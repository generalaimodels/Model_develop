# torchvision Module Structure

- datasets/
  - samplers/
    - __pycache__/
    - clip_sampler.py
      - Class: `DistributedSampler(dataset: Sized, num_replicas: Optional[int] = None, rank: Optional[int] = None, shuffle: bool = False, group_size: int = 1) -> None`
        - Description: Extension of DistributedSampler, as discussed in https://github.com/pytorch/pytorch/issues/23430 Example: dataset: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] num_replicas: 4 shuffle: False when group_size = 1 RANK | shard_dataset ========================= rank_0 | [0, 4, 8, 12] rank_1 | [1, 5, 9, 13] rank_2 | [2, 6, 10, 0] rank_3 | [3, 7, 11, 1] when group_size = 2 RANK | shard_dataset ========================= rank_0 | [0, 1, 8, 9] rank_1 | [2, 3, 10, 11] rank_2 | [4, 5, 12, 13] rank_3 | [6, 7, 0, 1]
      - Class: `RandomClipSampler(video_clips: torchvision.datasets.video_utils.VideoClips, max_clips_per_video: int) -> None`
        - Description: Samples at most `max_video_clips_per_video` clips for each video randomly Args: video_clips (VideoClips): video clips to sample from max_clips_per_video (int): maximum number of clips to be sampled per video
      - Class: `Sampler(data_source: Optional[Sized] = None) -> None`
        - Description: Base class for all Samplers. Every Sampler subclass has to provide an :meth:`__iter__` method, providing a way to iterate over indices or lists of indices (batches) of dataset elements, and a :meth:`__len__` method that returns the length of the returned iterators. Args: data_source (Dataset): This argument is not used and will be removed in 2.2.0. You may still have custom implementation that utilizes it. Example: >>> # xdoctest: +SKIP >>> class AccedingSequenceLengthSampler(Sampler[int]): >>> def __init__(self, data: List[str]) -> None: >>> self.data = data >>> >>> def __len__(self) -> int: >>> return len(self.data) >>> >>> def __iter__(self) -> Iterator[int]: >>> sizes = torch.tensor([len(x) for x in self.data]) >>> yield from torch.argsort(sizes).tolist() >>> >>> class AccedingSequenceLengthBatchSampler(Sampler[List[int]]): >>> def __init__(self, data: List[str], batch_size: int) -> None: >>> self.data = data >>> self.batch_size = batch_size >>> >>> def __len__(self) -> int: >>> return (len(self.data) + self.batch_size - 1) // self.batch_size >>> >>> def __iter__(self) -> Iterator[List[int]]: >>> sizes = torch.tensor([len(x) for x in self.data]) >>> for batch in torch.chunk(torch.argsort(sizes), len(self)): >>> yield batch.tolist() .. note:: The :meth:`__len__` method isn't strictly required by :class:`~torch.utils.data.DataLoader`, but is expected in any calculation involving the length of a :class:`~torch.utils.data.DataLoader`.
      - Class: `UniformClipSampler(video_clips: torchvision.datasets.video_utils.VideoClips, num_clips_per_video: int) -> None`
        - Description: Sample `num_video_clips_per_video` clips for each video, equally spaced. When number of unique clips in the video is fewer than num_video_clips_per_video, repeat the clips until `num_video_clips_per_video` clips are collected Args: video_clips (VideoClips): video clips to sample from num_clips_per_video (int): number of clips to be sampled per video
      - Class: `VideoClips(video_paths: List[str], clip_length_in_frames: int = 16, frames_between_clips: int = 1, frame_rate: Optional[int] = None, _precomputed_metadata: Optional[Dict[str, Any]] = None, num_workers: int = 0, _video_width: int = 0, _video_height: int = 0, _video_min_dimension: int = 0, _video_max_dimension: int = 0, _audio_samples: int = 0, _audio_channels: int = 0, output_format: str = 'THWC') -> None`
        - Description: Given a list of video files, computes all consecutive subvideos of size `clip_length_in_frames`, where the distance between each subvideo in the same video is defined by `frames_between_clips`. If `frame_rate` is specified, it will also resample all the videos to have the same frame rate, and the clips will refer to this frame rate. Creating this instance the first time is time-consuming, as it needs to decode all the videos in `video_paths`. It is recommended that you cache the results after instantiation of the class. Recreating the clips for different clip lengths is fast, and can be done with the `compute_clips` method. Args: video_paths (List[str]): paths to the video files clip_length_in_frames (int): size of a clip in number of frames frames_between_clips (int): step (in frames) between each clip frame_rate (int, optional): if specified, it will resample the video so that it has `frame_rate`, and then the clips will be defined on the resampled video num_workers (int): how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0) output_format (str): The format of the output video tensors. Can be either "THWC" (default) or "TCHW".
      - Function: `cast(typ, val)`
        - Description: Cast a value to a type. This returns the value unchanged. To the type checker this signals that the return value has the designated type, but at runtime we intentionally don't check anything (we want this to be as fast as possible).
  - __pycache__/
  - caltech.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Caltech101(root: str, target_type: Union[List[str], str] = 'category', transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `Caltech 101 <https://data.caltech.edu/records/20086>`_ Dataset. .. warning:: This class needs `scipy <https://docs.scipy.org/doc/>`_ to load target files from `.mat` format. Args: root (string): Root directory of dataset where directory ``caltech101`` exists or will be saved to if download is set to True. target_type (string or list, optional): Type of target to use, ``category`` or ``annotation``. Can also be a list to output a tuple with all specified target types. ``category`` represents the target class, and ``annotation`` is a list of points from a hand-generated outline. Defaults to ``category``. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `Caltech256(root: str, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `Caltech 256 <https://data.caltech.edu/records/20087>`_ Dataset. Args: root (string): Root directory of dataset where directory ``caltech256`` exists or will be saved to if download is set to True. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - celeba.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `CSV(header, index, data)`
      - Description: CSV(header, index, data)
    - Class: `CelebA(root: str, split: str = 'train', target_type: Union[List[str], str] = 'attr', transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `Large-scale CelebFaces Attributes (CelebA) Dataset <http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>`_ Dataset. Args: root (string): Root directory where images are downloaded to. split (string): One of {'train', 'valid', 'test', 'all'}. Accordingly dataset is selected. target_type (string or list, optional): Type of target to use, ``attr``, ``identity``, ``bbox``, or ``landmarks``. Can also be a list to output a tuple with all specified target types. The targets represent: - ``attr`` (Tensor shape=(40,) dtype=int): binary (0, 1) labels for attributes - ``identity`` (int): label for each person (data points with the same identity are the same person) - ``bbox`` (Tensor shape=(4,) dtype=int): bounding box (x, y, width, height) - ``landmarks`` (Tensor shape=(10,) dtype=int): landmark points (lefteye_x, lefteye_y, righteye_x, righteye_y, nose_x, nose_y, leftmouth_x, leftmouth_y, rightmouth_x, rightmouth_y) Defaults to ``attr``. If empty, ``None`` will be returned as target. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.PILToTensor`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `check_integrity(fpath: str, md5: Optional[str] = None) -> bool`
      - Description: No docstring available
    - Function: `download_file_from_google_drive(file_id: str, root: str, filename: Optional[str] = None, md5: Optional[str] = None)`
      - Description: Download a Google Drive file from and place it in root. Args: file_id (str): id of file to be downloaded root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the id of the file. md5 (str, optional): MD5 checksum of the download. If None, do not check
    - Function: `extract_archive(from_path: str, to_path: Optional[str] = None, remove_finished: bool = False) -> str`
      - Description: Extract an archive. The archive type and a possible compression is automatically detected from the file name. If the file is compressed but not an archive the call is dispatched to :func:`decompress`. Args: from_path (str): Path to the file to be extracted. to_path (str): Path to the directory the file will be extracted to. If omitted, the directory of the file is used. remove_finished (bool): If ``True``, remove the file after the extraction. Returns: (str): Path to the directory the file was extracted to.
    - Function: `namedtuple(typename, field_names, *, rename=False, defaults=None, module=None)`
      - Description: Returns a new subclass of tuple with named fields. >>> Point = namedtuple('Point', ['x', 'y']) >>> Point.__doc__ # docstring for the new class 'Point(x, y)' >>> p = Point(11, y=22) # instantiate with positional args or keywords >>> p[0] + p[1] # indexable like a plain tuple 33 >>> x, y = p # unpack like a regular tuple >>> x, y (11, 22) >>> p.x + p.y # fields also accessible by name 33 >>> d = p._asdict() # convert to a dictionary >>> d['x'] 11 >>> Point(**d) # convert from a dictionary Point(x=11, y=22) >>> p._replace(x=100) # _replace() is like str.replace() but targets named fields Point(x=100, y=22)
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - cifar.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `CIFAR10(root: str, train: bool = True, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset. Args: root (string): Root directory of dataset where directory ``cifar-10-batches-py`` exists or will be saved to if download is set to True. train (bool, optional): If True, creates dataset from training set, otherwise creates from test set. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `CIFAR100(root: str, train: bool = True, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset. This is a subclass of the `CIFAR10` Dataset.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `check_integrity(fpath: str, md5: Optional[str] = None) -> bool`
      - Description: No docstring available
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
  - cityscapes.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Cityscapes(root: str, split: str = 'train', mode: str = 'fine', target_type: Union[List[str], str] = 'instance', transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, transforms: Optional[Callable] = None) -> None`
      - Description: `Cityscapes <http://www.cityscapes-dataset.com/>`_ Dataset. Args: root (string): Root directory of dataset where directory ``leftImg8bit`` and ``gtFine`` or ``gtCoarse`` are located. split (string, optional): The image split to use, ``train``, ``test`` or ``val`` if mode="fine" otherwise ``train``, ``train_extra`` or ``val`` mode (string, optional): The quality mode to use, ``fine`` or ``coarse`` target_type (string or list, optional): Type of target to use, ``instance``, ``semantic``, ``polygon`` or ``color``. Can also be a list to output a tuple with all specified target types. transform (callable, optional): A function/transform that takes in a PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. transforms (callable, optional): A function/transform that takes input sample and its target as entry and returns a transformed version. Examples: Get semantic segmentation target .. code-block:: python dataset = Cityscapes('./data/cityscapes', split='train', mode='fine', target_type='semantic') img, smnt = dataset[0] Get multiple targets .. code-block:: python dataset = Cityscapes('./data/cityscapes', split='train', mode='fine', target_type=['instance', 'color', 'polygon']) img, (inst, col, poly) = dataset[0] Validate on the "coarse" set .. code-block:: python dataset = Cityscapes('./data/cityscapes', split='val', mode='coarse', target_type='semantic') img, smnt = dataset[0]
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `extract_archive(from_path: str, to_path: Optional[str] = None, remove_finished: bool = False) -> str`
      - Description: Extract an archive. The archive type and a possible compression is automatically detected from the file name. If the file is compressed but not an archive the call is dispatched to :func:`decompress`. Args: from_path (str): Path to the file to be extracted. to_path (str): Path to the directory the file will be extracted to. If omitted, the directory of the file is used. remove_finished (bool): If ``True``, remove the file after the extraction. Returns: (str): Path to the directory the file was extracted to.
    - Function: `iterable_to_str(iterable: Iterable) -> str`
      - Description: No docstring available
    - Function: `namedtuple(typename, field_names, *, rename=False, defaults=None, module=None)`
      - Description: Returns a new subclass of tuple with named fields. >>> Point = namedtuple('Point', ['x', 'y']) >>> Point.__doc__ # docstring for the new class 'Point(x, y)' >>> p = Point(11, y=22) # instantiate with positional args or keywords >>> p[0] + p[1] # indexable like a plain tuple 33 >>> x, y = p # unpack like a regular tuple >>> x, y (11, 22) >>> p.x + p.y # fields also accessible by name 33 >>> d = p._asdict() # convert to a dictionary >>> d['x'] 11 >>> Point(**d) # convert from a dictionary Point(x=11, y=22) >>> p._replace(x=100) # _replace() is like str.replace() but targets named fields Point(x=100, y=22)
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - clevr.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `CLEVRClassification(root: str, split: str = 'train', transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `CLEVR <https://cs.stanford.edu/people/jcjohns/clevr/>`_ classification dataset. The number of objects in a scene are used as label. Args: root (string): Root directory of dataset where directory ``root/clevr`` exists or will be saved to if download is set to True. split (string, optional): The dataset split, supports ``"train"`` (default), ``"val"``, or ``"test"``. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in them target and transforms it. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `urlparse(url, scheme='', allow_fragments=True)`
      - Description: Parse a URL into 6 components: <scheme>://<netloc>/<path>;<params>?<query>#<fragment> The result is a named 6-tuple with fields corresponding to the above. It is either a ParseResult or ParseResultBytes object, depending on the type of the url parameter. The username, password, hostname, and port sub-components of netloc can also be accessed as attributes of the returned object. The scheme argument provides the default value of the scheme component when no scheme is found in url. If allow_fragments is False, no attempt is made to separate the fragment component from the previous component, which can be either path or query. Note that % escapes are not expanded.
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - coco.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `CocoCaptions(root: str, annFile: str, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, transforms: Optional[Callable] = None) -> None`
      - Description: `MS Coco Captions <https://cocodataset.org/#captions-2015>`_ Dataset. It requires the `COCO API to be installed <https://github.com/pdollar/coco/tree/master/PythonAPI>`_. Args: root (string): Root directory where images are downloaded to. annFile (string): Path to json annotation file. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.PILToTensor`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. transforms (callable, optional): A function/transform that takes input sample and its target as entry and returns a transformed version. Example: .. code:: python import torchvision.datasets as dset import torchvision.transforms as transforms cap = dset.CocoCaptions(root = 'dir where images are', annFile = 'json annotation file', transform=transforms.PILToTensor()) print('Number of samples: ', len(cap)) img, target = cap[3] # load 4th sample print("Image Size: ", img.size()) print(target) Output: :: Number of samples: 82783 Image Size: (3L, 427L, 640L) [u'A plane emitting smoke stream flying over a mountain.', u'A plane darts across a bright blue sky behind a mountain covered in snow', u'A plane leaves a contrail above the snowy mountain top.', u'A mountain that has a plane flying overheard in the distance.', u'A mountain view with a plume of smoke in the background']
    - Class: `CocoDetection(root: str, annFile: str, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, transforms: Optional[Callable] = None) -> None`
      - Description: `MS Coco Detection <https://cocodataset.org/#detection-2016>`_ Dataset. It requires the `COCO API to be installed <https://github.com/pdollar/coco/tree/master/PythonAPI>`_. Args: root (string): Root directory where images are downloaded to. annFile (string): Path to json annotation file. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.PILToTensor`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. transforms (callable, optional): A function/transform that takes input sample and its target as entry and returns a transformed version.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
  - country211.py
    - Class: `Country211(root: str, split: str = 'train', transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `The Country211 Data Set <https://github.com/openai/CLIP/blob/main/data/country211.md>`_ from OpenAI. This dataset was built by filtering the images from the YFCC100m dataset that have GPS coordinate corresponding to a ISO-3166 country code. The dataset is balanced by sampling 150 train images, 50 validation images, and 100 test images for each country. Args: root (string): Root directory of the dataset. split (string, optional): The dataset split, supports ``"train"`` (default), ``"valid"`` and ``"test"``. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop``. target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If True, downloads the dataset from the internet and puts it into ``root/country211/``. If dataset is already downloaded, it is not downloaded again.
    - Class: `ImageFolder(root: str, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, loader: Callable[[str], Any] = <function default_loader at 0x00000208094BB880>, is_valid_file: Optional[Callable[[str], bool]] = None)`
      - Description: A generic data loader where the images are arranged in this way by default: :: root/dog/xxx.png root/dog/xxy.png root/dog/[...]/xxz.png root/cat/123.png root/cat/nsdf3.png root/cat/[...]/asd932_.png This class inherits from :class:`~torchvision.datasets.DatasetFolder` so the same methods can be overridden to customize the dataset. Args: root (string): Root directory path. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. loader (callable, optional): A function to load an image given its path. is_valid_file (callable, optional): A function that takes path of an Image file and check if the file is a valid file (used to check of corrupt files) Attributes: classes (list): List of the class names sorted alphabetically. class_to_idx (dict): Dict with items (class_name, class_index). imgs (list): List of (image path, class_index) tuples
    - Class: `Path(*args, **kwargs)`
      - Description: PurePath subclass that can make system calls. Path represents a filesystem path but unlike PurePath, also offers methods to do system calls on path objects. Depending on your system, instantiating a Path will return either a PosixPath or a WindowsPath object. You can also instantiate a PosixPath or WindowsPath directly, but cannot instantiate a WindowsPath on a POSIX system or vice versa.
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - dtd.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `DTD(root: str, split: str = 'train', partition: int = 1, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `Describable Textures Dataset (DTD) <https://www.robots.ox.ac.uk/~vgg/data/dtd/>`_. Args: root (string): Root directory of the dataset. split (string, optional): The dataset split, supports ``"train"`` (default), ``"val"``, or ``"test"``. partition (int, optional): The dataset partition. Should be ``1 <= partition <= 10``. Defaults to ``1``. .. note:: The partition only changes which split each image belongs to. Thus, regardless of the selected partition, combining all splits will result in all images. transform (callable, optional): A function/transform that takes in a PIL image and returns a transformed version. E.g, ``transforms.RandomCrop``. target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If True, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. Default is False.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - eurosat.py
    - Class: `EuroSAT(root: str, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: RGB version of the `EuroSAT <https://github.com/phelber/eurosat>`_ Dataset. Args: root (string): Root directory of dataset where ``root/eurosat`` exists. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If True, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. Default is False.
    - Class: `ImageFolder(root: str, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, loader: Callable[[str], Any] = <function default_loader at 0x00000208094BB880>, is_valid_file: Optional[Callable[[str], bool]] = None)`
      - Description: A generic data loader where the images are arranged in this way by default: :: root/dog/xxx.png root/dog/xxy.png root/dog/[...]/xxz.png root/cat/123.png root/cat/nsdf3.png root/cat/[...]/asd932_.png This class inherits from :class:`~torchvision.datasets.DatasetFolder` so the same methods can be overridden to customize the dataset. Args: root (string): Root directory path. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. loader (callable, optional): A function to load an image given its path. is_valid_file (callable, optional): A function that takes path of an Image file and check if the file is a valid file (used to check of corrupt files) Attributes: classes (list): List of the class names sorted alphabetically. class_to_idx (dict): Dict with items (class_name, class_index). imgs (list): List of (image path, class_index) tuples
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
  - fakedata.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `FakeData(size: int = 1000, image_size: Tuple[int, int, int] = (3, 224, 224), num_classes: int = 10, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, random_offset: int = 0) -> None`
      - Description: A fake dataset that returns randomly generated images and returns them as PIL images Args: size (int, optional): Size of the dataset. Default: 1000 images image_size(tuple, optional): Size if the returned images. Default: (3, 224, 224) num_classes(int, optional): Number of classes in the dataset. Default: 10 transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. random_offset (int): Offsets the index-based random seed used to generate each image. Default: 0
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
  - fer2013.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `FER2013(root: str, split: str = 'train', transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: `FER2013 <https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge>`_ Dataset. Args: root (string): Root directory of dataset where directory ``root/fer2013`` exists. split (string, optional): The dataset split, supports ``"train"`` (default), or ``"test"``. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `check_integrity(fpath: str, md5: Optional[str] = None) -> bool`
      - Description: No docstring available
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - fgvc_aircraft.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `FGVCAircraft(root: 'str', split: 'str' = 'trainval', annotation_level: 'str' = 'variant', transform: 'Optional[Callable]' = None, target_transform: 'Optional[Callable]' = None, download: 'bool' = False) -> 'None'`
      - Description: `FGVC Aircraft <https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/>`_ Dataset. The dataset contains 10,000 images of aircraft, with 100 images for each of 100 different aircraft model variants, most of which are airplanes. Aircraft models are organized in a three-levels hierarchy. The three levels, from finer to coarser, are: - ``variant``, e.g. Boeing 737-700. A variant collapses all the models that are visually indistinguishable into one class. The dataset comprises 100 different variants. - ``family``, e.g. Boeing 737. The dataset comprises 70 different families. - ``manufacturer``, e.g. Boeing. The dataset comprises 30 different manufacturers. Args: root (string): Root directory of the FGVC Aircraft dataset. split (string, optional): The dataset split, supports ``train``, ``val``, ``trainval`` and ``test``. annotation_level (str, optional): The annotation level, supports ``variant``, ``family`` and ``manufacturer``. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If True, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - flickr.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Flickr30k(root: str, ann_file: str, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: `Flickr30k Entities <https://bryanplummer.com/Flickr30kEntities/>`_ Dataset. Args: root (string): Root directory where images are downloaded to. ann_file (string): Path to annotation file. transform (callable, optional): A function/transform that takes in a PIL image and returns a transformed version. E.g, ``transforms.PILToTensor`` target_transform (callable, optional): A function/transform that takes in the target and transforms it.
    - Class: `Flickr8k(root: str, ann_file: str, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: `Flickr8k Entities <http://hockenmaier.cs.illinois.edu/8k-pictures.html>`_ Dataset. Args: root (string): Root directory where images are downloaded to. ann_file (string): Path to annotation file. transform (callable, optional): A function/transform that takes in a PIL image and returns a transformed version. E.g, ``transforms.PILToTensor`` target_transform (callable, optional): A function/transform that takes in the target and transforms it.
    - Class: `Flickr8kParser(root: str) -> None`
      - Description: Parser for extracting captions from the Flickr8k dataset web page.
    - Class: `HTMLParser(*, convert_charrefs=True)`
      - Description: Find tags and other markup and call handler functions. Usage: p = HTMLParser() p.feed(data) ... p.close() Start tags are handled by calling self.handle_starttag() or self.handle_startendtag(); end tags by self.handle_endtag(). The data between tags is passed from the parser to the derived class by calling self.handle_data() with the data as argument (the data may be split up in arbitrary chunks). If convert_charrefs is True the character references are converted automatically to the corresponding Unicode character (and self.handle_data() is no longer split in chunks), otherwise they are passed by calling self.handle_entityref() or self.handle_charref() with the string containing respectively the named or numeric reference as the argument.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Class: `defaultdict(Unable to retrieve signature)`
      - Description: defaultdict(default_factory=None, /, [...]) --> dict with default factory The default factory is called without arguments to produce a new value when a key is not present, in __getitem__ only. A defaultdict compares equal to a dict with the same items. All remaining arguments are treated the same as if they were passed to the dict constructor, including keyword arguments.
  - flowers102.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Flowers102(root: str, split: str = 'train', transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `Oxford 102 Flower <https://www.robots.ox.ac.uk/~vgg/data/flowers/102/>`_ Dataset. .. warning:: This class needs `scipy <https://docs.scipy.org/doc/>`_ to load target files from `.mat` format. Oxford 102 Flower is an image classification dataset consisting of 102 flower categories. The flowers were chosen to be flowers commonly occurring in the United Kingdom. Each class consists of between 40 and 258 images. The images have large scale, pose and light variations. In addition, there are categories that have large variations within the category, and several very similar categories. Args: root (string): Root directory of the dataset. split (string, optional): The dataset split, supports ``"train"`` (default), ``"val"``, or ``"test"``. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop``. target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `Path(*args, **kwargs)`
      - Description: PurePath subclass that can make system calls. Path represents a filesystem path but unlike PurePath, also offers methods to do system calls on path objects. Depending on your system, instantiating a Path will return either a PosixPath or a WindowsPath object. You can also instantiate a PosixPath or WindowsPath directly, but cannot instantiate a WindowsPath on a POSIX system or vice versa.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `check_integrity(fpath: str, md5: Optional[str] = None) -> bool`
      - Description: No docstring available
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `download_url(url: str, root: str, filename: Optional[str] = None, md5: Optional[str] = None, max_redirect_hops: int = 3) -> None`
      - Description: Download a file from a url and place it in root. Args: url (str): URL to download file from root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the basename of the URL md5 (str, optional): MD5 checksum of the download. If None, do not check max_redirect_hops (int, optional): Maximum number of redirect hops allowed
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - folder.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `DatasetFolder(root: str, loader: Callable[[str], Any], extensions: Optional[Tuple[str, ...]] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, is_valid_file: Optional[Callable[[str], bool]] = None) -> None`
      - Description: A generic data loader. This default directory structure can be customized by overriding the :meth:`find_classes` method. Args: root (string): Root directory path. loader (callable): A function to load a sample given its path. extensions (tuple[string]): A list of allowed extensions. both extensions and is_valid_file should not be passed. transform (callable, optional): A function/transform that takes in a sample and returns a transformed version. E.g, ``transforms.RandomCrop`` for images. target_transform (callable, optional): A function/transform that takes in the target and transforms it. is_valid_file (callable, optional): A function that takes path of a file and check if the file is a valid file (used to check of corrupt files) both extensions and is_valid_file should not be passed. Attributes: classes (list): List of the class names sorted alphabetically. class_to_idx (dict): Dict with items (class_name, class_index). samples (list): List of (sample path, class_index) tuples targets (list): The class_index value for each image in the dataset
    - Class: `ImageFolder(root: str, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, loader: Callable[[str], Any] = <function default_loader at 0x00000208094BB880>, is_valid_file: Optional[Callable[[str], bool]] = None)`
      - Description: A generic data loader where the images are arranged in this way by default: :: root/dog/xxx.png root/dog/xxy.png root/dog/[...]/xxz.png root/cat/123.png root/cat/nsdf3.png root/cat/[...]/asd932_.png This class inherits from :class:`~torchvision.datasets.DatasetFolder` so the same methods can be overridden to customize the dataset. Args: root (string): Root directory path. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. loader (callable, optional): A function to load an image given its path. is_valid_file (callable, optional): A function that takes path of an Image file and check if the file is a valid file (used to check of corrupt files) Attributes: classes (list): List of the class names sorted alphabetically. class_to_idx (dict): Dict with items (class_name, class_index). imgs (list): List of (image path, class_index) tuples
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `accimage_loader(path: str) -> Any`
      - Description: No docstring available
    - Function: `cast(typ, val)`
      - Description: Cast a value to a type. This returns the value unchanged. To the type checker this signals that the return value has the designated type, but at runtime we intentionally don't check anything (we want this to be as fast as possible).
    - Function: `default_loader(path: str) -> Any`
      - Description: No docstring available
    - Function: `find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]`
      - Description: Finds the class folders in a dataset. See :class:`DatasetFolder` for details.
    - Function: `has_file_allowed_extension(filename: str, extensions: Union[str, Tuple[str, ...]]) -> bool`
      - Description: Checks if a file is an allowed extension. Args: filename (string): path to a file extensions (tuple of strings): extensions to consider (lowercase) Returns: bool: True if the filename ends with one of given extensions
    - Function: `is_image_file(filename: str) -> bool`
      - Description: Checks if a file is an allowed image extension. Args: filename (string): path to a file Returns: bool: True if the filename ends with a known image extension
    - Function: `make_dataset(directory: str, class_to_idx: Optional[Dict[str, int]] = None, extensions: Union[str, Tuple[str, ...], NoneType] = None, is_valid_file: Optional[Callable[[str], bool]] = None) -> List[Tuple[str, int]]`
      - Description: Generates a list of samples of a form (path_to_sample, class). See :class:`DatasetFolder` for details. Note: The class_to_idx parameter is here optional and will use the logic of the ``find_classes`` function by default.
    - Function: `pil_loader(path: str) -> PIL.Image.Image`
      - Description: No docstring available
  - food101.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Food101(root: str, split: str = 'train', transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `The Food-101 Data Set <https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/>`_. The Food-101 is a challenging data set of 101 food categories with 101,000 images. For each class, 250 manually reviewed test images are provided as well as 750 training images. On purpose, the training images were not cleaned, and thus still contain some amount of noise. This comes mostly in the form of intense colors and sometimes wrong labels. All images were rescaled to have a maximum side length of 512 pixels. Args: root (string): Root directory of the dataset. split (string, optional): The dataset split, supports ``"train"`` (default) and ``"test"``. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop``. target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If True, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. Default is False.
    - Class: `Path(*args, **kwargs)`
      - Description: PurePath subclass that can make system calls. Path represents a filesystem path but unlike PurePath, also offers methods to do system calls on path objects. Depending on your system, instantiating a Path will return either a PosixPath or a WindowsPath object. You can also instantiate a PosixPath or WindowsPath directly, but cannot instantiate a WindowsPath on a POSIX system or vice versa.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - gtsrb.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `GTSRB(root: str, split: str = 'train', transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `German Traffic Sign Recognition Benchmark (GTSRB) <https://benchmark.ini.rub.de/>`_ Dataset. Args: root (string): Root directory of the dataset. split (string, optional): The dataset split, supports ``"train"`` (default), or ``"test"``. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop``. target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If True, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `make_dataset(directory: str, class_to_idx: Optional[Dict[str, int]] = None, extensions: Union[str, Tuple[str, ...], NoneType] = None, is_valid_file: Optional[Callable[[str], bool]] = None) -> List[Tuple[str, int]]`
      - Description: Generates a list of samples of a form (path_to_sample, class). See :class:`DatasetFolder` for details. Note: The class_to_idx parameter is here optional and will use the logic of the ``find_classes`` function by default.
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - hmdb51.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `HMDB51(root: str, annotation_path: str, frames_per_clip: int, step_between_clips: int = 1, frame_rate: Optional[int] = None, fold: int = 1, train: bool = True, transform: Optional[Callable] = None, _precomputed_metadata: Optional[Dict[str, Any]] = None, num_workers: int = 1, _video_width: int = 0, _video_height: int = 0, _video_min_dimension: int = 0, _audio_samples: int = 0, output_format: str = 'THWC') -> None`
      - Description: `HMDB51 <https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/>`_ dataset. HMDB51 is an action recognition video dataset. This dataset consider every video as a collection of video clips of fixed size, specified by ``frames_per_clip``, where the step in frames between each clip is given by ``step_between_clips``. To give an example, for 2 videos with 10 and 15 frames respectively, if ``frames_per_clip=5`` and ``step_between_clips=5``, the dataset size will be (2 + 3) = 5, where the first two elements will come from video 1, and the next three elements from video 2. Note that we drop clips which do not have exactly ``frames_per_clip`` elements, so not all frames in a video might be present. Internally, it uses a VideoClips object to handle clip creation. Args: root (string): Root directory of the HMDB51 Dataset. annotation_path (str): Path to the folder containing the split files. frames_per_clip (int): Number of frames in a clip. step_between_clips (int): Number of frames between each clip. fold (int, optional): Which fold to use. Should be between 1 and 3. train (bool, optional): If ``True``, creates a dataset from the train split, otherwise from the ``test`` split. transform (callable, optional): A function/transform that takes in a TxHxWxC video and returns a transformed version. output_format (str, optional): The format of the output video tensors (before transforms). Can be either "THWC" (default) or "TCHW". Returns: tuple: A 3-tuple with the following entries: - video (Tensor[T, H, W, C] or Tensor[T, C, H, W]): The `T` video frames - audio(Tensor[K, L]): the audio frames, where `K` is the number of channels and `L` is the number of points - label (int): class of the video clip
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `VideoClips(video_paths: List[str], clip_length_in_frames: int = 16, frames_between_clips: int = 1, frame_rate: Optional[int] = None, _precomputed_metadata: Optional[Dict[str, Any]] = None, num_workers: int = 0, _video_width: int = 0, _video_height: int = 0, _video_min_dimension: int = 0, _video_max_dimension: int = 0, _audio_samples: int = 0, _audio_channels: int = 0, output_format: str = 'THWC') -> None`
      - Description: Given a list of video files, computes all consecutive subvideos of size `clip_length_in_frames`, where the distance between each subvideo in the same video is defined by `frames_between_clips`. If `frame_rate` is specified, it will also resample all the videos to have the same frame rate, and the clips will refer to this frame rate. Creating this instance the first time is time-consuming, as it needs to decode all the videos in `video_paths`. It is recommended that you cache the results after instantiation of the class. Recreating the clips for different clip lengths is fast, and can be done with the `compute_clips` method. Args: video_paths (List[str]): paths to the video files clip_length_in_frames (int): size of a clip in number of frames frames_between_clips (int): step (in frames) between each clip frame_rate (int, optional): if specified, it will resample the video so that it has `frame_rate`, and then the clips will be defined on the resampled video num_workers (int): how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0) output_format (str): The format of the output video tensors. Can be either "THWC" (default) or "TCHW".
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]`
      - Description: Finds the class folders in a dataset. See :class:`DatasetFolder` for details.
    - Function: `make_dataset(directory: str, class_to_idx: Optional[Dict[str, int]] = None, extensions: Union[str, Tuple[str, ...], NoneType] = None, is_valid_file: Optional[Callable[[str], bool]] = None) -> List[Tuple[str, int]]`
      - Description: Generates a list of samples of a form (path_to_sample, class). See :class:`DatasetFolder` for details. Note: The class_to_idx parameter is here optional and will use the logic of the ``find_classes`` function by default.
  - imagenet.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `ImageFolder(root: str, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, loader: Callable[[str], Any] = <function default_loader at 0x00000208094BB880>, is_valid_file: Optional[Callable[[str], bool]] = None)`
      - Description: A generic data loader where the images are arranged in this way by default: :: root/dog/xxx.png root/dog/xxy.png root/dog/[...]/xxz.png root/cat/123.png root/cat/nsdf3.png root/cat/[...]/asd932_.png This class inherits from :class:`~torchvision.datasets.DatasetFolder` so the same methods can be overridden to customize the dataset. Args: root (string): Root directory path. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. loader (callable, optional): A function to load an image given its path. is_valid_file (callable, optional): A function that takes path of an Image file and check if the file is a valid file (used to check of corrupt files) Attributes: classes (list): List of the class names sorted alphabetically. class_to_idx (dict): Dict with items (class_name, class_index). imgs (list): List of (image path, class_index) tuples
    - Class: `ImageNet(root: str, split: str = 'train', **kwargs: Any) -> None`
      - Description: `ImageNet <http://image-net.org/>`_ 2012 Classification Dataset. .. note:: Before using this class, it is required to download ImageNet 2012 dataset from `here <https://image-net.org/challenges/LSVRC/2012/2012-downloads.php>`_ and place the files ``ILSVRC2012_devkit_t12.tar.gz`` and ``ILSVRC2012_img_train.tar`` or ``ILSVRC2012_img_val.tar`` based on ``split`` in the root directory. Args: root (string): Root directory of the ImageNet Dataset. split (string, optional): The dataset split, supports ``train``, or ``val``. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. loader (callable, optional): A function to load an image given its path. Attributes: classes (list): List of the class name tuples. class_to_idx (dict): Dict with items (class_name, class_index). wnids (list): List of the WordNet IDs. wnid_to_idx (dict): Dict with items (wordnet_id, class_index). imgs (list): List of (image path, class_index) tuples targets (list): The class_index value for each image in the dataset
    - Function: `_verify_archive(root: str, file: str, md5: str) -> None`
      - Description: No docstring available
    - Function: `check_integrity(fpath: str, md5: Optional[str] = None) -> bool`
      - Description: No docstring available
    - Function: `contextmanager(func)`
      - Description: @contextmanager decorator. Typical usage: @contextmanager def some_generator(<arguments>): <setup> try: yield <value> finally: <cleanup> This makes this: with some_generator(<arguments>) as <variable>: <body> equivalent to this: <setup> try: <variable> = <value> <body> finally: <cleanup>
    - Function: `extract_archive(from_path: str, to_path: Optional[str] = None, remove_finished: bool = False) -> str`
      - Description: Extract an archive. The archive type and a possible compression is automatically detected from the file name. If the file is compressed but not an archive the call is dispatched to :func:`decompress`. Args: from_path (str): Path to the file to be extracted. to_path (str): Path to the directory the file will be extracted to. If omitted, the directory of the file is used. remove_finished (bool): If ``True``, remove the file after the extraction. Returns: (str): Path to the directory the file was extracted to.
    - Function: `load_meta_file(root: str, file: Optional[str] = None) -> Tuple[Dict[str, str], List[str]]`
      - Description: No docstring available
    - Function: `parse_devkit_archive(root: str, file: Optional[str] = None) -> None`
      - Description: Parse the devkit archive of the ImageNet2012 classification dataset and save the meta information in a binary file. Args: root (str): Root directory containing the devkit archive file (str, optional): Name of devkit archive. Defaults to 'ILSVRC2012_devkit_t12.tar.gz'
    - Function: `parse_train_archive(root: str, file: Optional[str] = None, folder: str = 'train') -> None`
      - Description: Parse the train images archive of the ImageNet2012 classification dataset and prepare it for usage with the ImageNet dataset. Args: root (str): Root directory containing the train images archive file (str, optional): Name of train images archive. Defaults to 'ILSVRC2012_img_train.tar' folder (str, optional): Optional name for train images folder. Defaults to 'train'
    - Function: `parse_val_archive(root: str, file: Optional[str] = None, wnids: Optional[List[str]] = None, folder: str = 'val') -> None`
      - Description: Parse the validation images archive of the ImageNet2012 classification dataset and prepare it for usage with the ImageNet dataset. Args: root (str): Root directory containing the validation images archive file (str, optional): Name of validation images archive. Defaults to 'ILSVRC2012_img_val.tar' wnids (list, optional): List of WordNet IDs of the validation images. If None is given, the IDs are loaded from the meta file in the root directory folder (str, optional): Optional name for validation images folder. Defaults to 'val'
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - inaturalist.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `INaturalist(root: str, version: str = '2021_train', target_type: Union[List[str], str] = 'full', transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `iNaturalist <https://github.com/visipedia/inat_comp>`_ Dataset. Args: root (string): Root directory of dataset where the image files are stored. This class does not require/use annotation files. version (string, optional): Which version of the dataset to download/use. One of '2017', '2018', '2019', '2021_train', '2021_train_mini', '2021_valid'. Default: `2021_train`. target_type (string or list, optional): Type of target to use, for 2021 versions, one of: - ``full``: the full category (species) - ``kingdom``: e.g. "Animalia" - ``phylum``: e.g. "Arthropoda" - ``class``: e.g. "Insecta" - ``order``: e.g. "Coleoptera" - ``family``: e.g. "Cleridae" - ``genus``: e.g. "Trichodes" for 2017-2019 versions, one of: - ``full``: the full (numeric) category - ``super``: the super category, e.g. "Amphibians" Can also be a list to output a tuple with all specified target types. Defaults to ``full``. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - kinetics.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Kinetics(root: str, frames_per_clip: int, num_classes: str = '400', split: str = 'train', frame_rate: Optional[int] = None, step_between_clips: int = 1, transform: Optional[Callable] = None, extensions: Tuple[str, ...] = ('avi', 'mp4'), download: bool = False, num_download_workers: int = 1, num_workers: int = 1, _precomputed_metadata: Optional[Dict[str, Any]] = None, _video_width: int = 0, _video_height: int = 0, _video_min_dimension: int = 0, _audio_samples: int = 0, _audio_channels: int = 0, _legacy: bool = False, output_format: str = 'TCHW') -> None`
      - Description: `Generic Kinetics <https://www.deepmind.com/open-source/kinetics>`_ dataset. Kinetics-400/600/700 are action recognition video datasets. This dataset consider every video as a collection of video clips of fixed size, specified by ``frames_per_clip``, where the step in frames between each clip is given by ``step_between_clips``. To give an example, for 2 videos with 10 and 15 frames respectively, if ``frames_per_clip=5`` and ``step_between_clips=5``, the dataset size will be (2 + 3) = 5, where the first two elements will come from video 1, and the next three elements from video 2. Note that we drop clips which do not have exactly ``frames_per_clip`` elements, so not all frames in a video might be present. Args: root (string): Root directory of the Kinetics Dataset. Directory should be structured as follows: .. code:: root/  split   class1    clip1.mp4    clip2.mp4    clip3.mp4    ...   class2    clipx.mp4    ... Note: split is appended automatically using the split argument. frames_per_clip (int): number of frames in a clip num_classes (int): select between Kinetics-400 (default), Kinetics-600, and Kinetics-700 split (str): split of the dataset to consider; supports ``"train"`` (default) ``"val"`` ``"test"`` frame_rate (float): If omitted, interpolate different frame rate for each clip. step_between_clips (int): number of frames between each clip transform (callable, optional): A function/transform that takes in a TxHxWxC video and returns a transformed version. download (bool): Download the official version of the dataset to root folder. num_workers (int): Use multiple workers for VideoClips creation num_download_workers (int): Use multiprocessing in order to speed up download. output_format (str, optional): The format of the output video tensors (before transforms). Can be either "THWC" or "TCHW" (default). Note that in most other utils and datasets, the default is actually "THWC". Returns: tuple: A 3-tuple with the following entries: - video (Tensor[T, C, H, W] or Tensor[T, H, W, C]): the `T` video frames in torch.uint8 tensor - audio(Tensor[K, L]): the audio frames, where `K` is the number of channels and `L` is the number of points in torch.float tensor - label (int): class of the video clip Raises: RuntimeError: If ``download is True`` and the video archives are already extracted.
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `VideoClips(video_paths: List[str], clip_length_in_frames: int = 16, frames_between_clips: int = 1, frame_rate: Optional[int] = None, _precomputed_metadata: Optional[Dict[str, Any]] = None, num_workers: int = 0, _video_width: int = 0, _video_height: int = 0, _video_min_dimension: int = 0, _video_max_dimension: int = 0, _audio_samples: int = 0, _audio_channels: int = 0, output_format: str = 'THWC') -> None`
      - Description: Given a list of video files, computes all consecutive subvideos of size `clip_length_in_frames`, where the distance between each subvideo in the same video is defined by `frames_between_clips`. If `frame_rate` is specified, it will also resample all the videos to have the same frame rate, and the clips will refer to this frame rate. Creating this instance the first time is time-consuming, as it needs to decode all the videos in `video_paths`. It is recommended that you cache the results after instantiation of the class. Recreating the clips for different clip lengths is fast, and can be done with the `compute_clips` method. Args: video_paths (List[str]): paths to the video files clip_length_in_frames (int): size of a clip in number of frames frames_between_clips (int): step (in frames) between each clip frame_rate (int, optional): if specified, it will resample the video so that it has `frame_rate`, and then the clips will be defined on the resampled video num_workers (int): how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0) output_format (str): The format of the output video tensors. Can be either "THWC" (default) or "TCHW".
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `_dl_wrap(tarpath: str, videopath: str, line: str) -> None`
      - Description: No docstring available
    - Function: `check_integrity(fpath: str, md5: Optional[str] = None) -> bool`
      - Description: No docstring available
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `download_url(url: str, root: str, filename: Optional[str] = None, md5: Optional[str] = None, max_redirect_hops: int = 3) -> None`
      - Description: Download a file from a url and place it in root. Args: url (str): URL to download file from root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the basename of the URL md5 (str, optional): MD5 checksum of the download. If None, do not check max_redirect_hops (int, optional): Maximum number of redirect hops allowed
    - Function: `find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]`
      - Description: Finds the class folders in a dataset. See :class:`DatasetFolder` for details.
    - Function: `make_dataset(directory: str, class_to_idx: Optional[Dict[str, int]] = None, extensions: Union[str, Tuple[str, ...], NoneType] = None, is_valid_file: Optional[Callable[[str], bool]] = None) -> List[Tuple[str, int]]`
      - Description: Generates a list of samples of a form (path_to_sample, class). See :class:`DatasetFolder` for details. Note: The class_to_idx parameter is here optional and will use the logic of the ``find_classes`` function by default.
    - Class: `partial(Unable to retrieve signature)`
      - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - kitti.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Kitti(root: str, train: bool = True, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, transforms: Optional[Callable] = None, download: bool = False)`
      - Description: `KITTI <http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark>`_ Dataset. It corresponds to the "left color images of object" dataset, for object detection. Args: root (string): Root directory where images are downloaded to. Expects the following folder structure if download=False: .. code:: <root>  Kitti  raw  training |  image_2 |  label_2  testing  image_2 train (bool, optional): Use ``train`` split if true, else ``test`` split. Defaults to ``train``. transform (callable, optional): A function/transform that takes in a PIL image and returns a transformed version. E.g, ``transforms.PILToTensor`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. transforms (callable, optional): A function/transform that takes input sample and its target as entry and returns a transformed version. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
  - lfw.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `LFWPairs(root: str, split: str = '10fold', image_set: str = 'funneled', transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `LFW <http://vis-www.cs.umass.edu/lfw/>`_ Dataset. Args: root (string): Root directory of dataset where directory ``lfw-py`` exists or will be saved to if download is set to True. split (string, optional): The image split to use. Can be one of ``train``, ``test``, ``10fold``. Defaults to ``10fold``. image_set (str, optional): Type of image funneling to use, ``original``, ``funneled`` or ``deepfunneled``. Defaults to ``funneled``. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomRotation`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `LFWPeople(root: str, split: str = '10fold', image_set: str = 'funneled', transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `LFW <http://vis-www.cs.umass.edu/lfw/>`_ Dataset. Args: root (string): Root directory of dataset where directory ``lfw-py`` exists or will be saved to if download is set to True. split (string, optional): The image split to use. Can be one of ``train``, ``test``, ``10fold`` (default). image_set (str, optional): Type of image funneling to use, ``original``, ``funneled`` or ``deepfunneled``. Defaults to ``funneled``. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomRotation`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Class: `_LFW(root: str, split: str, image_set: str, view: str, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `check_integrity(fpath: str, md5: Optional[str] = None) -> bool`
      - Description: No docstring available
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `download_url(url: str, root: str, filename: Optional[str] = None, md5: Optional[str] = None, max_redirect_hops: int = 3) -> None`
      - Description: Download a file from a url and place it in root. Args: url (str): URL to download file from root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the basename of the URL md5 (str, optional): MD5 checksum of the download. If None, do not check max_redirect_hops (int, optional): Maximum number of redirect hops allowed
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - lsun.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Iterable()`
      - Description: No docstring available
    - Class: `LSUN(root: str, classes: Union[str, List[str]] = 'train', transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: `LSUN <https://www.yf.io/p/lsun>`_ dataset. You will need to install the ``lmdb`` package to use this dataset: run ``pip install lmdb`` Args: root (string): Root directory for the database files. classes (string or list): One of {'train', 'val', 'test'} or a list of categories to load. e,g. ['bedroom_train', 'church_outdoor_train']. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it.
    - Class: `LSUNClass(root: str, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `cast(typ, val)`
      - Description: Cast a value to a type. This returns the value unchanged. To the type checker this signals that the return value has the designated type, but at runtime we intentionally don't check anything (we want this to be as fast as possible).
    - Function: `iterable_to_str(iterable: Iterable) -> str`
      - Description: No docstring available
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - mnist.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `EMNIST(root: str, split: str, **kwargs: Any) -> None`
      - Description: `EMNIST <https://www.westernsydney.edu.au/bens/home/reproducible_research/emnist>`_ Dataset. Args: root (string): Root directory of dataset where ``EMNIST/raw/train-images-idx3-ubyte`` and ``EMNIST/raw/t10k-images-idx3-ubyte`` exist. split (string): The dataset has 6 different splits: ``byclass``, ``bymerge``, ``balanced``, ``letters``, ``digits`` and ``mnist``. This argument specifies which one to use. train (bool, optional): If True, creates dataset from ``training.pt``, otherwise from ``test.pt``. download (bool, optional): If True, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it.
    - Class: `FashionMNIST(root: str, train: bool = True, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `Fashion-MNIST <https://github.com/zalandoresearch/fashion-mnist>`_ Dataset. Args: root (string): Root directory of dataset where ``FashionMNIST/raw/train-images-idx3-ubyte`` and ``FashionMNIST/raw/t10k-images-idx3-ubyte`` exist. train (bool, optional): If True, creates dataset from ``train-images-idx3-ubyte``, otherwise from ``t10k-images-idx3-ubyte``. download (bool, optional): If True, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it.
    - Class: `KMNIST(root: str, train: bool = True, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `Kuzushiji-MNIST <https://github.com/rois-codh/kmnist>`_ Dataset. Args: root (string): Root directory of dataset where ``KMNIST/raw/train-images-idx3-ubyte`` and ``KMNIST/raw/t10k-images-idx3-ubyte`` exist. train (bool, optional): If True, creates dataset from ``train-images-idx3-ubyte``, otherwise from ``t10k-images-idx3-ubyte``. download (bool, optional): If True, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it.
    - Class: `MNIST(root: str, train: bool = True, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset. Args: root (string): Root directory of dataset where ``MNIST/raw/train-images-idx3-ubyte`` and ``MNIST/raw/t10k-images-idx3-ubyte`` exist. train (bool, optional): If True, creates dataset from ``train-images-idx3-ubyte``, otherwise from ``t10k-images-idx3-ubyte``. download (bool, optional): If True, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it.
    - Class: `QMNIST(root: str, what: Optional[str] = None, compat: bool = True, train: bool = True, **kwargs: Any) -> None`
      - Description: `QMNIST <https://github.com/facebookresearch/qmnist>`_ Dataset. Args: root (string): Root directory of dataset whose ``raw`` subdir contains binary files of the datasets. what (string,optional): Can be 'train', 'test', 'test10k', 'test50k', or 'nist' for respectively the mnist compatible training set, the 60k qmnist testing set, the 10k qmnist examples that match the mnist testing set, the 50k remaining qmnist testing examples, or all the nist digits. The default is to select 'train' or 'test' according to the compatibility argument 'train'. compat (bool,optional): A boolean that says whether the target for each example is class number (for compatibility with the MNIST dataloader) or a torch vector containing the full qmnist information. Default=True. download (bool, optional): If True, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. train (bool,optional,compatibility): When argument 'what' is not specified, this boolean decides whether to load the training set or the testing set. Default: True.
    - Class: `URLError(reason, filename=None)`
      - Description: Base class for I/O related errors.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `_flip_byte_order(t: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `check_integrity(fpath: str, md5: Optional[str] = None) -> bool`
      - Description: No docstring available
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `extract_archive(from_path: str, to_path: Optional[str] = None, remove_finished: bool = False) -> str`
      - Description: Extract an archive. The archive type and a possible compression is automatically detected from the file name. If the file is compressed but not an archive the call is dispatched to :func:`decompress`. Args: from_path (str): Path to the file to be extracted. to_path (str): Path to the directory the file will be extracted to. If omitted, the directory of the file is used. remove_finished (bool): If ``True``, remove the file after the extraction. Returns: (str): Path to the directory the file was extracted to.
    - Function: `get_int(b: bytes) -> int`
      - Description: No docstring available
    - Function: `read_image_file(path: str) -> torch.Tensor`
      - Description: No docstring available
    - Function: `read_label_file(path: str) -> torch.Tensor`
      - Description: No docstring available
    - Function: `read_sn3_pascalvincent_tensor(path: str, strict: bool = True) -> torch.Tensor`
      - Description: Read a SN3 file in "Pascal Vincent" format (Lush file 'libidx/idx-io.lsh'). Argument may be a filename, compressed filename, or file object.
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - moving_mnist.py
    - Class: `MovingMNIST(root: str, split: Optional[str] = None, split_ratio: int = 10, download: bool = False, transform: Optional[Callable] = None) -> None`
      - Description: `MovingMNIST <http://www.cs.toronto.edu/~nitish/unsupervised_video/>`_ Dataset. Args: root (string): Root directory of dataset where ``MovingMNIST/mnist_test_seq.npy`` exists. split (string, optional): The dataset split, supports ``None`` (default), ``"train"`` and ``"test"``. If ``split=None``, the full data is returned. split_ratio (int, optional): The split ratio of number of frames. If ``split="train"``, the first split frames ``data[:, :split_ratio]`` is returned. If ``split="test"``, the last split frames ``data[:, split_ratio:]`` is returned. If ``split=None``, this parameter is ignored and the all frames data is returned. transform (callable, optional): A function/transform that takes in an torch Tensor and returns a transformed version. E.g, ``transforms.RandomCrop`` download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `download_url(url: str, root: str, filename: Optional[str] = None, md5: Optional[str] = None, max_redirect_hops: int = 3) -> None`
      - Description: Download a file from a url and place it in root. Args: url (str): URL to download file from root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the basename of the URL md5 (str, optional): MD5 checksum of the download. If None, do not check max_redirect_hops (int, optional): Maximum number of redirect hops allowed
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - omniglot.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Omniglot(root: str, background: bool = True, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `Omniglot <https://github.com/brendenlake/omniglot>`_ Dataset. Args: root (string): Root directory of dataset where directory ``omniglot-py`` exists. background (bool, optional): If True, creates dataset from the "background" set, otherwise creates from the "evaluation" set. This terminology is defined by the authors. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If true, downloads the dataset zip files from the internet and puts it in root directory. If the zip files are already downloaded, they are not downloaded again.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `check_integrity(fpath: str, md5: Optional[str] = None) -> bool`
      - Description: No docstring available
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `join(path, *paths)`
      - Description: No docstring available
    - Function: `list_dir(root: str, prefix: bool = False) -> List[str]`
      - Description: List all directories at a given root Args: root (str): Path to directory whose folders need to be listed prefix (bool, optional): If true, prepends the path to each result, otherwise only returns the name of the directories found
    - Function: `list_files(root: str, suffix: str, prefix: bool = False) -> List[str]`
      - Description: List all files ending with a suffix at a given root Args: root (str): Path to directory whose folders need to be listed suffix (str or tuple): Suffix of the files to match, e.g. '.png' or ('.jpg', '.png'). It uses the Python "str.endswith" method and is passed directly prefix (bool, optional): If true, prepends the path to each result, otherwise only returns the name of the files found
  - oxford_iiit_pet.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `OxfordIIITPet(root: str, split: str = 'trainval', target_types: Union[Sequence[str], str] = 'category', transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False)`
      - Description: `Oxford-IIIT Pet Dataset <https://www.robots.ox.ac.uk/~vgg/data/pets/>`_. Args: root (string): Root directory of the dataset. split (string, optional): The dataset split, supports ``"trainval"`` (default) or ``"test"``. target_types (string, sequence of strings, optional): Types of target to use. Can be ``category`` (default) or ``segmentation``. Can also be a list to output a tuple with all specified target types. The types represent: - ``category`` (int): Label for one of the 37 pet categories. - ``segmentation`` (PIL image): Segmentation trimap of the image. If empty, ``None`` will be returned as target. transform (callable, optional): A function/transform that takes in a PIL image and returns a transformed version. E.g, ``transforms.RandomCrop``. target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If True, downloads the dataset from the internet and puts it into ``root/oxford-iiit-pet``. If dataset is already downloaded, it is not downloaded again.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - pcam.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `PCAM(root: str, split: str = 'train', transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False)`
      - Description: `PCAM Dataset <https://github.com/basveeling/pcam>`_. The PatchCamelyon dataset is a binary classification dataset with 327,680 color images (96px x 96px), extracted from histopathologic scans of lymph node sections. Each image is annotated with a binary label indicating presence of metastatic tissue. This dataset requires the ``h5py`` package which you can install with ``pip install h5py``. Args: root (string): Root directory of the dataset. split (string, optional): The dataset split, supports ``"train"`` (default), ``"test"`` or ``"val"``. transform (callable, optional): A function/transform that takes in a PIL image and returns a transformed version. E.g, ``transforms.RandomCrop``. target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If True, downloads the dataset from the internet and puts it into ``root/pcam``. If dataset is already downloaded, it is not downloaded again.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `_decompress(from_path: str, to_path: Optional[str] = None, remove_finished: bool = False) -> str`
      - Description: Decompress a file. The compression is automatically detected from the file name. Args: from_path (str): Path to the file to be decompressed. to_path (str): Path to the decompressed file. If omitted, ``from_path`` without compression extension is used. remove_finished (bool): If ``True``, remove the file after the extraction. Returns: (str): Path to the decompressed file.
    - Function: `download_file_from_google_drive(file_id: str, root: str, filename: Optional[str] = None, md5: Optional[str] = None)`
      - Description: Download a Google Drive file from and place it in root. Args: file_id (str): id of file to be downloaded root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the id of the file. md5 (str, optional): MD5 checksum of the download. If None, do not check
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - phototour.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `PhotoTour(root: str, name: str, train: bool = True, transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `Multi-view Stereo Correspondence <http://matthewalunbrown.com/patchdata/patchdata.html>`_ Dataset. .. note:: We only provide the newer version of the dataset, since the authors state that it is more suitable for training descriptors based on difference of Gaussian, or Harris corners, as the patches are centred on real interest point detections, rather than being projections of 3D points as is the case in the old dataset. The original dataset is available under http://phototour.cs.washington.edu/patches/default.htm. Args: root (string): Root directory where images are. name (string): Name of the dataset to load. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `download_url(url: str, root: str, filename: Optional[str] = None, md5: Optional[str] = None, max_redirect_hops: int = 3) -> None`
      - Description: Download a file from a url and place it in root. Args: url (str): URL to download file from root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the basename of the URL md5 (str, optional): MD5 checksum of the download. If None, do not check max_redirect_hops (int, optional): Maximum number of redirect hops allowed
    - Function: `read_image_file(data_dir: str, image_ext: str, n: int) -> torch.Tensor`
      - Description: Return a Tensor containing the patches
    - Function: `read_info_file(data_dir: str, info_file: str) -> torch.Tensor`
      - Description: Return a Tensor containing the list of labels Read the file and keep only the ID of the 3D point.
    - Function: `read_matches_files(data_dir: str, matches_file: str) -> torch.Tensor`
      - Description: Return a Tensor containing the ground truth matches Read the file and keep only 3D point ID. Matches are represented with a 1, non matches with a 0.
  - places365.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Places365(root: str, split: str = 'train-standard', small: bool = False, download: bool = False, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, loader: Callable[[str], Any] = <function default_loader at 0x00000208094BB880>) -> None`
      - Description: `Places365 <http://places2.csail.mit.edu/index.html>`_ classification dataset. Args: root (string): Root directory of the Places365 dataset. split (string, optional): The dataset split. Can be one of ``train-standard`` (default), ``train-challenge``, ``val``. small (bool, optional): If ``True``, uses the small images, i.e. resized to 256 x 256 pixels, instead of the high resolution ones. download (bool, optional): If ``True``, downloads the dataset components and places them in ``root``. Already downloaded archives are not downloaded again. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. loader (callable, optional): A function to load an image given its path. Attributes: classes (list): List of the class names. class_to_idx (dict): Dict with items (class_name, class_index). imgs (list): List of (image path, class_index) tuples targets (list): The class_index value for each image in the dataset Raises: RuntimeError: If ``download is False`` and the meta files, i.e. the devkit, are not present or corrupted. RuntimeError: If ``download is True`` and the image archive is already extracted.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `check_integrity(fpath: str, md5: Optional[str] = None) -> bool`
      - Description: No docstring available
    - Function: `default_loader(path: str) -> Any`
      - Description: No docstring available
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `urljoin(base, url, allow_fragments=True)`
      - Description: Join a base URL and a possibly relative URL to form an absolute interpretation of the latter.
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - rendered_sst2.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Path(*args, **kwargs)`
      - Description: PurePath subclass that can make system calls. Path represents a filesystem path but unlike PurePath, also offers methods to do system calls on path objects. Depending on your system, instantiating a Path will return either a PosixPath or a WindowsPath object. You can also instantiate a PosixPath or WindowsPath directly, but cannot instantiate a WindowsPath on a POSIX system or vice versa.
    - Class: `RenderedSST2(root: str, split: str = 'train', transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `The Rendered SST2 Dataset <https://github.com/openai/CLIP/blob/main/data/rendered-sst2.md>`_. Rendered SST2 is an image classification dataset used to evaluate the models capability on optical character recognition. This dataset was generated by rendering sentences in the Standford Sentiment Treebank v2 dataset. This dataset contains two classes (positive and negative) and is divided in three splits: a train split containing 6920 images (3610 positive and 3310 negative), a validation split containing 872 images (444 positive and 428 negative), and a test split containing 1821 images (909 positive and 912 negative). Args: root (string): Root directory of the dataset. split (string, optional): The dataset split, supports ``"train"`` (default), `"val"` and ``"test"``. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop``. target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If True, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. Default is False.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `make_dataset(directory: str, class_to_idx: Optional[Dict[str, int]] = None, extensions: Union[str, Tuple[str, ...], NoneType] = None, is_valid_file: Optional[Callable[[str], bool]] = None) -> List[Tuple[str, int]]`
      - Description: Generates a list of samples of a form (path_to_sample, class). See :class:`DatasetFolder` for details. Note: The class_to_idx parameter is here optional and will use the logic of the ``find_classes`` function by default.
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - sbd.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `SBDataset(root: str, image_set: str = 'train', mode: str = 'boundaries', download: bool = False, transforms: Optional[Callable] = None) -> None`
      - Description: `Semantic Boundaries Dataset <http://home.bharathh.info/pubs/codes/SBD/download.html>`_ The SBD currently contains annotations from 11355 images taken from the PASCAL VOC 2011 dataset. .. note :: Please note that the train and val splits included with this dataset are different from the splits in the PASCAL VOC dataset. In particular some "train" images might be part of VOC2012 val. If you are interested in testing on VOC 2012 val, then use `image_set='train_noval'`, which excludes all val images. .. warning:: This class needs `scipy <https://docs.scipy.org/doc/>`_ to load target files from `.mat` format. Args: root (string): Root directory of the Semantic Boundaries Dataset image_set (string, optional): Select the image_set to use, ``train``, ``val`` or ``train_noval``. Image set ``train_noval`` excludes VOC 2012 val images. mode (string, optional): Select target type. Possible values 'boundaries' or 'segmentation'. In case of 'boundaries', the target is an array of shape `[num_classes, H, W]`, where `num_classes=20`. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. transforms (callable, optional): A function/transform that takes input sample and its target as entry and returns a transformed version. Input sample is PIL image and target is a numpy array if `mode='boundaries'` or PIL image if `mode='segmentation'`.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `download_url(url: str, root: str, filename: Optional[str] = None, md5: Optional[str] = None, max_redirect_hops: int = 3) -> None`
      - Description: Download a file from a url and place it in root. Args: url (str): URL to download file from root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the basename of the URL md5 (str, optional): MD5 checksum of the download. If None, do not check max_redirect_hops (int, optional): Maximum number of redirect hops allowed
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - sbu.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `SBU(root: str, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = True) -> None`
      - Description: `SBU Captioned Photo <http://www.cs.virginia.edu/~vicente/sbucaptions/>`_ Dataset. Args: root (string): Root directory of dataset where tarball ``SBUCaptionedPhotoDataset.tar.gz`` exists. transform (callable, optional): A function/transform that takes in a PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If True, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `check_integrity(fpath: str, md5: Optional[str] = None) -> bool`
      - Description: No docstring available
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `download_url(url: str, root: str, filename: Optional[str] = None, md5: Optional[str] = None, max_redirect_hops: int = 3) -> None`
      - Description: Download a file from a url and place it in root. Args: url (str): URL to download file from root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the basename of the URL md5 (str, optional): MD5 checksum of the download. If None, do not check max_redirect_hops (int, optional): Maximum number of redirect hops allowed
  - semeion.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `SEMEION(root: str, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = True) -> None`
      - Description: `SEMEION <http://archive.ics.uci.edu/ml/datasets/semeion+handwritten+digit>`_ Dataset. Args: root (string): Root directory of dataset where directory ``semeion.py`` exists. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `check_integrity(fpath: str, md5: Optional[str] = None) -> bool`
      - Description: No docstring available
    - Function: `download_url(url: str, root: str, filename: Optional[str] = None, md5: Optional[str] = None, max_redirect_hops: int = 3) -> None`
      - Description: Download a file from a url and place it in root. Args: url (str): URL to download file from root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the basename of the URL md5 (str, optional): MD5 checksum of the download. If None, do not check max_redirect_hops (int, optional): Maximum number of redirect hops allowed
  - stanford_cars.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `StanfordCars(root: str, split: str = 'train', transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `Stanford Cars <https://ai.stanford.edu/~jkrause/cars/car_dataset.html>`_ Dataset The Cars dataset contains 16,185 images of 196 classes of cars. The data is split into 8,144 training images and 8,041 testing images, where each class has been split roughly in a 50-50 split .. note:: This class needs `scipy <https://docs.scipy.org/doc/>`_ to load target files from `.mat` format. Args: root (string): Root directory of dataset split (string, optional): The dataset split, supports ``"train"`` (default) or ``"test"``. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If True, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `download_url(url: str, root: str, filename: Optional[str] = None, md5: Optional[str] = None, max_redirect_hops: int = 3) -> None`
      - Description: Download a file from a url and place it in root. Args: url (str): URL to download file from root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the basename of the URL md5 (str, optional): MD5 checksum of the download. If None, do not check max_redirect_hops (int, optional): Maximum number of redirect hops allowed
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - stl10.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `STL10(root: str, split: str = 'train', folds: Optional[int] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `STL10 <https://cs.stanford.edu/~acoates/stl10/>`_ Dataset. Args: root (string): Root directory of dataset where directory ``stl10_binary`` exists. split (string): One of {'train', 'test', 'unlabeled', 'train+unlabeled'}. Accordingly, dataset is selected. folds (int, optional): One of {0-9} or None. For training, loads one of the 10 pre-defined folds of 1k samples for the standard evaluation procedure. If no value is passed, loads the 5k samples. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `cast(typ, val)`
      - Description: Cast a value to a type. This returns the value unchanged. To the type checker this signals that the return value has the designated type, but at runtime we intentionally don't check anything (we want this to be as fast as possible).
    - Function: `check_integrity(fpath: str, md5: Optional[str] = None) -> bool`
      - Description: No docstring available
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - sun397.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Path(*args, **kwargs)`
      - Description: PurePath subclass that can make system calls. Path represents a filesystem path but unlike PurePath, also offers methods to do system calls on path objects. Depending on your system, instantiating a Path will return either a PosixPath or a WindowsPath object. You can also instantiate a PosixPath or WindowsPath directly, but cannot instantiate a WindowsPath on a POSIX system or vice versa.
    - Class: `SUN397(root: str, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `The SUN397 Data Set <https://vision.princeton.edu/projects/2010/SUN/>`_. The SUN397 or Scene UNderstanding (SUN) is a dataset for scene recognition consisting of 397 categories with 108'754 images. Args: root (string): Root directory of the dataset. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop``. target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
  - svhn.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `SVHN(root: str, split: str = 'train', transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `SVHN <http://ufldl.stanford.edu/housenumbers/>`_ Dataset. Note: The SVHN dataset assigns the label `10` to the digit `0`. However, in this Dataset, we assign the label `0` to the digit `0` to be compatible with PyTorch loss functions which expect the class labels to be in the range `[0, C-1]` .. warning:: This class needs `scipy <https://docs.scipy.org/doc/>`_ to load data from `.mat` format. Args: root (string): Root directory of the dataset where the data is stored. split (string): One of {'train', 'test', 'extra'}. Accordingly dataset is selected. 'extra' is Extra training set. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `check_integrity(fpath: str, md5: Optional[str] = None) -> bool`
      - Description: No docstring available
    - Function: `download_url(url: str, root: str, filename: Optional[str] = None, md5: Optional[str] = None, max_redirect_hops: int = 3) -> None`
      - Description: Download a file from a url and place it in root. Args: url (str): URL to download file from root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the basename of the URL md5 (str, optional): MD5 checksum of the download. If None, do not check max_redirect_hops (int, optional): Maximum number of redirect hops allowed
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - ucf101.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `UCF101(root: str, annotation_path: str, frames_per_clip: int, step_between_clips: int = 1, frame_rate: Optional[int] = None, fold: int = 1, train: bool = True, transform: Optional[Callable] = None, _precomputed_metadata: Optional[Dict[str, Any]] = None, num_workers: int = 1, _video_width: int = 0, _video_height: int = 0, _video_min_dimension: int = 0, _audio_samples: int = 0, output_format: str = 'THWC') -> None`
      - Description: `UCF101 <https://www.crcv.ucf.edu/data/UCF101.php>`_ dataset. UCF101 is an action recognition video dataset. This dataset consider every video as a collection of video clips of fixed size, specified by ``frames_per_clip``, where the step in frames between each clip is given by ``step_between_clips``. The dataset itself can be downloaded from the dataset website; annotations that ``annotation_path`` should be pointing to can be downloaded from `here <https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip>`_. To give an example, for 2 videos with 10 and 15 frames respectively, if ``frames_per_clip=5`` and ``step_between_clips=5``, the dataset size will be (2 + 3) = 5, where the first two elements will come from video 1, and the next three elements from video 2. Note that we drop clips which do not have exactly ``frames_per_clip`` elements, so not all frames in a video might be present. Internally, it uses a VideoClips object to handle clip creation. Args: root (string): Root directory of the UCF101 Dataset. annotation_path (str): path to the folder containing the split files; see docstring above for download instructions of these files frames_per_clip (int): number of frames in a clip. step_between_clips (int, optional): number of frames between each clip. fold (int, optional): which fold to use. Should be between 1 and 3. train (bool, optional): if ``True``, creates a dataset from the train split, otherwise from the ``test`` split. transform (callable, optional): A function/transform that takes in a TxHxWxC video and returns a transformed version. output_format (str, optional): The format of the output video tensors (before transforms). Can be either "THWC" (default) or "TCHW". Returns: tuple: A 3-tuple with the following entries: - video (Tensor[T, H, W, C] or Tensor[T, C, H, W]): The `T` video frames - audio(Tensor[K, L]): the audio frames, where `K` is the number of channels and `L` is the number of points - label (int): class of the video clip
    - Class: `VideoClips(video_paths: List[str], clip_length_in_frames: int = 16, frames_between_clips: int = 1, frame_rate: Optional[int] = None, _precomputed_metadata: Optional[Dict[str, Any]] = None, num_workers: int = 0, _video_width: int = 0, _video_height: int = 0, _video_min_dimension: int = 0, _video_max_dimension: int = 0, _audio_samples: int = 0, _audio_channels: int = 0, output_format: str = 'THWC') -> None`
      - Description: Given a list of video files, computes all consecutive subvideos of size `clip_length_in_frames`, where the distance between each subvideo in the same video is defined by `frames_between_clips`. If `frame_rate` is specified, it will also resample all the videos to have the same frame rate, and the clips will refer to this frame rate. Creating this instance the first time is time-consuming, as it needs to decode all the videos in `video_paths`. It is recommended that you cache the results after instantiation of the class. Recreating the clips for different clip lengths is fast, and can be done with the `compute_clips` method. Args: video_paths (List[str]): paths to the video files clip_length_in_frames (int): size of a clip in number of frames frames_between_clips (int): step (in frames) between each clip frame_rate (int, optional): if specified, it will resample the video so that it has `frame_rate`, and then the clips will be defined on the resampled video num_workers (int): how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0) output_format (str): The format of the output video tensors. Can be either "THWC" (default) or "TCHW".
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]`
      - Description: Finds the class folders in a dataset. See :class:`DatasetFolder` for details.
    - Function: `make_dataset(directory: str, class_to_idx: Optional[Dict[str, int]] = None, extensions: Union[str, Tuple[str, ...], NoneType] = None, is_valid_file: Optional[Callable[[str], bool]] = None) -> List[Tuple[str, int]]`
      - Description: Generates a list of samples of a form (path_to_sample, class). See :class:`DatasetFolder` for details. Note: The class_to_idx parameter is here optional and will use the logic of the ``find_classes`` function by default.
  - usps.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `USPS(root: str, train: bool = True, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `USPS <https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#usps>`_ Dataset. The data-format is : [label [index:value ]*256 \n] * num_lines, where ``label`` lies in ``[1, 10]``. The value for each pixel lies in ``[-1, 1]``. Here we transform the ``label`` into ``[0, 9]`` and make pixel values in ``[0, 255]``. Args: root (string): Root directory of dataset to store``USPS`` data files. train (bool, optional): If True, creates dataset from ``usps.bz2``, otherwise from ``usps.t.bz2``. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `download_url(url: str, root: str, filename: Optional[str] = None, md5: Optional[str] = None, max_redirect_hops: int = 3) -> None`
      - Description: Download a file from a url and place it in root. Args: url (str): URL to download file from root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the basename of the URL md5 (str, optional): MD5 checksum of the download. If None, do not check max_redirect_hops (int, optional): Maximum number of redirect hops allowed
  - utils.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `IO()`
      - Description: Generic base class for TextIO and BinaryIO. This is an abstract, generic version of the return of open(). NOTE: This does not distinguish between the different possible classes (text vs. binary, read vs. write vs. read/write, append-only, unbuffered). The TextIO and BinaryIO subclasses below capture the distinctions between text vs. binary, which is pervasive in the interface; however we currently do not offer a way to track the other distinctions in the type system.
    - Class: `TypeVar(name, *constraints, bound=None, covariant=False, contravariant=False)`
      - Description: Type variable. Usage:: T = TypeVar('T') # Can be anything A = TypeVar('A', str, bytes) # Must be str or bytes Type variables exist primarily for the benefit of static type checkers. They serve as the parameters for generic types as well as for generic function definitions. See class Generic for more information on generic types. Generic functions work as follows: def repeat(x: T, n: int) -> List[T]: '''Return a list containing n references to x.''' return [x]*n def longest(x: A, y: A) -> A: '''Return the longest of two strings.''' return x if len(x) >= len(y) else y The latter example's signature is essentially the overloading of (str, str) -> str and (bytes, bytes) -> bytes. Also note that if the arguments are instances of some subclass of str, the return type is still plain str. At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError. Type variables defined with covariant=True or contravariant=True can be used to declare covariant or contravariant generic types. See PEP 484 for more details. By default generic types are invariant in all type variables. Type variables can be introspected. e.g.: T.__name__ == 'T' T.__constraints__ == () T.__covariant__ == False T.__contravariant__ = False A.__constraints__ == (str, bytes) Note that only type variables defined in global scope can be pickled.
    - Function: `_decompress(from_path: str, to_path: Optional[str] = None, remove_finished: bool = False) -> str`
      - Description: Decompress a file. The compression is automatically detected from the file name. Args: from_path (str): Path to the file to be decompressed. to_path (str): Path to the decompressed file. If omitted, ``from_path`` without compression extension is used. remove_finished (bool): If ``True``, remove the file after the extraction. Returns: (str): Path to the decompressed file.
    - Function: `_detect_file_type(file: str) -> Tuple[str, Optional[str], Optional[str]]`
      - Description: Detect the archive type and/or compression of a file. Args: file (str): the filename Returns: (tuple): tuple of suffix, archive type, and compression Raises: RuntimeError: if file has no suffix or suffix is not supported
    - Function: `_download_file_from_remote_location(fpath: str, url: str) -> None`
      - Description: No docstring available
    - Function: `_extract_gdrive_api_response(response, chunk_size: int = 32768) -> Tuple[bytes, Iterator[bytes]]`
      - Description: No docstring available
    - Function: `_extract_tar(from_path: str, to_path: str, compression: Optional[str]) -> None`
      - Description: No docstring available
    - Function: `_extract_zip(from_path: str, to_path: str, compression: Optional[str]) -> None`
      - Description: No docstring available
    - Function: `_flip_byte_order(t: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_get_google_drive_file_id(url: str) -> Optional[str]`
      - Description: No docstring available
    - Function: `_get_redirect_url(url: str, max_hops: int = 3) -> str`
      - Description: No docstring available
    - Function: `_is_remote_location_available() -> bool`
      - Description: No docstring available
    - Function: `_read_pfm(file_name: str, slice_channels: int = 2) -> numpy.ndarray`
      - Description: Read file in .pfm format. Might contain either 1 or 3 channels of data. Args: file_name (str): Path to the file. slice_channels (int): Number of channels to slice out of the file. Useful for reading different data formats stored in .pfm files: Optical Flows, Stereo Disparity Maps, etc.
    - Function: `_save_response_content(content: Iterator[bytes], destination: str, length: Optional[int] = None) -> None`
      - Description: No docstring available
    - Function: `_urlretrieve(url: str, filename: str, chunk_size: int = 32768) -> None`
      - Description: No docstring available
    - Function: `calculate_md5(fpath: str, chunk_size: int = 1048576) -> str`
      - Description: No docstring available
    - Function: `check_integrity(fpath: str, md5: Optional[str] = None) -> bool`
      - Description: No docstring available
    - Function: `check_md5(fpath: str, md5: str, **kwargs: Any) -> bool`
      - Description: No docstring available
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `download_file_from_google_drive(file_id: str, root: str, filename: Optional[str] = None, md5: Optional[str] = None)`
      - Description: Download a Google Drive file from and place it in root. Args: file_id (str): id of file to be downloaded root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the id of the file. md5 (str, optional): MD5 checksum of the download. If None, do not check
    - Function: `download_url(url: str, root: str, filename: Optional[str] = None, md5: Optional[str] = None, max_redirect_hops: int = 3) -> None`
      - Description: Download a file from a url and place it in root. Args: url (str): URL to download file from root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the basename of the URL md5 (str, optional): MD5 checksum of the download. If None, do not check max_redirect_hops (int, optional): Maximum number of redirect hops allowed
    - Function: `extract_archive(from_path: str, to_path: Optional[str] = None, remove_finished: bool = False) -> str`
      - Description: Extract an archive. The archive type and a possible compression is automatically detected from the file name. If the file is compressed but not an archive the call is dispatched to :func:`decompress`. Args: from_path (str): Path to the file to be extracted. to_path (str): Path to the directory the file will be extracted to. If omitted, the directory of the file is used. remove_finished (bool): If ``True``, remove the file after the extraction. Returns: (str): Path to the directory the file was extracted to.
    - Function: `iterable_to_str(iterable: Iterable) -> str`
      - Description: No docstring available
    - Function: `list_dir(root: str, prefix: bool = False) -> List[str]`
      - Description: List all directories at a given root Args: root (str): Path to directory whose folders need to be listed prefix (bool, optional): If true, prepends the path to each result, otherwise only returns the name of the directories found
    - Function: `list_files(root: str, suffix: str, prefix: bool = False) -> List[str]`
      - Description: List all files ending with a suffix at a given root Args: root (str): Path to directory whose folders need to be listed suffix (str or tuple): Suffix of the files to match, e.g. '.png' or ('.jpg', '.png'). It uses the Python "str.endswith" method and is passed directly prefix (bool, optional): If true, prepends the path to each result, otherwise only returns the name of the files found
    - Class: `tqdm(*_, **__)`
      - Description: Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested. Parameters ---------- iterable : iterable, optional Iterable to decorate with a progressbar. Leave blank to manually manage the updates. desc : str, optional Prefix for the progressbar. total : int or float, optional The number of expected iterations. If unspecified, len(iterable) is used if possible. If float("inf") or as a last resort, only basic progress statistics are displayed (no ETA, no progressbar). If `gui` is True and this parameter needs subsequent updating, specify an initial arbitrary large positive number, e.g. 9e9. leave : bool, optional If [default: True], keeps all traces of the progressbar upon termination of iteration. If `None`, will leave only if `position` is `0`. file : `io.TextIOWrapper` or `io.StringIO`, optional Specifies where to output the progress messages (default: sys.stderr). Uses `file.write(str)` and `file.flush()` methods. For encoding, see `write_bytes`. ncols : int, optional The width of the entire output message. If specified, dynamically resizes the progressbar to stay within this bound. If unspecified, attempts to use environment width. The fallback is a meter width of 10 and no limit for the counter and statistics. If 0, will not print any meter (only stats). mininterval : float, optional Minimum progress display update interval [default: 0.1] seconds. maxinterval : float, optional Maximum progress display update interval [default: 10] seconds. Automatically adjusts `miniters` to correspond to `mininterval` after long display update lag. Only works if `dynamic_miniters` or monitor thread is enabled. miniters : int or float, optional Minimum progress display update interval, in iterations. If 0 and `dynamic_miniters`, will automatically adjust to equal `mininterval` (more CPU efficient, good for tight loops). If > 0, will skip display of specified number of iterations. Tweak this and `mininterval` to get very efficient loops. If your progress is erratic with both fast and slow iterations (network, skipping items, etc) you should set miniters=1. ascii : bool or str, optional If unspecified or False, use unicode (smooth blocks) to fill the meter. The fallback is to use ASCII characters " 123456789#". disable : bool, optional Whether to disable the entire progressbar wrapper [default: False]. If set to None, disable on non-TTY. unit : str, optional String that will be used to define the unit of each iteration [default: it]. unit_scale : bool or int or float, optional If 1 or True, the number of iterations will be reduced/scaled automatically and a metric prefix following the International System of Units standard will be added (kilo, mega, etc.) [default: False]. If any other non-zero number, will scale `total` and `n`. dynamic_ncols : bool, optional If set, constantly alters `ncols` and `nrows` to the environment (allowing for window resizes) [default: False]. smoothing : float, optional Exponential moving average smoothing factor for speed estimates (ignored in GUI mode). Ranges from 0 (average speed) to 1 (current/instantaneous speed) [default: 0.3]. bar_format : str, optional Specify a custom bar string formatting. May impact performance. [default: '{l_bar}{bar}{r_bar}'], where l_bar='{desc}: {percentage:3.0f}%|' and r_bar='| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, ' '{rate_fmt}{postfix}]' Possible vars: l_bar, bar, r_bar, n, n_fmt, total, total_fmt, percentage, elapsed, elapsed_s, ncols, nrows, desc, unit, rate, rate_fmt, rate_noinv, rate_noinv_fmt, rate_inv, rate_inv_fmt, postfix, unit_divisor, remaining, remaining_s, eta. Note that a trailing ": " is automatically removed after {desc} if the latter is empty. initial : int or float, optional The initial counter value. Useful when restarting a progress bar [default: 0]. If using float, consider specifying `{n:.3f}` or similar in `bar_format`, or specifying `unit_scale`. position : int, optional Specify the line offset to print this bar (starting from 0) Automatic if unspecified. Useful to manage multiple bars at once (eg, from threads). postfix : dict or *, optional Specify additional stats to display at the end of the bar. Calls `set_postfix(**postfix)` if possible (dict). unit_divisor : float, optional [default: 1000], ignored unless `unit_scale` is True. write_bytes : bool, optional Whether to write bytes. If (default: False) will write unicode. lock_args : tuple, optional Passed to `refresh` for intermediate output (initialisation, iterating, and updating). nrows : int, optional The screen height. If specified, hides nested bars outside this bound. If unspecified, attempts to use environment height. The fallback is 20. colour : str, optional Bar colour (e.g. 'green', '#00ff00'). delay : float, optional Don't display until [default: 0] seconds have elapsed. gui : bool, optional WARNING: internal parameter - do not use. Use tqdm.gui.tqdm(...) instead. If set, will attempt to use matplotlib animations for a graphical output [default: False]. Returns ------- out : decorated iterator.
    - Function: `urlparse(url, scheme='', allow_fragments=True)`
      - Description: Parse a URL into 6 components: <scheme>://<netloc>/<path>;<params>?<query>#<fragment> The result is a named 6-tuple with fields corresponding to the above. It is either a ParseResult or ParseResultBytes object, depending on the type of the url parameter. The username, password, hostname, and port sub-components of netloc can also be accessed as attributes of the returned object. The scheme argument provides the default value of the scheme component when no scheme is found in url. If allow_fragments is False, no attempt is made to separate the fragment component from the previous component, which can be either path or query. Note that % escapes are not expanded.
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - video_utils.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Fraction(numerator=0, denominator=None, *, _normalize=True)`
      - Description: This class implements rational numbers. In the two-argument form of the constructor, Fraction(8, 6) will produce a rational number equivalent to 4/3. Both arguments must be Rational. The numerator defaults to 0 and the denominator defaults to 1 so that Fraction(3) == 3 and Fraction() == 0. Fractions can also be constructed from: - numeric strings similar to those accepted by the float constructor (for example, '-2.3' or '1e10') - strings of the form '123/456' - float and Decimal instances - other Rational instances (including integers)
    - Class: `TypeVar(name, *constraints, bound=None, covariant=False, contravariant=False)`
      - Description: Type variable. Usage:: T = TypeVar('T') # Can be anything A = TypeVar('A', str, bytes) # Must be str or bytes Type variables exist primarily for the benefit of static type checkers. They serve as the parameters for generic types as well as for generic function definitions. See class Generic for more information on generic types. Generic functions work as follows: def repeat(x: T, n: int) -> List[T]: '''Return a list containing n references to x.''' return [x]*n def longest(x: A, y: A) -> A: '''Return the longest of two strings.''' return x if len(x) >= len(y) else y The latter example's signature is essentially the overloading of (str, str) -> str and (bytes, bytes) -> bytes. Also note that if the arguments are instances of some subclass of str, the return type is still plain str. At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError. Type variables defined with covariant=True or contravariant=True can be used to declare covariant or contravariant generic types. See PEP 484 for more details. By default generic types are invariant in all type variables. Type variables can be introspected. e.g.: T.__name__ == 'T' T.__constraints__ == () T.__covariant__ == False T.__contravariant__ = False A.__constraints__ == (str, bytes) Note that only type variables defined in global scope can be pickled.
    - Class: `VideoClips(video_paths: List[str], clip_length_in_frames: int = 16, frames_between_clips: int = 1, frame_rate: Optional[int] = None, _precomputed_metadata: Optional[Dict[str, Any]] = None, num_workers: int = 0, _video_width: int = 0, _video_height: int = 0, _video_min_dimension: int = 0, _video_max_dimension: int = 0, _audio_samples: int = 0, _audio_channels: int = 0, output_format: str = 'THWC') -> None`
      - Description: Given a list of video files, computes all consecutive subvideos of size `clip_length_in_frames`, where the distance between each subvideo in the same video is defined by `frames_between_clips`. If `frame_rate` is specified, it will also resample all the videos to have the same frame rate, and the clips will refer to this frame rate. Creating this instance the first time is time-consuming, as it needs to decode all the videos in `video_paths`. It is recommended that you cache the results after instantiation of the class. Recreating the clips for different clip lengths is fast, and can be done with the `compute_clips` method. Args: video_paths (List[str]): paths to the video files clip_length_in_frames (int): size of a clip in number of frames frames_between_clips (int): step (in frames) between each clip frame_rate (int, optional): if specified, it will resample the video so that it has `frame_rate`, and then the clips will be defined on the resampled video num_workers (int): how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0) output_format (str): The format of the output video tensors. Can be either "THWC" (default) or "TCHW".
    - Class: `_VideoTimestampsDataset(video_paths: List[str]) -> None`
      - Description: Dataset used to parallelize the reading of the timestamps of a list of videos, given their paths in the filesystem. Used in VideoClips and defined at top level, so it can be pickled when forking.
    - Function: `_collate_fn(x: ~T) -> ~T`
      - Description: Dummy collate function to be used with _VideoTimestampsDataset
    - Function: `_probe_video_from_file(filename: str) -> torchvision.io._video_opt.VideoMetaData`
      - Description: Probe a video file and return VideoMetaData with info about the video
    - Function: `_read_video_from_file(filename: str, seek_frame_margin: float = 0.25, read_video_stream: bool = True, video_width: int = 0, video_height: int = 0, video_min_dimension: int = 0, video_max_dimension: int = 0, video_pts_range: Tuple[int, int] = (0, -1), video_timebase: fractions.Fraction = Fraction(0, 1), read_audio_stream: bool = True, audio_samples: int = 0, audio_channels: int = 0, audio_pts_range: Tuple[int, int] = (0, -1), audio_timebase: fractions.Fraction = Fraction(0, 1)) -> Tuple[torch.Tensor, torch.Tensor, torchvision.io._video_opt.VideoMetaData]`
      - Description: Reads a video from a file, returning both the video frames and the audio frames Args: filename (str): path to the video file seek_frame_margin (double, optional): seeking frame in the stream is imprecise. Thus, when video_start_pts is specified, we seek the pts earlier by seek_frame_margin seconds read_video_stream (int, optional): whether read video stream. If yes, set to 1. Otherwise, 0 video_width/video_height/video_min_dimension/video_max_dimension (int): together decide the size of decoded frames: - When video_width = 0, video_height = 0, video_min_dimension = 0, and video_max_dimension = 0, keep the original frame resolution - When video_width = 0, video_height = 0, video_min_dimension != 0, and video_max_dimension = 0, keep the aspect ratio and resize the frame so that shorter edge size is video_min_dimension - When video_width = 0, video_height = 0, video_min_dimension = 0, and video_max_dimension != 0, keep the aspect ratio and resize the frame so that longer edge size is video_max_dimension - When video_width = 0, video_height = 0, video_min_dimension != 0, and video_max_dimension != 0, resize the frame so that shorter edge size is video_min_dimension, and longer edge size is video_max_dimension. The aspect ratio may not be preserved - When video_width = 0, video_height != 0, video_min_dimension = 0, and video_max_dimension = 0, keep the aspect ratio and resize the frame so that frame video_height is $video_height - When video_width != 0, video_height == 0, video_min_dimension = 0, and video_max_dimension = 0, keep the aspect ratio and resize the frame so that frame video_width is $video_width - When video_width != 0, video_height != 0, video_min_dimension = 0, and video_max_dimension = 0, resize the frame so that frame video_width and video_height are set to $video_width and $video_height, respectively video_pts_range (list(int), optional): the start and end presentation timestamp of video stream video_timebase (Fraction, optional): a Fraction rational number which denotes timebase in video stream read_audio_stream (int, optional): whether read audio stream. If yes, set to 1. Otherwise, 0 audio_samples (int, optional): audio sampling rate audio_channels (int optional): audio channels audio_pts_range (list(int), optional): the start and end presentation timestamp of audio stream audio_timebase (Fraction, optional): a Fraction rational number which denotes time base in audio stream Returns vframes (Tensor[T, H, W, C]): the `T` video frames aframes (Tensor[L, K]): the audio frames, where `L` is the number of points and `K` is the number of audio_channels info (Dict): metadata for the video and audio. Can contain the fields video_fps (float) and audio_fps (int)
    - Function: `cast(typ, val)`
      - Description: Cast a value to a type. This returns the value unchanged. To the type checker this signals that the return value has the designated type, but at runtime we intentionally don't check anything (we want this to be as fast as possible).
    - Function: `pts_convert(pts: int, timebase_from: fractions.Fraction, timebase_to: fractions.Fraction, round_func: Callable = <built-in function floor>) -> int`
      - Description: convert pts between different time bases Args: pts: presentation timestamp, float timebase_from: original timebase. Fraction timebase_to: new timebase. Fraction round_func: rounding function.
    - Function: `read_video(filename: str, start_pts: Union[float, fractions.Fraction] = 0, end_pts: Union[float, fractions.Fraction, NoneType] = None, pts_unit: str = 'pts', output_format: str = 'THWC') -> Tuple[torch.Tensor, torch.Tensor, Dict[str, Any]]`
      - Description: Reads a video from a file, returning both the video frames and the audio frames Args: filename (str): path to the video file start_pts (int if pts_unit = 'pts', float / Fraction if pts_unit = 'sec', optional): The start presentation time of the video end_pts (int if pts_unit = 'pts', float / Fraction if pts_unit = 'sec', optional): The end presentation time pts_unit (str, optional): unit in which start_pts and end_pts values will be interpreted, either 'pts' or 'sec'. Defaults to 'pts'. output_format (str, optional): The format of the output video tensors. Can be either "THWC" (default) or "TCHW". Returns: vframes (Tensor[T, H, W, C] or Tensor[T, C, H, W]): the `T` video frames aframes (Tensor[K, L]): the audio frames, where `K` is the number of channels and `L` is the number of points info (Dict): metadata for the video and audio. Can contain the fields video_fps (float) and audio_fps (int)
    - Function: `read_video_timestamps(filename: str, pts_unit: str = 'pts') -> Tuple[List[int], Optional[float]]`
      - Description: List the video frames timestamps. Note that the function decodes the whole video frame-by-frame. Args: filename (str): path to the video file pts_unit (str, optional): unit in which timestamp values will be returned either 'pts' or 'sec'. Defaults to 'pts'. Returns: pts (List[int] if pts_unit = 'pts', List[Fraction] if pts_unit = 'sec'): presentation timestamps for each one of the frames in the video. video_fps (float, optional): the frame rate for the video
    - Class: `tqdm(*_, **__)`
      - Description: Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested. Parameters ---------- iterable : iterable, optional Iterable to decorate with a progressbar. Leave blank to manually manage the updates. desc : str, optional Prefix for the progressbar. total : int or float, optional The number of expected iterations. If unspecified, len(iterable) is used if possible. If float("inf") or as a last resort, only basic progress statistics are displayed (no ETA, no progressbar). If `gui` is True and this parameter needs subsequent updating, specify an initial arbitrary large positive number, e.g. 9e9. leave : bool, optional If [default: True], keeps all traces of the progressbar upon termination of iteration. If `None`, will leave only if `position` is `0`. file : `io.TextIOWrapper` or `io.StringIO`, optional Specifies where to output the progress messages (default: sys.stderr). Uses `file.write(str)` and `file.flush()` methods. For encoding, see `write_bytes`. ncols : int, optional The width of the entire output message. If specified, dynamically resizes the progressbar to stay within this bound. If unspecified, attempts to use environment width. The fallback is a meter width of 10 and no limit for the counter and statistics. If 0, will not print any meter (only stats). mininterval : float, optional Minimum progress display update interval [default: 0.1] seconds. maxinterval : float, optional Maximum progress display update interval [default: 10] seconds. Automatically adjusts `miniters` to correspond to `mininterval` after long display update lag. Only works if `dynamic_miniters` or monitor thread is enabled. miniters : int or float, optional Minimum progress display update interval, in iterations. If 0 and `dynamic_miniters`, will automatically adjust to equal `mininterval` (more CPU efficient, good for tight loops). If > 0, will skip display of specified number of iterations. Tweak this and `mininterval` to get very efficient loops. If your progress is erratic with both fast and slow iterations (network, skipping items, etc) you should set miniters=1. ascii : bool or str, optional If unspecified or False, use unicode (smooth blocks) to fill the meter. The fallback is to use ASCII characters " 123456789#". disable : bool, optional Whether to disable the entire progressbar wrapper [default: False]. If set to None, disable on non-TTY. unit : str, optional String that will be used to define the unit of each iteration [default: it]. unit_scale : bool or int or float, optional If 1 or True, the number of iterations will be reduced/scaled automatically and a metric prefix following the International System of Units standard will be added (kilo, mega, etc.) [default: False]. If any other non-zero number, will scale `total` and `n`. dynamic_ncols : bool, optional If set, constantly alters `ncols` and `nrows` to the environment (allowing for window resizes) [default: False]. smoothing : float, optional Exponential moving average smoothing factor for speed estimates (ignored in GUI mode). Ranges from 0 (average speed) to 1 (current/instantaneous speed) [default: 0.3]. bar_format : str, optional Specify a custom bar string formatting. May impact performance. [default: '{l_bar}{bar}{r_bar}'], where l_bar='{desc}: {percentage:3.0f}%|' and r_bar='| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, ' '{rate_fmt}{postfix}]' Possible vars: l_bar, bar, r_bar, n, n_fmt, total, total_fmt, percentage, elapsed, elapsed_s, ncols, nrows, desc, unit, rate, rate_fmt, rate_noinv, rate_noinv_fmt, rate_inv, rate_inv_fmt, postfix, unit_divisor, remaining, remaining_s, eta. Note that a trailing ": " is automatically removed after {desc} if the latter is empty. initial : int or float, optional The initial counter value. Useful when restarting a progress bar [default: 0]. If using float, consider specifying `{n:.3f}` or similar in `bar_format`, or specifying `unit_scale`. position : int, optional Specify the line offset to print this bar (starting from 0) Automatic if unspecified. Useful to manage multiple bars at once (eg, from threads). postfix : dict or *, optional Specify additional stats to display at the end of the bar. Calls `set_postfix(**postfix)` if possible (dict). unit_divisor : float, optional [default: 1000], ignored unless `unit_scale` is True. write_bytes : bool, optional Whether to write bytes. If (default: False) will write unicode. lock_args : tuple, optional Passed to `refresh` for intermediate output (initialisation, iterating, and updating). nrows : int, optional The screen height. If specified, hides nested bars outside this bound. If unspecified, attempts to use environment height. The fallback is 20. colour : str, optional Bar colour (e.g. 'green', '#00ff00'). delay : float, optional Don't display until [default: 0] seconds have elapsed. gui : bool, optional WARNING: internal parameter - do not use. Use tqdm.gui.tqdm(...) instead. If set, will attempt to use matplotlib animations for a graphical output [default: False]. Returns ------- out : decorated iterator.
    - Function: `unfold(tensor: torch.Tensor, size: int, step: int, dilation: int = 1) -> torch.Tensor`
      - Description: similar to tensor.unfold, but with the dilation and specialized for 1d tensors Returns all consecutive windows of `size` elements, with `step` between windows. The distance between each element in a window is given by `dilation`.
  - vision.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `StandardTransform(transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: No docstring available
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
  - voc.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `ET_Element(Unable to retrieve signature)`
      - Description: No docstring available
    - Function: `ET_parse(source, parser=None, forbid_dtd=False, forbid_entities=True, forbid_external=True)`
      - Description: No docstring available
    - Class: `VOCDetection(root: str, year: str = '2012', image_set: str = 'train', download: bool = False, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, transforms: Optional[Callable] = None)`
      - Description: `Pascal VOC <http://host.robots.ox.ac.uk/pascal/VOC/>`_ Detection Dataset. Args: root (string): Root directory of the VOC Dataset. year (string, optional): The dataset year, supports years ``"2007"`` to ``"2012"``. image_set (string, optional): Select the image_set to use, ``"train"``, ``"trainval"`` or ``"val"``. If ``year=="2007"``, can also be ``"test"``. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. (default: alphabetic indexing of VOC's 20 classes). transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, required): A function/transform that takes in the target and transforms it. transforms (callable, optional): A function/transform that takes input sample and its target as entry and returns a transformed version.
    - Class: `VOCSegmentation(root: str, year: str = '2012', image_set: str = 'train', download: bool = False, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, transforms: Optional[Callable] = None)`
      - Description: `Pascal VOC <http://host.robots.ox.ac.uk/pascal/VOC/>`_ Segmentation Dataset. Args: root (string): Root directory of the VOC Dataset. year (string, optional): The dataset year, supports years ``"2007"`` to ``"2012"``. image_set (string, optional): Select the image_set to use, ``"train"``, ``"trainval"`` or ``"val"``. If ``year=="2007"``, can also be ``"test"``. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. transforms (callable, optional): A function/transform that takes input sample and its target as entry and returns a transformed version.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Class: `_VOCBase(root: str, year: str = '2012', image_set: str = 'train', download: bool = False, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, transforms: Optional[Callable] = None)`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - widerface.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Class: `WIDERFace(root: str, split: str = 'train', transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None`
      - Description: `WIDERFace <http://shuoyang1213.me/WIDERFACE/>`_ Dataset. Args: root (string): Root directory where images and annotations are downloaded to. Expects the following folder structure if download=False: .. code:: <root>  widerface  wider_face_split ('wider_face_split.zip' if compressed)  WIDER_train ('WIDER_train.zip' if compressed)  WIDER_val ('WIDER_val.zip' if compressed)  WIDER_test ('WIDER_test.zip' if compressed) split (string): The dataset split to use. One of {``train``, ``val``, ``test``}. Defaults to ``train``. transform (callable, optional): A function/transform that takes in a PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. download (bool, optional): If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.
    - Function: `abspath(path)`
      - Description: Return the absolute version of a path.
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `download_file_from_google_drive(file_id: str, root: str, filename: Optional[str] = None, md5: Optional[str] = None)`
      - Description: Download a Google Drive file from and place it in root. Args: file_id (str): id of file to be downloaded root (str): Directory to place downloaded file in filename (str, optional): Name to save the file under. If None, use the id of the file. md5 (str, optional): MD5 checksum of the download. If None, do not check
    - Function: `expanduser(path)`
      - Description: Expand ~ and ~user constructs. If user or $HOME is unknown, do nothing.
    - Function: `extract_archive(from_path: str, to_path: Optional[str] = None, remove_finished: bool = False) -> str`
      - Description: Extract an archive. The archive type and a possible compression is automatically detected from the file name. If the file is compressed but not an archive the call is dispatched to :func:`decompress`. Args: from_path (str): Path to the file to be extracted. to_path (str): Path to the directory the file will be extracted to. If omitted, the directory of the file is used. remove_finished (bool): If ``True``, remove the file after the extraction. Returns: (str): Path to the directory the file was extracted to.
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - _optical_flow.py
    - Class: `ABC()`
      - Description: Helper class that provides a standard way to create an ABC using inheritance.
    - Class: `FlowDataset(root: str, transforms: Optional[Callable] = None) -> None`
      - Description: Helper class that provides a standard way to create an ABC using inheritance.
    - Class: `FlyingChairs(root: str, split: str = 'train', transforms: Optional[Callable] = None) -> None`
      - Description: `FlyingChairs <https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html#flyingchairs>`_ Dataset for optical flow. You will also need to download the FlyingChairs_train_val.txt file from the dataset page. The dataset is expected to have the following structure: :: root FlyingChairs data 00001_flow.flo 00001_img1.ppm 00001_img2.ppm ... FlyingChairs_train_val.txt Args: root (string): Root directory of the FlyingChairs Dataset. split (string, optional): The dataset split, either "train" (default) or "val" transforms (callable, optional): A function/transform that takes in ``img1, img2, flow, valid_flow_mask`` and returns a transformed version. ``valid_flow_mask`` is expected for consistency with other datasets which return a built-in valid mask, such as :class:`~torchvision.datasets.KittiFlow`.
    - Class: `FlyingThings3D(root: str, split: str = 'train', pass_name: str = 'clean', camera: str = 'left', transforms: Optional[Callable] = None) -> None`
      - Description: `FlyingThings3D <https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html>`_ dataset for optical flow. The dataset is expected to have the following structure: :: root FlyingThings3D frames_cleanpass TEST TRAIN frames_finalpass TEST TRAIN optical_flow TEST TRAIN Args: root (string): Root directory of the intel FlyingThings3D Dataset. split (string, optional): The dataset split, either "train" (default) or "test" pass_name (string, optional): The pass to use, either "clean" (default) or "final" or "both". See link above for details on the different passes. camera (string, optional): Which camera to return images from. Can be either "left" (default) or "right" or "both". transforms (callable, optional): A function/transform that takes in ``img1, img2, flow, valid_flow_mask`` and returns a transformed version. ``valid_flow_mask`` is expected for consistency with other datasets which return a built-in valid mask, such as :class:`~torchvision.datasets.KittiFlow`.
    - Class: `HD1K(root: str, split: str = 'train', transforms: Optional[Callable] = None) -> None`
      - Description: `HD1K <http://hci-benchmark.iwr.uni-heidelberg.de/>`__ dataset for optical flow. The dataset is expected to have the following structure: :: root hd1k hd1k_challenge image_2 hd1k_flow_gt flow_occ hd1k_input image_2 Args: root (string): Root directory of the HD1K Dataset. split (string, optional): The dataset split, either "train" (default) or "test" transforms (callable, optional): A function/transform that takes in ``img1, img2, flow, valid_flow_mask`` and returns a transformed version.
    - Class: `KittiFlow(root: str, split: str = 'train', transforms: Optional[Callable] = None) -> None`
      - Description: `KITTI <http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow>`__ dataset for optical flow (2015). The dataset is expected to have the following structure: :: root KittiFlow testing image_2 training image_2 flow_occ Args: root (string): Root directory of the KittiFlow Dataset. split (string, optional): The dataset split, either "train" (default) or "test" transforms (callable, optional): A function/transform that takes in ``img1, img2, flow, valid_flow_mask`` and returns a transformed version.
    - Class: `Path(*args, **kwargs)`
      - Description: PurePath subclass that can make system calls. Path represents a filesystem path but unlike PurePath, also offers methods to do system calls on path objects. Depending on your system, instantiating a Path will return either a PosixPath or a WindowsPath object. You can also instantiate a PosixPath or WindowsPath directly, but cannot instantiate a WindowsPath on a POSIX system or vice versa.
    - Class: `Sintel(root: str, split: str = 'train', pass_name: str = 'clean', transforms: Optional[Callable] = None) -> None`
      - Description: `Sintel <http://sintel.is.tue.mpg.de/>`_ Dataset for optical flow. The dataset is expected to have the following structure: :: root Sintel testing clean scene_1 scene_2 ... final scene_1 scene_2 ... training clean scene_1 scene_2 ... final scene_1 scene_2 ... flow scene_1 scene_2 ... Args: root (string): Root directory of the Sintel Dataset. split (string, optional): The dataset split, either "train" (default) or "test" pass_name (string, optional): The pass to use, either "clean" (default), "final", or "both". See link above for details on the different passes. transforms (callable, optional): A function/transform that takes in ``img1, img2, flow, valid_flow_mask`` and returns a transformed version. ``valid_flow_mask`` is expected for consistency with other datasets which return a built-in valid mask, such as :class:`~torchvision.datasets.KittiFlow`.
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `_read_16bits_png_with_flow_and_valid_mask(file_name: str) -> Tuple[numpy.ndarray, numpy.ndarray]`
      - Description: No docstring available
    - Function: `_read_flo(file_name: str) -> numpy.ndarray`
      - Description: Read .flo file in Middlebury format
    - Function: `_read_pfm(file_name: str, slice_channels: int = 2) -> numpy.ndarray`
      - Description: Read file in .pfm format. Might contain either 1 or 3 channels of data. Args: file_name (str): Path to the file. slice_channels (int): Number of channels to slice out of the file. Useful for reading different data formats stored in .pfm files: Optical Flows, Stereo Disparity Maps, etc.
    - Function: `_read_png_16(path: str, mode: torchvision.io.image.ImageReadMode = <ImageReadMode.UNCHANGED: 0>) -> torch.Tensor`
      - Description: No docstring available
    - Function: `abstractmethod(funcobj)`
      - Description: A decorator indicating abstract methods. Requires that the metaclass is ABCMeta or derived from it. A class that has a metaclass derived from ABCMeta cannot be instantiated unless all of its abstract methods are overridden. The abstract methods can be called using any of the normal 'super' call mechanisms. abstractmethod() may be used to declare abstract methods for properties and descriptors. Usage: class C(metaclass=ABCMeta): @abstractmethod def my_abstract_method(self, arg1, arg2, argN): ...
    - Function: `glob(pathname, *, root_dir=None, dir_fd=None, recursive=False, include_hidden=False)`
      - Description: Return a list of paths matching a pathname pattern. The pattern may contain simple shell-style wildcards a la fnmatch. Unlike fnmatch, filenames starting with a dot are special cases that are not matched by '*' and '?' patterns by default. If `include_hidden` is true, the patterns '*', '?', '**' will match hidden directories. If `recursive` is true, the pattern '**' will match any files and zero or more directories and subdirectories.
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
  - _stereo_matching.py
    - Class: `ABC()`
      - Description: Helper class that provides a standard way to create an ABC using inheritance.
    - Class: `CREStereo(root: str, transforms: Optional[Callable] = None) -> None`
      - Description: Synthetic dataset used in training the `CREStereo <https://arxiv.org/pdf/2203.11483.pdf>`_ architecture. Dataset details on the official paper `repo <https://github.com/megvii-research/CREStereo>`_. The dataset is expected to have the following structure: :: root CREStereo tree img1_left.jpg img1_right.jpg img1_left.disp.jpg img1_right.disp.jpg img2_left.jpg img2_right.jpg img2_left.disp.jpg img2_right.disp.jpg ... shapenet img1_left.jpg img1_right.jpg img1_left.disp.jpg img1_right.disp.jpg ... reflective img1_left.jpg img1_right.jpg img1_left.disp.jpg img1_right.disp.jpg ... hole img1_left.jpg img1_right.jpg img1_left.disp.jpg img1_right.disp.jpg ... Args: root (str): Root directory of the dataset. transforms (callable, optional): A function/transform that takes in a sample and returns a transformed version.
    - Class: `CarlaStereo(root: str, transforms: Optional[Callable] = None) -> None`
      - Description: Carla simulator data linked in the `CREStereo github repo <https://github.com/megvii-research/CREStereo>`_. The dataset is expected to have the following structure: :: root carla-highres trainingF scene1 img0.png img1.png disp0GT.pfm disp1GT.pfm calib.txt scene2 img0.png img1.png disp0GT.pfm disp1GT.pfm calib.txt ... Args: root (string): Root directory where `carla-highres` is located. transforms (callable, optional): A function/transform that takes in a sample and returns a transformed version.
    - Class: `ETH3DStereo(root: str, split: str = 'train', transforms: Optional[Callable] = None) -> None`
      - Description: ETH3D `Low-Res Two-View <https://www.eth3d.net/datasets>`_ dataset. The dataset is expected to have the following structure: :: root ETH3D two_view_training scene1 im1.png im0.png images.txt cameras.txt calib.txt scene2 im1.png im0.png images.txt cameras.txt calib.txt ... two_view_training_gt scene1 disp0GT.pfm mask0nocc.png scene2 disp0GT.pfm mask0nocc.png ... two_view_testing scene1 im1.png im0.png images.txt cameras.txt calib.txt scene2 im1.png im0.png images.txt cameras.txt calib.txt ... Args: root (string): Root directory of the ETH3D Dataset. split (string, optional): The dataset split of scenes, either "train" (default) or "test". transforms (callable, optional): A function/transform that takes in a sample and returns a transformed version.
    - Class: `FallingThingsStereo(root: str, variant: str = 'single', transforms: Optional[Callable] = None) -> None`
      - Description: `FallingThings <https://research.nvidia.com/publication/2018-06_falling-things-synthetic-dataset-3d-object-detection-and-pose-estimation>`_ dataset. The dataset is expected to have the following structure: :: root FallingThings single dir1 scene1 _object_settings.json _camera_settings.json image1.left.depth.png image1.right.depth.png image1.left.jpg image1.right.jpg image2.left.depth.png image2.right.depth.png image2.left.jpg image2.right ... scene2 ... mixed scene1 _object_settings.json _camera_settings.json image1.left.depth.png image1.right.depth.png image1.left.jpg image1.right.jpg image2.left.depth.png image2.right.depth.png image2.left.jpg image2.right ... scene2 ... Args: root (string): Root directory where FallingThings is located. variant (string): Which variant to use. Either "single", "mixed", or "both". transforms (callable, optional): A function/transform that takes in a sample and returns a transformed version.
    - Class: `InStereo2k(root: str, split: str = 'train', transforms: Optional[Callable] = None) -> None`
      - Description: `InStereo2k <https://github.com/YuhuaXu/StereoDataset>`_ dataset. The dataset is expected to have the following structure: :: root InStereo2k train scene1 left.png right.png left_disp.png right_disp.png ... scene2 ... test scene1 left.png right.png left_disp.png right_disp.png ... scene2 ... Args: root (string): Root directory where InStereo2k is located. split (string): Either "train" or "test". transforms (callable, optional): A function/transform that takes in a sample and returns a transformed version.
    - Class: `Kitti2012Stereo(root: str, split: str = 'train', transforms: Optional[Callable] = None) -> None`
      - Description: KITTI dataset from the `2012 stereo evaluation benchmark <http://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php>`_. Uses the RGB images for consistency with KITTI 2015. The dataset is expected to have the following structure: :: root Kitti2012 testing colored_0 1_10.png 2_10.png ... colored_1 1_10.png 2_10.png ... training colored_0 1_10.png 2_10.png ... colored_1 1_10.png 2_10.png ... disp_noc 1.png 2.png ... calib Args: root (string): Root directory where `Kitti2012` is located. split (string, optional): The dataset split of scenes, either "train" (default) or "test". transforms (callable, optional): A function/transform that takes in a sample and returns a transformed version.
    - Class: `Kitti2015Stereo(root: str, split: str = 'train', transforms: Optional[Callable] = None) -> None`
      - Description: KITTI dataset from the `2015 stereo evaluation benchmark <http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php>`_. The dataset is expected to have the following structure: :: root Kitti2015 testing image_2 img1.png img2.png ... image_3 img1.png img2.png ... training image_2 img1.png img2.png ... image_3 img1.png img2.png ... disp_occ_0 img1.png img2.png ... disp_occ_1 img1.png img2.png ... calib Args: root (string): Root directory where `Kitti2015` is located. split (string, optional): The dataset split of scenes, either "train" (default) or "test". transforms (callable, optional): A function/transform that takes in a sample and returns a transformed version.
    - Class: `Middlebury2014Stereo(root: str, split: str = 'train', calibration: Optional[str] = 'perfect', use_ambient_views: bool = False, transforms: Optional[Callable] = None, download: bool = False) -> None`
      - Description: Publicly available scenes from the Middlebury dataset `2014 version <https://vision.middlebury.edu/stereo/data/scenes2014/>`. The dataset mostly follows the original format, without containing the ambient subdirectories. : :: root Middlebury2014 train scene1-{perfect,imperfect} calib.txt im{0,1}.png im1E.png im1L.png disp{0,1}.pfm disp{0,1}-n.png disp{0,1}-sd.pfm disp{0,1}y.pfm scene2-{perfect,imperfect} calib.txt im{0,1}.png im1E.png im1L.png disp{0,1}.pfm disp{0,1}-n.png disp{0,1}-sd.pfm disp{0,1}y.pfm ... additional scene1-{perfect,imperfect} calib.txt im{0,1}.png im1E.png im1L.png disp{0,1}.pfm disp{0,1}-n.png disp{0,1}-sd.pfm disp{0,1}y.pfm ... test scene1 calib.txt im{0,1}.png scene2 calib.txt im{0,1}.png ... Args: root (string): Root directory of the Middleburry 2014 Dataset. split (string, optional): The dataset split of scenes, either "train" (default), "test", or "additional" use_ambient_views (boolean, optional): Whether to use different expose or lightning views when possible. The dataset samples with equal probability between ``[im1.png, im1E.png, im1L.png]``. calibration (string, optional): Whether or not to use the calibrated (default) or uncalibrated scenes. transforms (callable, optional): A function/transform that takes in a sample and returns a transformed version. download (boolean, optional): Whether or not to download the dataset in the ``root`` directory.
    - Class: `Path(*args, **kwargs)`
      - Description: PurePath subclass that can make system calls. Path represents a filesystem path but unlike PurePath, also offers methods to do system calls on path objects. Depending on your system, instantiating a Path will return either a PosixPath or a WindowsPath object. You can also instantiate a PosixPath or WindowsPath directly, but cannot instantiate a WindowsPath on a POSIX system or vice versa.
    - Class: `SceneFlowStereo(root: str, variant: str = 'FlyingThings3D', pass_name: str = 'clean', transforms: Optional[Callable] = None) -> None`
      - Description: Dataset interface for `Scene Flow <https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html>`_ datasets. This interface provides access to the `FlyingThings3D, `Monkaa` and `Driving` datasets. The dataset is expected to have the following structure: :: root SceneFlow Monkaa frames_cleanpass scene1 left img1.png img2.png right img1.png img2.png scene2 left img1.png img2.png right img1.png img2.png frames_finalpass scene1 left img1.png img2.png right img1.png img2.png ... ... disparity scene1 left img1.pfm img2.pfm right img1.pfm img2.pfm FlyingThings3D ... ... Args: root (string): Root directory where SceneFlow is located. variant (string): Which dataset variant to user, "FlyingThings3D" (default), "Monkaa" or "Driving". pass_name (string): Which pass to use, "clean" (default), "final" or "both". transforms (callable, optional): A function/transform that takes in a sample and returns a transformed version.
    - Class: `SintelStereo(root: str, pass_name: str = 'final', transforms: Optional[Callable] = None) -> None`
      - Description: Sintel `Stereo Dataset <http://sintel.is.tue.mpg.de/stereo>`_. The dataset is expected to have the following structure: :: root Sintel training final_left scene1 img1.png img2.png ... ... final_right scene2 img1.png img2.png ... ... disparities scene1 img1.png img2.png ... ... occlusions scene1 img1.png img2.png ... ... outofframe scene1 img1.png img2.png ... ... Args: root (string): Root directory where Sintel Stereo is located. pass_name (string): The name of the pass to use, either "final", "clean" or "both". transforms (callable, optional): A function/transform that takes in a sample and returns a transformed version.
    - Class: `StereoMatchingDataset(root: str, transforms: Optional[Callable] = None) -> None`
      - Description: Base interface for Stereo matching datasets
    - Class: `VisionDataset(root: str, transforms: Optional[Callable] = None, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None`
      - Description: Base Class For making datasets which are compatible with torchvision. It is necessary to override the ``__getitem__`` and ``__len__`` method. Args: root (string): Root directory of dataset. transforms (callable, optional): A function/transforms that takes in an image and a label and returns the transformed versions of both. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. .. note:: :attr:`transforms` and the combination of :attr:`transform` and :attr:`target_transform` are mutually exclusive.
    - Function: `_read_pfm(file_name: str, slice_channels: int = 2) -> numpy.ndarray`
      - Description: Read file in .pfm format. Might contain either 1 or 3 channels of data. Args: file_name (str): Path to the file. slice_channels (int): Number of channels to slice out of the file. Useful for reading different data formats stored in .pfm files: Optical Flows, Stereo Disparity Maps, etc.
    - Function: `abstractmethod(funcobj)`
      - Description: A decorator indicating abstract methods. Requires that the metaclass is ABCMeta or derived from it. A class that has a metaclass derived from ABCMeta cannot be instantiated unless all of its abstract methods are overridden. The abstract methods can be called using any of the normal 'super' call mechanisms. abstractmethod() may be used to declare abstract methods for properties and descriptors. Usage: class C(metaclass=ABCMeta): @abstractmethod def my_abstract_method(self, arg1, arg2, argN): ...
    - Function: `cast(typ, val)`
      - Description: Cast a value to a type. This returns the value unchanged. To the type checker this signals that the return value has the designated type, but at runtime we intentionally don't check anything (we want this to be as fast as possible).
    - Function: `download_and_extract_archive(url: str, download_root: str, extract_root: Optional[str] = None, filename: Optional[str] = None, md5: Optional[str] = None, remove_finished: bool = False) -> None`
      - Description: No docstring available
    - Function: `glob(pathname, *, root_dir=None, dir_fd=None, recursive=False, include_hidden=False)`
      - Description: Return a list of paths matching a pathname pattern. The pattern may contain simple shell-style wildcards a la fnmatch. Unlike fnmatch, filenames starting with a dot are special cases that are not matched by '*' and '?' patterns by default. If `include_hidden` is true, the patterns '*', '?', '**' will match hidden directories. If `recursive` is true, the pattern '**' will match any files and zero or more directories and subdirectories.
    - Function: `verify_str_arg(value: ~T, arg: Optional[str] = None, valid_values: Optional[Iterable[~T]] = None, custom_msg: Optional[str] = None) -> ~T`
      - Description: No docstring available
- io/
  - __pycache__/
  - image.py
    - Class: `Enum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: Create a collection of name/value pairs. Example enumeration: >>> class Color(Enum): ... RED = 1 ... BLUE = 2 ... GREEN = 3 Access them by: - attribute access:: >>> Color.RED <Color.RED: 1> - value lookup: >>> Color(1) <Color.RED: 1> - name lookup: >>> Color['RED'] <Color.RED: 1> Enumerations can be iterated over, and know how many members they have: >>> len(Color) 3 >>> list(Color) [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>] Methods can be added to enumerations, and members can have their own attributes -- see the documentation for details.
    - Class: `ImageReadMode(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: Support for various modes while reading images. Use ``ImageReadMode.UNCHANGED`` for loading the image as-is, ``ImageReadMode.GRAY`` for converting to grayscale, ``ImageReadMode.GRAY_ALPHA`` for grayscale with transparency, ``ImageReadMode.RGB`` for RGB and ``ImageReadMode.RGB_ALPHA`` for RGB with transparency.
    - Function: `_load_library(lib_name)`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_read_png_16(path: str, mode: torchvision.io.image.ImageReadMode = <ImageReadMode.UNCHANGED: 0>) -> torch.Tensor`
      - Description: No docstring available
    - Function: `decode_image(input: torch.Tensor, mode: torchvision.io.image.ImageReadMode = <ImageReadMode.UNCHANGED: 0>) -> torch.Tensor`
      - Description: Detects whether an image is a JPEG or PNG and performs the appropriate operation to decode the image into a 3 dimensional RGB or grayscale Tensor. Optionally converts the image to the desired format. The values of the output tensor are uint8 in [0, 255]. Args: input (Tensor): a one dimensional uint8 tensor containing the raw bytes of the PNG or JPEG image. mode (ImageReadMode): the read mode used for optionally converting the image. Default: ``ImageReadMode.UNCHANGED``. See ``ImageReadMode`` class for more information on various available modes. Returns: output (Tensor[image_channels, image_height, image_width])
    - Function: `decode_jpeg(input: torch.Tensor, mode: torchvision.io.image.ImageReadMode = <ImageReadMode.UNCHANGED: 0>, device: str = 'cpu') -> torch.Tensor`
      - Description: Decodes a JPEG image into a 3 dimensional RGB or grayscale Tensor. Optionally converts the image to the desired format. The values of the output tensor are uint8 between 0 and 255. Args: input (Tensor[1]): a one dimensional uint8 tensor containing the raw bytes of the JPEG image. This tensor must be on CPU, regardless of the ``device`` parameter. mode (ImageReadMode): the read mode used for optionally converting the image. The supported modes are: ``ImageReadMode.UNCHANGED``, ``ImageReadMode.GRAY`` and ``ImageReadMode.RGB`` Default: ``ImageReadMode.UNCHANGED``. See ``ImageReadMode`` class for more information on various available modes. device (str or torch.device): The device on which the decoded image will be stored. If a cuda device is specified, the image will be decoded with `nvjpeg <https://developer.nvidia.com/nvjpeg>`_. This is only supported for CUDA version >= 10.1 .. betastatus:: device parameter .. warning:: There is a memory leak in the nvjpeg library for CUDA versions < 11.6. Make sure to rely on CUDA 11.6 or above before using ``device="cuda"``. Returns: output (Tensor[image_channels, image_height, image_width])
    - Function: `decode_png(input: torch.Tensor, mode: torchvision.io.image.ImageReadMode = <ImageReadMode.UNCHANGED: 0>) -> torch.Tensor`
      - Description: Decodes a PNG image into a 3 dimensional RGB or grayscale Tensor. Optionally converts the image to the desired format. The values of the output tensor are uint8 in [0, 255]. Args: input (Tensor[1]): a one dimensional uint8 tensor containing the raw bytes of the PNG image. mode (ImageReadMode): the read mode used for optionally converting the image. Default: ``ImageReadMode.UNCHANGED``. See `ImageReadMode` class for more information on various available modes. Returns: output (Tensor[image_channels, image_height, image_width])
    - Function: `encode_jpeg(input: torch.Tensor, quality: int = 75) -> torch.Tensor`
      - Description: Takes an input tensor in CHW layout and returns a buffer with the contents of its corresponding JPEG file. Args: input (Tensor[channels, image_height, image_width])): int8 image tensor of ``c`` channels, where ``c`` must be 1 or 3. quality (int): Quality of the resulting JPEG file, it must be a number between 1 and 100. Default: 75 Returns: output (Tensor[1]): A one dimensional int8 tensor that contains the raw bytes of the JPEG file.
    - Function: `encode_png(input: torch.Tensor, compression_level: int = 6) -> torch.Tensor`
      - Description: Takes an input tensor in CHW layout and returns a buffer with the contents of its corresponding PNG file. Args: input (Tensor[channels, image_height, image_width]): int8 image tensor of ``c`` channels, where ``c`` must 3 or 1. compression_level (int): Compression factor for the resulting file, it must be a number between 0 and 9. Default: 6 Returns: Tensor[1]: A one dimensional int8 tensor that contains the raw bytes of the PNG file.
    - Function: `read_file(path: str) -> torch.Tensor`
      - Description: Reads and outputs the bytes contents of a file as a uint8 Tensor with one dimension. Args: path (str): the path to the file to be read Returns: data (Tensor)
    - Function: `read_image(path: str, mode: torchvision.io.image.ImageReadMode = <ImageReadMode.UNCHANGED: 0>) -> torch.Tensor`
      - Description: Reads a JPEG or PNG image into a 3 dimensional RGB or grayscale Tensor. Optionally converts the image to the desired format. The values of the output tensor are uint8 in [0, 255]. Args: path (str): path of the JPEG or PNG image. mode (ImageReadMode): the read mode used for optionally converting the image. Default: ``ImageReadMode.UNCHANGED``. See ``ImageReadMode`` class for more information on various available modes. Returns: output (Tensor[image_channels, image_height, image_width])
    - Function: `write_file(filename: str, data: torch.Tensor) -> None`
      - Description: Writes the contents of an uint8 tensor with one dimension to a file. Args: filename (str): the path to the file to be written data (Tensor): the contents to be written to the output file
    - Function: `write_jpeg(input: torch.Tensor, filename: str, quality: int = 75)`
      - Description: Takes an input tensor in CHW layout and saves it in a JPEG file. Args: input (Tensor[channels, image_height, image_width]): int8 image tensor of ``c`` channels, where ``c`` must be 1 or 3. filename (str): Path to save the image. quality (int): Quality of the resulting JPEG file, it must be a number between 1 and 100. Default: 75
    - Function: `write_png(input: torch.Tensor, filename: str, compression_level: int = 6)`
      - Description: Takes an input tensor in CHW layout (or HW in the case of grayscale images) and saves it in a PNG file. Args: input (Tensor[channels, image_height, image_width]): int8 image tensor of ``c`` channels, where ``c`` must be 1 or 3. filename (str): Path to save the image. compression_level (int): Compression factor for the resulting file, it must be a number between 0 and 9. Default: 6
  - video.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Fraction(numerator=0, denominator=None, *, _normalize=True)`
      - Description: This class implements rational numbers. In the two-argument form of the constructor, Fraction(8, 6) will produce a rational number equivalent to 4/3. Both arguments must be Rational. The numerator defaults to 0 and the denominator defaults to 1 so that Fraction(3) == 3 and Fraction() == 0. Fractions can also be constructed from: - numeric strings similar to those accepted by the float constructor (for example, '-2.3' or '1e10') - strings of the form '123/456' - float and Decimal instances - other Rational instances (including integers)
    - Function: `_align_audio_frames(aframes: torch.Tensor, audio_frames: List[ForwardRef('av.frame.Frame')], ref_start: int, ref_end: float) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_av_available() -> bool`
      - Description: No docstring available
    - Function: `_can_read_timestamps_from_packets(container: 'av.container.Container') -> bool`
      - Description: No docstring available
    - Function: `_check_av_available() -> None`
      - Description: No docstring available
    - Function: `_decode_video_timestamps(container: 'av.container.Container') -> List[int]`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_read_from_stream(container: 'av.container.Container', start_offset: float, end_offset: float, pts_unit: str, stream: 'av.stream.Stream', stream_name: Dict[str, Union[int, List[int], Tuple[int, ...], NoneType]]) -> List[ForwardRef('av.frame.Frame')]`
      - Description: No docstring available
    - Function: `read_video(filename: str, start_pts: Union[float, fractions.Fraction] = 0, end_pts: Union[float, fractions.Fraction, NoneType] = None, pts_unit: str = 'pts', output_format: str = 'THWC') -> Tuple[torch.Tensor, torch.Tensor, Dict[str, Any]]`
      - Description: Reads a video from a file, returning both the video frames and the audio frames Args: filename (str): path to the video file start_pts (int if pts_unit = 'pts', float / Fraction if pts_unit = 'sec', optional): The start presentation time of the video end_pts (int if pts_unit = 'pts', float / Fraction if pts_unit = 'sec', optional): The end presentation time pts_unit (str, optional): unit in which start_pts and end_pts values will be interpreted, either 'pts' or 'sec'. Defaults to 'pts'. output_format (str, optional): The format of the output video tensors. Can be either "THWC" (default) or "TCHW". Returns: vframes (Tensor[T, H, W, C] or Tensor[T, C, H, W]): the `T` video frames aframes (Tensor[K, L]): the audio frames, where `K` is the number of channels and `L` is the number of points info (Dict): metadata for the video and audio. Can contain the fields video_fps (float) and audio_fps (int)
    - Function: `read_video_timestamps(filename: str, pts_unit: str = 'pts') -> Tuple[List[int], Optional[float]]`
      - Description: List the video frames timestamps. Note that the function decodes the whole video frame-by-frame. Args: filename (str): path to the video file pts_unit (str, optional): unit in which timestamp values will be returned either 'pts' or 'sec'. Defaults to 'pts'. Returns: pts (List[int] if pts_unit = 'pts', List[Fraction] if pts_unit = 'sec'): presentation timestamps for each one of the frames in the video. video_fps (float, optional): the frame rate for the video
    - Function: `write_video(filename: str, video_array: torch.Tensor, fps: float, video_codec: str = 'libx264', options: Optional[Dict[str, Any]] = None, audio_array: Optional[torch.Tensor] = None, audio_fps: Optional[float] = None, audio_codec: Optional[str] = None, audio_options: Optional[Dict[str, Any]] = None) -> None`
      - Description: Writes a 4d tensor in [T, H, W, C] format in a video file Args: filename (str): path where the video will be saved video_array (Tensor[T, H, W, C]): tensor containing the individual frames, as a uint8 tensor in [T, H, W, C] format fps (Number): video frames per second video_codec (str): the name of the video codec, i.e. "libx264", "h264", etc. options (Dict): dictionary containing options to be passed into the PyAV video stream audio_array (Tensor[C, N]): tensor containing the audio, where C is the number of channels and N is the number of samples audio_fps (Number): audio sample rate, typically 44100 or 48000 audio_codec (str): the name of the audio codec, i.e. "mp3", "aac", etc. audio_options (Dict): dictionary containing options to be passed into the PyAV audio stream
  - video_reader.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `VideoReader(src: str = '', stream: str = 'video', num_threads: int = 0, path: Optional[str] = None) -> None`
      - Description: Fine-grained video-reading API. Supports frame-by-frame reading of various streams from a single video container. Much like previous video_reader API it supports the following backends: video_reader, pyav, and cuda. Backends can be set via `torchvision.set_video_backend` function. .. betastatus:: VideoReader class Example: The following examples creates a :mod:`VideoReader` object, seeks into 2s point, and returns a single frame:: import torchvision video_path = "path_to_a_test_video" reader = torchvision.io.VideoReader(video_path, "video") reader.seek(2.0) frame = next(reader) :mod:`VideoReader` implements the iterable API, which makes it suitable to using it in conjunction with :mod:`itertools` for more advanced reading. As such, we can use a :mod:`VideoReader` instance inside for loops:: reader.seek(2) for frame in reader: frames.append(frame['data']) # additionally, `seek` implements a fluent API, so we can do for frame in reader.seek(2): frames.append(frame['data']) With :mod:`itertools`, we can read all frames between 2 and 5 seconds with the following code:: for frame in itertools.takewhile(lambda x: x['pts'] <= 5, reader.seek(2)): frames.append(frame['data']) and similarly, reading 10 frames after the 2s timestamp can be achieved as follows:: for frame in itertools.islice(reader.seek(2), 10): frames.append(frame['data']) .. note:: Each stream descriptor consists of two parts: stream type (e.g. 'video') and a unique stream id (which are determined by the video encoding). In this way, if the video container contains multiple streams of the same type, users can access the one they want. If only stream type is passed, the decoder auto-detects first stream of that type. Args: src (string, bytes object, or tensor): The media source. If string-type, it must be a file path supported by FFMPEG. If bytes, should be an in-memory representation of a file supported by FFMPEG. If Tensor, it is interpreted internally as byte buffer. It must be one-dimensional, of type ``torch.uint8``. stream (string, optional): descriptor of the required stream, followed by the stream id, in the format ``{stream_type}:{stream_id}``. Defaults to ``"video:0"``. Currently available options include ``['video', 'audio']`` num_threads (int, optional): number of threads used by the codec to decode video. Default value (0) enables multithreading with codec-dependent heuristic. The performance will depend on the version of FFMPEG codecs supported. path (str, optional): .. warning: This parameter was deprecated in ``0.15`` and will be removed in ``0.17``. Please use ``src`` instead.
    - Function: `_has_video_opt() -> bool`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
  - _load_gpu_decoder.py
    - Function: `_load_library(lib_name)`
      - Description: No docstring available
  - _video_opt.py
    - Class: `Fraction(numerator=0, denominator=None, *, _normalize=True)`
      - Description: This class implements rational numbers. In the two-argument form of the constructor, Fraction(8, 6) will produce a rational number equivalent to 4/3. Both arguments must be Rational. The numerator defaults to 0 and the denominator defaults to 1 so that Fraction(3) == 3 and Fraction() == 0. Fractions can also be constructed from: - numeric strings similar to those accepted by the float constructor (for example, '-2.3' or '1e10') - strings of the form '123/456' - float and Decimal instances - other Rational instances (including integers)
    - Class: `Timebase(numerator: int, denominator: int) -> None`
      - Description: No docstring available
    - Class: `VideoMetaData() -> None`
      - Description: No docstring available
    - Function: `_align_audio_frames(aframes: torch.Tensor, aframe_pts: torch.Tensor, audio_pts_range: Tuple[int, int]) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_fill_info(vtimebase: torch.Tensor, vfps: torch.Tensor, vduration: torch.Tensor, atimebase: torch.Tensor, asample_rate: torch.Tensor, aduration: torch.Tensor) -> torchvision.io._video_opt.VideoMetaData`
      - Description: Build update VideoMetaData struct with info about the video
    - Function: `_load_library(lib_name)`
      - Description: No docstring available
    - Function: `_probe_video_from_file(filename: str) -> torchvision.io._video_opt.VideoMetaData`
      - Description: Probe a video file and return VideoMetaData with info about the video
    - Function: `_probe_video_from_memory(video_data: torch.Tensor) -> torchvision.io._video_opt.VideoMetaData`
      - Description: Probe a video in memory and return VideoMetaData with info about the video This function is torchscriptable
    - Function: `_read_video(filename: str, start_pts: Union[float, fractions.Fraction] = 0, end_pts: Union[float, fractions.Fraction, NoneType] = None, pts_unit: str = 'pts') -> Tuple[torch.Tensor, torch.Tensor, Dict[str, float]]`
      - Description: No docstring available
    - Function: `_read_video_from_file(filename: str, seek_frame_margin: float = 0.25, read_video_stream: bool = True, video_width: int = 0, video_height: int = 0, video_min_dimension: int = 0, video_max_dimension: int = 0, video_pts_range: Tuple[int, int] = (0, -1), video_timebase: fractions.Fraction = Fraction(0, 1), read_audio_stream: bool = True, audio_samples: int = 0, audio_channels: int = 0, audio_pts_range: Tuple[int, int] = (0, -1), audio_timebase: fractions.Fraction = Fraction(0, 1)) -> Tuple[torch.Tensor, torch.Tensor, torchvision.io._video_opt.VideoMetaData]`
      - Description: Reads a video from a file, returning both the video frames and the audio frames Args: filename (str): path to the video file seek_frame_margin (double, optional): seeking frame in the stream is imprecise. Thus, when video_start_pts is specified, we seek the pts earlier by seek_frame_margin seconds read_video_stream (int, optional): whether read video stream. If yes, set to 1. Otherwise, 0 video_width/video_height/video_min_dimension/video_max_dimension (int): together decide the size of decoded frames: - When video_width = 0, video_height = 0, video_min_dimension = 0, and video_max_dimension = 0, keep the original frame resolution - When video_width = 0, video_height = 0, video_min_dimension != 0, and video_max_dimension = 0, keep the aspect ratio and resize the frame so that shorter edge size is video_min_dimension - When video_width = 0, video_height = 0, video_min_dimension = 0, and video_max_dimension != 0, keep the aspect ratio and resize the frame so that longer edge size is video_max_dimension - When video_width = 0, video_height = 0, video_min_dimension != 0, and video_max_dimension != 0, resize the frame so that shorter edge size is video_min_dimension, and longer edge size is video_max_dimension. The aspect ratio may not be preserved - When video_width = 0, video_height != 0, video_min_dimension = 0, and video_max_dimension = 0, keep the aspect ratio and resize the frame so that frame video_height is $video_height - When video_width != 0, video_height == 0, video_min_dimension = 0, and video_max_dimension = 0, keep the aspect ratio and resize the frame so that frame video_width is $video_width - When video_width != 0, video_height != 0, video_min_dimension = 0, and video_max_dimension = 0, resize the frame so that frame video_width and video_height are set to $video_width and $video_height, respectively video_pts_range (list(int), optional): the start and end presentation timestamp of video stream video_timebase (Fraction, optional): a Fraction rational number which denotes timebase in video stream read_audio_stream (int, optional): whether read audio stream. If yes, set to 1. Otherwise, 0 audio_samples (int, optional): audio sampling rate audio_channels (int optional): audio channels audio_pts_range (list(int), optional): the start and end presentation timestamp of audio stream audio_timebase (Fraction, optional): a Fraction rational number which denotes time base in audio stream Returns vframes (Tensor[T, H, W, C]): the `T` video frames aframes (Tensor[L, K]): the audio frames, where `L` is the number of points and `K` is the number of audio_channels info (Dict): metadata for the video and audio. Can contain the fields video_fps (float) and audio_fps (int)
    - Function: `_read_video_from_memory(video_data: torch.Tensor, seek_frame_margin: float = 0.25, read_video_stream: int = 1, video_width: int = 0, video_height: int = 0, video_min_dimension: int = 0, video_max_dimension: int = 0, video_pts_range: Tuple[int, int] = (0, -1), video_timebase_numerator: int = 0, video_timebase_denominator: int = 1, read_audio_stream: int = 1, audio_samples: int = 0, audio_channels: int = 0, audio_pts_range: Tuple[int, int] = (0, -1), audio_timebase_numerator: int = 0, audio_timebase_denominator: int = 1) -> Tuple[torch.Tensor, torch.Tensor]`
      - Description: Reads a video from memory, returning both the video frames as the audio frames This function is torchscriptable. Args: video_data (data type could be 1) torch.Tensor, dtype=torch.int8 or 2) python bytes): compressed video content stored in either 1) torch.Tensor 2) python bytes seek_frame_margin (double, optional): seeking frame in the stream is imprecise. Thus, when video_start_pts is specified, we seek the pts earlier by seek_frame_margin seconds read_video_stream (int, optional): whether read video stream. If yes, set to 1. Otherwise, 0 video_width/video_height/video_min_dimension/video_max_dimension (int): together decide the size of decoded frames: - When video_width = 0, video_height = 0, video_min_dimension = 0, and video_max_dimension = 0, keep the original frame resolution - When video_width = 0, video_height = 0, video_min_dimension != 0, and video_max_dimension = 0, keep the aspect ratio and resize the frame so that shorter edge size is video_min_dimension - When video_width = 0, video_height = 0, video_min_dimension = 0, and video_max_dimension != 0, keep the aspect ratio and resize the frame so that longer edge size is video_max_dimension - When video_width = 0, video_height = 0, video_min_dimension != 0, and video_max_dimension != 0, resize the frame so that shorter edge size is video_min_dimension, and longer edge size is video_max_dimension. The aspect ratio may not be preserved - When video_width = 0, video_height != 0, video_min_dimension = 0, and video_max_dimension = 0, keep the aspect ratio and resize the frame so that frame video_height is $video_height - When video_width != 0, video_height == 0, video_min_dimension = 0, and video_max_dimension = 0, keep the aspect ratio and resize the frame so that frame video_width is $video_width - When video_width != 0, video_height != 0, video_min_dimension = 0, and video_max_dimension = 0, resize the frame so that frame video_width and video_height are set to $video_width and $video_height, respectively video_pts_range (list(int), optional): the start and end presentation timestamp of video stream video_timebase_numerator / video_timebase_denominator (float, optional): a rational number which denotes timebase in video stream read_audio_stream (int, optional): whether read audio stream. If yes, set to 1. Otherwise, 0 audio_samples (int, optional): audio sampling rate audio_channels (int optional): audio audio_channels audio_pts_range (list(int), optional): the start and end presentation timestamp of audio stream audio_timebase_numerator / audio_timebase_denominator (float, optional): a rational number which denotes time base in audio stream Returns: vframes (Tensor[T, H, W, C]): the `T` video frames aframes (Tensor[L, K]): the audio frames, where `L` is the number of points and `K` is the number of channels
    - Function: `_read_video_timestamps(filename: str, pts_unit: str = 'pts') -> Tuple[Union[List[int], List[fractions.Fraction]], Optional[float]]`
      - Description: No docstring available
    - Function: `_read_video_timestamps_from_file(filename: str) -> Tuple[List[int], List[int], torchvision.io._video_opt.VideoMetaData]`
      - Description: Decode all video- and audio frames in the video. Only pts (presentation timestamp) is returned. The actual frame pixel data is not copied. Thus, it is much faster than read_video(...)
    - Function: `_read_video_timestamps_from_memory(video_data: torch.Tensor) -> Tuple[List[int], List[int], torchvision.io._video_opt.VideoMetaData]`
      - Description: Decode all frames in the video. Only pts (presentation timestamp) is returned. The actual frame pixel data is not copied. Thus, read_video_timestamps(...) is much faster than read_video(...)
    - Function: `_validate_pts(pts_range: Tuple[int, int]) -> None`
      - Description: No docstring available
- models/
  - detection/
    - __pycache__/
    - anchor_utils.py
      - Class: `AnchorGenerator(sizes=((128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),))`
        - Description: Module that generates anchors for a set of feature maps and image sizes. The module support computing anchors at multiple sizes and aspect ratios per feature map. This module assumes aspect ratio = height / width for each anchor. sizes and aspect_ratios should have the same number of elements, and it should correspond to the number of feature maps. sizes[i] and aspect_ratios[i] can have an arbitrary number of elements, and AnchorGenerator will output a set of sizes[i] * aspect_ratios[i] anchors per spatial location for feature map i. Args: sizes (Tuple[Tuple[int]]): aspect_ratios (Tuple[Tuple[float]]):
      - Class: `DefaultBoxGenerator(aspect_ratios: List[List[int]], min_ratio: float = 0.15, max_ratio: float = 0.9, scales: Optional[List[float]] = None, steps: Optional[List[int]] = None, clip: bool = True)`
        - Description: This module generates the default boxes of SSD for a set of feature maps and image sizes. Args: aspect_ratios (List[List[int]]): A list with all the aspect ratios used in each feature map. min_ratio (float): The minimum scale :math:` ext{s}_{ ext{min}}` of the default boxes used in the estimation of the scales of each feature map. It is used only if the ``scales`` parameter is not provided. max_ratio (float): The maximum scale :math:` ext{s}_{ ext{max}}` of the default boxes used in the estimation of the scales of each feature map. It is used only if the ``scales`` parameter is not provided. scales (List[float]], optional): The scales of the default boxes. If not provided it will be estimated using the ``min_ratio`` and ``max_ratio`` parameters. steps (List[int]], optional): It's a hyper-parameter that affects the tiling of default boxes. If not provided it will be estimated from the data. clip (bool): Whether the standardized values of default boxes should be clipped between 0 and 1. The clipping is applied while the boxes are encoded in format ``(cx, cy, w, h)``.
      - Class: `ImageList(tensors: torch.Tensor, image_sizes: List[Tuple[int, int]]) -> None`
        - Description: Structure that holds a list of images (of possibly varying sizes) as a single tensor. This works by padding the images to the same size, and storing in a field the original sizes of each image Args: tensors (tensor): Tensor containing images. image_sizes (list[tuple[int, int]]): List of Tuples each containing size of images.
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
    - backbone_utils.py
      - Class: `BackboneWithFPN(backbone: torch.nn.modules.module.Module, return_layers: Dict[str, str], in_channels_list: List[int], out_channels: int, extra_blocks: Optional[torchvision.ops.feature_pyramid_network.ExtraFPNBlock] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
        - Description: Adds a FPN on top of a model. Internally, it uses torchvision.models._utils.IntermediateLayerGetter to extract a submodel that returns the feature maps specified in return_layers. The same limitations of IntermediateLayerGetter apply here. Args: backbone (nn.Module) return_layers (Dict[name, new_name]): a dict containing the names of the modules for which the activations will be returned as the key of the dict, and the value of the dict is the name of the returned activation (which the user can specify). in_channels_list (List[int]): number of channels for each feature map that is returned, in the order they are present in the OrderedDict out_channels (int): number of channels in the FPN. norm_layer (callable, optional): Module specifying the normalization layer to use. Default: None Attributes: out_channels (int): the number of channels in the FPN
      - Class: `ExtraFPNBlock(*args, **kwargs) -> None`
        - Description: Base class for the extra block in the FPN. Args: results (List[Tensor]): the result of the FPN x (List[Tensor]): the original feature maps names (List[str]): the names for each one of the original feature maps Returns: results (List[Tensor]): the extended set of results of the FPN names (List[str]): the extended set of names for the results
      - Class: `FeaturePyramidNetwork(in_channels_list: List[int], out_channels: int, extra_blocks: Optional[torchvision.ops.feature_pyramid_network.ExtraFPNBlock] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None)`
        - Description: Module that adds a FPN from on top of a set of feature maps. This is based on `"Feature Pyramid Network for Object Detection" <https://arxiv.org/abs/1612.03144>`_. The feature maps are currently supposed to be in increasing depth order. The input to the model is expected to be an OrderedDict[Tensor], containing the feature maps on top of which the FPN will be added. Args: in_channels_list (list[int]): number of channels for each feature map that is passed to the module out_channels (int): number of channels of the FPN representation extra_blocks (ExtraFPNBlock or None): if provided, extra operations will be performed. It is expected to take the fpn features, the original features and the names of the original features as input, and returns a new list of feature maps and their corresponding names norm_layer (callable, optional): Module specifying the normalization layer to use. Default: None Examples:: >>> m = torchvision.ops.FeaturePyramidNetwork([10, 20, 30], 5) >>> # get some dummy data >>> x = OrderedDict() >>> x['feat0'] = torch.rand(1, 10, 64, 64) >>> x['feat2'] = torch.rand(1, 20, 16, 16) >>> x['feat3'] = torch.rand(1, 30, 8, 8) >>> # compute the FPN on top of x >>> output = m(x) >>> print([(k, v.shape) for k, v in output.items()]) >>> # returns >>> [('feat0', torch.Size([1, 5, 64, 64])), >>> ('feat2', torch.Size([1, 5, 16, 16])), >>> ('feat3', torch.Size([1, 5, 8, 8]))]
      - Class: `IntermediateLayerGetter(model: torch.nn.modules.module.Module, return_layers: Dict[str, str]) -> None`
        - Description: Module wrapper that returns intermediate layers from a model It has a strong assumption that the modules have been registered into the model in the same order as they are used. This means that one should **not** reuse the same nn.Module twice in the forward if you want this to work. Additionally, it is only able to query submodules that are directly assigned to the model. So if `model` is passed, `model.feature1` can be returned, but not `model.feature1.layer2`. Args: model (nn.Module): model on which we will extract the features return_layers (Dict[name, new_name]): a dict containing the names of the modules for which the activations will be returned as the key of the dict, and the value of the dict is the name of the returned activation (which the user can specify). Examples:: >>> m = torchvision.models.resnet18(weights=ResNet18_Weights.DEFAULT) >>> # extract layer1 and layer3, giving as names `feat1` and feat2` >>> new_m = torchvision.models._utils.IntermediateLayerGetter(m, >>> {'layer1': 'feat1', 'layer3': 'feat2'}) >>> out = new_m(torch.rand(1, 3, 224, 224)) >>> print([(k, v.shape) for k, v in out.items()]) >>> [('feat1', torch.Size([1, 64, 56, 56])), >>> ('feat2', torch.Size([1, 256, 14, 14]))]
      - Class: `LastLevelMaxPool(*args, **kwargs) -> None`
        - Description: Applies a max_pool2d (not actual max_pool2d, we just subsample) on top of the last feature map
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Function: `_get_enum_from_fn(fn: Callable) -> Type[torchvision.models._api.WeightsEnum]`
        - Description: Internal method that gets the weight enum of a specific model builder method. Args: fn (Callable): The builder method used to create the model. Returns: WeightsEnum: The requested weight enum.
      - Function: `_mobilenet_extractor(backbone: Union[torchvision.models.mobilenetv2.MobileNetV2, torchvision.models.mobilenetv3.MobileNetV3], fpn: bool, trainable_layers: int, returned_layers: Optional[List[int]] = None, extra_blocks: Optional[torchvision.ops.feature_pyramid_network.ExtraFPNBlock] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> torch.nn.modules.module.Module`
        - Description: No docstring available
      - Function: `_resnet_fpn_extractor(backbone: torchvision.models.resnet.ResNet, trainable_layers: int, returned_layers: Optional[List[int]] = None, extra_blocks: Optional[torchvision.ops.feature_pyramid_network.ExtraFPNBlock] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> torchvision.models.detection.backbone_utils.BackboneWithFPN`
        - Description: No docstring available
      - Function: `_validate_trainable_layers(is_trained: bool, trainable_backbone_layers: Optional[int], max_value: int, default_value: int) -> int`
        - Description: No docstring available
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Function: `mobilenet_backbone(*, backbone_name: str, weights: Optional[torchvision.models._api.WeightsEnum], fpn: bool, norm_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torchvision.ops.misc.FrozenBatchNorm2d'>, trainable_layers: int = 2, returned_layers: Optional[List[int]] = None, extra_blocks: Optional[torchvision.ops.feature_pyramid_network.ExtraFPNBlock] = None) -> torch.nn.modules.module.Module`
        - Description: No docstring available
      - Function: `resnet_fpn_backbone(*, backbone_name: str, weights: Optional[torchvision.models._api.WeightsEnum], norm_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torchvision.ops.misc.FrozenBatchNorm2d'>, trainable_layers: int = 3, returned_layers: Optional[List[int]] = None, extra_blocks: Optional[torchvision.ops.feature_pyramid_network.ExtraFPNBlock] = None) -> torchvision.models.detection.backbone_utils.BackboneWithFPN`
        - Description: Constructs a specified ResNet backbone with FPN on top. Freezes the specified number of layers in the backbone. Examples:: >>> from torchvision.models.detection.backbone_utils import resnet_fpn_backbone >>> backbone = resnet_fpn_backbone('resnet50', weights=ResNet50_Weights.DEFAULT, trainable_layers=3) >>> # get some dummy image >>> x = torch.rand(1,3,64,64) >>> # compute the output >>> output = backbone(x) >>> print([(k, v.shape) for k, v in output.items()]) >>> # returns >>> [('0', torch.Size([1, 256, 16, 16])), >>> ('1', torch.Size([1, 256, 8, 8])), >>> ('2', torch.Size([1, 256, 4, 4])), >>> ('3', torch.Size([1, 256, 2, 2])), >>> ('pool', torch.Size([1, 256, 1, 1]))] Args: backbone_name (string): resnet architecture. Possible values are 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnext50_32x4d', 'resnext101_32x8d', 'wide_resnet50_2', 'wide_resnet101_2' weights (WeightsEnum, optional): The pretrained weights for the model norm_layer (callable): it is recommended to use the default value. For details visit: (https://github.com/facebookresearch/maskrcnn-benchmark/issues/267) trainable_layers (int): number of trainable (not frozen) layers starting from final block. Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable. returned_layers (list of int): The layers of the network to return. Each entry must be in ``[1, 4]``. By default, all layers are returned. extra_blocks (ExtraFPNBlock or None): if provided, extra operations will be performed. It is expected to take the fpn features, the original features and the names of the original features as input, and returns a new list of feature maps and their corresponding names. By default, a ``LastLevelMaxPool`` is used.
    - faster_rcnn.py
      - Class: `AnchorGenerator(sizes=((128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),))`
        - Description: Module that generates anchors for a set of feature maps and image sizes. The module support computing anchors at multiple sizes and aspect ratios per feature map. This module assumes aspect ratio = height / width for each anchor. sizes and aspect_ratios should have the same number of elements, and it should correspond to the number of feature maps. sizes[i] and aspect_ratios[i] can have an arbitrary number of elements, and AnchorGenerator will output a set of sizes[i] * aspect_ratios[i] anchors per spatial location for feature map i. Args: sizes (Tuple[Tuple[int]]): aspect_ratios (Tuple[Tuple[float]]):
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `FastRCNNConvFCHead(input_size: Tuple[int, int, int], conv_layers: List[int], fc_layers: List[int], norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None)`
        - Description: A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ``OrderedDict`` of modules can be passed in. The ``forward()`` method of ``Sequential`` accepts any input and forwards it to the first module it contains. It then "chains" outputs to inputs sequentially for each subsequent module, finally returning the output of the last module. The value a ``Sequential`` provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the ``Sequential`` applies to each of the modules it stores (which are each a registered submodule of the ``Sequential``). What's the difference between a ``Sequential`` and a :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it sounds like--a list for storing ``Module`` s! On the other hand, the layers in a ``Sequential`` are connected in a cascading way. Example:: # Using Sequential to create a small model. When `model` is run, # input will first be passed to `Conv2d(1,20,5)`. The output of # `Conv2d(1,20,5)` will be used as the input to the first # `ReLU`; the output of the first `ReLU` will become the input # for `Conv2d(20,64,5)`. Finally, the output of # `Conv2d(20,64,5)` will be used as input to the second `ReLU` model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # Using Sequential with OrderedDict. This is functionally the # same as the above code model = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ]))
      - Class: `FastRCNNPredictor(in_channels, num_classes)`
        - Description: Standard classification + bounding box regression layers for Fast R-CNN. Args: in_channels (int): number of input channels num_classes (int): number of output classes (including background)
      - Class: `FasterRCNN(backbone, num_classes=None, min_size=800, max_size=1333, image_mean=None, image_std=None, rpn_anchor_generator=None, rpn_head=None, rpn_pre_nms_top_n_train=2000, rpn_pre_nms_top_n_test=1000, rpn_post_nms_top_n_train=2000, rpn_post_nms_top_n_test=1000, rpn_nms_thresh=0.7, rpn_fg_iou_thresh=0.7, rpn_bg_iou_thresh=0.3, rpn_batch_size_per_image=256, rpn_positive_fraction=0.5, rpn_score_thresh=0.0, box_roi_pool=None, box_head=None, box_predictor=None, box_score_thresh=0.05, box_nms_thresh=0.5, box_detections_per_img=100, box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5, box_batch_size_per_image=512, box_positive_fraction=0.25, bbox_reg_weights=None, **kwargs)`
        - Description: Implements Faster R-CNN. The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each image, and should be in 0-1 range. Different images can have different sizes. The behavior of the model changes depending on if it is in training or evaluation mode. During training, the model expects both the input tensors and targets (list of dictionary), containing: - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (Int64Tensor[N]): the class label for each ground-truth box The model returns a Dict[Tensor] during training, containing the classification and regression losses for both the RPN and the R-CNN. During inference, the model requires only the input tensors, and returns the post-processed predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as follows: - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (Int64Tensor[N]): the predicted labels for each image - scores (Tensor[N]): the scores or each prediction Args: backbone (nn.Module): the network used to compute the features for the model. It should contain an out_channels attribute, which indicates the number of output channels that each feature map has (and it should be the same for all feature maps). The backbone should return a single Tensor or and OrderedDict[Tensor]. num_classes (int): number of output classes of the model (including the background). If box_predictor is specified, num_classes should be None. min_size (int): minimum size of the image to be rescaled before feeding it to the backbone max_size (int): maximum size of the image to be rescaled before feeding it to the backbone image_mean (Tuple[float, float, float]): mean values used for input normalization. They are generally the mean values of the dataset on which the backbone has been trained on image_std (Tuple[float, float, float]): std values used for input normalization. They are generally the std values of the dataset on which the backbone has been trained on rpn_anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature maps. rpn_head (nn.Module): module that computes the objectness and regression deltas from the RPN rpn_pre_nms_top_n_train (int): number of proposals to keep before applying NMS during training rpn_pre_nms_top_n_test (int): number of proposals to keep before applying NMS during testing rpn_post_nms_top_n_train (int): number of proposals to keep after applying NMS during training rpn_post_nms_top_n_test (int): number of proposals to keep after applying NMS during testing rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be considered as positive during training of the RPN. rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be considered as negative during training of the RPN. rpn_batch_size_per_image (int): number of anchors that are sampled during training of the RPN for computing the loss rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training of the RPN rpn_score_thresh (float): during inference, only return proposals with a classification score greater than rpn_score_thresh box_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in the locations indicated by the bounding boxes box_head (nn.Module): module that takes the cropped feature maps as input box_predictor (nn.Module): module that takes the output of box_head and returns the classification logits and box regression deltas. box_score_thresh (float): during inference, only return proposals with a classification score greater than box_score_thresh box_nms_thresh (float): NMS threshold for the prediction head. Used during inference box_detections_per_img (int): maximum number of detections per image, for all classes. box_fg_iou_thresh (float): minimum IoU between the proposals and the GT box so that they can be considered as positive during training of the classification head box_bg_iou_thresh (float): maximum IoU between the proposals and the GT box so that they can be considered as negative during training of the classification head box_batch_size_per_image (int): number of proposals that are sampled during training of the classification head box_positive_fraction (float): proportion of positive proposals in a mini-batch during training of the classification head bbox_reg_weights (Tuple[float, float, float, float]): weights for the encoding/decoding of the bounding boxes Example:: >>> import torch >>> import torchvision >>> from torchvision.models.detection import FasterRCNN >>> from torchvision.models.detection.rpn import AnchorGenerator >>> # load a pre-trained model for classification and return >>> # only the features >>> backbone = torchvision.models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).features >>> # FasterRCNN needs to know the number of >>> # output channels in a backbone. For mobilenet_v2, it's 1280, >>> # so we need to add it here >>> backbone.out_channels = 1280 >>> >>> # let's make the RPN generate 5 x 3 anchors per spatial >>> # location, with 5 different sizes and 3 different aspect >>> # ratios. We have a Tuple[Tuple[int]] because each feature >>> # map could potentially have different sizes and >>> # aspect ratios >>> anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), >>> aspect_ratios=((0.5, 1.0, 2.0),)) >>> >>> # let's define what are the feature maps that we will >>> # use to perform the region of interest cropping, as well as >>> # the size of the crop after rescaling. >>> # if your backbone returns a Tensor, featmap_names is expected to >>> # be ['0']. More generally, the backbone should return an >>> # OrderedDict[Tensor], and in featmap_names you can choose which >>> # feature maps to use. >>> roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], >>> output_size=7, >>> sampling_ratio=2) >>> >>> # put the pieces together inside a FasterRCNN model >>> model = FasterRCNN(backbone, >>> num_classes=2, >>> rpn_anchor_generator=anchor_generator, >>> box_roi_pool=roi_pooler) >>> model.eval() >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] >>> predictions = model(x)
      - Class: `FasterRCNN_MobileNet_V3_Large_320_FPN_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `FasterRCNN_MobileNet_V3_Large_FPN_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `FasterRCNN_ResNet50_FPN_V2_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `FasterRCNN_ResNet50_FPN_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `GeneralizedRCNN(backbone: torch.nn.modules.module.Module, rpn: torch.nn.modules.module.Module, roi_heads: torch.nn.modules.module.Module, transform: torch.nn.modules.module.Module) -> None`
        - Description: Main class for Generalized R-CNN. Args: backbone (nn.Module): rpn (nn.Module): roi_heads (nn.Module): takes the features + the proposals from the RPN and computes detections / masks from it. transform (nn.Module): performs the data transformation from the inputs to feed into the model
      - Class: `GeneralizedRCNNTransform(min_size: int, max_size: int, image_mean: List[float], image_std: List[float], size_divisible: int = 32, fixed_size: Optional[Tuple[int, int]] = None, **kwargs: Any)`
        - Description: Performs input / target transformation before feeding the data to a GeneralizedRCNN model. The transformations it performs are: - input normalization (mean subtraction and std division) - input / target resizing to match min_size / max_size It returns a ImageList for the inputs, and a List[Dict[Tensor]] for the targets
      - Class: `MobileNet_V3_Large_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `MultiScaleRoIAlign(featmap_names: List[str], output_size: Union[int, Tuple[int], List[int]], sampling_ratio: int, *, canonical_scale: int = 224, canonical_level: int = 4)`
        - Description: Multi-scale RoIAlign pooling, which is useful for detection with or without FPN. It infers the scale of the pooling via the heuristics specified in eq. 1 of the `Feature Pyramid Network paper <https://arxiv.org/abs/1612.03144>`_. They keyword-only parameters ``canonical_scale`` and ``canonical_level`` correspond respectively to ``224`` and ``k0=4`` in eq. 1, and have the following meaning: ``canonical_level`` is the target level of the pyramid from which to pool a region of interest with ``w x h = canonical_scale x canonical_scale``. Args: featmap_names (List[str]): the names of the feature maps that will be used for the pooling. output_size (List[Tuple[int, int]] or List[int]): output size for the pooled region sampling_ratio (int): sampling ratio for ROIAlign canonical_scale (int, optional): canonical_scale for LevelMapper canonical_level (int, optional): canonical_level for LevelMapper Examples:: >>> m = torchvision.ops.MultiScaleRoIAlign(['feat1', 'feat3'], 3, 2) >>> i = OrderedDict() >>> i['feat1'] = torch.rand(1, 5, 64, 64) >>> i['feat2'] = torch.rand(1, 5, 32, 32) # this feature won't be used in the pooling >>> i['feat3'] = torch.rand(1, 5, 16, 16) >>> # create some random bounding boxes >>> boxes = torch.rand(6, 4) * 256; boxes[:, 2:] += boxes[:, :2] >>> # original image size, before computing the feature maps >>> image_sizes = [(512, 512)] >>> output = m(i, [boxes], image_sizes) >>> print(output.shape) >>> torch.Size([6, 5, 3, 3])
      - Class: `ObjectDetection(*args, **kwargs) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `RPNHead(in_channels: int, num_anchors: int, conv_depth=1) -> None`
        - Description: Adds a simple RPN Head with classification and regression heads Args: in_channels (int): number of channels of the input feature num_anchors (int): number of anchors to be predicted conv_depth (int, optional): number of convolutions
      - Class: `RegionProposalNetwork(anchor_generator: torchvision.models.detection.anchor_utils.AnchorGenerator, head: torch.nn.modules.module.Module, fg_iou_thresh: float, bg_iou_thresh: float, batch_size_per_image: int, positive_fraction: float, pre_nms_top_n: Dict[str, int], post_nms_top_n: Dict[str, int], nms_thresh: float, score_thresh: float = 0.0) -> None`
        - Description: Implements Region Proposal Network (RPN). Args: anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature maps. head (nn.Module): module that computes the objectness and regression deltas fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be considered as positive during training of the RPN. bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be considered as negative during training of the RPN. batch_size_per_image (int): number of anchors that are sampled during training of the RPN for computing the loss positive_fraction (float): proportion of positive anchors in a mini-batch during training of the RPN pre_nms_top_n (Dict[str, int]): number of proposals to keep before applying NMS. It should contain two fields: training and testing, to allow for different values depending on training or evaluation post_nms_top_n (Dict[str, int]): number of proposals to keep after applying NMS. It should contain two fields: training and testing, to allow for different values depending on training or evaluation nms_thresh (float): NMS threshold used for postprocessing the RPN proposals
      - Class: `ResNet50_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `RoIHeads(box_roi_pool, box_head, box_predictor, fg_iou_thresh, bg_iou_thresh, batch_size_per_image, positive_fraction, bbox_reg_weights, score_thresh, nms_thresh, detections_per_img, mask_roi_pool=None, mask_head=None, mask_predictor=None, keypoint_roi_pool=None, keypoint_head=None, keypoint_predictor=None)`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `TwoMLPHead(in_channels, representation_size)`
        - Description: Standard heads for FPN-based models Args: in_channels (int): number of input channels representation_size (int): size of the intermediate representation
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Function: `_default_anchorgen()`
        - Description: No docstring available
      - Function: `_fasterrcnn_mobilenet_v3_large_fpn(*, weights: Union[torchvision.models.detection.faster_rcnn.FasterRCNN_MobileNet_V3_Large_FPN_Weights, torchvision.models.detection.faster_rcnn.FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, NoneType], progress: bool, num_classes: Optional[int], weights_backbone: Optional[torchvision.models.mobilenetv3.MobileNet_V3_Large_Weights], trainable_backbone_layers: Optional[int], **kwargs: Any) -> torchvision.models.detection.faster_rcnn.FasterRCNN`
        - Description: No docstring available
      - Function: `_mobilenet_extractor(backbone: Union[torchvision.models.mobilenetv2.MobileNetV2, torchvision.models.mobilenetv3.MobileNetV3], fpn: bool, trainable_layers: int, returned_layers: Optional[List[int]] = None, extra_blocks: Optional[torchvision.ops.feature_pyramid_network.ExtraFPNBlock] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> torch.nn.modules.module.Module`
        - Description: No docstring available
      - Function: `_ovewrite_value_param(param: str, actual: Optional[~V], expected: ~V) -> ~V`
        - Description: No docstring available
      - Function: `_resnet_fpn_extractor(backbone: torchvision.models.resnet.ResNet, trainable_layers: int, returned_layers: Optional[List[int]] = None, extra_blocks: Optional[torchvision.ops.feature_pyramid_network.ExtraFPNBlock] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> torchvision.models.detection.backbone_utils.BackboneWithFPN`
        - Description: No docstring available
      - Function: `_validate_trainable_layers(is_trained: bool, trainable_backbone_layers: Optional[int], max_value: int, default_value: int) -> int`
        - Description: No docstring available
      - Function: `fasterrcnn_mobilenet_v3_large_320_fpn(*, weights: Optional[torchvision.models.detection.faster_rcnn.FasterRCNN_MobileNet_V3_Large_320_FPN_Weights] = None, progress: bool = True, num_classes: Optional[int] = None, weights_backbone: Optional[torchvision.models.mobilenetv3.MobileNet_V3_Large_Weights] = MobileNet_V3_Large_Weights.IMAGENET1K_V1, trainable_backbone_layers: Optional[int] = None, **kwargs: Any) -> torchvision.models.detection.faster_rcnn.FasterRCNN`
        - Description: Low resolution Faster R-CNN model with a MobileNetV3-Large backbone tuned for mobile use cases. .. betastatus:: detection module It works similarly to Faster R-CNN with ResNet-50 FPN backbone. See :func:`~torchvision.models.detection.fasterrcnn_resnet50_fpn` for more details. Example:: >>> model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(weights=FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.DEFAULT) >>> model.eval() >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] >>> predictions = model(x) Args: weights (:class:`~torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_320_FPN_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_320_FPN_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. num_classes (int, optional): number of output classes of the model (including the background) weights_backbone (:class:`~torchvision.models.MobileNet_V3_Large_Weights`, optional): The pretrained weights for the backbone. trainable_backbone_layers (int, optional): number of trainable (not frozen) layers starting from final block. Valid values are between 0 and 6, with 6 meaning all backbone layers are trainable. If ``None`` is passed (the default) this value is set to 3. **kwargs: parameters passed to the ``torchvision.models.detection.faster_rcnn.FasterRCNN`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py>`_ for more details about this class. .. autoclass:: torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_320_FPN_Weights :members:
      - Function: `fasterrcnn_mobilenet_v3_large_fpn(*, weights: Optional[torchvision.models.detection.faster_rcnn.FasterRCNN_MobileNet_V3_Large_FPN_Weights] = None, progress: bool = True, num_classes: Optional[int] = None, weights_backbone: Optional[torchvision.models.mobilenetv3.MobileNet_V3_Large_Weights] = MobileNet_V3_Large_Weights.IMAGENET1K_V1, trainable_backbone_layers: Optional[int] = None, **kwargs: Any) -> torchvision.models.detection.faster_rcnn.FasterRCNN`
        - Description: Constructs a high resolution Faster R-CNN model with a MobileNetV3-Large FPN backbone. .. betastatus:: detection module It works similarly to Faster R-CNN with ResNet-50 FPN backbone. See :func:`~torchvision.models.detection.fasterrcnn_resnet50_fpn` for more details. Example:: >>> model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT) >>> model.eval() >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] >>> predictions = model(x) Args: weights (:class:`~torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. num_classes (int, optional): number of output classes of the model (including the background) weights_backbone (:class:`~torchvision.models.MobileNet_V3_Large_Weights`, optional): The pretrained weights for the backbone. trainable_backbone_layers (int, optional): number of trainable (not frozen) layers starting from final block. Valid values are between 0 and 6, with 6 meaning all backbone layers are trainable. If ``None`` is passed (the default) this value is set to 3. **kwargs: parameters passed to the ``torchvision.models.detection.faster_rcnn.FasterRCNN`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py>`_ for more details about this class. .. autoclass:: torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights :members:
      - Function: `fasterrcnn_resnet50_fpn(*, weights: Optional[torchvision.models.detection.faster_rcnn.FasterRCNN_ResNet50_FPN_Weights] = None, progress: bool = True, num_classes: Optional[int] = None, weights_backbone: Optional[torchvision.models.resnet.ResNet50_Weights] = ResNet50_Weights.IMAGENET1K_V1, trainable_backbone_layers: Optional[int] = None, **kwargs: Any) -> torchvision.models.detection.faster_rcnn.FasterRCNN`
        - Description: Faster R-CNN model with a ResNet-50-FPN backbone from the `Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks <https://arxiv.org/abs/1506.01497>`__ paper. .. betastatus:: detection module The input to the model is expected to be a list of tensors, each of shape ``[C, H, W]``, one for each image, and should be in ``0-1`` range. Different images can have different sizes. The behavior of the model changes depending on if it is in training or evaluation mode. During training, the model expects both the input tensors and a targets (list of dictionary), containing: - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (``Int64Tensor[N]``): the class label for each ground-truth box The model returns a ``Dict[Tensor]`` during training, containing the classification and regression losses for both the RPN and the R-CNN. During inference, the model requires only the input tensors, and returns the post-processed predictions as a ``List[Dict[Tensor]]``, one for each input image. The fields of the ``Dict`` are as follows, where ``N`` is the number of detections: - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (``Int64Tensor[N]``): the predicted labels for each detection - scores (``Tensor[N]``): the scores of each detection For more details on the output, you may refer to :ref:`instance_seg_output`. Faster R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size. Example:: >>> model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT) >>> # For training >>> images, boxes = torch.rand(4, 3, 600, 1200), torch.rand(4, 11, 4) >>> boxes[:, :, 2:4] = boxes[:, :, 0:2] + boxes[:, :, 2:4] >>> labels = torch.randint(1, 91, (4, 11)) >>> images = list(image for image in images) >>> targets = [] >>> for i in range(len(images)): >>> d = {} >>> d['boxes'] = boxes[i] >>> d['labels'] = labels[i] >>> targets.append(d) >>> output = model(images, targets) >>> # For inference >>> model.eval() >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] >>> predictions = model(x) >>> >>> # optionally, if you want to export the model to ONNX: >>> torch.onnx.export(model, x, "faster_rcnn.onnx", opset_version = 11) Args: weights (:class:`~torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. num_classes (int, optional): number of output classes of the model (including the background) weights_backbone (:class:`~torchvision.models.ResNet50_Weights`, optional): The pretrained weights for the backbone. trainable_backbone_layers (int, optional): number of trainable (not frozen) layers starting from final block. Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable. If ``None`` is passed (the default) this value is set to 3. **kwargs: parameters passed to the ``torchvision.models.detection.faster_rcnn.FasterRCNN`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py>`_ for more details about this class. .. autoclass:: torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights :members:
      - Function: `fasterrcnn_resnet50_fpn_v2(*, weights: Optional[torchvision.models.detection.faster_rcnn.FasterRCNN_ResNet50_FPN_V2_Weights] = None, progress: bool = True, num_classes: Optional[int] = None, weights_backbone: Optional[torchvision.models.resnet.ResNet50_Weights] = None, trainable_backbone_layers: Optional[int] = None, **kwargs: Any) -> torchvision.models.detection.faster_rcnn.FasterRCNN`
        - Description: Constructs an improved Faster R-CNN model with a ResNet-50-FPN backbone from `Benchmarking Detection Transfer Learning with Vision Transformers <https://arxiv.org/abs/2111.11429>`__ paper. .. betastatus:: detection module It works similarly to Faster R-CNN with ResNet-50 FPN backbone. See :func:`~torchvision.models.detection.fasterrcnn_resnet50_fpn` for more details. Args: weights (:class:`~torchvision.models.detection.FasterRCNN_ResNet50_FPN_V2_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.detection.FasterRCNN_ResNet50_FPN_V2_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. num_classes (int, optional): number of output classes of the model (including the background) weights_backbone (:class:`~torchvision.models.ResNet50_Weights`, optional): The pretrained weights for the backbone. trainable_backbone_layers (int, optional): number of trainable (not frozen) layers starting from final block. Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable. If ``None`` is passed (the default) this value is set to 3. **kwargs: parameters passed to the ``torchvision.models.detection.faster_rcnn.FasterRCNN`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py>`_ for more details about this class. .. autoclass:: torchvision.models.detection.FasterRCNN_ResNet50_FPN_V2_Weights :members:
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Function: `mobilenet_v3_large(*, weights: Optional[torchvision.models.mobilenetv3.MobileNet_V3_Large_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.mobilenetv3.MobileNetV3`
        - Description: Constructs a large MobileNetV3 architecture from `Searching for MobileNetV3 <https://arxiv.org/abs/1905.02244>`__. Args: weights (:class:`~torchvision.models.MobileNet_V3_Large_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.MobileNet_V3_Large_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.mobilenet.MobileNetV3`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv3.py>`_ for more details about this class. .. autoclass:: torchvision.models.MobileNet_V3_Large_Weights :members:
      - Function: `overwrite_eps(model: torch.nn.modules.module.Module, eps: float) -> None`
        - Description: This method overwrites the default eps values of all the FrozenBatchNorm2d layers of the model with the provided value. This is necessary to address the BC-breaking change introduced by the bug-fix at pytorch/vision#2933. The overwrite is applied only when the pretrained weights are loaded to maintain compatibility with previous versions. Args: model (nn.Module): The model on which we perform the overwrite. eps (float): The new value of eps.
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
      - Function: `resnet50(*, weights: Optional[torchvision.models.resnet.ResNet50_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.resnet.ResNet`
        - Description: ResNet-50 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__. .. note:: The bottleneck of TorchVision places the stride for downsampling to the second 3x3 convolution while the original paper places it to the first 1x1 convolution. This variant improves the accuracy and is known as `ResNet V1.5 <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_. Args: weights (:class:`~torchvision.models.ResNet50_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ResNet50_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.ResNet50_Weights :members:
    - fcos.py
      - Class: `AnchorGenerator(sizes=((128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),))`
        - Description: Module that generates anchors for a set of feature maps and image sizes. The module support computing anchors at multiple sizes and aspect ratios per feature map. This module assumes aspect ratio = height / width for each anchor. sizes and aspect_ratios should have the same number of elements, and it should correspond to the number of feature maps. sizes[i] and aspect_ratios[i] can have an arbitrary number of elements, and AnchorGenerator will output a set of sizes[i] * aspect_ratios[i] anchors per spatial location for feature map i. Args: sizes (Tuple[Tuple[int]]): aspect_ratios (Tuple[Tuple[float]]):
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `FCOS(backbone: torch.nn.modules.module.Module, num_classes: int, min_size: int = 800, max_size: int = 1333, image_mean: Optional[List[float]] = None, image_std: Optional[List[float]] = None, anchor_generator: Optional[torchvision.models.detection.anchor_utils.AnchorGenerator] = None, head: Optional[torch.nn.modules.module.Module] = None, center_sampling_radius: float = 1.5, score_thresh: float = 0.2, nms_thresh: float = 0.6, detections_per_img: int = 100, topk_candidates: int = 1000, **kwargs)`
        - Description: Implements FCOS. The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each image, and should be in 0-1 range. Different images can have different sizes. The behavior of the model changes depending on if it is in training or evaluation mode. During training, the model expects both the input tensors and targets (list of dictionary), containing: - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (Int64Tensor[N]): the class label for each ground-truth box The model returns a Dict[Tensor] during training, containing the classification, regression and centerness losses. During inference, the model requires only the input tensors, and returns the post-processed predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as follows: - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (Int64Tensor[N]): the predicted labels for each image - scores (Tensor[N]): the scores for each prediction Args: backbone (nn.Module): the network used to compute the features for the model. It should contain an out_channels attribute, which indicates the number of output channels that each feature map has (and it should be the same for all feature maps). The backbone should return a single Tensor or an OrderedDict[Tensor]. num_classes (int): number of output classes of the model (including the background). min_size (int): minimum size of the image to be rescaled before feeding it to the backbone max_size (int): maximum size of the image to be rescaled before feeding it to the backbone image_mean (Tuple[float, float, float]): mean values used for input normalization. They are generally the mean values of the dataset on which the backbone has been trained on image_std (Tuple[float, float, float]): std values used for input normalization. They are generally the std values of the dataset on which the backbone has been trained on anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature maps. For FCOS, only set one anchor for per position of each level, the width and height equal to the stride of feature map, and set aspect ratio = 1.0, so the center of anchor is equivalent to the point in FCOS paper. head (nn.Module): Module run on top of the feature pyramid. Defaults to a module containing a classification and regression module. center_sampling_radius (int): radius of the "center" of a groundtruth box, within which all anchor points are labeled positive. score_thresh (float): Score threshold used for postprocessing the detections. nms_thresh (float): NMS threshold used for postprocessing the detections. detections_per_img (int): Number of best detections to keep after NMS. topk_candidates (int): Number of best detections to keep before NMS. Example: >>> import torch >>> import torchvision >>> from torchvision.models.detection import FCOS >>> from torchvision.models.detection.anchor_utils import AnchorGenerator >>> # load a pre-trained model for classification and return >>> # only the features >>> backbone = torchvision.models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).features >>> # FCOS needs to know the number of >>> # output channels in a backbone. For mobilenet_v2, it's 1280, >>> # so we need to add it here >>> backbone.out_channels = 1280 >>> >>> # let's make the network generate 5 x 3 anchors per spatial >>> # location, with 5 different sizes and 3 different aspect >>> # ratios. We have a Tuple[Tuple[int]] because each feature >>> # map could potentially have different sizes and >>> # aspect ratios >>> anchor_generator = AnchorGenerator( >>> sizes=((8,), (16,), (32,), (64,), (128,)), >>> aspect_ratios=((1.0,),) >>> ) >>> >>> # put the pieces together inside a FCOS model >>> model = FCOS( >>> backbone, >>> num_classes=80, >>> anchor_generator=anchor_generator, >>> ) >>> model.eval() >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] >>> predictions = model(x)
      - Class: `FCOSClassificationHead(in_channels: int, num_anchors: int, num_classes: int, num_convs: int = 4, prior_probability: float = 0.01, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
        - Description: A classification head for use in FCOS. Args: in_channels (int): number of channels of the input feature. num_anchors (int): number of anchors to be predicted. num_classes (int): number of classes to be predicted. num_convs (Optional[int]): number of conv layer. Default: 4. prior_probability (Optional[float]): probability of prior. Default: 0.01. norm_layer: Module specifying the normalization layer to use.
      - Class: `FCOSHead(in_channels: int, num_anchors: int, num_classes: int, num_convs: Optional[int] = 4) -> None`
        - Description: A regression and classification head for use in FCOS. Args: in_channels (int): number of channels of the input feature num_anchors (int): number of anchors to be predicted num_classes (int): number of classes to be predicted num_convs (Optional[int]): number of conv layer of head. Default: 4.
      - Class: `FCOSRegressionHead(in_channels: int, num_anchors: int, num_convs: int = 4, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None)`
        - Description: A regression head for use in FCOS, which combines regression branch and center-ness branch. This can obtain better performance. Reference: `FCOS: A simple and strong anchor-free object detector <https://arxiv.org/abs/2006.09214>`_. Args: in_channels (int): number of channels of the input feature num_anchors (int): number of anchors to be predicted num_convs (Optional[int]): number of conv layer. Default: 4. norm_layer: Module specifying the normalization layer to use.
      - Class: `FCOS_ResNet50_FPN_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `GeneralizedRCNNTransform(min_size: int, max_size: int, image_mean: List[float], image_std: List[float], size_divisible: int = 32, fixed_size: Optional[Tuple[int, int]] = None, **kwargs: Any)`
        - Description: Performs input / target transformation before feeding the data to a GeneralizedRCNN model. The transformations it performs are: - input normalization (mean subtraction and std division) - input / target resizing to match min_size / max_size It returns a ImageList for the inputs, and a List[Dict[Tensor]] for the targets
      - Class: `LastLevelP6P7(in_channels: int, out_channels: int)`
        - Description: This module is used in RetinaNet to generate extra layers, P6 and P7.
      - Class: `ObjectDetection(*args, **kwargs) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `OrderedDict(Unable to retrieve signature)`
        - Description: Dictionary that remembers insertion order
      - Class: `ResNet50_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Function: `_log_api_usage_once(obj: Any) -> None`
        - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
      - Function: `_ovewrite_value_param(param: str, actual: Optional[~V], expected: ~V) -> ~V`
        - Description: No docstring available
      - Function: `_resnet_fpn_extractor(backbone: torchvision.models.resnet.ResNet, trainable_layers: int, returned_layers: Optional[List[int]] = None, extra_blocks: Optional[torchvision.ops.feature_pyramid_network.ExtraFPNBlock] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> torchvision.models.detection.backbone_utils.BackboneWithFPN`
        - Description: No docstring available
      - Function: `_validate_trainable_layers(is_trained: bool, trainable_backbone_layers: Optional[int], max_value: int, default_value: int) -> int`
        - Description: No docstring available
      - Function: `fcos_resnet50_fpn(*, weights: Optional[torchvision.models.detection.fcos.FCOS_ResNet50_FPN_Weights] = None, progress: bool = True, num_classes: Optional[int] = None, weights_backbone: Optional[torchvision.models.resnet.ResNet50_Weights] = ResNet50_Weights.IMAGENET1K_V1, trainable_backbone_layers: Optional[int] = None, **kwargs: Any) -> torchvision.models.detection.fcos.FCOS`
        - Description: Constructs a FCOS model with a ResNet-50-FPN backbone. .. betastatus:: detection module Reference: `FCOS: Fully Convolutional One-Stage Object Detection <https://arxiv.org/abs/1904.01355>`_. `FCOS: A simple and strong anchor-free object detector <https://arxiv.org/abs/2006.09214>`_. The input to the model is expected to be a list of tensors, each of shape ``[C, H, W]``, one for each image, and should be in ``0-1`` range. Different images can have different sizes. The behavior of the model changes depending on if it is in training or evaluation mode. During training, the model expects both the input tensors and targets (list of dictionary), containing: - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (``Int64Tensor[N]``): the class label for each ground-truth box The model returns a ``Dict[Tensor]`` during training, containing the classification and regression losses. During inference, the model requires only the input tensors, and returns the post-processed predictions as a ``List[Dict[Tensor]]``, one for each input image. The fields of the ``Dict`` are as follows, where ``N`` is the number of detections: - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (``Int64Tensor[N]``): the predicted labels for each detection - scores (``Tensor[N]``): the scores of each detection For more details on the output, you may refer to :ref:`instance_seg_output`. Example: >>> model = torchvision.models.detection.fcos_resnet50_fpn(weights=FCOS_ResNet50_FPN_Weights.DEFAULT) >>> model.eval() >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] >>> predictions = model(x) Args: weights (:class:`~torchvision.models.detection.FCOS_ResNet50_FPN_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.detection.FCOS_ResNet50_FPN_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool): If True, displays a progress bar of the download to stderr num_classes (int, optional): number of output classes of the model (including the background) weights_backbone (:class:`~torchvision.models.ResNet50_Weights`, optional): The pretrained weights for the backbone. trainable_backbone_layers (int, optional): number of trainable (not frozen) resnet layers starting from final block. Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable. If ``None`` is passed (the default) this value is set to 3. Default: None **kwargs: parameters passed to the ``torchvision.models.detection.FCOS`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/detection/fcos.py>`_ for more details about this class. .. autoclass:: torchvision.models.detection.FCOS_ResNet50_FPN_Weights :members:
      - Function: `generalized_box_iou_loss(boxes1: torch.Tensor, boxes2: torch.Tensor, reduction: str = 'none', eps: float = 1e-07) -> torch.Tensor`
        - Description: Gradient-friendly IoU loss with an additional penalty that is non-zero when the boxes do not overlap and scales with the size of their smallest enclosing box. This loss is symmetric, so the boxes1 and boxes2 arguments are interchangeable. Both sets of boxes are expected to be in ``(x1, y1, x2, y2)`` format with ``0 <= x1 < x2`` and ``0 <= y1 < y2``, and The two boxes should have the same dimensions. Args: boxes1 (Tensor[N, 4] or Tensor[4]): first set of boxes boxes2 (Tensor[N, 4] or Tensor[4]): second set of boxes reduction (string, optional): Specifies the reduction to apply to the output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: No reduction will be applied to the output. ``'mean'``: The output will be averaged. ``'sum'``: The output will be summed. Default: ``'none'`` eps (float): small number to prevent division by zero. Default: 1e-7 Returns: Tensor: Loss tensor with the reduction option applied. Reference: Hamid Rezatofighi et al.: Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression: https://arxiv.org/abs/1902.09630
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Class: `partial(Unable to retrieve signature)`
        - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
      - Function: `resnet50(*, weights: Optional[torchvision.models.resnet.ResNet50_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.resnet.ResNet`
        - Description: ResNet-50 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__. .. note:: The bottleneck of TorchVision places the stride for downsampling to the second 3x3 convolution while the original paper places it to the first 1x1 convolution. This variant improves the accuracy and is known as `ResNet V1.5 <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_. Args: weights (:class:`~torchvision.models.ResNet50_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ResNet50_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.ResNet50_Weights :members:
      - Function: `sigmoid_focal_loss(inputs: torch.Tensor, targets: torch.Tensor, alpha: float = 0.25, gamma: float = 2, reduction: str = 'none') -> torch.Tensor`
        - Description: Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002. Args: inputs (Tensor): A float tensor of arbitrary shape. The predictions for each example. targets (Tensor): A float tensor with the same shape as inputs. Stores the binary classification label for each element in inputs (0 for the negative class and 1 for the positive class). alpha (float): Weighting factor in range (0,1) to balance positive vs negative examples or -1 for ignore. Default: ``0.25``. gamma (float): Exponent of the modulating factor (1 - p_t) to balance easy vs hard examples. Default: ``2``. reduction (string): ``'none'`` | ``'mean'`` | ``'sum'`` ``'none'``: No reduction will be applied to the output. ``'mean'``: The output will be averaged. ``'sum'``: The output will be summed. Default: ``'none'``. Returns: Loss tensor with the reduction option applied.
    - generalized_rcnn.py
      - Class: `GeneralizedRCNN(backbone: torch.nn.modules.module.Module, rpn: torch.nn.modules.module.Module, roi_heads: torch.nn.modules.module.Module, transform: torch.nn.modules.module.Module) -> None`
        - Description: Main class for Generalized R-CNN. Args: backbone (nn.Module): rpn (nn.Module): roi_heads (nn.Module): takes the features + the proposals from the RPN and computes detections / masks from it. transform (nn.Module): performs the data transformation from the inputs to feed into the model
      - Class: `OrderedDict(Unable to retrieve signature)`
        - Description: Dictionary that remembers insertion order
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Function: `_log_api_usage_once(obj: Any) -> None`
        - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - image_list.py
      - Class: `ImageList(tensors: torch.Tensor, image_sizes: List[Tuple[int, int]]) -> None`
        - Description: Structure that holds a list of images (of possibly varying sizes) as a single tensor. This works by padding the images to the same size, and storing in a field the original sizes of each image Args: tensors (tensor): Tensor containing images. image_sizes (list[tuple[int, int]]): List of Tuples each containing size of images.
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
    - keypoint_rcnn.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `FasterRCNN(backbone, num_classes=None, min_size=800, max_size=1333, image_mean=None, image_std=None, rpn_anchor_generator=None, rpn_head=None, rpn_pre_nms_top_n_train=2000, rpn_pre_nms_top_n_test=1000, rpn_post_nms_top_n_train=2000, rpn_post_nms_top_n_test=1000, rpn_nms_thresh=0.7, rpn_fg_iou_thresh=0.7, rpn_bg_iou_thresh=0.3, rpn_batch_size_per_image=256, rpn_positive_fraction=0.5, rpn_score_thresh=0.0, box_roi_pool=None, box_head=None, box_predictor=None, box_score_thresh=0.05, box_nms_thresh=0.5, box_detections_per_img=100, box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5, box_batch_size_per_image=512, box_positive_fraction=0.25, bbox_reg_weights=None, **kwargs)`
        - Description: Implements Faster R-CNN. The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each image, and should be in 0-1 range. Different images can have different sizes. The behavior of the model changes depending on if it is in training or evaluation mode. During training, the model expects both the input tensors and targets (list of dictionary), containing: - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (Int64Tensor[N]): the class label for each ground-truth box The model returns a Dict[Tensor] during training, containing the classification and regression losses for both the RPN and the R-CNN. During inference, the model requires only the input tensors, and returns the post-processed predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as follows: - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (Int64Tensor[N]): the predicted labels for each image - scores (Tensor[N]): the scores or each prediction Args: backbone (nn.Module): the network used to compute the features for the model. It should contain an out_channels attribute, which indicates the number of output channels that each feature map has (and it should be the same for all feature maps). The backbone should return a single Tensor or and OrderedDict[Tensor]. num_classes (int): number of output classes of the model (including the background). If box_predictor is specified, num_classes should be None. min_size (int): minimum size of the image to be rescaled before feeding it to the backbone max_size (int): maximum size of the image to be rescaled before feeding it to the backbone image_mean (Tuple[float, float, float]): mean values used for input normalization. They are generally the mean values of the dataset on which the backbone has been trained on image_std (Tuple[float, float, float]): std values used for input normalization. They are generally the std values of the dataset on which the backbone has been trained on rpn_anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature maps. rpn_head (nn.Module): module that computes the objectness and regression deltas from the RPN rpn_pre_nms_top_n_train (int): number of proposals to keep before applying NMS during training rpn_pre_nms_top_n_test (int): number of proposals to keep before applying NMS during testing rpn_post_nms_top_n_train (int): number of proposals to keep after applying NMS during training rpn_post_nms_top_n_test (int): number of proposals to keep after applying NMS during testing rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be considered as positive during training of the RPN. rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be considered as negative during training of the RPN. rpn_batch_size_per_image (int): number of anchors that are sampled during training of the RPN for computing the loss rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training of the RPN rpn_score_thresh (float): during inference, only return proposals with a classification score greater than rpn_score_thresh box_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in the locations indicated by the bounding boxes box_head (nn.Module): module that takes the cropped feature maps as input box_predictor (nn.Module): module that takes the output of box_head and returns the classification logits and box regression deltas. box_score_thresh (float): during inference, only return proposals with a classification score greater than box_score_thresh box_nms_thresh (float): NMS threshold for the prediction head. Used during inference box_detections_per_img (int): maximum number of detections per image, for all classes. box_fg_iou_thresh (float): minimum IoU between the proposals and the GT box so that they can be considered as positive during training of the classification head box_bg_iou_thresh (float): maximum IoU between the proposals and the GT box so that they can be considered as negative during training of the classification head box_batch_size_per_image (int): number of proposals that are sampled during training of the classification head box_positive_fraction (float): proportion of positive proposals in a mini-batch during training of the classification head bbox_reg_weights (Tuple[float, float, float, float]): weights for the encoding/decoding of the bounding boxes Example:: >>> import torch >>> import torchvision >>> from torchvision.models.detection import FasterRCNN >>> from torchvision.models.detection.rpn import AnchorGenerator >>> # load a pre-trained model for classification and return >>> # only the features >>> backbone = torchvision.models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).features >>> # FasterRCNN needs to know the number of >>> # output channels in a backbone. For mobilenet_v2, it's 1280, >>> # so we need to add it here >>> backbone.out_channels = 1280 >>> >>> # let's make the RPN generate 5 x 3 anchors per spatial >>> # location, with 5 different sizes and 3 different aspect >>> # ratios. We have a Tuple[Tuple[int]] because each feature >>> # map could potentially have different sizes and >>> # aspect ratios >>> anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), >>> aspect_ratios=((0.5, 1.0, 2.0),)) >>> >>> # let's define what are the feature maps that we will >>> # use to perform the region of interest cropping, as well as >>> # the size of the crop after rescaling. >>> # if your backbone returns a Tensor, featmap_names is expected to >>> # be ['0']. More generally, the backbone should return an >>> # OrderedDict[Tensor], and in featmap_names you can choose which >>> # feature maps to use. >>> roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], >>> output_size=7, >>> sampling_ratio=2) >>> >>> # put the pieces together inside a FasterRCNN model >>> model = FasterRCNN(backbone, >>> num_classes=2, >>> rpn_anchor_generator=anchor_generator, >>> box_roi_pool=roi_pooler) >>> model.eval() >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] >>> predictions = model(x)
      - Class: `KeypointRCNN(backbone, num_classes=None, min_size=None, max_size=1333, image_mean=None, image_std=None, rpn_anchor_generator=None, rpn_head=None, rpn_pre_nms_top_n_train=2000, rpn_pre_nms_top_n_test=1000, rpn_post_nms_top_n_train=2000, rpn_post_nms_top_n_test=1000, rpn_nms_thresh=0.7, rpn_fg_iou_thresh=0.7, rpn_bg_iou_thresh=0.3, rpn_batch_size_per_image=256, rpn_positive_fraction=0.5, rpn_score_thresh=0.0, box_roi_pool=None, box_head=None, box_predictor=None, box_score_thresh=0.05, box_nms_thresh=0.5, box_detections_per_img=100, box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5, box_batch_size_per_image=512, box_positive_fraction=0.25, bbox_reg_weights=None, keypoint_roi_pool=None, keypoint_head=None, keypoint_predictor=None, num_keypoints=None, **kwargs)`
        - Description: Implements Keypoint R-CNN. The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each image, and should be in 0-1 range. Different images can have different sizes. The behavior of the model changes depending on if it is in training or evaluation mode. During training, the model expects both the input tensors and targets (list of dictionary), containing: - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (Int64Tensor[N]): the class label for each ground-truth box - keypoints (FloatTensor[N, K, 3]): the K keypoints location for each of the N instances, in the format [x, y, visibility], where visibility=0 means that the keypoint is not visible. The model returns a Dict[Tensor] during training, containing the classification and regression losses for both the RPN and the R-CNN, and the keypoint loss. During inference, the model requires only the input tensors, and returns the post-processed predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as follows: - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (Int64Tensor[N]): the predicted labels for each image - scores (Tensor[N]): the scores or each prediction - keypoints (FloatTensor[N, K, 3]): the locations of the predicted keypoints, in [x, y, v] format. Args: backbone (nn.Module): the network used to compute the features for the model. It should contain an out_channels attribute, which indicates the number of output channels that each feature map has (and it should be the same for all feature maps). The backbone should return a single Tensor or and OrderedDict[Tensor]. num_classes (int): number of output classes of the model (including the background). If box_predictor is specified, num_classes should be None. min_size (int): minimum size of the image to be rescaled before feeding it to the backbone max_size (int): maximum size of the image to be rescaled before feeding it to the backbone image_mean (Tuple[float, float, float]): mean values used for input normalization. They are generally the mean values of the dataset on which the backbone has been trained on image_std (Tuple[float, float, float]): std values used for input normalization. They are generally the std values of the dataset on which the backbone has been trained on rpn_anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature maps. rpn_head (nn.Module): module that computes the objectness and regression deltas from the RPN rpn_pre_nms_top_n_train (int): number of proposals to keep before applying NMS during training rpn_pre_nms_top_n_test (int): number of proposals to keep before applying NMS during testing rpn_post_nms_top_n_train (int): number of proposals to keep after applying NMS during training rpn_post_nms_top_n_test (int): number of proposals to keep after applying NMS during testing rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be considered as positive during training of the RPN. rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be considered as negative during training of the RPN. rpn_batch_size_per_image (int): number of anchors that are sampled during training of the RPN for computing the loss rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training of the RPN rpn_score_thresh (float): during inference, only return proposals with a classification score greater than rpn_score_thresh box_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in the locations indicated by the bounding boxes box_head (nn.Module): module that takes the cropped feature maps as input box_predictor (nn.Module): module that takes the output of box_head and returns the classification logits and box regression deltas. box_score_thresh (float): during inference, only return proposals with a classification score greater than box_score_thresh box_nms_thresh (float): NMS threshold for the prediction head. Used during inference box_detections_per_img (int): maximum number of detections per image, for all classes. box_fg_iou_thresh (float): minimum IoU between the proposals and the GT box so that they can be considered as positive during training of the classification head box_bg_iou_thresh (float): maximum IoU between the proposals and the GT box so that they can be considered as negative during training of the classification head box_batch_size_per_image (int): number of proposals that are sampled during training of the classification head box_positive_fraction (float): proportion of positive proposals in a mini-batch during training of the classification head bbox_reg_weights (Tuple[float, float, float, float]): weights for the encoding/decoding of the bounding boxes keypoint_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in the locations indicated by the bounding boxes, which will be used for the keypoint head. keypoint_head (nn.Module): module that takes the cropped feature maps as input keypoint_predictor (nn.Module): module that takes the output of the keypoint_head and returns the heatmap logits Example:: >>> import torch >>> import torchvision >>> from torchvision.models.detection import KeypointRCNN >>> from torchvision.models.detection.anchor_utils import AnchorGenerator >>> >>> # load a pre-trained model for classification and return >>> # only the features >>> backbone = torchvision.models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).features >>> # KeypointRCNN needs to know the number of >>> # output channels in a backbone. For mobilenet_v2, it's 1280, >>> # so we need to add it here >>> backbone.out_channels = 1280 >>> >>> # let's make the RPN generate 5 x 3 anchors per spatial >>> # location, with 5 different sizes and 3 different aspect >>> # ratios. We have a Tuple[Tuple[int]] because each feature >>> # map could potentially have different sizes and >>> # aspect ratios >>> anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), >>> aspect_ratios=((0.5, 1.0, 2.0),)) >>> >>> # let's define what are the feature maps that we will >>> # use to perform the region of interest cropping, as well as >>> # the size of the crop after rescaling. >>> # if your backbone returns a Tensor, featmap_names is expected to >>> # be ['0']. More generally, the backbone should return an >>> # OrderedDict[Tensor], and in featmap_names you can choose which >>> # feature maps to use. >>> roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], >>> output_size=7, >>> sampling_ratio=2) >>> >>> keypoint_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], >>> output_size=14, >>> sampling_ratio=2) >>> # put the pieces together inside a KeypointRCNN model >>> model = KeypointRCNN(backbone, >>> num_classes=2, >>> rpn_anchor_generator=anchor_generator, >>> box_roi_pool=roi_pooler, >>> keypoint_roi_pool=keypoint_roi_pooler) >>> model.eval() >>> model.eval() >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] >>> predictions = model(x)
      - Class: `KeypointRCNNHeads(in_channels, layers)`
        - Description: A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ``OrderedDict`` of modules can be passed in. The ``forward()`` method of ``Sequential`` accepts any input and forwards it to the first module it contains. It then "chains" outputs to inputs sequentially for each subsequent module, finally returning the output of the last module. The value a ``Sequential`` provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the ``Sequential`` applies to each of the modules it stores (which are each a registered submodule of the ``Sequential``). What's the difference between a ``Sequential`` and a :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it sounds like--a list for storing ``Module`` s! On the other hand, the layers in a ``Sequential`` are connected in a cascading way. Example:: # Using Sequential to create a small model. When `model` is run, # input will first be passed to `Conv2d(1,20,5)`. The output of # `Conv2d(1,20,5)` will be used as the input to the first # `ReLU`; the output of the first `ReLU` will become the input # for `Conv2d(20,64,5)`. Finally, the output of # `Conv2d(20,64,5)` will be used as input to the second `ReLU` model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # Using Sequential with OrderedDict. This is functionally the # same as the above code model = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ]))
      - Class: `KeypointRCNNPredictor(in_channels, num_keypoints)`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `KeypointRCNN_ResNet50_FPN_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `MultiScaleRoIAlign(featmap_names: List[str], output_size: Union[int, Tuple[int], List[int]], sampling_ratio: int, *, canonical_scale: int = 224, canonical_level: int = 4)`
        - Description: Multi-scale RoIAlign pooling, which is useful for detection with or without FPN. It infers the scale of the pooling via the heuristics specified in eq. 1 of the `Feature Pyramid Network paper <https://arxiv.org/abs/1612.03144>`_. They keyword-only parameters ``canonical_scale`` and ``canonical_level`` correspond respectively to ``224`` and ``k0=4`` in eq. 1, and have the following meaning: ``canonical_level`` is the target level of the pyramid from which to pool a region of interest with ``w x h = canonical_scale x canonical_scale``. Args: featmap_names (List[str]): the names of the feature maps that will be used for the pooling. output_size (List[Tuple[int, int]] or List[int]): output size for the pooled region sampling_ratio (int): sampling ratio for ROIAlign canonical_scale (int, optional): canonical_scale for LevelMapper canonical_level (int, optional): canonical_level for LevelMapper Examples:: >>> m = torchvision.ops.MultiScaleRoIAlign(['feat1', 'feat3'], 3, 2) >>> i = OrderedDict() >>> i['feat1'] = torch.rand(1, 5, 64, 64) >>> i['feat2'] = torch.rand(1, 5, 32, 32) # this feature won't be used in the pooling >>> i['feat3'] = torch.rand(1, 5, 16, 16) >>> # create some random bounding boxes >>> boxes = torch.rand(6, 4) * 256; boxes[:, 2:] += boxes[:, :2] >>> # original image size, before computing the feature maps >>> image_sizes = [(512, 512)] >>> output = m(i, [boxes], image_sizes) >>> print(output.shape) >>> torch.Size([6, 5, 3, 3])
      - Class: `ObjectDetection(*args, **kwargs) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `ResNet50_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Function: `_ovewrite_value_param(param: str, actual: Optional[~V], expected: ~V) -> ~V`
        - Description: No docstring available
      - Function: `_resnet_fpn_extractor(backbone: torchvision.models.resnet.ResNet, trainable_layers: int, returned_layers: Optional[List[int]] = None, extra_blocks: Optional[torchvision.ops.feature_pyramid_network.ExtraFPNBlock] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> torchvision.models.detection.backbone_utils.BackboneWithFPN`
        - Description: No docstring available
      - Function: `_validate_trainable_layers(is_trained: bool, trainable_backbone_layers: Optional[int], max_value: int, default_value: int) -> int`
        - Description: No docstring available
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Function: `keypointrcnn_resnet50_fpn(*, weights: Optional[torchvision.models.detection.keypoint_rcnn.KeypointRCNN_ResNet50_FPN_Weights] = None, progress: bool = True, num_classes: Optional[int] = None, num_keypoints: Optional[int] = None, weights_backbone: Optional[torchvision.models.resnet.ResNet50_Weights] = ResNet50_Weights.IMAGENET1K_V1, trainable_backbone_layers: Optional[int] = None, **kwargs: Any) -> torchvision.models.detection.keypoint_rcnn.KeypointRCNN`
        - Description: Constructs a Keypoint R-CNN model with a ResNet-50-FPN backbone. .. betastatus:: detection module Reference: `Mask R-CNN <https://arxiv.org/abs/1703.06870>`__. The input to the model is expected to be a list of tensors, each of shape ``[C, H, W]``, one for each image, and should be in ``0-1`` range. Different images can have different sizes. The behavior of the model changes depending on if it is in training or evaluation mode. During training, the model expects both the input tensors and targets (list of dictionary), containing: - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (``Int64Tensor[N]``): the class label for each ground-truth box - keypoints (``FloatTensor[N, K, 3]``): the ``K`` keypoints location for each of the ``N`` instances, in the format ``[x, y, visibility]``, where ``visibility=0`` means that the keypoint is not visible. The model returns a ``Dict[Tensor]`` during training, containing the classification and regression losses for both the RPN and the R-CNN, and the keypoint loss. During inference, the model requires only the input tensors, and returns the post-processed predictions as a ``List[Dict[Tensor]]``, one for each input image. The fields of the ``Dict`` are as follows, where ``N`` is the number of detected instances: - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (``Int64Tensor[N]``): the predicted labels for each instance - scores (``Tensor[N]``): the scores or each instance - keypoints (``FloatTensor[N, K, 3]``): the locations of the predicted keypoints, in ``[x, y, v]`` format. For more details on the output, you may refer to :ref:`instance_seg_output`. Keypoint R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size. Example:: >>> model = torchvision.models.detection.keypointrcnn_resnet50_fpn(weights=KeypointRCNN_ResNet50_FPN_Weights.DEFAULT) >>> model.eval() >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] >>> predictions = model(x) >>> >>> # optionally, if you want to export the model to ONNX: >>> torch.onnx.export(model, x, "keypoint_rcnn.onnx", opset_version = 11) Args: weights (:class:`~torchvision.models.detection.KeypointRCNN_ResNet50_FPN_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.detection.KeypointRCNN_ResNet50_FPN_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool): If True, displays a progress bar of the download to stderr num_classes (int, optional): number of output classes of the model (including the background) num_keypoints (int, optional): number of keypoints weights_backbone (:class:`~torchvision.models.ResNet50_Weights`, optional): The pretrained weights for the backbone. trainable_backbone_layers (int, optional): number of trainable (not frozen) layers starting from final block. Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable. If ``None`` is passed (the default) this value is set to 3. .. autoclass:: torchvision.models.detection.KeypointRCNN_ResNet50_FPN_Weights :members:
      - Function: `overwrite_eps(model: torch.nn.modules.module.Module, eps: float) -> None`
        - Description: This method overwrites the default eps values of all the FrozenBatchNorm2d layers of the model with the provided value. This is necessary to address the BC-breaking change introduced by the bug-fix at pytorch/vision#2933. The overwrite is applied only when the pretrained weights are loaded to maintain compatibility with previous versions. Args: model (nn.Module): The model on which we perform the overwrite. eps (float): The new value of eps.
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
      - Function: `resnet50(*, weights: Optional[torchvision.models.resnet.ResNet50_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.resnet.ResNet`
        - Description: ResNet-50 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__. .. note:: The bottleneck of TorchVision places the stride for downsampling to the second 3x3 convolution while the original paper places it to the first 1x1 convolution. This variant improves the accuracy and is known as `ResNet V1.5 <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_. Args: weights (:class:`~torchvision.models.ResNet50_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ResNet50_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.ResNet50_Weights :members:
    - mask_rcnn.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `FastRCNNConvFCHead(input_size: Tuple[int, int, int], conv_layers: List[int], fc_layers: List[int], norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None)`
        - Description: A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ``OrderedDict`` of modules can be passed in. The ``forward()`` method of ``Sequential`` accepts any input and forwards it to the first module it contains. It then "chains" outputs to inputs sequentially for each subsequent module, finally returning the output of the last module. The value a ``Sequential`` provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the ``Sequential`` applies to each of the modules it stores (which are each a registered submodule of the ``Sequential``). What's the difference between a ``Sequential`` and a :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it sounds like--a list for storing ``Module`` s! On the other hand, the layers in a ``Sequential`` are connected in a cascading way. Example:: # Using Sequential to create a small model. When `model` is run, # input will first be passed to `Conv2d(1,20,5)`. The output of # `Conv2d(1,20,5)` will be used as the input to the first # `ReLU`; the output of the first `ReLU` will become the input # for `Conv2d(20,64,5)`. Finally, the output of # `Conv2d(20,64,5)` will be used as input to the second `ReLU` model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # Using Sequential with OrderedDict. This is functionally the # same as the above code model = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ]))
      - Class: `FasterRCNN(backbone, num_classes=None, min_size=800, max_size=1333, image_mean=None, image_std=None, rpn_anchor_generator=None, rpn_head=None, rpn_pre_nms_top_n_train=2000, rpn_pre_nms_top_n_test=1000, rpn_post_nms_top_n_train=2000, rpn_post_nms_top_n_test=1000, rpn_nms_thresh=0.7, rpn_fg_iou_thresh=0.7, rpn_bg_iou_thresh=0.3, rpn_batch_size_per_image=256, rpn_positive_fraction=0.5, rpn_score_thresh=0.0, box_roi_pool=None, box_head=None, box_predictor=None, box_score_thresh=0.05, box_nms_thresh=0.5, box_detections_per_img=100, box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5, box_batch_size_per_image=512, box_positive_fraction=0.25, bbox_reg_weights=None, **kwargs)`
        - Description: Implements Faster R-CNN. The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each image, and should be in 0-1 range. Different images can have different sizes. The behavior of the model changes depending on if it is in training or evaluation mode. During training, the model expects both the input tensors and targets (list of dictionary), containing: - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (Int64Tensor[N]): the class label for each ground-truth box The model returns a Dict[Tensor] during training, containing the classification and regression losses for both the RPN and the R-CNN. During inference, the model requires only the input tensors, and returns the post-processed predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as follows: - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (Int64Tensor[N]): the predicted labels for each image - scores (Tensor[N]): the scores or each prediction Args: backbone (nn.Module): the network used to compute the features for the model. It should contain an out_channels attribute, which indicates the number of output channels that each feature map has (and it should be the same for all feature maps). The backbone should return a single Tensor or and OrderedDict[Tensor]. num_classes (int): number of output classes of the model (including the background). If box_predictor is specified, num_classes should be None. min_size (int): minimum size of the image to be rescaled before feeding it to the backbone max_size (int): maximum size of the image to be rescaled before feeding it to the backbone image_mean (Tuple[float, float, float]): mean values used for input normalization. They are generally the mean values of the dataset on which the backbone has been trained on image_std (Tuple[float, float, float]): std values used for input normalization. They are generally the std values of the dataset on which the backbone has been trained on rpn_anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature maps. rpn_head (nn.Module): module that computes the objectness and regression deltas from the RPN rpn_pre_nms_top_n_train (int): number of proposals to keep before applying NMS during training rpn_pre_nms_top_n_test (int): number of proposals to keep before applying NMS during testing rpn_post_nms_top_n_train (int): number of proposals to keep after applying NMS during training rpn_post_nms_top_n_test (int): number of proposals to keep after applying NMS during testing rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be considered as positive during training of the RPN. rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be considered as negative during training of the RPN. rpn_batch_size_per_image (int): number of anchors that are sampled during training of the RPN for computing the loss rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training of the RPN rpn_score_thresh (float): during inference, only return proposals with a classification score greater than rpn_score_thresh box_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in the locations indicated by the bounding boxes box_head (nn.Module): module that takes the cropped feature maps as input box_predictor (nn.Module): module that takes the output of box_head and returns the classification logits and box regression deltas. box_score_thresh (float): during inference, only return proposals with a classification score greater than box_score_thresh box_nms_thresh (float): NMS threshold for the prediction head. Used during inference box_detections_per_img (int): maximum number of detections per image, for all classes. box_fg_iou_thresh (float): minimum IoU between the proposals and the GT box so that they can be considered as positive during training of the classification head box_bg_iou_thresh (float): maximum IoU between the proposals and the GT box so that they can be considered as negative during training of the classification head box_batch_size_per_image (int): number of proposals that are sampled during training of the classification head box_positive_fraction (float): proportion of positive proposals in a mini-batch during training of the classification head bbox_reg_weights (Tuple[float, float, float, float]): weights for the encoding/decoding of the bounding boxes Example:: >>> import torch >>> import torchvision >>> from torchvision.models.detection import FasterRCNN >>> from torchvision.models.detection.rpn import AnchorGenerator >>> # load a pre-trained model for classification and return >>> # only the features >>> backbone = torchvision.models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).features >>> # FasterRCNN needs to know the number of >>> # output channels in a backbone. For mobilenet_v2, it's 1280, >>> # so we need to add it here >>> backbone.out_channels = 1280 >>> >>> # let's make the RPN generate 5 x 3 anchors per spatial >>> # location, with 5 different sizes and 3 different aspect >>> # ratios. We have a Tuple[Tuple[int]] because each feature >>> # map could potentially have different sizes and >>> # aspect ratios >>> anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), >>> aspect_ratios=((0.5, 1.0, 2.0),)) >>> >>> # let's define what are the feature maps that we will >>> # use to perform the region of interest cropping, as well as >>> # the size of the crop after rescaling. >>> # if your backbone returns a Tensor, featmap_names is expected to >>> # be ['0']. More generally, the backbone should return an >>> # OrderedDict[Tensor], and in featmap_names you can choose which >>> # feature maps to use. >>> roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], >>> output_size=7, >>> sampling_ratio=2) >>> >>> # put the pieces together inside a FasterRCNN model >>> model = FasterRCNN(backbone, >>> num_classes=2, >>> rpn_anchor_generator=anchor_generator, >>> box_roi_pool=roi_pooler) >>> model.eval() >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] >>> predictions = model(x)
      - Class: `MaskRCNN(backbone, num_classes=None, min_size=800, max_size=1333, image_mean=None, image_std=None, rpn_anchor_generator=None, rpn_head=None, rpn_pre_nms_top_n_train=2000, rpn_pre_nms_top_n_test=1000, rpn_post_nms_top_n_train=2000, rpn_post_nms_top_n_test=1000, rpn_nms_thresh=0.7, rpn_fg_iou_thresh=0.7, rpn_bg_iou_thresh=0.3, rpn_batch_size_per_image=256, rpn_positive_fraction=0.5, rpn_score_thresh=0.0, box_roi_pool=None, box_head=None, box_predictor=None, box_score_thresh=0.05, box_nms_thresh=0.5, box_detections_per_img=100, box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5, box_batch_size_per_image=512, box_positive_fraction=0.25, bbox_reg_weights=None, mask_roi_pool=None, mask_head=None, mask_predictor=None, **kwargs)`
        - Description: Implements Mask R-CNN. The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each image, and should be in 0-1 range. Different images can have different sizes. The behavior of the model changes depending on if it is in training or evaluation mode. During training, the model expects both the input tensors and targets (list of dictionary), containing: - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (Int64Tensor[N]): the class label for each ground-truth box - masks (UInt8Tensor[N, H, W]): the segmentation binary masks for each instance The model returns a Dict[Tensor] during training, containing the classification and regression losses for both the RPN and the R-CNN, and the mask loss. During inference, the model requires only the input tensors, and returns the post-processed predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as follows: - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (Int64Tensor[N]): the predicted labels for each image - scores (Tensor[N]): the scores or each prediction - masks (UInt8Tensor[N, 1, H, W]): the predicted masks for each instance, in 0-1 range. In order to obtain the final segmentation masks, the soft masks can be thresholded, generally with a value of 0.5 (mask >= 0.5) Args: backbone (nn.Module): the network used to compute the features for the model. It should contain an out_channels attribute, which indicates the number of output channels that each feature map has (and it should be the same for all feature maps). The backbone should return a single Tensor or and OrderedDict[Tensor]. num_classes (int): number of output classes of the model (including the background). If box_predictor is specified, num_classes should be None. min_size (int): minimum size of the image to be rescaled before feeding it to the backbone max_size (int): maximum size of the image to be rescaled before feeding it to the backbone image_mean (Tuple[float, float, float]): mean values used for input normalization. They are generally the mean values of the dataset on which the backbone has been trained on image_std (Tuple[float, float, float]): std values used for input normalization. They are generally the std values of the dataset on which the backbone has been trained on rpn_anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature maps. rpn_head (nn.Module): module that computes the objectness and regression deltas from the RPN rpn_pre_nms_top_n_train (int): number of proposals to keep before applying NMS during training rpn_pre_nms_top_n_test (int): number of proposals to keep before applying NMS during testing rpn_post_nms_top_n_train (int): number of proposals to keep after applying NMS during training rpn_post_nms_top_n_test (int): number of proposals to keep after applying NMS during testing rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be considered as positive during training of the RPN. rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be considered as negative during training of the RPN. rpn_batch_size_per_image (int): number of anchors that are sampled during training of the RPN for computing the loss rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training of the RPN rpn_score_thresh (float): during inference, only return proposals with a classification score greater than rpn_score_thresh box_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in the locations indicated by the bounding boxes box_head (nn.Module): module that takes the cropped feature maps as input box_predictor (nn.Module): module that takes the output of box_head and returns the classification logits and box regression deltas. box_score_thresh (float): during inference, only return proposals with a classification score greater than box_score_thresh box_nms_thresh (float): NMS threshold for the prediction head. Used during inference box_detections_per_img (int): maximum number of detections per image, for all classes. box_fg_iou_thresh (float): minimum IoU between the proposals and the GT box so that they can be considered as positive during training of the classification head box_bg_iou_thresh (float): maximum IoU between the proposals and the GT box so that they can be considered as negative during training of the classification head box_batch_size_per_image (int): number of proposals that are sampled during training of the classification head box_positive_fraction (float): proportion of positive proposals in a mini-batch during training of the classification head bbox_reg_weights (Tuple[float, float, float, float]): weights for the encoding/decoding of the bounding boxes mask_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in the locations indicated by the bounding boxes, which will be used for the mask head. mask_head (nn.Module): module that takes the cropped feature maps as input mask_predictor (nn.Module): module that takes the output of the mask_head and returns the segmentation mask logits Example:: >>> import torch >>> import torchvision >>> from torchvision.models.detection import MaskRCNN >>> from torchvision.models.detection.anchor_utils import AnchorGenerator >>> >>> # load a pre-trained model for classification and return >>> # only the features >>> backbone = torchvision.models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).features >>> # MaskRCNN needs to know the number of >>> # output channels in a backbone. For mobilenet_v2, it's 1280 >>> # so we need to add it here, >>> backbone.out_channels = 1280 >>> >>> # let's make the RPN generate 5 x 3 anchors per spatial >>> # location, with 5 different sizes and 3 different aspect >>> # ratios. We have a Tuple[Tuple[int]] because each feature >>> # map could potentially have different sizes and >>> # aspect ratios >>> anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), >>> aspect_ratios=((0.5, 1.0, 2.0),)) >>> >>> # let's define what are the feature maps that we will >>> # use to perform the region of interest cropping, as well as >>> # the size of the crop after rescaling. >>> # if your backbone returns a Tensor, featmap_names is expected to >>> # be ['0']. More generally, the backbone should return an >>> # OrderedDict[Tensor], and in featmap_names you can choose which >>> # feature maps to use. >>> roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], >>> output_size=7, >>> sampling_ratio=2) >>> >>> mask_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], >>> output_size=14, >>> sampling_ratio=2) >>> # put the pieces together inside a MaskRCNN model >>> model = MaskRCNN(backbone, >>> num_classes=2, >>> rpn_anchor_generator=anchor_generator, >>> box_roi_pool=roi_pooler, >>> mask_roi_pool=mask_roi_pooler) >>> model.eval() >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] >>> predictions = model(x)
      - Class: `MaskRCNNHeads(in_channels, layers, dilation, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None)`
        - Description: A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ``OrderedDict`` of modules can be passed in. The ``forward()`` method of ``Sequential`` accepts any input and forwards it to the first module it contains. It then "chains" outputs to inputs sequentially for each subsequent module, finally returning the output of the last module. The value a ``Sequential`` provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the ``Sequential`` applies to each of the modules it stores (which are each a registered submodule of the ``Sequential``). What's the difference between a ``Sequential`` and a :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it sounds like--a list for storing ``Module`` s! On the other hand, the layers in a ``Sequential`` are connected in a cascading way. Example:: # Using Sequential to create a small model. When `model` is run, # input will first be passed to `Conv2d(1,20,5)`. The output of # `Conv2d(1,20,5)` will be used as the input to the first # `ReLU`; the output of the first `ReLU` will become the input # for `Conv2d(20,64,5)`. Finally, the output of # `Conv2d(20,64,5)` will be used as input to the second `ReLU` model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # Using Sequential with OrderedDict. This is functionally the # same as the above code model = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ]))
      - Class: `MaskRCNNPredictor(in_channels, dim_reduced, num_classes)`
        - Description: A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ``OrderedDict`` of modules can be passed in. The ``forward()`` method of ``Sequential`` accepts any input and forwards it to the first module it contains. It then "chains" outputs to inputs sequentially for each subsequent module, finally returning the output of the last module. The value a ``Sequential`` provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the ``Sequential`` applies to each of the modules it stores (which are each a registered submodule of the ``Sequential``). What's the difference between a ``Sequential`` and a :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it sounds like--a list for storing ``Module`` s! On the other hand, the layers in a ``Sequential`` are connected in a cascading way. Example:: # Using Sequential to create a small model. When `model` is run, # input will first be passed to `Conv2d(1,20,5)`. The output of # `Conv2d(1,20,5)` will be used as the input to the first # `ReLU`; the output of the first `ReLU` will become the input # for `Conv2d(20,64,5)`. Finally, the output of # `Conv2d(20,64,5)` will be used as input to the second `ReLU` model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # Using Sequential with OrderedDict. This is functionally the # same as the above code model = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ]))
      - Class: `MaskRCNN_ResNet50_FPN_V2_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `MaskRCNN_ResNet50_FPN_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `MultiScaleRoIAlign(featmap_names: List[str], output_size: Union[int, Tuple[int], List[int]], sampling_ratio: int, *, canonical_scale: int = 224, canonical_level: int = 4)`
        - Description: Multi-scale RoIAlign pooling, which is useful for detection with or without FPN. It infers the scale of the pooling via the heuristics specified in eq. 1 of the `Feature Pyramid Network paper <https://arxiv.org/abs/1612.03144>`_. They keyword-only parameters ``canonical_scale`` and ``canonical_level`` correspond respectively to ``224`` and ``k0=4`` in eq. 1, and have the following meaning: ``canonical_level`` is the target level of the pyramid from which to pool a region of interest with ``w x h = canonical_scale x canonical_scale``. Args: featmap_names (List[str]): the names of the feature maps that will be used for the pooling. output_size (List[Tuple[int, int]] or List[int]): output size for the pooled region sampling_ratio (int): sampling ratio for ROIAlign canonical_scale (int, optional): canonical_scale for LevelMapper canonical_level (int, optional): canonical_level for LevelMapper Examples:: >>> m = torchvision.ops.MultiScaleRoIAlign(['feat1', 'feat3'], 3, 2) >>> i = OrderedDict() >>> i['feat1'] = torch.rand(1, 5, 64, 64) >>> i['feat2'] = torch.rand(1, 5, 32, 32) # this feature won't be used in the pooling >>> i['feat3'] = torch.rand(1, 5, 16, 16) >>> # create some random bounding boxes >>> boxes = torch.rand(6, 4) * 256; boxes[:, 2:] += boxes[:, :2] >>> # original image size, before computing the feature maps >>> image_sizes = [(512, 512)] >>> output = m(i, [boxes], image_sizes) >>> print(output.shape) >>> torch.Size([6, 5, 3, 3])
      - Class: `ObjectDetection(*args, **kwargs) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `OrderedDict(Unable to retrieve signature)`
        - Description: Dictionary that remembers insertion order
      - Class: `RPNHead(in_channels: int, num_anchors: int, conv_depth=1) -> None`
        - Description: Adds a simple RPN Head with classification and regression heads Args: in_channels (int): number of channels of the input feature num_anchors (int): number of anchors to be predicted conv_depth (int, optional): number of convolutions
      - Class: `ResNet50_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Function: `_default_anchorgen()`
        - Description: No docstring available
      - Function: `_ovewrite_value_param(param: str, actual: Optional[~V], expected: ~V) -> ~V`
        - Description: No docstring available
      - Function: `_resnet_fpn_extractor(backbone: torchvision.models.resnet.ResNet, trainable_layers: int, returned_layers: Optional[List[int]] = None, extra_blocks: Optional[torchvision.ops.feature_pyramid_network.ExtraFPNBlock] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> torchvision.models.detection.backbone_utils.BackboneWithFPN`
        - Description: No docstring available
      - Function: `_validate_trainable_layers(is_trained: bool, trainable_backbone_layers: Optional[int], max_value: int, default_value: int) -> int`
        - Description: No docstring available
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Function: `maskrcnn_resnet50_fpn(*, weights: Optional[torchvision.models.detection.mask_rcnn.MaskRCNN_ResNet50_FPN_Weights] = None, progress: bool = True, num_classes: Optional[int] = None, weights_backbone: Optional[torchvision.models.resnet.ResNet50_Weights] = ResNet50_Weights.IMAGENET1K_V1, trainable_backbone_layers: Optional[int] = None, **kwargs: Any) -> torchvision.models.detection.mask_rcnn.MaskRCNN`
        - Description: Mask R-CNN model with a ResNet-50-FPN backbone from the `Mask R-CNN <https://arxiv.org/abs/1703.06870>`_ paper. .. betastatus:: detection module The input to the model is expected to be a list of tensors, each of shape ``[C, H, W]``, one for each image, and should be in ``0-1`` range. Different images can have different sizes. The behavior of the model changes depending on if it is in training or evaluation mode. During training, the model expects both the input tensors and targets (list of dictionary), containing: - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (``Int64Tensor[N]``): the class label for each ground-truth box - masks (``UInt8Tensor[N, H, W]``): the segmentation binary masks for each instance The model returns a ``Dict[Tensor]`` during training, containing the classification and regression losses for both the RPN and the R-CNN, and the mask loss. During inference, the model requires only the input tensors, and returns the post-processed predictions as a ``List[Dict[Tensor]]``, one for each input image. The fields of the ``Dict`` are as follows, where ``N`` is the number of detected instances: - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (``Int64Tensor[N]``): the predicted labels for each instance - scores (``Tensor[N]``): the scores or each instance - masks (``UInt8Tensor[N, 1, H, W]``): the predicted masks for each instance, in ``0-1`` range. In order to obtain the final segmentation masks, the soft masks can be thresholded, generally with a value of 0.5 (``mask >= 0.5``) For more details on the output and on how to plot the masks, you may refer to :ref:`instance_seg_output`. Mask R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size. Example:: >>> model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT) >>> model.eval() >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] >>> predictions = model(x) >>> >>> # optionally, if you want to export the model to ONNX: >>> torch.onnx.export(model, x, "mask_rcnn.onnx", opset_version = 11) Args: weights (:class:`~torchvision.models.detection.MaskRCNN_ResNet50_FPN_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.detection.MaskRCNN_ResNet50_FPN_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. num_classes (int, optional): number of output classes of the model (including the background) weights_backbone (:class:`~torchvision.models.ResNet50_Weights`, optional): The pretrained weights for the backbone. trainable_backbone_layers (int, optional): number of trainable (not frozen) layers starting from final block. Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable. If ``None`` is passed (the default) this value is set to 3. **kwargs: parameters passed to the ``torchvision.models.detection.mask_rcnn.MaskRCNN`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/detection/mask_rcnn.py>`_ for more details about this class. .. autoclass:: torchvision.models.detection.MaskRCNN_ResNet50_FPN_Weights :members:
      - Function: `maskrcnn_resnet50_fpn_v2(*, weights: Optional[torchvision.models.detection.mask_rcnn.MaskRCNN_ResNet50_FPN_V2_Weights] = None, progress: bool = True, num_classes: Optional[int] = None, weights_backbone: Optional[torchvision.models.resnet.ResNet50_Weights] = None, trainable_backbone_layers: Optional[int] = None, **kwargs: Any) -> torchvision.models.detection.mask_rcnn.MaskRCNN`
        - Description: Improved Mask R-CNN model with a ResNet-50-FPN backbone from the `Benchmarking Detection Transfer Learning with Vision Transformers <https://arxiv.org/abs/2111.11429>`_ paper. .. betastatus:: detection module :func:`~torchvision.models.detection.maskrcnn_resnet50_fpn` for more details. Args: weights (:class:`~torchvision.models.detection.MaskRCNN_ResNet50_FPN_V2_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.detection.MaskRCNN_ResNet50_FPN_V2_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. num_classes (int, optional): number of output classes of the model (including the background) weights_backbone (:class:`~torchvision.models.ResNet50_Weights`, optional): The pretrained weights for the backbone. trainable_backbone_layers (int, optional): number of trainable (not frozen) layers starting from final block. Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable. If ``None`` is passed (the default) this value is set to 3. **kwargs: parameters passed to the ``torchvision.models.detection.mask_rcnn.MaskRCNN`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/detection/mask_rcnn.py>`_ for more details about this class. .. autoclass:: torchvision.models.detection.MaskRCNN_ResNet50_FPN_V2_Weights :members:
      - Function: `overwrite_eps(model: torch.nn.modules.module.Module, eps: float) -> None`
        - Description: This method overwrites the default eps values of all the FrozenBatchNorm2d layers of the model with the provided value. This is necessary to address the BC-breaking change introduced by the bug-fix at pytorch/vision#2933. The overwrite is applied only when the pretrained weights are loaded to maintain compatibility with previous versions. Args: model (nn.Module): The model on which we perform the overwrite. eps (float): The new value of eps.
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
      - Function: `resnet50(*, weights: Optional[torchvision.models.resnet.ResNet50_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.resnet.ResNet`
        - Description: ResNet-50 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__. .. note:: The bottleneck of TorchVision places the stride for downsampling to the second 3x3 convolution while the original paper places it to the first 1x1 convolution. This variant improves the accuracy and is known as `ResNet V1.5 <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_. Args: weights (:class:`~torchvision.models.ResNet50_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ResNet50_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.ResNet50_Weights :members:
    - retinanet.py
      - Class: `AnchorGenerator(sizes=((128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),))`
        - Description: Module that generates anchors for a set of feature maps and image sizes. The module support computing anchors at multiple sizes and aspect ratios per feature map. This module assumes aspect ratio = height / width for each anchor. sizes and aspect_ratios should have the same number of elements, and it should correspond to the number of feature maps. sizes[i] and aspect_ratios[i] can have an arbitrary number of elements, and AnchorGenerator will output a set of sizes[i] * aspect_ratios[i] anchors per spatial location for feature map i. Args: sizes (Tuple[Tuple[int]]): aspect_ratios (Tuple[Tuple[float]]):
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `GeneralizedRCNNTransform(min_size: int, max_size: int, image_mean: List[float], image_std: List[float], size_divisible: int = 32, fixed_size: Optional[Tuple[int, int]] = None, **kwargs: Any)`
        - Description: Performs input / target transformation before feeding the data to a GeneralizedRCNN model. The transformations it performs are: - input normalization (mean subtraction and std division) - input / target resizing to match min_size / max_size It returns a ImageList for the inputs, and a List[Dict[Tensor]] for the targets
      - Class: `LastLevelP6P7(in_channels: int, out_channels: int)`
        - Description: This module is used in RetinaNet to generate extra layers, P6 and P7.
      - Class: `ObjectDetection(*args, **kwargs) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `OrderedDict(Unable to retrieve signature)`
        - Description: Dictionary that remembers insertion order
      - Class: `ResNet50_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `RetinaNet(backbone, num_classes, min_size=800, max_size=1333, image_mean=None, image_std=None, anchor_generator=None, head=None, proposal_matcher=None, score_thresh=0.05, nms_thresh=0.5, detections_per_img=300, fg_iou_thresh=0.5, bg_iou_thresh=0.4, topk_candidates=1000, **kwargs)`
        - Description: Implements RetinaNet. The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each image, and should be in 0-1 range. Different images can have different sizes. The behavior of the model changes depending on if it is in training or evaluation mode. During training, the model expects both the input tensors and targets (list of dictionary), containing: - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (Int64Tensor[N]): the class label for each ground-truth box The model returns a Dict[Tensor] during training, containing the classification and regression losses. During inference, the model requires only the input tensors, and returns the post-processed predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as follows: - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (Int64Tensor[N]): the predicted labels for each image - scores (Tensor[N]): the scores for each prediction Args: backbone (nn.Module): the network used to compute the features for the model. It should contain an out_channels attribute, which indicates the number of output channels that each feature map has (and it should be the same for all feature maps). The backbone should return a single Tensor or an OrderedDict[Tensor]. num_classes (int): number of output classes of the model (including the background). min_size (int): minimum size of the image to be rescaled before feeding it to the backbone max_size (int): maximum size of the image to be rescaled before feeding it to the backbone image_mean (Tuple[float, float, float]): mean values used for input normalization. They are generally the mean values of the dataset on which the backbone has been trained on image_std (Tuple[float, float, float]): std values used for input normalization. They are generally the std values of the dataset on which the backbone has been trained on anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature maps. head (nn.Module): Module run on top of the feature pyramid. Defaults to a module containing a classification and regression module. score_thresh (float): Score threshold used for postprocessing the detections. nms_thresh (float): NMS threshold used for postprocessing the detections. detections_per_img (int): Number of best detections to keep after NMS. fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be considered as positive during training. bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be considered as negative during training. topk_candidates (int): Number of best detections to keep before NMS. Example: >>> import torch >>> import torchvision >>> from torchvision.models.detection import RetinaNet >>> from torchvision.models.detection.anchor_utils import AnchorGenerator >>> # load a pre-trained model for classification and return >>> # only the features >>> backbone = torchvision.models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).features >>> # RetinaNet needs to know the number of >>> # output channels in a backbone. For mobilenet_v2, it's 1280, >>> # so we need to add it here >>> backbone.out_channels = 1280 >>> >>> # let's make the network generate 5 x 3 anchors per spatial >>> # location, with 5 different sizes and 3 different aspect >>> # ratios. We have a Tuple[Tuple[int]] because each feature >>> # map could potentially have different sizes and >>> # aspect ratios >>> anchor_generator = AnchorGenerator( >>> sizes=((32, 64, 128, 256, 512),), >>> aspect_ratios=((0.5, 1.0, 2.0),) >>> ) >>> >>> # put the pieces together inside a RetinaNet model >>> model = RetinaNet(backbone, >>> num_classes=2, >>> anchor_generator=anchor_generator) >>> model.eval() >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] >>> predictions = model(x)
      - Class: `RetinaNetClassificationHead(in_channels, num_anchors, num_classes, prior_probability=0.01, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None)`
        - Description: A classification head for use in RetinaNet. Args: in_channels (int): number of channels of the input feature num_anchors (int): number of anchors to be predicted num_classes (int): number of classes to be predicted norm_layer (callable, optional): Module specifying the normalization layer to use. Default: None
      - Class: `RetinaNetHead(in_channels, num_anchors, num_classes, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None)`
        - Description: A regression and classification head for use in RetinaNet. Args: in_channels (int): number of channels of the input feature num_anchors (int): number of anchors to be predicted num_classes (int): number of classes to be predicted norm_layer (callable, optional): Module specifying the normalization layer to use. Default: None
      - Class: `RetinaNetRegressionHead(in_channels, num_anchors, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None)`
        - Description: A regression head for use in RetinaNet. Args: in_channels (int): number of channels of the input feature num_anchors (int): number of anchors to be predicted norm_layer (callable, optional): Module specifying the normalization layer to use. Default: None
      - Class: `RetinaNet_ResNet50_FPN_V2_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `RetinaNet_ResNet50_FPN_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Function: `_box_loss(type: str, box_coder: torchvision.models.detection._utils.BoxCoder, anchors_per_image: torch.Tensor, matched_gt_boxes_per_image: torch.Tensor, bbox_regression_per_image: torch.Tensor, cnf: Optional[Dict[str, float]] = None) -> torch.Tensor`
        - Description: No docstring available
      - Function: `_default_anchorgen()`
        - Description: No docstring available
      - Function: `_log_api_usage_once(obj: Any) -> None`
        - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
      - Function: `_ovewrite_value_param(param: str, actual: Optional[~V], expected: ~V) -> ~V`
        - Description: No docstring available
      - Function: `_resnet_fpn_extractor(backbone: torchvision.models.resnet.ResNet, trainable_layers: int, returned_layers: Optional[List[int]] = None, extra_blocks: Optional[torchvision.ops.feature_pyramid_network.ExtraFPNBlock] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> torchvision.models.detection.backbone_utils.BackboneWithFPN`
        - Description: No docstring available
      - Function: `_sum(x: List[torch.Tensor]) -> torch.Tensor`
        - Description: No docstring available
      - Function: `_v1_to_v2_weights(state_dict, prefix)`
        - Description: No docstring available
      - Function: `_validate_trainable_layers(is_trained: bool, trainable_backbone_layers: Optional[int], max_value: int, default_value: int) -> int`
        - Description: No docstring available
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Function: `overwrite_eps(model: torch.nn.modules.module.Module, eps: float) -> None`
        - Description: This method overwrites the default eps values of all the FrozenBatchNorm2d layers of the model with the provided value. This is necessary to address the BC-breaking change introduced by the bug-fix at pytorch/vision#2933. The overwrite is applied only when the pretrained weights are loaded to maintain compatibility with previous versions. Args: model (nn.Module): The model on which we perform the overwrite. eps (float): The new value of eps.
      - Class: `partial(Unable to retrieve signature)`
        - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
      - Function: `resnet50(*, weights: Optional[torchvision.models.resnet.ResNet50_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.resnet.ResNet`
        - Description: ResNet-50 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__. .. note:: The bottleneck of TorchVision places the stride for downsampling to the second 3x3 convolution while the original paper places it to the first 1x1 convolution. This variant improves the accuracy and is known as `ResNet V1.5 <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_. Args: weights (:class:`~torchvision.models.ResNet50_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ResNet50_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.ResNet50_Weights :members:
      - Function: `retinanet_resnet50_fpn(*, weights: Optional[torchvision.models.detection.retinanet.RetinaNet_ResNet50_FPN_Weights] = None, progress: bool = True, num_classes: Optional[int] = None, weights_backbone: Optional[torchvision.models.resnet.ResNet50_Weights] = ResNet50_Weights.IMAGENET1K_V1, trainable_backbone_layers: Optional[int] = None, **kwargs: Any) -> torchvision.models.detection.retinanet.RetinaNet`
        - Description: Constructs a RetinaNet model with a ResNet-50-FPN backbone. .. betastatus:: detection module Reference: `Focal Loss for Dense Object Detection <https://arxiv.org/abs/1708.02002>`_. The input to the model is expected to be a list of tensors, each of shape ``[C, H, W]``, one for each image, and should be in ``0-1`` range. Different images can have different sizes. The behavior of the model changes depending on if it is in training or evaluation mode. During training, the model expects both the input tensors and targets (list of dictionary), containing: - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (``Int64Tensor[N]``): the class label for each ground-truth box The model returns a ``Dict[Tensor]`` during training, containing the classification and regression losses. During inference, the model requires only the input tensors, and returns the post-processed predictions as a ``List[Dict[Tensor]]``, one for each input image. The fields of the ``Dict`` are as follows, where ``N`` is the number of detections: - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (``Int64Tensor[N]``): the predicted labels for each detection - scores (``Tensor[N]``): the scores of each detection For more details on the output, you may refer to :ref:`instance_seg_output`. Example:: >>> model = torchvision.models.detection.retinanet_resnet50_fpn(weights=RetinaNet_ResNet50_FPN_Weights.DEFAULT) >>> model.eval() >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] >>> predictions = model(x) Args: weights (:class:`~torchvision.models.detection.RetinaNet_ResNet50_FPN_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.detection.RetinaNet_ResNet50_FPN_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool): If True, displays a progress bar of the download to stderr. Default is True. num_classes (int, optional): number of output classes of the model (including the background) weights_backbone (:class:`~torchvision.models.ResNet50_Weights`, optional): The pretrained weights for the backbone. trainable_backbone_layers (int, optional): number of trainable (not frozen) layers starting from final block. Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable. If ``None`` is passed (the default) this value is set to 3. **kwargs: parameters passed to the ``torchvision.models.detection.RetinaNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/detection/retinanet.py>`_ for more details about this class. .. autoclass:: torchvision.models.detection.RetinaNet_ResNet50_FPN_Weights :members:
      - Function: `retinanet_resnet50_fpn_v2(*, weights: Optional[torchvision.models.detection.retinanet.RetinaNet_ResNet50_FPN_V2_Weights] = None, progress: bool = True, num_classes: Optional[int] = None, weights_backbone: Optional[torchvision.models.resnet.ResNet50_Weights] = None, trainable_backbone_layers: Optional[int] = None, **kwargs: Any) -> torchvision.models.detection.retinanet.RetinaNet`
        - Description: Constructs an improved RetinaNet model with a ResNet-50-FPN backbone. .. betastatus:: detection module Reference: `Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection <https://arxiv.org/abs/1912.02424>`_. :func:`~torchvision.models.detection.retinanet_resnet50_fpn` for more details. Args: weights (:class:`~torchvision.models.detection.RetinaNet_ResNet50_FPN_V2_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.detection.RetinaNet_ResNet50_FPN_V2_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool): If True, displays a progress bar of the download to stderr. Default is True. num_classes (int, optional): number of output classes of the model (including the background) weights_backbone (:class:`~torchvision.models.ResNet50_Weights`, optional): The pretrained weights for the backbone. trainable_backbone_layers (int, optional): number of trainable (not frozen) layers starting from final block. Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable. If ``None`` is passed (the default) this value is set to 3. **kwargs: parameters passed to the ``torchvision.models.detection.RetinaNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/detection/retinanet.py>`_ for more details about this class. .. autoclass:: torchvision.models.detection.RetinaNet_ResNet50_FPN_V2_Weights :members:
      - Function: `sigmoid_focal_loss(inputs: torch.Tensor, targets: torch.Tensor, alpha: float = 0.25, gamma: float = 2, reduction: str = 'none') -> torch.Tensor`
        - Description: Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002. Args: inputs (Tensor): A float tensor of arbitrary shape. The predictions for each example. targets (Tensor): A float tensor with the same shape as inputs. Stores the binary classification label for each element in inputs (0 for the negative class and 1 for the positive class). alpha (float): Weighting factor in range (0,1) to balance positive vs negative examples or -1 for ignore. Default: ``0.25``. gamma (float): Exponent of the modulating factor (1 - p_t) to balance easy vs hard examples. Default: ``2``. reduction (string): ``'none'`` | ``'mean'`` | ``'sum'`` ``'none'``: No reduction will be applied to the output. ``'mean'``: The output will be averaged. ``'sum'``: The output will be summed. Default: ``'none'``. Returns: Loss tensor with the reduction option applied.
    - roi_heads.py
      - Class: `RoIHeads(box_roi_pool, box_head, box_predictor, fg_iou_thresh, bg_iou_thresh, batch_size_per_image, positive_fraction, bbox_reg_weights, score_thresh, nms_thresh, detections_per_img, mask_roi_pool=None, mask_head=None, mask_predictor=None, keypoint_roi_pool=None, keypoint_head=None, keypoint_predictor=None)`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Function: `_onnx_expand_boxes(boxes, scale)`
        - Description: No docstring available
      - Function: `_onnx_heatmaps_to_keypoints(maps, maps_i, roi_map_width, roi_map_height, widths_i, heights_i, offset_x_i, offset_y_i)`
        - Description: No docstring available
      - Function: `_onnx_heatmaps_to_keypoints_loop(maps, rois, widths_ceil, heights_ceil, widths, heights, offset_x, offset_y, num_keypoints)`
        - Description: No docstring available
      - Function: `_onnx_paste_mask_in_image(mask, box, im_h, im_w)`
        - Description: No docstring available
      - Function: `_onnx_paste_masks_in_image_loop(masks, boxes, im_h, im_w)`
        - Description: No docstring available
      - Function: `expand_boxes(boxes, scale)`
        - Description: No docstring available
      - Function: `expand_masks(mask, padding)`
        - Description: No docstring available
      - Function: `expand_masks_tracing_scale(M, padding)`
        - Description: No docstring available
      - Function: `fastrcnn_loss(class_logits, box_regression, labels, regression_targets)`
        - Description: Computes the loss for Faster R-CNN. Args: class_logits (Tensor) box_regression (Tensor) labels (list[BoxList]) regression_targets (Tensor) Returns: classification_loss (Tensor) box_loss (Tensor)
      - Function: `heatmaps_to_keypoints(maps, rois)`
        - Description: Extract predicted keypoint locations from heatmaps. Output has shape (#rois, 4, #keypoints) with the 4 rows corresponding to (x, y, logit, prob) for each keypoint.
      - Function: `keypointrcnn_inference(x, boxes)`
        - Description: No docstring available
      - Function: `keypointrcnn_loss(keypoint_logits, proposals, gt_keypoints, keypoint_matched_idxs)`
        - Description: No docstring available
      - Function: `keypoints_to_heatmap(keypoints, rois, heatmap_size)`
        - Description: No docstring available
      - Function: `maskrcnn_inference(x, labels)`
        - Description: From the results of the CNN, post process the masks by taking the mask corresponding to the class with max probability (which are of fixed size and directly output by the CNN) and return the masks in the mask field of the BoxList. Args: x (Tensor): the mask logits labels (list[BoxList]): bounding boxes that are used as reference, one for ech image Returns: results (list[BoxList]): one BoxList for each image, containing the extra field mask
      - Function: `maskrcnn_loss(mask_logits, proposals, gt_masks, gt_labels, mask_matched_idxs)`
        - Description: Args: proposals (list[BoxList]) mask_logits (Tensor) targets (list[BoxList]) Return: mask_loss (Tensor): scalar tensor containing the loss
      - Function: `paste_mask_in_image(mask, box, im_h, im_w)`
        - Description: No docstring available
      - Function: `paste_masks_in_image(masks, boxes, img_shape, padding=1)`
        - Description: No docstring available
      - Function: `project_masks_on_boxes(gt_masks, boxes, matched_idxs, M)`
        - Description: Given segmentation masks and the bounding boxes corresponding to the location of the masks in the image, this function crops and resizes the masks in the position defined by the boxes. This prepares the masks for them to be fed to the loss computation as the targets.
      - Function: `roi_align(input: torch.Tensor, boxes: Union[torch.Tensor, List[torch.Tensor]], output_size: None, spatial_scale: float = 1.0, sampling_ratio: int = -1, aligned: bool = False) -> torch.Tensor`
        - Description: Performs Region of Interest (RoI) Align operator with average pooling, as described in Mask R-CNN. Args: input (Tensor[N, C, H, W]): The input tensor, i.e. a batch with ``N`` elements. Each element contains ``C`` feature maps of dimensions ``H x W``. If the tensor is quantized, we expect a batch size of ``N == 1``. boxes (Tensor[K, 5] or List[Tensor[L, 4]]): the box coordinates in (x1, y1, x2, y2) format where the regions will be taken from. The coordinate must satisfy ``0 <= x1 < x2`` and ``0 <= y1 < y2``. If a single Tensor is passed, then the first column should contain the index of the corresponding element in the batch, i.e. a number in ``[0, N - 1]``. If a list of Tensors is passed, then each Tensor will correspond to the boxes for an element i in the batch. output_size (int or Tuple[int, int]): the size of the output (in bins or pixels) after the pooling is performed, as (height, width). spatial_scale (float): a scaling factor that maps the box coordinates to the input coordinates. For example, if your boxes are defined on the scale of a 224x224 image and your input is a 112x112 feature map (resulting from a 0.5x scaling of the original image), you'll want to set this to 0.5. Default: 1.0 sampling_ratio (int): number of sampling points in the interpolation grid used to compute the output value of each pooled output bin. If > 0, then exactly ``sampling_ratio x sampling_ratio`` sampling points per bin are used. If <= 0, then an adaptive number of grid points are used (computed as ``ceil(roi_width / output_width)``, and likewise for height). Default: -1 aligned (bool): If False, use the legacy implementation. If True, pixel shift the box coordinates it by -0.5 for a better alignment with the two neighboring pixel indices. This version is used in Detectron2 Returns: Tensor[K, C, output_size[0], output_size[1]]: The pooled RoIs.
    - rpn.py
      - Class: `AnchorGenerator(sizes=((128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),))`
        - Description: Module that generates anchors for a set of feature maps and image sizes. The module support computing anchors at multiple sizes and aspect ratios per feature map. This module assumes aspect ratio = height / width for each anchor. sizes and aspect_ratios should have the same number of elements, and it should correspond to the number of feature maps. sizes[i] and aspect_ratios[i] can have an arbitrary number of elements, and AnchorGenerator will output a set of sizes[i] * aspect_ratios[i] anchors per spatial location for feature map i. Args: sizes (Tuple[Tuple[int]]): aspect_ratios (Tuple[Tuple[float]]):
      - Class: `Conv2dNormActivation(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]] = 3, stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int], str, NoneType] = None, groups: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, dilation: Union[int, Tuple[int, int]] = 1, inplace: Optional[bool] = True, bias: Optional[bool] = None) -> None`
        - Description: Configurable block used for Convolution2d-Normalization-Activation blocks. Args: in_channels (int): Number of channels in the input image out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block kernel_size: (int, optional): Size of the convolving kernel. Default: 3 stride (int, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation`` groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm2d`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` dilation (int): Spacing between kernel elements. Default: 1 inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True`` bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.
      - Class: `ImageList(tensors: torch.Tensor, image_sizes: List[Tuple[int, int]]) -> None`
        - Description: Structure that holds a list of images (of possibly varying sizes) as a single tensor. This works by padding the images to the same size, and storing in a field the original sizes of each image Args: tensors (tensor): Tensor containing images. image_sizes (list[tuple[int, int]]): List of Tuples each containing size of images.
      - Class: `RPNHead(in_channels: int, num_anchors: int, conv_depth=1) -> None`
        - Description: Adds a simple RPN Head with classification and regression heads Args: in_channels (int): number of channels of the input feature num_anchors (int): number of anchors to be predicted conv_depth (int, optional): number of convolutions
      - Class: `RegionProposalNetwork(anchor_generator: torchvision.models.detection.anchor_utils.AnchorGenerator, head: torch.nn.modules.module.Module, fg_iou_thresh: float, bg_iou_thresh: float, batch_size_per_image: int, positive_fraction: float, pre_nms_top_n: Dict[str, int], post_nms_top_n: Dict[str, int], nms_thresh: float, score_thresh: float = 0.0) -> None`
        - Description: Implements Region Proposal Network (RPN). Args: anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature maps. head (nn.Module): module that computes the objectness and regression deltas fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be considered as positive during training of the RPN. bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be considered as negative during training of the RPN. batch_size_per_image (int): number of anchors that are sampled during training of the RPN for computing the loss positive_fraction (float): proportion of positive anchors in a mini-batch during training of the RPN pre_nms_top_n (Dict[str, int]): number of proposals to keep before applying NMS. It should contain two fields: training and testing, to allow for different values depending on training or evaluation post_nms_top_n (Dict[str, int]): number of proposals to keep after applying NMS. It should contain two fields: training and testing, to allow for different values depending on training or evaluation nms_thresh (float): NMS threshold used for postprocessing the RPN proposals
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Function: `concat_box_prediction_layers(box_cls: List[torch.Tensor], box_regression: List[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]`
        - Description: No docstring available
      - Function: `permute_and_flatten(layer: torch.Tensor, N: int, A: int, C: int, H: int, W: int) -> torch.Tensor`
        - Description: No docstring available
    - ssd.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `DefaultBoxGenerator(aspect_ratios: List[List[int]], min_ratio: float = 0.15, max_ratio: float = 0.9, scales: Optional[List[float]] = None, steps: Optional[List[int]] = None, clip: bool = True)`
        - Description: This module generates the default boxes of SSD for a set of feature maps and image sizes. Args: aspect_ratios (List[List[int]]): A list with all the aspect ratios used in each feature map. min_ratio (float): The minimum scale :math:` ext{s}_{ ext{min}}` of the default boxes used in the estimation of the scales of each feature map. It is used only if the ``scales`` parameter is not provided. max_ratio (float): The maximum scale :math:` ext{s}_{ ext{max}}` of the default boxes used in the estimation of the scales of each feature map. It is used only if the ``scales`` parameter is not provided. scales (List[float]], optional): The scales of the default boxes. If not provided it will be estimated using the ``min_ratio`` and ``max_ratio`` parameters. steps (List[int]], optional): It's a hyper-parameter that affects the tiling of default boxes. If not provided it will be estimated from the data. clip (bool): Whether the standardized values of default boxes should be clipped between 0 and 1. The clipping is applied while the boxes are encoded in format ``(cx, cy, w, h)``.
      - Class: `GeneralizedRCNNTransform(min_size: int, max_size: int, image_mean: List[float], image_std: List[float], size_divisible: int = 32, fixed_size: Optional[Tuple[int, int]] = None, **kwargs: Any)`
        - Description: Performs input / target transformation before feeding the data to a GeneralizedRCNN model. The transformations it performs are: - input normalization (mean subtraction and std division) - input / target resizing to match min_size / max_size It returns a ImageList for the inputs, and a List[Dict[Tensor]] for the targets
      - Class: `ObjectDetection(*args, **kwargs) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `OrderedDict(Unable to retrieve signature)`
        - Description: Dictionary that remembers insertion order
      - Class: `SSD(backbone: torch.nn.modules.module.Module, anchor_generator: torchvision.models.detection.anchor_utils.DefaultBoxGenerator, size: Tuple[int, int], num_classes: int, image_mean: Optional[List[float]] = None, image_std: Optional[List[float]] = None, head: Optional[torch.nn.modules.module.Module] = None, score_thresh: float = 0.01, nms_thresh: float = 0.45, detections_per_img: int = 200, iou_thresh: float = 0.5, topk_candidates: int = 400, positive_fraction: float = 0.25, **kwargs: Any)`
        - Description: Implements SSD architecture from `"SSD: Single Shot MultiBox Detector" <https://arxiv.org/abs/1512.02325>`_. The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each image, and should be in 0-1 range. Different images can have different sizes, but they will be resized to a fixed size before passing it to the backbone. The behavior of the model changes depending on if it is in training or evaluation mode. During training, the model expects both the input tensors and targets (list of dictionary), containing: - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (Int64Tensor[N]): the class label for each ground-truth box The model returns a Dict[Tensor] during training, containing the classification and regression losses. During inference, the model requires only the input tensors, and returns the post-processed predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as follows, where ``N`` is the number of detections: - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (Int64Tensor[N]): the predicted labels for each detection - scores (Tensor[N]): the scores for each detection Args: backbone (nn.Module): the network used to compute the features for the model. It should contain an out_channels attribute with the list of the output channels of each feature map. The backbone should return a single Tensor or an OrderedDict[Tensor]. anchor_generator (DefaultBoxGenerator): module that generates the default boxes for a set of feature maps. size (Tuple[int, int]): the width and height to which images will be rescaled before feeding them to the backbone. num_classes (int): number of output classes of the model (including the background). image_mean (Tuple[float, float, float]): mean values used for input normalization. They are generally the mean values of the dataset on which the backbone has been trained on image_std (Tuple[float, float, float]): std values used for input normalization. They are generally the std values of the dataset on which the backbone has been trained on head (nn.Module, optional): Module run on top of the backbone features. Defaults to a module containing a classification and regression module. score_thresh (float): Score threshold used for postprocessing the detections. nms_thresh (float): NMS threshold used for postprocessing the detections. detections_per_img (int): Number of best detections to keep after NMS. iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be considered as positive during training. topk_candidates (int): Number of best detections to keep before NMS. positive_fraction (float): a number between 0 and 1 which indicates the proportion of positive proposals used during the training of the classification head. It is used to estimate the negative to positive ratio.
      - Class: `SSD300_VGG16_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `SSDClassificationHead(in_channels: List[int], num_anchors: List[int], num_classes: int)`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `SSDFeatureExtractorVGG(backbone: torch.nn.modules.module.Module, highres: bool)`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `SSDHead(in_channels: List[int], num_anchors: List[int], num_classes: int)`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `SSDRegressionHead(in_channels: List[int], num_anchors: List[int])`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `SSDScoringHead(module_list: torch.nn.modules.container.ModuleList, num_columns: int)`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Class: `VGG(features: torch.nn.modules.module.Module, num_classes: int = 1000, init_weights: bool = True, dropout: float = 0.5) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `VGG16_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Function: `_log_api_usage_once(obj: Any) -> None`
        - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
      - Function: `_ovewrite_value_param(param: str, actual: Optional[~V], expected: ~V) -> ~V`
        - Description: No docstring available
      - Function: `_validate_trainable_layers(is_trained: bool, trainable_backbone_layers: Optional[int], max_value: int, default_value: int) -> int`
        - Description: No docstring available
      - Function: `_vgg_extractor(backbone: torchvision.models.vgg.VGG, highres: bool, trainable_layers: int)`
        - Description: No docstring available
      - Function: `_xavier_init(conv: torch.nn.modules.module.Module)`
        - Description: No docstring available
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
      - Function: `ssd300_vgg16(*, weights: Optional[torchvision.models.detection.ssd.SSD300_VGG16_Weights] = None, progress: bool = True, num_classes: Optional[int] = None, weights_backbone: Optional[torchvision.models.vgg.VGG16_Weights] = VGG16_Weights.IMAGENET1K_FEATURES, trainable_backbone_layers: Optional[int] = None, **kwargs: Any) -> torchvision.models.detection.ssd.SSD`
        - Description: The SSD300 model is based on the `SSD: Single Shot MultiBox Detector <https://arxiv.org/abs/1512.02325>`_ paper. .. betastatus:: detection module The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each image, and should be in 0-1 range. Different images can have different sizes, but they will be resized to a fixed size before passing it to the backbone. The behavior of the model changes depending on if it is in training or evaluation mode. During training, the model expects both the input tensors and targets (list of dictionary), containing: - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (Int64Tensor[N]): the class label for each ground-truth box The model returns a Dict[Tensor] during training, containing the classification and regression losses. During inference, the model requires only the input tensors, and returns the post-processed predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as follows, where ``N`` is the number of detections: - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (Int64Tensor[N]): the predicted labels for each detection - scores (Tensor[N]): the scores for each detection Example: >>> model = torchvision.models.detection.ssd300_vgg16(weights=SSD300_VGG16_Weights.DEFAULT) >>> model.eval() >>> x = [torch.rand(3, 300, 300), torch.rand(3, 500, 400)] >>> predictions = model(x) Args: weights (:class:`~torchvision.models.detection.SSD300_VGG16_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.detection.SSD300_VGG16_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr Default is True. num_classes (int, optional): number of output classes of the model (including the background) weights_backbone (:class:`~torchvision.models.VGG16_Weights`, optional): The pretrained weights for the backbone trainable_backbone_layers (int, optional): number of trainable (not frozen) layers starting from final block. Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable. If ``None`` is passed (the default) this value is set to 4. **kwargs: parameters passed to the ``torchvision.models.detection.SSD`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/detection/ssd.py>`_ for more details about this class. .. autoclass:: torchvision.models.detection.SSD300_VGG16_Weights :members:
      - Function: `vgg16(*, weights: Optional[torchvision.models.vgg.VGG16_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.vgg.VGG`
        - Description: VGG-16 from `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/abs/1409.1556>`__. Args: weights (:class:`~torchvision.models.VGG16_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.VGG16_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.vgg.VGG`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py>`_ for more details about this class. .. autoclass:: torchvision.models.VGG16_Weights :members:
    - ssdlite.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `Conv2dNormActivation(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]] = 3, stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int], str, NoneType] = None, groups: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, dilation: Union[int, Tuple[int, int]] = 1, inplace: Optional[bool] = True, bias: Optional[bool] = None) -> None`
        - Description: Configurable block used for Convolution2d-Normalization-Activation blocks. Args: in_channels (int): Number of channels in the input image out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block kernel_size: (int, optional): Size of the convolving kernel. Default: 3 stride (int, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation`` groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm2d`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` dilation (int): Spacing between kernel elements. Default: 1 inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True`` bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.
      - Class: `DefaultBoxGenerator(aspect_ratios: List[List[int]], min_ratio: float = 0.15, max_ratio: float = 0.9, scales: Optional[List[float]] = None, steps: Optional[List[int]] = None, clip: bool = True)`
        - Description: This module generates the default boxes of SSD for a set of feature maps and image sizes. Args: aspect_ratios (List[List[int]]): A list with all the aspect ratios used in each feature map. min_ratio (float): The minimum scale :math:` ext{s}_{ ext{min}}` of the default boxes used in the estimation of the scales of each feature map. It is used only if the ``scales`` parameter is not provided. max_ratio (float): The maximum scale :math:` ext{s}_{ ext{max}}` of the default boxes used in the estimation of the scales of each feature map. It is used only if the ``scales`` parameter is not provided. scales (List[float]], optional): The scales of the default boxes. If not provided it will be estimated using the ``min_ratio`` and ``max_ratio`` parameters. steps (List[int]], optional): It's a hyper-parameter that affects the tiling of default boxes. If not provided it will be estimated from the data. clip (bool): Whether the standardized values of default boxes should be clipped between 0 and 1. The clipping is applied while the boxes are encoded in format ``(cx, cy, w, h)``.
      - Class: `MobileNet_V3_Large_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `ObjectDetection(*args, **kwargs) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `OrderedDict(Unable to retrieve signature)`
        - Description: Dictionary that remembers insertion order
      - Class: `SSD(backbone: torch.nn.modules.module.Module, anchor_generator: torchvision.models.detection.anchor_utils.DefaultBoxGenerator, size: Tuple[int, int], num_classes: int, image_mean: Optional[List[float]] = None, image_std: Optional[List[float]] = None, head: Optional[torch.nn.modules.module.Module] = None, score_thresh: float = 0.01, nms_thresh: float = 0.45, detections_per_img: int = 200, iou_thresh: float = 0.5, topk_candidates: int = 400, positive_fraction: float = 0.25, **kwargs: Any)`
        - Description: Implements SSD architecture from `"SSD: Single Shot MultiBox Detector" <https://arxiv.org/abs/1512.02325>`_. The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each image, and should be in 0-1 range. Different images can have different sizes, but they will be resized to a fixed size before passing it to the backbone. The behavior of the model changes depending on if it is in training or evaluation mode. During training, the model expects both the input tensors and targets (list of dictionary), containing: - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (Int64Tensor[N]): the class label for each ground-truth box The model returns a Dict[Tensor] during training, containing the classification and regression losses. During inference, the model requires only the input tensors, and returns the post-processed predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as follows, where ``N`` is the number of detections: - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``. - labels (Int64Tensor[N]): the predicted labels for each detection - scores (Tensor[N]): the scores for each detection Args: backbone (nn.Module): the network used to compute the features for the model. It should contain an out_channels attribute with the list of the output channels of each feature map. The backbone should return a single Tensor or an OrderedDict[Tensor]. anchor_generator (DefaultBoxGenerator): module that generates the default boxes for a set of feature maps. size (Tuple[int, int]): the width and height to which images will be rescaled before feeding them to the backbone. num_classes (int): number of output classes of the model (including the background). image_mean (Tuple[float, float, float]): mean values used for input normalization. They are generally the mean values of the dataset on which the backbone has been trained on image_std (Tuple[float, float, float]): std values used for input normalization. They are generally the std values of the dataset on which the backbone has been trained on head (nn.Module, optional): Module run on top of the backbone features. Defaults to a module containing a classification and regression module. score_thresh (float): Score threshold used for postprocessing the detections. nms_thresh (float): NMS threshold used for postprocessing the detections. detections_per_img (int): Number of best detections to keep after NMS. iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be considered as positive during training. topk_candidates (int): Number of best detections to keep before NMS. positive_fraction (float): a number between 0 and 1 which indicates the proportion of positive proposals used during the training of the classification head. It is used to estimate the negative to positive ratio.
      - Class: `SSDLite320_MobileNet_V3_Large_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `SSDLiteClassificationHead(in_channels: List[int], num_anchors: List[int], num_classes: int, norm_layer: Callable[..., torch.nn.modules.module.Module])`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `SSDLiteFeatureExtractorMobileNet(backbone: torch.nn.modules.module.Module, c4_pos: int, norm_layer: Callable[..., torch.nn.modules.module.Module], width_mult: float = 1.0, min_depth: int = 16)`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `SSDLiteHead(in_channels: List[int], num_anchors: List[int], num_classes: int, norm_layer: Callable[..., torch.nn.modules.module.Module])`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `SSDLiteRegressionHead(in_channels: List[int], num_anchors: List[int], norm_layer: Callable[..., torch.nn.modules.module.Module])`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `SSDScoringHead(module_list: torch.nn.modules.container.ModuleList, num_columns: int)`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Function: `_extra_block(in_channels: int, out_channels: int, norm_layer: Callable[..., torch.nn.modules.module.Module]) -> torch.nn.modules.container.Sequential`
        - Description: No docstring available
      - Function: `_log_api_usage_once(obj: Any) -> None`
        - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
      - Function: `_mobilenet_extractor(backbone: Union[torchvision.models.mobilenetv2.MobileNetV2, torchvision.models.mobilenetv3.MobileNetV3], trainable_layers: int, norm_layer: Callable[..., torch.nn.modules.module.Module])`
        - Description: No docstring available
      - Function: `_normal_init(conv: torch.nn.modules.module.Module)`
        - Description: No docstring available
      - Function: `_ovewrite_value_param(param: str, actual: Optional[~V], expected: ~V) -> ~V`
        - Description: No docstring available
      - Function: `_prediction_block(in_channels: int, out_channels: int, kernel_size: int, norm_layer: Callable[..., torch.nn.modules.module.Module]) -> torch.nn.modules.container.Sequential`
        - Description: No docstring available
      - Function: `_validate_trainable_layers(is_trained: bool, trainable_backbone_layers: Optional[int], max_value: int, default_value: int) -> int`
        - Description: No docstring available
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Function: `mobilenet_v3_large(*, weights: Optional[torchvision.models.mobilenetv3.MobileNet_V3_Large_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.mobilenetv3.MobileNetV3`
        - Description: Constructs a large MobileNetV3 architecture from `Searching for MobileNetV3 <https://arxiv.org/abs/1905.02244>`__. Args: weights (:class:`~torchvision.models.MobileNet_V3_Large_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.MobileNet_V3_Large_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.mobilenet.MobileNetV3`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv3.py>`_ for more details about this class. .. autoclass:: torchvision.models.MobileNet_V3_Large_Weights :members:
      - Class: `partial(Unable to retrieve signature)`
        - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
      - Function: `ssdlite320_mobilenet_v3_large(*, weights: Optional[torchvision.models.detection.ssdlite.SSDLite320_MobileNet_V3_Large_Weights] = None, progress: bool = True, num_classes: Optional[int] = None, weights_backbone: Optional[torchvision.models.mobilenetv3.MobileNet_V3_Large_Weights] = MobileNet_V3_Large_Weights.IMAGENET1K_V1, trainable_backbone_layers: Optional[int] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None, **kwargs: Any) -> torchvision.models.detection.ssd.SSD`
        - Description: SSDlite model architecture with input size 320x320 and a MobileNetV3 Large backbone, as described at `Searching for MobileNetV3 <https://arxiv.org/abs/1905.02244>`__ and `MobileNetV2: Inverted Residuals and Linear Bottlenecks <https://arxiv.org/abs/1801.04381>`__. .. betastatus:: detection module See :func:`~torchvision.models.detection.ssd300_vgg16` for more details. Example: >>> model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=SSDLite320_MobileNet_V3_Large_Weights.DEFAULT) >>> model.eval() >>> x = [torch.rand(3, 320, 320), torch.rand(3, 500, 400)] >>> predictions = model(x) Args: weights (:class:`~torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. num_classes (int, optional): number of output classes of the model (including the background). weights_backbone (:class:`~torchvision.models.MobileNet_V3_Large_Weights`, optional): The pretrained weights for the backbone. trainable_backbone_layers (int, optional): number of trainable (not frozen) layers starting from final block. Valid values are between 0 and 6, with 6 meaning all backbone layers are trainable. If ``None`` is passed (the default) this value is set to 6. norm_layer (callable, optional): Module specifying the normalization layer to use. **kwargs: parameters passed to the ``torchvision.models.detection.ssd.SSD`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/detection/ssd.py>`_ for more details about this class. .. autoclass:: torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights :members:
    - transform.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `GeneralizedRCNNTransform(min_size: int, max_size: int, image_mean: List[float], image_std: List[float], size_divisible: int = 32, fixed_size: Optional[Tuple[int, int]] = None, **kwargs: Any)`
        - Description: Performs input / target transformation before feeding the data to a GeneralizedRCNN model. The transformations it performs are: - input normalization (mean subtraction and std division) - input / target resizing to match min_size / max_size It returns a ImageList for the inputs, and a List[Dict[Tensor]] for the targets
      - Class: `ImageList(tensors: torch.Tensor, image_sizes: List[Tuple[int, int]]) -> None`
        - Description: Structure that holds a list of images (of possibly varying sizes) as a single tensor. This works by padding the images to the same size, and storing in a field the original sizes of each image Args: tensors (tensor): Tensor containing images. image_sizes (list[tuple[int, int]]): List of Tuples each containing size of images.
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Function: `_fake_cast_onnx(v: torch.Tensor) -> float`
        - Description: No docstring available
      - Function: `_get_shape_onnx(image: torch.Tensor) -> torch.Tensor`
        - Description: No docstring available
      - Function: `_resize_image_and_masks(image: torch.Tensor, self_min_size: int, self_max_size: int, target: Optional[Dict[str, torch.Tensor]] = None, fixed_size: Optional[Tuple[int, int]] = None) -> Tuple[torch.Tensor, Optional[Dict[str, torch.Tensor]]]`
        - Description: No docstring available
      - Function: `paste_masks_in_image(masks, boxes, img_shape, padding=1)`
        - Description: No docstring available
      - Function: `resize_boxes(boxes: torch.Tensor, original_size: List[int], new_size: List[int]) -> torch.Tensor`
        - Description: No docstring available
      - Function: `resize_keypoints(keypoints: torch.Tensor, original_size: List[int], new_size: List[int]) -> torch.Tensor`
        - Description: No docstring available
    - _utils.py
      - Class: `BalancedPositiveNegativeSampler(batch_size_per_image: int, positive_fraction: float) -> None`
        - Description: This class samples batches, ensuring that they contain a fixed proportion of positives
      - Class: `BoxCoder(weights: Tuple[float, float, float, float], bbox_xform_clip: float = 4.135166556742356) -> None`
        - Description: This class encodes and decodes a set of bounding boxes into the representation used for training the regressors.
      - Class: `BoxLinearCoder(normalize_by_size: bool = True) -> None`
        - Description: The linear box-to-box transform defined in FCOS. The transformation is parameterized by the distance from the center of (square) src box to 4 edges of the target box.
      - Class: `FrozenBatchNorm2d(num_features: int, eps: float = 1e-05)`
        - Description: BatchNorm2d where the batch statistics and the affine parameters are fixed Args: num_features (int): Number of features ``C`` from an expected input of size ``(N, C, H, W)`` eps (float): a value added to the denominator for numerical stability. Default: 1e-5
      - Class: `Matcher(high_threshold: float, low_threshold: float, allow_low_quality_matches: bool = False) -> None`
        - Description: This class assigns to each predicted "element" (e.g., a box) a ground-truth element. Each predicted element will have exactly zero or one matches; each ground-truth element may be assigned to zero or more predicted elements. Matching is based on the MxN match_quality_matrix, that characterizes how well each (ground-truth, predicted)-pair match. For example, if the elements are boxes, the matrix may contain box IoU overlap values. The matcher returns a tensor of size N containing the index of the ground-truth element m that matches to prediction n. If there is no match, a negative value is returned.
      - Class: `OrderedDict(Unable to retrieve signature)`
        - Description: Dictionary that remembers insertion order
      - Class: `SSDMatcher(threshold: float) -> None`
        - Description: This class assigns to each predicted "element" (e.g., a box) a ground-truth element. Each predicted element will have exactly zero or one matches; each ground-truth element may be assigned to zero or more predicted elements. Matching is based on the MxN match_quality_matrix, that characterizes how well each (ground-truth, predicted)-pair match. For example, if the elements are boxes, the matrix may contain box IoU overlap values. The matcher returns a tensor of size N containing the index of the ground-truth element m that matches to prediction n. If there is no match, a negative value is returned.
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Function: `_box_loss(type: str, box_coder: torchvision.models.detection._utils.BoxCoder, anchors_per_image: torch.Tensor, matched_gt_boxes_per_image: torch.Tensor, bbox_regression_per_image: torch.Tensor, cnf: Optional[Dict[str, float]] = None) -> torch.Tensor`
        - Description: No docstring available
      - Function: `_fake_cast_onnx(v: torch.Tensor) -> int`
        - Description: No docstring available
      - Function: `_topk_min(input: torch.Tensor, orig_kval: int, axis: int) -> int`
        - Description: ONNX spec requires the k-value to be less than or equal to the number of inputs along provided dim. Certain models use the number of elements along a particular axis instead of K if K exceeds the number of elements along that axis. Previously, python's min() function was used to determine whether to use the provided k-value or the specified dim axis value. However, in cases where the model is being exported in tracing mode, python min() is static causing the model to be traced incorrectly and eventually fail at the topk node. In order to avoid this situation, in tracing mode, torch.min() is used instead. Args: input (Tensor): The original input tensor. orig_kval (int): The provided k-value. axis(int): Axis along which we retrieve the input size. Returns: min_kval (int): Appropriately selected k-value.
      - Function: `complete_box_iou_loss(boxes1: torch.Tensor, boxes2: torch.Tensor, reduction: str = 'none', eps: float = 1e-07) -> torch.Tensor`
        - Description: Gradient-friendly IoU loss with an additional penalty that is non-zero when the boxes do not overlap. This loss function considers important geometrical factors such as overlap area, normalized central point distance and aspect ratio. This loss is symmetric, so the boxes1 and boxes2 arguments are interchangeable. Both sets of boxes are expected to be in ``(x1, y1, x2, y2)`` format with ``0 <= x1 < x2`` and ``0 <= y1 < y2``, and The two boxes should have the same dimensions. Args: boxes1 : (Tensor[N, 4] or Tensor[4]) first set of boxes boxes2 : (Tensor[N, 4] or Tensor[4]) second set of boxes reduction : (string, optional) Specifies the reduction to apply to the output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: No reduction will be applied to the output. ``'mean'``: The output will be averaged. ``'sum'``: The output will be summed. Default: ``'none'`` eps : (float): small number to prevent division by zero. Default: 1e-7 Returns: Tensor: Loss tensor with the reduction option applied. Reference: Zhaohui Zheng et al.: Complete Intersection over Union Loss: https://arxiv.org/abs/1911.08287
      - Function: `distance_box_iou_loss(boxes1: torch.Tensor, boxes2: torch.Tensor, reduction: str = 'none', eps: float = 1e-07) -> torch.Tensor`
        - Description: Gradient-friendly IoU loss with an additional penalty that is non-zero when the distance between boxes' centers isn't zero. Indeed, for two exactly overlapping boxes, the distance IoU is the same as the IoU loss. This loss is symmetric, so the boxes1 and boxes2 arguments are interchangeable. Both sets of boxes are expected to be in ``(x1, y1, x2, y2)`` format with ``0 <= x1 < x2`` and ``0 <= y1 < y2``, and The two boxes should have the same dimensions. Args: boxes1 (Tensor[N, 4]): first set of boxes boxes2 (Tensor[N, 4]): second set of boxes reduction (string, optional): Specifies the reduction to apply to the output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: No reduction will be applied to the output. ``'mean'``: The output will be averaged. ``'sum'``: The output will be summed. Default: ``'none'`` eps (float, optional): small number to prevent division by zero. Default: 1e-7 Returns: Tensor: Loss tensor with the reduction option applied. Reference: Zhaohui Zheng et al.: Distance Intersection over Union Loss: https://arxiv.org/abs/1911.08287
      - Function: `encode_boxes(reference_boxes: torch.Tensor, proposals: torch.Tensor, weights: torch.Tensor) -> torch.Tensor`
        - Description: Encode a set of proposals with respect to some reference boxes Args: reference_boxes (Tensor): reference boxes proposals (Tensor): boxes to be encoded weights (Tensor[4]): the weights for ``(x, y, w, h)``
      - Function: `generalized_box_iou_loss(boxes1: torch.Tensor, boxes2: torch.Tensor, reduction: str = 'none', eps: float = 1e-07) -> torch.Tensor`
        - Description: Gradient-friendly IoU loss with an additional penalty that is non-zero when the boxes do not overlap and scales with the size of their smallest enclosing box. This loss is symmetric, so the boxes1 and boxes2 arguments are interchangeable. Both sets of boxes are expected to be in ``(x1, y1, x2, y2)`` format with ``0 <= x1 < x2`` and ``0 <= y1 < y2``, and The two boxes should have the same dimensions. Args: boxes1 (Tensor[N, 4] or Tensor[4]): first set of boxes boxes2 (Tensor[N, 4] or Tensor[4]): second set of boxes reduction (string, optional): Specifies the reduction to apply to the output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: No reduction will be applied to the output. ``'mean'``: The output will be averaged. ``'sum'``: The output will be summed. Default: ``'none'`` eps (float): small number to prevent division by zero. Default: 1e-7 Returns: Tensor: Loss tensor with the reduction option applied. Reference: Hamid Rezatofighi et al.: Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression: https://arxiv.org/abs/1902.09630
      - Function: `overwrite_eps(model: torch.nn.modules.module.Module, eps: float) -> None`
        - Description: This method overwrites the default eps values of all the FrozenBatchNorm2d layers of the model with the provided value. This is necessary to address the BC-breaking change introduced by the bug-fix at pytorch/vision#2933. The overwrite is applied only when the pretrained weights are loaded to maintain compatibility with previous versions. Args: model (nn.Module): The model on which we perform the overwrite. eps (float): The new value of eps.
      - Function: `retrieve_out_channels(model: torch.nn.modules.module.Module, size: Tuple[int, int]) -> List[int]`
        - Description: This method retrieves the number of output channels of a specific model. Args: model (nn.Module): The model for which we estimate the out_channels. It should return a single Tensor or an OrderedDict[Tensor]. size (Tuple[int, int]): The size (wxh) of the input. Returns: out_channels (List[int]): A list of the output channels of the model.
  - optical_flow/
    - __pycache__/
    - raft.py
      - Class: `BatchNorm2d(num_features: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = True, track_running_stats: bool = True, device=None, dtype=None) -> None`
        - Description: Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper `Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ . .. math:: y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta The mean and standard-deviation are calculated per-dimension over the mini-batches and :math:`\gamma` and :math:`\beta` are learnable parameter vectors of size `C` (where `C` is the input size). By default, the elements of :math:`\gamma` are set to 1 and the elements of :math:`\beta` are set to 0. At train time in the forward pass, the standard-deviation is calculated via the biased estimator, equivalent to ``torch.var(input, unbiased=False)``. However, the value stored in the moving average of the standard-deviation is calculated via the unbiased estimator, equivalent to ``torch.var(input, unbiased=True)``. Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default :attr:`momentum` of 0.1. If :attr:`track_running_stats` is set to ``False``, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well. .. note:: This :attr:`momentum` argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is :math:`\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t`, where :math:`\hat{x}` is the estimated statistic and :math:`x_t` is the new observed value. Because the Batch Normalization is done over the `C` dimension, computing statistics on `(N, H, W)` slices, it's common terminology to call this Spatial Batch Normalization. Args: num_features: :math:`C` from an expected input of size :math:`(N, C, H, W)` eps: a value added to the denominator for numerical stability. Default: 1e-5 momentum: the value used for the running_mean and running_var computation. Can be set to ``None`` for cumulative moving average (i.e. simple average). Default: 0.1 affine: a boolean value that when set to ``True``, this module has learnable affine parameters. Default: ``True`` track_running_stats: a boolean value that when set to ``True``, this module tracks the running mean and variance, and when set to ``False``, this module does not track such statistics, and initializes statistics buffers :attr:`running_mean` and :attr:`running_var` as ``None``. When these buffers are ``None``, this module always uses batch statistics. in both training and eval modes. Default: ``True`` Shape: - Input: :math:`(N, C, H, W)` - Output: :math:`(N, C, H, W)` (same shape as input) Examples:: >>> # With Learnable Parameters >>> m = nn.BatchNorm2d(100) >>> # Without Learnable Parameters >>> m = nn.BatchNorm2d(100, affine=False) >>> input = torch.randn(20, 100, 35, 45) >>> output = m(input)
      - Class: `BottleneckBlock(in_channels, out_channels, *, norm_layer, stride=1)`
        - Description: Slightly modified BottleNeck block (extra relu and biases)
      - Class: `Conv2dNormActivation(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]] = 3, stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int], str, NoneType] = None, groups: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, dilation: Union[int, Tuple[int, int]] = 1, inplace: Optional[bool] = True, bias: Optional[bool] = None) -> None`
        - Description: Configurable block used for Convolution2d-Normalization-Activation blocks. Args: in_channels (int): Number of channels in the input image out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block kernel_size: (int, optional): Size of the convolving kernel. Default: 3 stride (int, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation`` groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm2d`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` dilation (int): Spacing between kernel elements. Default: 1 inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True`` bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.
      - Class: `ConvGRU(*, input_size, hidden_size, kernel_size, padding)`
        - Description: Convolutional Gru unit.
      - Class: `CorrBlock(*, num_levels: int = 4, radius: int = 4)`
        - Description: The correlation block. Creates a correlation pyramid with ``num_levels`` levels from the outputs of the feature encoder, and then indexes from this pyramid to create correlation features. The "indexing" of a given centroid pixel x' is done by concatenating its surrounding neighbors that are within a ``radius``, according to the infinity norm (see paper section 3.2). Note: typo in the paper, it should be infinity norm, not 1-norm.
      - Class: `FeatureEncoder(*, block=<class 'torchvision.models.optical_flow.raft.ResidualBlock'>, layers=(64, 64, 96, 128, 256), strides=(2, 1, 2, 2), norm_layer=<class 'torch.nn.modules.batchnorm.BatchNorm2d'>)`
        - Description: The feature encoder, used both as the actual feature encoder, and as the context encoder. It must downsample its input by 8.
      - Class: `FlowHead(*, in_channels, hidden_size)`
        - Description: Flow head, part of the update block. Takes the hidden state of the recurrent unit as input, and outputs the predicted "delta flow".
      - Class: `InstanceNorm2d(num_features: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = False, track_running_stats: bool = False, device=None, dtype=None) -> None`
        - Description: Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper `Instance Normalization: The Missing Ingredient for Fast Stylization <https://arxiv.org/abs/1607.08022>`__. .. math:: y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. :math:`\gamma` and :math:`\beta` are learnable parameter vectors of size `C` (where `C` is the input size) if :attr:`affine` is ``True``. The standard-deviation is calculated via the biased estimator, equivalent to `torch.var(input, unbiased=False)`. By default, this layer uses instance statistics computed from input data in both training and evaluation modes. If :attr:`track_running_stats` is set to ``True``, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default :attr:`momentum` of 0.1. .. note:: This :attr:`momentum` argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is :math:`\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t`, where :math:`\hat{x}` is the estimated statistic and :math:`x_t` is the new observed value. .. note:: :class:`InstanceNorm2d` and :class:`LayerNorm` are very similar, but have some subtle differences. :class:`InstanceNorm2d` is applied on each channel of channeled data like RGB images, but :class:`LayerNorm` is usually applied on entire sample and often in NLP tasks. Additionally, :class:`LayerNorm` applies elementwise affine transform, while :class:`InstanceNorm2d` usually don't apply affine transform. Args: num_features: :math:`C` from an expected input of size :math:`(N, C, H, W)` or :math:`(C, H, W)` eps: a value added to the denominator for numerical stability. Default: 1e-5 momentum: the value used for the running_mean and running_var computation. Default: 0.1 affine: a boolean value that when set to ``True``, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: ``False``. track_running_stats: a boolean value that when set to ``True``, this module tracks the running mean and variance, and when set to ``False``, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: ``False`` Shape: - Input: :math:`(N, C, H, W)` or :math:`(C, H, W)` - Output: :math:`(N, C, H, W)` or :math:`(C, H, W)` (same shape as input) Examples:: >>> # Without Learnable Parameters >>> m = nn.InstanceNorm2d(100) >>> # With Learnable Parameters >>> m = nn.InstanceNorm2d(100, affine=True) >>> input = torch.randn(20, 100, 35, 45) >>> output = m(input)
      - Class: `MaskPredictor(*, in_channels, hidden_size, multiplier=0.25)`
        - Description: Mask predictor to be used when upsampling the predicted flow. It takes the hidden state of the recurrent unit as input and outputs the mask. This is not used in the raft-small model.
      - Class: `MotionEncoder(*, in_channels_corr, corr_layers=(256, 192), flow_layers=(128, 64), out_channels=128)`
        - Description: The motion encoder, part of the update block. Takes the current predicted flow and the correlation features as input and returns an encoded version of these.
      - Class: `OpticalFlow(*args, **kwargs) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `RAFT(*, feature_encoder, context_encoder, corr_block, update_block, mask_predictor=None)`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Raft_Large_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: The metrics reported here are as follows. ``epe`` is the "end-point-error" and indicates how far (in pixels) the predicted flow is from its true value. This is averaged over all pixels of all images. ``per_image_epe`` is similar, but the average is different: the epe is first computed on each image independently, and then averaged over all images. This corresponds to "Fl-epe" (sometimes written "F1-epe") in the original paper, and it's only used on Kitti. ``fl-all`` is also a Kitti-specific metric, defined by the author of the dataset and used for the Kitti leaderboard. It corresponds to the average of pixels whose epe is either <3px, or <5% of flow's 2-norm.
      - Class: `Raft_Small_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: The metrics reported here are as follows. ``epe`` is the "end-point-error" and indicates how far (in pixels) the predicted flow is from its true value. This is averaged over all pixels of all images. ``per_image_epe`` is similar, but the average is different: the epe is first computed on each image independently, and then averaged over all images. This corresponds to "Fl-epe" (sometimes written "F1-epe") in the original paper, and it's only used on Kitti. ``fl-all`` is also a Kitti-specific metric, defined by the author of the dataset and used for the Kitti leaderboard. It corresponds to the average of pixels whose epe is either <3px, or <5% of flow's 2-norm.
      - Class: `RecurrentBlock(*, input_size, hidden_size, kernel_size=((1, 5), (5, 1)), padding=((0, 2), (2, 0)))`
        - Description: Recurrent block, part of the update block. Takes the current hidden state and the concatenation of (motion encoder output, context) as input. Returns an updated hidden state.
      - Class: `ResidualBlock(in_channels, out_channels, *, norm_layer, stride=1, always_project: bool = False)`
        - Description: Slightly modified Residual block with extra relu and biases.
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Class: `UpdateBlock(*, motion_encoder, recurrent_block, flow_head)`
        - Description: The update block which contains the motion encoder, the recurrent block, and the flow head. It must expose a ``hidden_state_size`` attribute which is the hidden state size of its recurrent block.
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Function: `_log_api_usage_once(obj: Any) -> None`
        - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
      - Function: `_pass_through_h(h, _)`
        - Description: No docstring available
      - Function: `_raft(*, weights=None, progress=False, feature_encoder_layers, feature_encoder_block, feature_encoder_norm_layer, context_encoder_layers, context_encoder_block, context_encoder_norm_layer, corr_block_num_levels, corr_block_radius, motion_encoder_corr_layers, motion_encoder_flow_layers, motion_encoder_out_channels, recurrent_block_hidden_state_size, recurrent_block_kernel_size, recurrent_block_padding, flow_head_hidden_size, use_mask_predictor, **kwargs)`
        - Description: No docstring available
      - Function: `grid_sample(img: torch.Tensor, absolute_grid: torch.Tensor, mode: str = 'bilinear', align_corners: Optional[bool] = None)`
        - Description: Same as torch's grid_sample, with absolute pixel coordinates instead of normalized coordinates.
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Function: `make_coords_grid(batch_size: int, h: int, w: int, device: str = 'cpu')`
        - Description: No docstring available
      - Function: `raft_large(*, weights: Optional[torchvision.models.optical_flow.raft.Raft_Large_Weights] = None, progress=True, **kwargs) -> torchvision.models.optical_flow.raft.RAFT`
        - Description: RAFT model from `RAFT: Recurrent All Pairs Field Transforms for Optical Flow <https://arxiv.org/abs/2003.12039>`_. Please see the example below for a tutorial on how to use this model. Args: weights(:class:`~torchvision.models.optical_flow.Raft_Large_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.optical_flow.Raft_Large_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.optical_flow.RAFT`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/optical_flow/raft.py>`_ for more details about this class. .. autoclass:: torchvision.models.optical_flow.Raft_Large_Weights :members:
      - Function: `raft_small(*, weights: Optional[torchvision.models.optical_flow.raft.Raft_Small_Weights] = None, progress=True, **kwargs) -> torchvision.models.optical_flow.raft.RAFT`
        - Description: RAFT "small" model from `RAFT: Recurrent All Pairs Field Transforms for Optical Flow <https://arxiv.org/abs/2003.12039>`__. Please see the example below for a tutorial on how to use this model. Args: weights(:class:`~torchvision.models.optical_flow.Raft_Small_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.optical_flow.Raft_Small_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.optical_flow.RAFT`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/optical_flow/raft.py>`_ for more details about this class. .. autoclass:: torchvision.models.optical_flow.Raft_Small_Weights :members:
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
      - Function: `upsample_flow(flow, up_mask: Optional[torch.Tensor] = None, factor: int = 8)`
        - Description: Upsample flow by the input factor (default 8). If up_mask is None we just interpolate. If up_mask is specified, we upsample using a convex combination of its weights. See paper page 8 and appendix B. Note that in appendix B the picture assumes a downsample factor of 4 instead of 8.
    - _utils.py
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Function: `grid_sample(img: torch.Tensor, absolute_grid: torch.Tensor, mode: str = 'bilinear', align_corners: Optional[bool] = None)`
        - Description: Same as torch's grid_sample, with absolute pixel coordinates instead of normalized coordinates.
      - Function: `make_coords_grid(batch_size: int, h: int, w: int, device: str = 'cpu')`
        - Description: No docstring available
      - Function: `upsample_flow(flow, up_mask: Optional[torch.Tensor] = None, factor: int = 8)`
        - Description: Upsample flow by the input factor (default 8). If up_mask is None we just interpolate. If up_mask is specified, we upsample using a convex combination of its weights. See paper page 8 and appendix B. Note that in appendix B the picture assumes a downsample factor of 4 instead of 8.
  - quantization/
    - __pycache__/
    - googlenet.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `BasicConv2d(in_channels: int, out_channels: int, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `GoogLeNet(num_classes: int = 1000, aux_logits: bool = True, transform_input: bool = False, init_weights: Optional[bool] = None, blocks: Optional[List[Callable[..., torch.nn.modules.module.Module]]] = None, dropout: float = 0.2, dropout_aux: float = 0.7) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `GoogLeNetOutputs(logits, aux_logits2, aux_logits1)`
        - Description: GoogLeNetOutputs(logits, aux_logits2, aux_logits1)
      - Class: `GoogLeNet_QuantizedWeights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `GoogLeNet_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Inception(in_channels: int, ch1x1: int, ch3x3red: int, ch3x3: int, ch5x5red: int, ch5x5: int, pool_proj: int, conv_block: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `InceptionAux(in_channels: int, num_classes: int, conv_block: Optional[Callable[..., torch.nn.modules.module.Module]] = None, dropout: float = 0.7) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `QuantizableBasicConv2d(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `QuantizableGoogLeNet(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `QuantizableInception(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `QuantizableInceptionAux(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Function: `_fuse_modules(model: torch.nn.modules.module.Module, modules_to_fuse: Union[List[str], List[List[str]]], is_qat: Optional[bool], **kwargs: Any)`
        - Description: No docstring available
      - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
        - Description: No docstring available
      - Function: `_replace_relu(module: torch.nn.modules.module.Module) -> None`
        - Description: No docstring available
      - Function: `googlenet(*, weights: Union[torchvision.models.quantization.googlenet.GoogLeNet_QuantizedWeights, torchvision.models.googlenet.GoogLeNet_Weights, NoneType] = None, progress: bool = True, quantize: bool = False, **kwargs: Any) -> torchvision.models.quantization.googlenet.QuantizableGoogLeNet`
        - Description: GoogLeNet (Inception v1) model architecture from `Going Deeper with Convolutions <http://arxiv.org/abs/1409.4842>`__. .. note:: Note that ``quantize = True`` returns a quantized model with 8 bit weights. Quantized models only support inference and run on CPUs. GPU inference is not yet supported. Args: weights (:class:`~torchvision.models.quantization.GoogLeNet_QuantizedWeights` or :class:`~torchvision.models.GoogLeNet_Weights`, optional): The pretrained weights for the model. See :class:`~torchvision.models.quantization.GoogLeNet_QuantizedWeights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. quantize (bool, optional): If True, return a quantized version of the model. Default is False. **kwargs: parameters passed to the ``torchvision.models.quantization.QuantizableGoogLeNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/googlenet.py>`_ for more details about this class. .. autoclass:: torchvision.models.quantization.GoogLeNet_QuantizedWeights :members: .. autoclass:: torchvision.models.GoogLeNet_Weights :members: :noindex:
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Class: `partial(Unable to retrieve signature)`
        - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
      - Function: `quantize_model(model: torch.nn.modules.module.Module, backend: str) -> None`
        - Description: No docstring available
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
    - inception.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `InceptionOutputs(logits, aux_logits)`
        - Description: InceptionOutputs(logits, aux_logits)
      - Class: `Inception_V3_QuantizedWeights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `Inception_V3_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `QuantizableBasicConv2d(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `QuantizableInception3(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `QuantizableInceptionA(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `QuantizableInceptionAux(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `QuantizableInceptionB(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `QuantizableInceptionC(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `QuantizableInceptionD(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `QuantizableInceptionE(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Function: `_fuse_modules(model: torch.nn.modules.module.Module, modules_to_fuse: Union[List[str], List[List[str]]], is_qat: Optional[bool], **kwargs: Any)`
        - Description: No docstring available
      - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
        - Description: No docstring available
      - Function: `_replace_relu(module: torch.nn.modules.module.Module) -> None`
        - Description: No docstring available
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Function: `inception_v3(*, weights: Union[torchvision.models.quantization.inception.Inception_V3_QuantizedWeights, torchvision.models.inception.Inception_V3_Weights, NoneType] = None, progress: bool = True, quantize: bool = False, **kwargs: Any) -> torchvision.models.quantization.inception.QuantizableInception3`
        - Description: Inception v3 model architecture from `Rethinking the Inception Architecture for Computer Vision <http://arxiv.org/abs/1512.00567>`__. .. note:: **Important**: In contrast to the other models the inception_v3 expects tensors with a size of N x 3 x 299 x 299, so ensure your images are sized accordingly. .. note:: Note that ``quantize = True`` returns a quantized model with 8 bit weights. Quantized models only support inference and run on CPUs. GPU inference is not yet supported. Args: weights (:class:`~torchvision.models.quantization.Inception_V3_QuantizedWeights` or :class:`~torchvision.models.Inception_V3_Weights`, optional): The pretrained weights for the model. See :class:`~torchvision.models.quantization.Inception_V3_QuantizedWeights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. quantize (bool, optional): If True, return a quantized version of the model. Default is False. **kwargs: parameters passed to the ``torchvision.models.quantization.QuantizableInception3`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/inception.py>`_ for more details about this class. .. autoclass:: torchvision.models.quantization.Inception_V3_QuantizedWeights :members: .. autoclass:: torchvision.models.Inception_V3_Weights :members: :noindex:
      - Class: `partial(Unable to retrieve signature)`
        - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
      - Function: `quantize_model(model: torch.nn.modules.module.Module, backend: str) -> None`
        - Description: No docstring available
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
    - mobilenet.py
      - Class: `MobileNet_V2_QuantizedWeights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `MobileNet_V3_Large_QuantizedWeights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `QuantizableMobileNetV2(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `QuantizableMobileNetV3(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Function: `mobilenet_v2(*, weights: Union[torchvision.models.quantization.mobilenetv2.MobileNet_V2_QuantizedWeights, torchvision.models.mobilenetv2.MobileNet_V2_Weights, NoneType] = None, progress: bool = True, quantize: bool = False, **kwargs: Any) -> torchvision.models.quantization.mobilenetv2.QuantizableMobileNetV2`
        - Description: Constructs a MobileNetV2 architecture from `MobileNetV2: Inverted Residuals and Linear Bottlenecks <https://arxiv.org/abs/1801.04381>`_. .. note:: Note that ``quantize = True`` returns a quantized model with 8 bit weights. Quantized models only support inference and run on CPUs. GPU inference is not yet supported. Args: weights (:class:`~torchvision.models.quantization.MobileNet_V2_QuantizedWeights` or :class:`~torchvision.models.MobileNet_V2_Weights`, optional): The pretrained weights for the model. See :class:`~torchvision.models.quantization.MobileNet_V2_QuantizedWeights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. quantize (bool, optional): If True, returns a quantized version of the model. Default is False. **kwargs: parameters passed to the ``torchvision.models.quantization.QuantizableMobileNetV2`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/mobilenetv2.py>`_ for more details about this class. .. autoclass:: torchvision.models.quantization.MobileNet_V2_QuantizedWeights :members: .. autoclass:: torchvision.models.MobileNet_V2_Weights :members: :noindex:
      - Function: `mobilenet_v3_large(*, weights: Union[torchvision.models.quantization.mobilenetv3.MobileNet_V3_Large_QuantizedWeights, torchvision.models.mobilenetv3.MobileNet_V3_Large_Weights, NoneType] = None, progress: bool = True, quantize: bool = False, **kwargs: Any) -> torchvision.models.quantization.mobilenetv3.QuantizableMobileNetV3`
        - Description: MobileNetV3 (Large) model from `Searching for MobileNetV3 <https://arxiv.org/abs/1905.02244>`_. .. note:: Note that ``quantize = True`` returns a quantized model with 8 bit weights. Quantized models only support inference and run on CPUs. GPU inference is not yet supported. Args: weights (:class:`~torchvision.models.quantization.MobileNet_V3_Large_QuantizedWeights` or :class:`~torchvision.models.MobileNet_V3_Large_Weights`, optional): The pretrained weights for the model. See :class:`~torchvision.models.quantization.MobileNet_V3_Large_QuantizedWeights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool): If True, displays a progress bar of the download to stderr. Default is True. quantize (bool): If True, return a quantized version of the model. Default is False. **kwargs: parameters passed to the ``torchvision.models.quantization.MobileNet_V3_Large_QuantizedWeights`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/mobilenetv3.py>`_ for more details about this class. .. autoclass:: torchvision.models.quantization.MobileNet_V3_Large_QuantizedWeights :members: .. autoclass:: torchvision.models.MobileNet_V3_Large_Weights :members: :noindex:
    - mobilenetv2.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `Conv2dNormActivation(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]] = 3, stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int], str, NoneType] = None, groups: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, dilation: Union[int, Tuple[int, int]] = 1, inplace: Optional[bool] = True, bias: Optional[bool] = None) -> None`
        - Description: Configurable block used for Convolution2d-Normalization-Activation blocks. Args: in_channels (int): Number of channels in the input image out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block kernel_size: (int, optional): Size of the convolving kernel. Default: 3 stride (int, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation`` groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm2d`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` dilation (int): Spacing between kernel elements. Default: 1 inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True`` bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.
      - Class: `DeQuantStub(qconfig=None)`
        - Description: Dequantize stub module, before calibration, this is same as identity, this will be swapped as `nnq.DeQuantize` in `convert`. Args: qconfig: quantization configuration for the tensor, if qconfig is not provided, we will get qconfig from parent modules
      - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `InvertedResidual(inp: int, oup: int, stride: int, expand_ratio: int, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `MobileNetV2(num_classes: int = 1000, width_mult: float = 1.0, inverted_residual_setting: Optional[List[List[int]]] = None, round_nearest: int = 8, block: Optional[Callable[..., torch.nn.modules.module.Module]] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None, dropout: float = 0.2) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `MobileNet_V2_QuantizedWeights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `MobileNet_V2_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `QuantStub(qconfig=None)`
        - Description: Quantize stub module, before calibration, this is same as an observer, it will be swapped as `nnq.Quantize` in `convert`. Args: qconfig: quantization configuration for the tensor, if qconfig is not provided, we will get qconfig from parent modules
      - Class: `QuantizableInvertedResidual(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `QuantizableMobileNetV2(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Function: `_fuse_modules(model: torch.nn.modules.module.Module, modules_to_fuse: Union[List[str], List[List[str]]], is_qat: Optional[bool], **kwargs: Any)`
        - Description: No docstring available
      - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
        - Description: No docstring available
      - Function: `_replace_relu(module: torch.nn.modules.module.Module) -> None`
        - Description: No docstring available
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Function: `mobilenet_v2(*, weights: Union[torchvision.models.quantization.mobilenetv2.MobileNet_V2_QuantizedWeights, torchvision.models.mobilenetv2.MobileNet_V2_Weights, NoneType] = None, progress: bool = True, quantize: bool = False, **kwargs: Any) -> torchvision.models.quantization.mobilenetv2.QuantizableMobileNetV2`
        - Description: Constructs a MobileNetV2 architecture from `MobileNetV2: Inverted Residuals and Linear Bottlenecks <https://arxiv.org/abs/1801.04381>`_. .. note:: Note that ``quantize = True`` returns a quantized model with 8 bit weights. Quantized models only support inference and run on CPUs. GPU inference is not yet supported. Args: weights (:class:`~torchvision.models.quantization.MobileNet_V2_QuantizedWeights` or :class:`~torchvision.models.MobileNet_V2_Weights`, optional): The pretrained weights for the model. See :class:`~torchvision.models.quantization.MobileNet_V2_QuantizedWeights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. quantize (bool, optional): If True, returns a quantized version of the model. Default is False. **kwargs: parameters passed to the ``torchvision.models.quantization.QuantizableMobileNetV2`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/mobilenetv2.py>`_ for more details about this class. .. autoclass:: torchvision.models.quantization.MobileNet_V2_QuantizedWeights :members: .. autoclass:: torchvision.models.MobileNet_V2_Weights :members: :noindex:
      - Class: `partial(Unable to retrieve signature)`
        - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
      - Function: `quantize_model(model: torch.nn.modules.module.Module, backend: str) -> None`
        - Description: No docstring available
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
    - mobilenetv3.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `Conv2dNormActivation(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]] = 3, stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int], str, NoneType] = None, groups: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, dilation: Union[int, Tuple[int, int]] = 1, inplace: Optional[bool] = True, bias: Optional[bool] = None) -> None`
        - Description: Configurable block used for Convolution2d-Normalization-Activation blocks. Args: in_channels (int): Number of channels in the input image out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block kernel_size: (int, optional): Size of the convolving kernel. Default: 3 stride (int, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation`` groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm2d`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` dilation (int): Spacing between kernel elements. Default: 1 inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True`` bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.
      - Class: `DeQuantStub(qconfig=None)`
        - Description: Dequantize stub module, before calibration, this is same as identity, this will be swapped as `nnq.DeQuantize` in `convert`. Args: qconfig: quantization configuration for the tensor, if qconfig is not provided, we will get qconfig from parent modules
      - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `InvertedResidual(cnf: torchvision.models.mobilenetv3.InvertedResidualConfig, norm_layer: Callable[..., torch.nn.modules.module.Module], se_layer: Callable[..., torch.nn.modules.module.Module] = functools.partial(<class 'torchvision.ops.misc.SqueezeExcitation'>, scale_activation=<class 'torch.nn.modules.activation.Hardsigmoid'>))`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `InvertedResidualConfig(input_channels: int, kernel: int, expanded_channels: int, out_channels: int, use_se: bool, activation: str, stride: int, dilation: int, width_mult: float)`
        - Description: No docstring available
      - Class: `MobileNetV3(inverted_residual_setting: List[torchvision.models.mobilenetv3.InvertedResidualConfig], last_channel: int, num_classes: int = 1000, block: Optional[Callable[..., torch.nn.modules.module.Module]] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None, dropout: float = 0.2, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `MobileNet_V3_Large_QuantizedWeights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `MobileNet_V3_Large_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `QuantStub(qconfig=None)`
        - Description: Quantize stub module, before calibration, this is same as an observer, it will be swapped as `nnq.Quantize` in `convert`. Args: qconfig: quantization configuration for the tensor, if qconfig is not provided, we will get qconfig from parent modules
      - Class: `QuantizableInvertedResidual(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `QuantizableMobileNetV3(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `QuantizableSqueezeExcitation(*args: Any, **kwargs: Any) -> None`
        - Description: This block implements the Squeeze-and-Excitation block from https://arxiv.org/abs/1709.01507 (see Fig. 1). Parameters ``activation``, and ``scale_activation`` correspond to ``delta`` and ``sigma`` in eq. 3. Args: input_channels (int): Number of channels in the input image squeeze_channels (int): Number of squeeze channels activation (Callable[..., torch.nn.Module], optional): ``delta`` activation. Default: ``torch.nn.ReLU`` scale_activation (Callable[..., torch.nn.Module]): ``sigma`` activation. Default: ``torch.nn.Sigmoid``
      - Class: `SqueezeExcitation(input_channels: int, squeeze_channels: int, activation: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.activation.ReLU'>, scale_activation: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.activation.Sigmoid'>) -> None`
        - Description: This block implements the Squeeze-and-Excitation block from https://arxiv.org/abs/1709.01507 (see Fig. 1). Parameters ``activation``, and ``scale_activation`` correspond to ``delta`` and ``sigma`` in eq. 3. Args: input_channels (int): Number of channels in the input image squeeze_channels (int): Number of squeeze channels activation (Callable[..., torch.nn.Module], optional): ``delta`` activation. Default: ``torch.nn.ReLU`` scale_activation (Callable[..., torch.nn.Module]): ``sigma`` activation. Default: ``torch.nn.Sigmoid``
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Function: `_fuse_modules(model: torch.nn.modules.module.Module, modules_to_fuse: Union[List[str], List[List[str]]], is_qat: Optional[bool], **kwargs: Any)`
        - Description: No docstring available
      - Function: `_mobilenet_v3_conf(arch: str, width_mult: float = 1.0, reduced_tail: bool = False, dilated: bool = False, **kwargs: Any)`
        - Description: No docstring available
      - Function: `_mobilenet_v3_model(inverted_residual_setting: List[torchvision.models.mobilenetv3.InvertedResidualConfig], last_channel: int, weights: Optional[torchvision.models._api.WeightsEnum], progress: bool, quantize: bool, **kwargs: Any) -> torchvision.models.quantization.mobilenetv3.QuantizableMobileNetV3`
        - Description: No docstring available
      - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
        - Description: No docstring available
      - Function: `_replace_relu(module: torch.nn.modules.module.Module) -> None`
        - Description: No docstring available
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Function: `mobilenet_v3_large(*, weights: Union[torchvision.models.quantization.mobilenetv3.MobileNet_V3_Large_QuantizedWeights, torchvision.models.mobilenetv3.MobileNet_V3_Large_Weights, NoneType] = None, progress: bool = True, quantize: bool = False, **kwargs: Any) -> torchvision.models.quantization.mobilenetv3.QuantizableMobileNetV3`
        - Description: MobileNetV3 (Large) model from `Searching for MobileNetV3 <https://arxiv.org/abs/1905.02244>`_. .. note:: Note that ``quantize = True`` returns a quantized model with 8 bit weights. Quantized models only support inference and run on CPUs. GPU inference is not yet supported. Args: weights (:class:`~torchvision.models.quantization.MobileNet_V3_Large_QuantizedWeights` or :class:`~torchvision.models.MobileNet_V3_Large_Weights`, optional): The pretrained weights for the model. See :class:`~torchvision.models.quantization.MobileNet_V3_Large_QuantizedWeights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool): If True, displays a progress bar of the download to stderr. Default is True. quantize (bool): If True, return a quantized version of the model. Default is False. **kwargs: parameters passed to the ``torchvision.models.quantization.MobileNet_V3_Large_QuantizedWeights`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/mobilenetv3.py>`_ for more details about this class. .. autoclass:: torchvision.models.quantization.MobileNet_V3_Large_QuantizedWeights :members: .. autoclass:: torchvision.models.MobileNet_V3_Large_Weights :members: :noindex:
      - Class: `partial(Unable to retrieve signature)`
        - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
    - resnet.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `BasicBlock(inplanes: int, planes: int, stride: int = 1, downsample: Optional[torch.nn.modules.module.Module] = None, groups: int = 1, base_width: int = 64, dilation: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Bottleneck(inplanes: int, planes: int, stride: int = 1, downsample: Optional[torch.nn.modules.module.Module] = None, groups: int = 1, base_width: int = 64, dilation: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `QuantizableBasicBlock(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `QuantizableBottleneck(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `QuantizableResNet(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `ResNeXt101_32X8D_QuantizedWeights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `ResNeXt101_32X8D_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `ResNeXt101_64X4D_QuantizedWeights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `ResNeXt101_64X4D_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `ResNet(block: Type[Union[torchvision.models.resnet.BasicBlock, torchvision.models.resnet.Bottleneck]], layers: List[int], num_classes: int = 1000, zero_init_residual: bool = False, groups: int = 1, width_per_group: int = 64, replace_stride_with_dilation: Optional[List[bool]] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `ResNet18_QuantizedWeights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `ResNet18_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `ResNet50_QuantizedWeights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `ResNet50_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Function: `_fuse_modules(model: torch.nn.modules.module.Module, modules_to_fuse: Union[List[str], List[List[str]]], is_qat: Optional[bool], **kwargs: Any)`
        - Description: No docstring available
      - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
        - Description: No docstring available
      - Function: `_replace_relu(module: torch.nn.modules.module.Module) -> None`
        - Description: No docstring available
      - Function: `_resnet(block: Type[Union[torchvision.models.quantization.resnet.QuantizableBasicBlock, torchvision.models.quantization.resnet.QuantizableBottleneck]], layers: List[int], weights: Optional[torchvision.models._api.WeightsEnum], progress: bool, quantize: bool, **kwargs: Any) -> torchvision.models.quantization.resnet.QuantizableResNet`
        - Description: No docstring available
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Class: `partial(Unable to retrieve signature)`
        - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
      - Function: `quantize_model(model: torch.nn.modules.module.Module, backend: str) -> None`
        - Description: No docstring available
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
      - Function: `resnet18(*, weights: Union[torchvision.models.quantization.resnet.ResNet18_QuantizedWeights, torchvision.models.resnet.ResNet18_Weights, NoneType] = None, progress: bool = True, quantize: bool = False, **kwargs: Any) -> torchvision.models.quantization.resnet.QuantizableResNet`
        - Description: ResNet-18 model from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`_ .. note:: Note that ``quantize = True`` returns a quantized model with 8 bit weights. Quantized models only support inference and run on CPUs. GPU inference is not yet supported. Args: weights (:class:`~torchvision.models.quantization.ResNet18_QuantizedWeights` or :class:`~torchvision.models.ResNet18_Weights`, optional): The pretrained weights for the model. See :class:`~torchvision.models.quantization.ResNet18_QuantizedWeights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. quantize (bool, optional): If True, return a quantized version of the model. Default is False. **kwargs: parameters passed to the ``torchvision.models.quantization.QuantizableResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.quantization.ResNet18_QuantizedWeights :members: .. autoclass:: torchvision.models.ResNet18_Weights :members: :noindex:
      - Function: `resnet50(*, weights: Union[torchvision.models.quantization.resnet.ResNet50_QuantizedWeights, torchvision.models.resnet.ResNet50_Weights, NoneType] = None, progress: bool = True, quantize: bool = False, **kwargs: Any) -> torchvision.models.quantization.resnet.QuantizableResNet`
        - Description: ResNet-50 model from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`_ .. note:: Note that ``quantize = True`` returns a quantized model with 8 bit weights. Quantized models only support inference and run on CPUs. GPU inference is not yet supported. Args: weights (:class:`~torchvision.models.quantization.ResNet50_QuantizedWeights` or :class:`~torchvision.models.ResNet50_Weights`, optional): The pretrained weights for the model. See :class:`~torchvision.models.quantization.ResNet50_QuantizedWeights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. quantize (bool, optional): If True, return a quantized version of the model. Default is False. **kwargs: parameters passed to the ``torchvision.models.quantization.QuantizableResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.quantization.ResNet50_QuantizedWeights :members: .. autoclass:: torchvision.models.ResNet50_Weights :members: :noindex:
      - Function: `resnext101_32x8d(*, weights: Union[torchvision.models.quantization.resnet.ResNeXt101_32X8D_QuantizedWeights, torchvision.models.resnet.ResNeXt101_32X8D_Weights, NoneType] = None, progress: bool = True, quantize: bool = False, **kwargs: Any) -> torchvision.models.quantization.resnet.QuantizableResNet`
        - Description: ResNeXt-101 32x8d model from `Aggregated Residual Transformation for Deep Neural Networks <https://arxiv.org/abs/1611.05431>`_ .. note:: Note that ``quantize = True`` returns a quantized model with 8 bit weights. Quantized models only support inference and run on CPUs. GPU inference is not yet supported. Args: weights (:class:`~torchvision.models.quantization.ResNeXt101_32X8D_QuantizedWeights` or :class:`~torchvision.models.ResNeXt101_32X8D_Weights`, optional): The pretrained weights for the model. See :class:`~torchvision.models.quantization.ResNet101_32X8D_QuantizedWeights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. quantize (bool, optional): If True, return a quantized version of the model. Default is False. **kwargs: parameters passed to the ``torchvision.models.quantization.QuantizableResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.quantization.ResNeXt101_32X8D_QuantizedWeights :members: .. autoclass:: torchvision.models.ResNeXt101_32X8D_Weights :members: :noindex:
      - Function: `resnext101_64x4d(*, weights: Union[torchvision.models.quantization.resnet.ResNeXt101_64X4D_QuantizedWeights, torchvision.models.resnet.ResNeXt101_64X4D_Weights, NoneType] = None, progress: bool = True, quantize: bool = False, **kwargs: Any) -> torchvision.models.quantization.resnet.QuantizableResNet`
        - Description: ResNeXt-101 64x4d model from `Aggregated Residual Transformation for Deep Neural Networks <https://arxiv.org/abs/1611.05431>`_ .. note:: Note that ``quantize = True`` returns a quantized model with 8 bit weights. Quantized models only support inference and run on CPUs. GPU inference is not yet supported. Args: weights (:class:`~torchvision.models.quantization.ResNeXt101_64X4D_QuantizedWeights` or :class:`~torchvision.models.ResNeXt101_64X4D_Weights`, optional): The pretrained weights for the model. See :class:`~torchvision.models.quantization.ResNet101_64X4D_QuantizedWeights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. quantize (bool, optional): If True, return a quantized version of the model. Default is False. **kwargs: parameters passed to the ``torchvision.models.quantization.QuantizableResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.quantization.ResNeXt101_64X4D_QuantizedWeights :members: .. autoclass:: torchvision.models.ResNeXt101_64X4D_Weights :members: :noindex:
    - shufflenetv2.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `QuantizableInvertedResidual(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `QuantizableShuffleNetV2(*args: Any, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `ShuffleNet_V2_X0_5_QuantizedWeights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `ShuffleNet_V2_X0_5_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `ShuffleNet_V2_X1_0_QuantizedWeights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `ShuffleNet_V2_X1_0_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `ShuffleNet_V2_X1_5_QuantizedWeights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `ShuffleNet_V2_X1_5_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `ShuffleNet_V2_X2_0_QuantizedWeights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `ShuffleNet_V2_X2_0_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Function: `_fuse_modules(model: torch.nn.modules.module.Module, modules_to_fuse: Union[List[str], List[List[str]]], is_qat: Optional[bool], **kwargs: Any)`
        - Description: No docstring available
      - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
        - Description: No docstring available
      - Function: `_replace_relu(module: torch.nn.modules.module.Module) -> None`
        - Description: No docstring available
      - Function: `_shufflenetv2(stages_repeats: List[int], stages_out_channels: List[int], *, weights: Optional[torchvision.models._api.WeightsEnum], progress: bool, quantize: bool, **kwargs: Any) -> torchvision.models.quantization.shufflenetv2.QuantizableShuffleNetV2`
        - Description: No docstring available
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Class: `partial(Unable to retrieve signature)`
        - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
      - Function: `quantize_model(model: torch.nn.modules.module.Module, backend: str) -> None`
        - Description: No docstring available
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
      - Function: `shufflenet_v2_x0_5(*, weights: Union[torchvision.models.quantization.shufflenetv2.ShuffleNet_V2_X0_5_QuantizedWeights, torchvision.models.shufflenetv2.ShuffleNet_V2_X0_5_Weights, NoneType] = None, progress: bool = True, quantize: bool = False, **kwargs: Any) -> torchvision.models.quantization.shufflenetv2.QuantizableShuffleNetV2`
        - Description: Constructs a ShuffleNetV2 with 0.5x output channels, as described in `ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design <https://arxiv.org/abs/1807.11164>`__. .. note:: Note that ``quantize = True`` returns a quantized model with 8 bit weights. Quantized models only support inference and run on CPUs. GPU inference is not yet supported. Args: weights (:class:`~torchvision.models.quantization.ShuffleNet_V2_X0_5_QuantizedWeights` or :class:`~torchvision.models.ShuffleNet_V2_X0_5_Weights`, optional): The pretrained weights for the model. See :class:`~torchvision.models.quantization.ShuffleNet_V2_X0_5_QuantizedWeights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. quantize (bool, optional): If True, return a quantized version of the model. Default is False. **kwargs: parameters passed to the ``torchvision.models.quantization.ShuffleNet_V2_X0_5_QuantizedWeights`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/shufflenetv2.py>`_ for more details about this class. .. autoclass:: torchvision.models.quantization.ShuffleNet_V2_X0_5_QuantizedWeights :members: .. autoclass:: torchvision.models.ShuffleNet_V2_X0_5_Weights :members: :noindex:
      - Function: `shufflenet_v2_x1_0(*, weights: Union[torchvision.models.quantization.shufflenetv2.ShuffleNet_V2_X1_0_QuantizedWeights, torchvision.models.shufflenetv2.ShuffleNet_V2_X1_0_Weights, NoneType] = None, progress: bool = True, quantize: bool = False, **kwargs: Any) -> torchvision.models.quantization.shufflenetv2.QuantizableShuffleNetV2`
        - Description: Constructs a ShuffleNetV2 with 1.0x output channels, as described in `ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design <https://arxiv.org/abs/1807.11164>`__. .. note:: Note that ``quantize = True`` returns a quantized model with 8 bit weights. Quantized models only support inference and run on CPUs. GPU inference is not yet supported. Args: weights (:class:`~torchvision.models.quantization.ShuffleNet_V2_X1_0_QuantizedWeights` or :class:`~torchvision.models.ShuffleNet_V2_X1_0_Weights`, optional): The pretrained weights for the model. See :class:`~torchvision.models.quantization.ShuffleNet_V2_X1_0_QuantizedWeights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. quantize (bool, optional): If True, return a quantized version of the model. Default is False. **kwargs: parameters passed to the ``torchvision.models.quantization.ShuffleNet_V2_X1_0_QuantizedWeights`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/shufflenetv2.py>`_ for more details about this class. .. autoclass:: torchvision.models.quantization.ShuffleNet_V2_X1_0_QuantizedWeights :members: .. autoclass:: torchvision.models.ShuffleNet_V2_X1_0_Weights :members: :noindex:
      - Function: `shufflenet_v2_x1_5(*, weights: Union[torchvision.models.quantization.shufflenetv2.ShuffleNet_V2_X1_5_QuantizedWeights, torchvision.models.shufflenetv2.ShuffleNet_V2_X1_5_Weights, NoneType] = None, progress: bool = True, quantize: bool = False, **kwargs: Any) -> torchvision.models.quantization.shufflenetv2.QuantizableShuffleNetV2`
        - Description: Constructs a ShuffleNetV2 with 1.5x output channels, as described in `ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design <https://arxiv.org/abs/1807.11164>`__. .. note:: Note that ``quantize = True`` returns a quantized model with 8 bit weights. Quantized models only support inference and run on CPUs. GPU inference is not yet supported. Args: weights (:class:`~torchvision.models.quantization.ShuffleNet_V2_X1_5_QuantizedWeights` or :class:`~torchvision.models.ShuffleNet_V2_X1_5_Weights`, optional): The pretrained weights for the model. See :class:`~torchvision.models.quantization.ShuffleNet_V2_X1_5_QuantizedWeights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. quantize (bool, optional): If True, return a quantized version of the model. Default is False. **kwargs: parameters passed to the ``torchvision.models.quantization.ShuffleNet_V2_X1_5_QuantizedWeights`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/shufflenetv2.py>`_ for more details about this class. .. autoclass:: torchvision.models.quantization.ShuffleNet_V2_X1_5_QuantizedWeights :members: .. autoclass:: torchvision.models.ShuffleNet_V2_X1_5_Weights :members: :noindex:
      - Function: `shufflenet_v2_x2_0(*, weights: Union[torchvision.models.quantization.shufflenetv2.ShuffleNet_V2_X2_0_QuantizedWeights, torchvision.models.shufflenetv2.ShuffleNet_V2_X2_0_Weights, NoneType] = None, progress: bool = True, quantize: bool = False, **kwargs: Any) -> torchvision.models.quantization.shufflenetv2.QuantizableShuffleNetV2`
        - Description: Constructs a ShuffleNetV2 with 2.0x output channels, as described in `ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design <https://arxiv.org/abs/1807.11164>`__. .. note:: Note that ``quantize = True`` returns a quantized model with 8 bit weights. Quantized models only support inference and run on CPUs. GPU inference is not yet supported. Args: weights (:class:`~torchvision.models.quantization.ShuffleNet_V2_X2_0_QuantizedWeights` or :class:`~torchvision.models.ShuffleNet_V2_X2_0_Weights`, optional): The pretrained weights for the model. See :class:`~torchvision.models.quantization.ShuffleNet_V2_X2_0_QuantizedWeights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. quantize (bool, optional): If True, return a quantized version of the model. Default is False. **kwargs: parameters passed to the ``torchvision.models.quantization.ShuffleNet_V2_X2_0_QuantizedWeights`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/shufflenetv2.py>`_ for more details about this class. .. autoclass:: torchvision.models.quantization.ShuffleNet_V2_X2_0_QuantizedWeights :members: .. autoclass:: torchvision.models.ShuffleNet_V2_X2_0_Weights :members: :noindex:
    - utils.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Function: `_fuse_modules(model: torch.nn.modules.module.Module, modules_to_fuse: Union[List[str], List[List[str]]], is_qat: Optional[bool], **kwargs: Any)`
        - Description: No docstring available
      - Function: `_replace_relu(module: torch.nn.modules.module.Module) -> None`
        - Description: No docstring available
      - Function: `quantize_model(model: torch.nn.modules.module.Module, backend: str) -> None`
        - Description: No docstring available
  - segmentation/
    - __pycache__/
    - deeplabv3.py
      - Class: `ASPP(in_channels: int, atrous_rates: List[int], out_channels: int = 256) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `ASPPConv(in_channels: int, out_channels: int, dilation: int) -> None`
        - Description: A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ``OrderedDict`` of modules can be passed in. The ``forward()`` method of ``Sequential`` accepts any input and forwards it to the first module it contains. It then "chains" outputs to inputs sequentially for each subsequent module, finally returning the output of the last module. The value a ``Sequential`` provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the ``Sequential`` applies to each of the modules it stores (which are each a registered submodule of the ``Sequential``). What's the difference between a ``Sequential`` and a :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it sounds like--a list for storing ``Module`` s! On the other hand, the layers in a ``Sequential`` are connected in a cascading way. Example:: # Using Sequential to create a small model. When `model` is run, # input will first be passed to `Conv2d(1,20,5)`. The output of # `Conv2d(1,20,5)` will be used as the input to the first # `ReLU`; the output of the first `ReLU` will become the input # for `Conv2d(20,64,5)`. Finally, the output of # `Conv2d(20,64,5)` will be used as input to the second `ReLU` model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # Using Sequential with OrderedDict. This is functionally the # same as the above code model = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ]))
      - Class: `ASPPPooling(in_channels: int, out_channels: int) -> None`
        - Description: A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ``OrderedDict`` of modules can be passed in. The ``forward()`` method of ``Sequential`` accepts any input and forwards it to the first module it contains. It then "chains" outputs to inputs sequentially for each subsequent module, finally returning the output of the last module. The value a ``Sequential`` provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the ``Sequential`` applies to each of the modules it stores (which are each a registered submodule of the ``Sequential``). What's the difference between a ``Sequential`` and a :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it sounds like--a list for storing ``Module`` s! On the other hand, the layers in a ``Sequential`` are connected in a cascading way. Example:: # Using Sequential to create a small model. When `model` is run, # input will first be passed to `Conv2d(1,20,5)`. The output of # `Conv2d(1,20,5)` will be used as the input to the first # `ReLU`; the output of the first `ReLU` will become the input # for `Conv2d(20,64,5)`. Finally, the output of # `Conv2d(20,64,5)` will be used as input to the second `ReLU` model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # Using Sequential with OrderedDict. This is functionally the # same as the above code model = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ]))
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `DeepLabHead(in_channels: int, num_classes: int) -> None`
        - Description: A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ``OrderedDict`` of modules can be passed in. The ``forward()`` method of ``Sequential`` accepts any input and forwards it to the first module it contains. It then "chains" outputs to inputs sequentially for each subsequent module, finally returning the output of the last module. The value a ``Sequential`` provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the ``Sequential`` applies to each of the modules it stores (which are each a registered submodule of the ``Sequential``). What's the difference between a ``Sequential`` and a :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it sounds like--a list for storing ``Module`` s! On the other hand, the layers in a ``Sequential`` are connected in a cascading way. Example:: # Using Sequential to create a small model. When `model` is run, # input will first be passed to `Conv2d(1,20,5)`. The output of # `Conv2d(1,20,5)` will be used as the input to the first # `ReLU`; the output of the first `ReLU` will become the input # for `Conv2d(20,64,5)`. Finally, the output of # `Conv2d(20,64,5)` will be used as input to the second `ReLU` model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # Using Sequential with OrderedDict. This is functionally the # same as the above code model = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ]))
      - Class: `DeepLabV3(backbone: torch.nn.modules.module.Module, classifier: torch.nn.modules.module.Module, aux_classifier: Optional[torch.nn.modules.module.Module] = None) -> None`
        - Description: Implements DeepLabV3 model from `"Rethinking Atrous Convolution for Semantic Image Segmentation" <https://arxiv.org/abs/1706.05587>`_. Args: backbone (nn.Module): the network used to compute the features for the model. The backbone should return an OrderedDict[Tensor], with the key being "out" for the last feature map used, and "aux" if an auxiliary classifier is used. classifier (nn.Module): module that takes the "out" element returned from the backbone and returns a dense prediction. aux_classifier (nn.Module, optional): auxiliary classifier used during training
      - Class: `DeepLabV3_MobileNet_V3_Large_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `DeepLabV3_ResNet101_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `DeepLabV3_ResNet50_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `FCNHead(in_channels: int, channels: int) -> None`
        - Description: A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ``OrderedDict`` of modules can be passed in. The ``forward()`` method of ``Sequential`` accepts any input and forwards it to the first module it contains. It then "chains" outputs to inputs sequentially for each subsequent module, finally returning the output of the last module. The value a ``Sequential`` provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the ``Sequential`` applies to each of the modules it stores (which are each a registered submodule of the ``Sequential``). What's the difference between a ``Sequential`` and a :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it sounds like--a list for storing ``Module`` s! On the other hand, the layers in a ``Sequential`` are connected in a cascading way. Example:: # Using Sequential to create a small model. When `model` is run, # input will first be passed to `Conv2d(1,20,5)`. The output of # `Conv2d(1,20,5)` will be used as the input to the first # `ReLU`; the output of the first `ReLU` will become the input # for `Conv2d(20,64,5)`. Finally, the output of # `Conv2d(20,64,5)` will be used as input to the second `ReLU` model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # Using Sequential with OrderedDict. This is functionally the # same as the above code model = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ]))
      - Class: `IntermediateLayerGetter(model: torch.nn.modules.module.Module, return_layers: Dict[str, str]) -> None`
        - Description: Module wrapper that returns intermediate layers from a model It has a strong assumption that the modules have been registered into the model in the same order as they are used. This means that one should **not** reuse the same nn.Module twice in the forward if you want this to work. Additionally, it is only able to query submodules that are directly assigned to the model. So if `model` is passed, `model.feature1` can be returned, but not `model.feature1.layer2`. Args: model (nn.Module): model on which we will extract the features return_layers (Dict[name, new_name]): a dict containing the names of the modules for which the activations will be returned as the key of the dict, and the value of the dict is the name of the returned activation (which the user can specify). Examples:: >>> m = torchvision.models.resnet18(weights=ResNet18_Weights.DEFAULT) >>> # extract layer1 and layer3, giving as names `feat1` and feat2` >>> new_m = torchvision.models._utils.IntermediateLayerGetter(m, >>> {'layer1': 'feat1', 'layer3': 'feat2'}) >>> out = new_m(torch.rand(1, 3, 224, 224)) >>> print([(k, v.shape) for k, v in out.items()]) >>> [('feat1', torch.Size([1, 64, 56, 56])), >>> ('feat2', torch.Size([1, 256, 14, 14]))]
      - Class: `MobileNetV3(inverted_residual_setting: List[torchvision.models.mobilenetv3.InvertedResidualConfig], last_channel: int, num_classes: int = 1000, block: Optional[Callable[..., torch.nn.modules.module.Module]] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None, dropout: float = 0.2, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `MobileNet_V3_Large_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `ResNet(block: Type[Union[torchvision.models.resnet.BasicBlock, torchvision.models.resnet.Bottleneck]], layers: List[int], num_classes: int = 1000, zero_init_residual: bool = False, groups: int = 1, width_per_group: int = 64, replace_stride_with_dilation: Optional[List[bool]] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `ResNet101_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `ResNet50_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `SemanticSegmentation(*, resize_size: Optional[int], mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `_SimpleSegmentationModel(backbone: torch.nn.modules.module.Module, classifier: torch.nn.modules.module.Module, aux_classifier: Optional[torch.nn.modules.module.Module] = None) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Function: `_deeplabv3_mobilenetv3(backbone: torchvision.models.mobilenetv3.MobileNetV3, num_classes: int, aux: Optional[bool]) -> torchvision.models.segmentation.deeplabv3.DeepLabV3`
        - Description: No docstring available
      - Function: `_deeplabv3_resnet(backbone: torchvision.models.resnet.ResNet, num_classes: int, aux: Optional[bool]) -> torchvision.models.segmentation.deeplabv3.DeepLabV3`
        - Description: No docstring available
      - Function: `_ovewrite_value_param(param: str, actual: Optional[~V], expected: ~V) -> ~V`
        - Description: No docstring available
      - Function: `deeplabv3_mobilenet_v3_large(*, weights: Optional[torchvision.models.segmentation.deeplabv3.DeepLabV3_MobileNet_V3_Large_Weights] = None, progress: bool = True, num_classes: Optional[int] = None, aux_loss: Optional[bool] = None, weights_backbone: Optional[torchvision.models.mobilenetv3.MobileNet_V3_Large_Weights] = MobileNet_V3_Large_Weights.IMAGENET1K_V1, **kwargs: Any) -> torchvision.models.segmentation.deeplabv3.DeepLabV3`
        - Description: Constructs a DeepLabV3 model with a MobileNetV3-Large backbone. Reference: `Rethinking Atrous Convolution for Semantic Image Segmentation <https://arxiv.org/abs/1706.05587>`__. Args: weights (:class:`~torchvision.models.segmentation.DeepLabV3_MobileNet_V3_Large_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.segmentation.DeepLabV3_MobileNet_V3_Large_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. num_classes (int, optional): number of output classes of the model (including the background) aux_loss (bool, optional): If True, it uses an auxiliary loss weights_backbone (:class:`~torchvision.models.MobileNet_V3_Large_Weights`, optional): The pretrained weights for the backbone **kwargs: unused .. autoclass:: torchvision.models.segmentation.DeepLabV3_MobileNet_V3_Large_Weights :members:
      - Function: `deeplabv3_resnet101(*, weights: Optional[torchvision.models.segmentation.deeplabv3.DeepLabV3_ResNet101_Weights] = None, progress: bool = True, num_classes: Optional[int] = None, aux_loss: Optional[bool] = None, weights_backbone: Optional[torchvision.models.resnet.ResNet101_Weights] = ResNet101_Weights.IMAGENET1K_V1, **kwargs: Any) -> torchvision.models.segmentation.deeplabv3.DeepLabV3`
        - Description: Constructs a DeepLabV3 model with a ResNet-101 backbone. .. betastatus:: segmentation module Reference: `Rethinking Atrous Convolution for Semantic Image Segmentation <https://arxiv.org/abs/1706.05587>`__. Args: weights (:class:`~torchvision.models.segmentation.DeepLabV3_ResNet101_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.segmentation.DeepLabV3_ResNet101_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. num_classes (int, optional): number of output classes of the model (including the background) aux_loss (bool, optional): If True, it uses an auxiliary loss weights_backbone (:class:`~torchvision.models.ResNet101_Weights`, optional): The pretrained weights for the backbone **kwargs: unused .. autoclass:: torchvision.models.segmentation.DeepLabV3_ResNet101_Weights :members:
      - Function: `deeplabv3_resnet50(*, weights: Optional[torchvision.models.segmentation.deeplabv3.DeepLabV3_ResNet50_Weights] = None, progress: bool = True, num_classes: Optional[int] = None, aux_loss: Optional[bool] = None, weights_backbone: Optional[torchvision.models.resnet.ResNet50_Weights] = ResNet50_Weights.IMAGENET1K_V1, **kwargs: Any) -> torchvision.models.segmentation.deeplabv3.DeepLabV3`
        - Description: Constructs a DeepLabV3 model with a ResNet-50 backbone. .. betastatus:: segmentation module Reference: `Rethinking Atrous Convolution for Semantic Image Segmentation <https://arxiv.org/abs/1706.05587>`__. Args: weights (:class:`~torchvision.models.segmentation.DeepLabV3_ResNet50_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.segmentation.DeepLabV3_ResNet50_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. num_classes (int, optional): number of output classes of the model (including the background) aux_loss (bool, optional): If True, it uses an auxiliary loss weights_backbone (:class:`~torchvision.models.ResNet50_Weights`, optional): The pretrained weights for the backbone **kwargs: unused .. autoclass:: torchvision.models.segmentation.DeepLabV3_ResNet50_Weights :members:
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Function: `mobilenet_v3_large(*, weights: Optional[torchvision.models.mobilenetv3.MobileNet_V3_Large_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.mobilenetv3.MobileNetV3`
        - Description: Constructs a large MobileNetV3 architecture from `Searching for MobileNetV3 <https://arxiv.org/abs/1905.02244>`__. Args: weights (:class:`~torchvision.models.MobileNet_V3_Large_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.MobileNet_V3_Large_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.mobilenet.MobileNetV3`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv3.py>`_ for more details about this class. .. autoclass:: torchvision.models.MobileNet_V3_Large_Weights :members:
      - Class: `partial(Unable to retrieve signature)`
        - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
      - Function: `resnet101(*, weights: Optional[torchvision.models.resnet.ResNet101_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.resnet.ResNet`
        - Description: ResNet-101 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__. .. note:: The bottleneck of TorchVision places the stride for downsampling to the second 3x3 convolution while the original paper places it to the first 1x1 convolution. This variant improves the accuracy and is known as `ResNet V1.5 <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_. Args: weights (:class:`~torchvision.models.ResNet101_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ResNet101_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.ResNet101_Weights :members:
      - Function: `resnet50(*, weights: Optional[torchvision.models.resnet.ResNet50_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.resnet.ResNet`
        - Description: ResNet-50 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__. .. note:: The bottleneck of TorchVision places the stride for downsampling to the second 3x3 convolution while the original paper places it to the first 1x1 convolution. This variant improves the accuracy and is known as `ResNet V1.5 <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_. Args: weights (:class:`~torchvision.models.ResNet50_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ResNet50_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.ResNet50_Weights :members:
    - fcn.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `FCN(backbone: torch.nn.modules.module.Module, classifier: torch.nn.modules.module.Module, aux_classifier: Optional[torch.nn.modules.module.Module] = None) -> None`
        - Description: Implements FCN model from `"Fully Convolutional Networks for Semantic Segmentation" <https://arxiv.org/abs/1411.4038>`_. Args: backbone (nn.Module): the network used to compute the features for the model. The backbone should return an OrderedDict[Tensor], with the key being "out" for the last feature map used, and "aux" if an auxiliary classifier is used. classifier (nn.Module): module that takes the "out" element returned from the backbone and returns a dense prediction. aux_classifier (nn.Module, optional): auxiliary classifier used during training
      - Class: `FCNHead(in_channels: int, channels: int) -> None`
        - Description: A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ``OrderedDict`` of modules can be passed in. The ``forward()`` method of ``Sequential`` accepts any input and forwards it to the first module it contains. It then "chains" outputs to inputs sequentially for each subsequent module, finally returning the output of the last module. The value a ``Sequential`` provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the ``Sequential`` applies to each of the modules it stores (which are each a registered submodule of the ``Sequential``). What's the difference between a ``Sequential`` and a :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it sounds like--a list for storing ``Module`` s! On the other hand, the layers in a ``Sequential`` are connected in a cascading way. Example:: # Using Sequential to create a small model. When `model` is run, # input will first be passed to `Conv2d(1,20,5)`. The output of # `Conv2d(1,20,5)` will be used as the input to the first # `ReLU`; the output of the first `ReLU` will become the input # for `Conv2d(20,64,5)`. Finally, the output of # `Conv2d(20,64,5)` will be used as input to the second `ReLU` model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # Using Sequential with OrderedDict. This is functionally the # same as the above code model = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ]))
      - Class: `FCN_ResNet101_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `FCN_ResNet50_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `IntermediateLayerGetter(model: torch.nn.modules.module.Module, return_layers: Dict[str, str]) -> None`
        - Description: Module wrapper that returns intermediate layers from a model It has a strong assumption that the modules have been registered into the model in the same order as they are used. This means that one should **not** reuse the same nn.Module twice in the forward if you want this to work. Additionally, it is only able to query submodules that are directly assigned to the model. So if `model` is passed, `model.feature1` can be returned, but not `model.feature1.layer2`. Args: model (nn.Module): model on which we will extract the features return_layers (Dict[name, new_name]): a dict containing the names of the modules for which the activations will be returned as the key of the dict, and the value of the dict is the name of the returned activation (which the user can specify). Examples:: >>> m = torchvision.models.resnet18(weights=ResNet18_Weights.DEFAULT) >>> # extract layer1 and layer3, giving as names `feat1` and feat2` >>> new_m = torchvision.models._utils.IntermediateLayerGetter(m, >>> {'layer1': 'feat1', 'layer3': 'feat2'}) >>> out = new_m(torch.rand(1, 3, 224, 224)) >>> print([(k, v.shape) for k, v in out.items()]) >>> [('feat1', torch.Size([1, 64, 56, 56])), >>> ('feat2', torch.Size([1, 256, 14, 14]))]
      - Class: `ResNet(block: Type[Union[torchvision.models.resnet.BasicBlock, torchvision.models.resnet.Bottleneck]], layers: List[int], num_classes: int = 1000, zero_init_residual: bool = False, groups: int = 1, width_per_group: int = 64, replace_stride_with_dilation: Optional[List[bool]] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `ResNet101_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `ResNet50_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `SemanticSegmentation(*, resize_size: Optional[int], mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `_SimpleSegmentationModel(backbone: torch.nn.modules.module.Module, classifier: torch.nn.modules.module.Module, aux_classifier: Optional[torch.nn.modules.module.Module] = None) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Function: `_fcn_resnet(backbone: torchvision.models.resnet.ResNet, num_classes: int, aux: Optional[bool]) -> torchvision.models.segmentation.fcn.FCN`
        - Description: No docstring available
      - Function: `_ovewrite_value_param(param: str, actual: Optional[~V], expected: ~V) -> ~V`
        - Description: No docstring available
      - Function: `fcn_resnet101(*, weights: Optional[torchvision.models.segmentation.fcn.FCN_ResNet101_Weights] = None, progress: bool = True, num_classes: Optional[int] = None, aux_loss: Optional[bool] = None, weights_backbone: Optional[torchvision.models.resnet.ResNet101_Weights] = ResNet101_Weights.IMAGENET1K_V1, **kwargs: Any) -> torchvision.models.segmentation.fcn.FCN`
        - Description: Fully-Convolutional Network model with a ResNet-101 backbone from the `Fully Convolutional Networks for Semantic Segmentation <https://arxiv.org/abs/1411.4038>`_ paper. .. betastatus:: segmentation module Args: weights (:class:`~torchvision.models.segmentation.FCN_ResNet101_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.segmentation.FCN_ResNet101_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. num_classes (int, optional): number of output classes of the model (including the background). aux_loss (bool, optional): If True, it uses an auxiliary loss. weights_backbone (:class:`~torchvision.models.ResNet101_Weights`, optional): The pretrained weights for the backbone. **kwargs: parameters passed to the ``torchvision.models.segmentation.fcn.FCN`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/segmentation/fcn.py>`_ for more details about this class. .. autoclass:: torchvision.models.segmentation.FCN_ResNet101_Weights :members:
      - Function: `fcn_resnet50(*, weights: Optional[torchvision.models.segmentation.fcn.FCN_ResNet50_Weights] = None, progress: bool = True, num_classes: Optional[int] = None, aux_loss: Optional[bool] = None, weights_backbone: Optional[torchvision.models.resnet.ResNet50_Weights] = ResNet50_Weights.IMAGENET1K_V1, **kwargs: Any) -> torchvision.models.segmentation.fcn.FCN`
        - Description: Fully-Convolutional Network model with a ResNet-50 backbone from the `Fully Convolutional Networks for Semantic Segmentation <https://arxiv.org/abs/1411.4038>`_ paper. .. betastatus:: segmentation module Args: weights (:class:`~torchvision.models.segmentation.FCN_ResNet50_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.segmentation.FCN_ResNet50_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. num_classes (int, optional): number of output classes of the model (including the background). aux_loss (bool, optional): If True, it uses an auxiliary loss. weights_backbone (:class:`~torchvision.models.ResNet50_Weights`, optional): The pretrained weights for the backbone. **kwargs: parameters passed to the ``torchvision.models.segmentation.fcn.FCN`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/segmentation/fcn.py>`_ for more details about this class. .. autoclass:: torchvision.models.segmentation.FCN_ResNet50_Weights :members:
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Class: `partial(Unable to retrieve signature)`
        - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
      - Function: `resnet101(*, weights: Optional[torchvision.models.resnet.ResNet101_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.resnet.ResNet`
        - Description: ResNet-101 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__. .. note:: The bottleneck of TorchVision places the stride for downsampling to the second 3x3 convolution while the original paper places it to the first 1x1 convolution. This variant improves the accuracy and is known as `ResNet V1.5 <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_. Args: weights (:class:`~torchvision.models.ResNet101_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ResNet101_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.ResNet101_Weights :members:
      - Function: `resnet50(*, weights: Optional[torchvision.models.resnet.ResNet50_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.resnet.ResNet`
        - Description: ResNet-50 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__. .. note:: The bottleneck of TorchVision places the stride for downsampling to the second 3x3 convolution while the original paper places it to the first 1x1 convolution. This variant improves the accuracy and is known as `ResNet V1.5 <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_. Args: weights (:class:`~torchvision.models.ResNet50_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ResNet50_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.ResNet50_Weights :members:
    - lraspp.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `IntermediateLayerGetter(model: torch.nn.modules.module.Module, return_layers: Dict[str, str]) -> None`
        - Description: Module wrapper that returns intermediate layers from a model It has a strong assumption that the modules have been registered into the model in the same order as they are used. This means that one should **not** reuse the same nn.Module twice in the forward if you want this to work. Additionally, it is only able to query submodules that are directly assigned to the model. So if `model` is passed, `model.feature1` can be returned, but not `model.feature1.layer2`. Args: model (nn.Module): model on which we will extract the features return_layers (Dict[name, new_name]): a dict containing the names of the modules for which the activations will be returned as the key of the dict, and the value of the dict is the name of the returned activation (which the user can specify). Examples:: >>> m = torchvision.models.resnet18(weights=ResNet18_Weights.DEFAULT) >>> # extract layer1 and layer3, giving as names `feat1` and feat2` >>> new_m = torchvision.models._utils.IntermediateLayerGetter(m, >>> {'layer1': 'feat1', 'layer3': 'feat2'}) >>> out = new_m(torch.rand(1, 3, 224, 224)) >>> print([(k, v.shape) for k, v in out.items()]) >>> [('feat1', torch.Size([1, 64, 56, 56])), >>> ('feat2', torch.Size([1, 256, 14, 14]))]
      - Class: `LRASPP(backbone: torch.nn.modules.module.Module, low_channels: int, high_channels: int, num_classes: int, inter_channels: int = 128) -> None`
        - Description: Implements a Lite R-ASPP Network for semantic segmentation from `"Searching for MobileNetV3" <https://arxiv.org/abs/1905.02244>`_. Args: backbone (nn.Module): the network used to compute the features for the model. The backbone should return an OrderedDict[Tensor], with the key being "high" for the high level feature map and "low" for the low level feature map. low_channels (int): the number of channels of the low level features. high_channels (int): the number of channels of the high level features. num_classes (int, optional): number of output classes of the model (including the background). inter_channels (int, optional): the number of channels for intermediate computations.
      - Class: `LRASPPHead(low_channels: int, high_channels: int, num_classes: int, inter_channels: int) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `LRASPP_MobileNet_V3_Large_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `MobileNetV3(inverted_residual_setting: List[torchvision.models.mobilenetv3.InvertedResidualConfig], last_channel: int, num_classes: int = 1000, block: Optional[Callable[..., torch.nn.modules.module.Module]] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None, dropout: float = 0.2, **kwargs: Any) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `MobileNet_V3_Large_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `OrderedDict(Unable to retrieve signature)`
        - Description: Dictionary that remembers insertion order
      - Class: `SemanticSegmentation(*, resize_size: Optional[int], mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Function: `_log_api_usage_once(obj: Any) -> None`
        - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
      - Function: `_lraspp_mobilenetv3(backbone: torchvision.models.mobilenetv3.MobileNetV3, num_classes: int) -> torchvision.models.segmentation.lraspp.LRASPP`
        - Description: No docstring available
      - Function: `_ovewrite_value_param(param: str, actual: Optional[~V], expected: ~V) -> ~V`
        - Description: No docstring available
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Function: `lraspp_mobilenet_v3_large(*, weights: Optional[torchvision.models.segmentation.lraspp.LRASPP_MobileNet_V3_Large_Weights] = None, progress: bool = True, num_classes: Optional[int] = None, weights_backbone: Optional[torchvision.models.mobilenetv3.MobileNet_V3_Large_Weights] = MobileNet_V3_Large_Weights.IMAGENET1K_V1, **kwargs: Any) -> torchvision.models.segmentation.lraspp.LRASPP`
        - Description: Constructs a Lite R-ASPP Network model with a MobileNetV3-Large backbone from `Searching for MobileNetV3 <https://arxiv.org/abs/1905.02244>`_ paper. .. betastatus:: segmentation module Args: weights (:class:`~torchvision.models.segmentation.LRASPP_MobileNet_V3_Large_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.segmentation.LRASPP_MobileNet_V3_Large_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. num_classes (int, optional): number of output classes of the model (including the background). aux_loss (bool, optional): If True, it uses an auxiliary loss. weights_backbone (:class:`~torchvision.models.MobileNet_V3_Large_Weights`, optional): The pretrained weights for the backbone. **kwargs: parameters passed to the ``torchvision.models.segmentation.LRASPP`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/segmentation/lraspp.py>`_ for more details about this class. .. autoclass:: torchvision.models.segmentation.LRASPP_MobileNet_V3_Large_Weights :members:
      - Function: `mobilenet_v3_large(*, weights: Optional[torchvision.models.mobilenetv3.MobileNet_V3_Large_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.mobilenetv3.MobileNetV3`
        - Description: Constructs a large MobileNetV3 architecture from `Searching for MobileNetV3 <https://arxiv.org/abs/1905.02244>`__. Args: weights (:class:`~torchvision.models.MobileNet_V3_Large_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.MobileNet_V3_Large_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.mobilenet.MobileNetV3`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv3.py>`_ for more details about this class. .. autoclass:: torchvision.models.MobileNet_V3_Large_Weights :members:
      - Class: `partial(Unable to retrieve signature)`
        - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
    - _utils.py
      - Class: `OrderedDict(Unable to retrieve signature)`
        - Description: Dictionary that remembers insertion order
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Class: `_SimpleSegmentationModel(backbone: torch.nn.modules.module.Module, classifier: torch.nn.modules.module.Module, aux_classifier: Optional[torch.nn.modules.module.Module] = None) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Function: `_log_api_usage_once(obj: Any) -> None`
        - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
  - video/
    - __pycache__/
    - mvit.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `MLP(in_channels: int, hidden_channels: List[int], norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, inplace: Optional[bool] = None, bias: bool = True, dropout: float = 0.0)`
        - Description: This block implements the multi-layer perceptron (MLP) module. Args: in_channels (int): Number of channels of the input hidden_channels (List[int]): List of the hidden channel dimensions norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the linear layer. If ``None`` this layer won't be used. Default: ``None`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the linear layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` inplace (bool, optional): Parameter for the activation layer, which can optionally do the operation in-place. Default is ``None``, which uses the respective default values of the ``activation_layer`` and Dropout layer. bias (bool): Whether to use bias in the linear layer. Default ``True`` dropout (float): The probability for the dropout layer. Default: 0.0
      - Class: `MSBlockConfig(num_heads: int, input_channels: int, output_channels: int, kernel_q: List[int], kernel_kv: List[int], stride_q: List[int], stride_kv: List[int]) -> None`
        - Description: MSBlockConfig(num_heads: int, input_channels: int, output_channels: int, kernel_q: List[int], kernel_kv: List[int], stride_q: List[int], stride_kv: List[int])
      - Class: `MViT(spatial_size: Tuple[int, int], temporal_size: int, block_setting: Sequence[torchvision.models.video.mvit.MSBlockConfig], residual_pool: bool, residual_with_cls_embed: bool, rel_pos_embed: bool, proj_after_attn: bool, dropout: float = 0.5, attention_dropout: float = 0.0, stochastic_depth_prob: float = 0.0, num_classes: int = 400, block: Optional[Callable[..., torch.nn.modules.module.Module]] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None, patch_embed_kernel: Tuple[int, int, int] = (3, 7, 7), patch_embed_stride: Tuple[int, int, int] = (2, 4, 4), patch_embed_padding: Tuple[int, int, int] = (1, 3, 3)) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `MViT_V1_B_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `MViT_V2_S_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `MultiscaleAttention(input_size: List[int], embed_dim: int, output_dim: int, num_heads: int, kernel_q: List[int], kernel_kv: List[int], stride_q: List[int], stride_kv: List[int], residual_pool: bool, residual_with_cls_embed: bool, rel_pos_embed: bool, dropout: float = 0.0, norm_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.normalization.LayerNorm'>) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `MultiscaleBlock(input_size: List[int], cnf: torchvision.models.video.mvit.MSBlockConfig, residual_pool: bool, residual_with_cls_embed: bool, rel_pos_embed: bool, proj_after_attn: bool, dropout: float = 0.0, stochastic_depth_prob: float = 0.0, norm_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.normalization.LayerNorm'>) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Pool(pool: torch.nn.modules.module.Module, norm: Optional[torch.nn.modules.module.Module], activation: Optional[torch.nn.modules.module.Module] = None, norm_before_pool: bool = False) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `PositionalEncoding(embed_size: int, spatial_size: Tuple[int, int], temporal_size: int, rel_pos_embed: bool) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `StochasticDepth(p: float, mode: str) -> None`
        - Description: See :func:`stochastic_depth`.
      - Class: `VideoClassification(*, crop_size: Tuple[int, int], resize_size: Tuple[int, int], mean: Tuple[float, ...] = (0.43216, 0.394666, 0.37645), std: Tuple[float, ...] = (0.22803, 0.22145, 0.216989), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Function: `_add_rel_pos(attn: torch.Tensor, q: torch.Tensor, q_thw: Tuple[int, int, int], k_thw: Tuple[int, int, int], rel_pos_h: torch.Tensor, rel_pos_w: torch.Tensor, rel_pos_t: torch.Tensor) -> torch.Tensor`
        - Description: No docstring available
      - Function: `_add_shortcut(x: torch.Tensor, shortcut: torch.Tensor, residual_with_cls_embed: bool)`
        - Description: No docstring available
      - Function: `_interpolate(embedding: torch.Tensor, d: int) -> torch.Tensor`
        - Description: No docstring available
      - Function: `_log_api_usage_once(obj: Any) -> None`
        - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
      - Function: `_mvit(block_setting: List[torchvision.models.video.mvit.MSBlockConfig], stochastic_depth_prob: float, weights: Optional[torchvision.models._api.WeightsEnum], progress: bool, **kwargs: Any) -> torchvision.models.video.mvit.MViT`
        - Description: No docstring available
      - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
        - Description: No docstring available
      - Function: `_prod(s: Sequence[int]) -> int`
        - Description: No docstring available
      - Function: `_squeeze(x: torch.Tensor, target_dim: int, expand_dim: int, tensor_dim: int) -> torch.Tensor`
        - Description: No docstring available
      - Function: `_unsqueeze(x: torch.Tensor, target_dim: int, expand_dim: int) -> Tuple[torch.Tensor, int]`
        - Description: No docstring available
      - Function: `dataclass(cls=None, /, *, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False, match_args=True, kw_only=False, slots=False, weakref_slot=False)`
        - Description: Add dunder methods based on the fields defined in the class. Examines PEP 526 __annotations__ to determine fields. If init is true, an __init__() method is added to the class. If repr is true, a __repr__() method is added. If order is true, rich comparison dunder methods are added. If unsafe_hash is true, a __hash__() method is added. If frozen is true, fields may not be assigned to after instance creation. If match_args is true, the __match_args__ tuple is added. If kw_only is true, then by default all fields are keyword-only. If slots is true, a new class with a __slots__ attribute is returned.
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Function: `mvit_v1_b(*, weights: Optional[torchvision.models.video.mvit.MViT_V1_B_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.video.mvit.MViT`
        - Description: Constructs a base MViTV1 architecture from `Multiscale Vision Transformers <https://arxiv.org/abs/2104.11227>`__. .. betastatus:: video module Args: weights (:class:`~torchvision.models.video.MViT_V1_B_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.video.MViT_V1_B_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.video.MViT`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/video/mvit.py>`_ for more details about this class. .. autoclass:: torchvision.models.video.MViT_V1_B_Weights :members:
      - Function: `mvit_v2_s(*, weights: Optional[torchvision.models.video.mvit.MViT_V2_S_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.video.mvit.MViT`
        - Description: Constructs a small MViTV2 architecture from `Multiscale Vision Transformers <https://arxiv.org/abs/2104.11227>`__ and `MViTv2: Improved Multiscale Vision Transformers for Classification and Detection <https://arxiv.org/abs/2112.01526>`__. .. betastatus:: video module Args: weights (:class:`~torchvision.models.video.MViT_V2_S_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.video.MViT_V2_S_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.video.MViT`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/video/mvit.py>`_ for more details about this class. .. autoclass:: torchvision.models.video.MViT_V2_S_Weights :members:
      - Class: `partial(Unable to retrieve signature)`
        - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
    - resnet.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `BasicBlock(inplanes: int, planes: int, conv_builder: Callable[..., torch.nn.modules.module.Module], stride: int = 1, downsample: Optional[torch.nn.modules.module.Module] = None) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `BasicStem() -> None`
        - Description: The default conv-batchnorm-relu stem
      - Class: `Bottleneck(inplanes: int, planes: int, conv_builder: Callable[..., torch.nn.modules.module.Module], stride: int = 1, downsample: Optional[torch.nn.modules.module.Module] = None) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Conv2Plus1D(in_planes: int, out_planes: int, midplanes: int, stride: int = 1, padding: int = 1) -> None`
        - Description: A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ``OrderedDict`` of modules can be passed in. The ``forward()`` method of ``Sequential`` accepts any input and forwards it to the first module it contains. It then "chains" outputs to inputs sequentially for each subsequent module, finally returning the output of the last module. The value a ``Sequential`` provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the ``Sequential`` applies to each of the modules it stores (which are each a registered submodule of the ``Sequential``). What's the difference between a ``Sequential`` and a :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it sounds like--a list for storing ``Module`` s! On the other hand, the layers in a ``Sequential`` are connected in a cascading way. Example:: # Using Sequential to create a small model. When `model` is run, # input will first be passed to `Conv2d(1,20,5)`. The output of # `Conv2d(1,20,5)` will be used as the input to the first # `ReLU`; the output of the first `ReLU` will become the input # for `Conv2d(20,64,5)`. Finally, the output of # `Conv2d(20,64,5)` will be used as input to the second `ReLU` model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # Using Sequential with OrderedDict. This is functionally the # same as the above code model = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ]))
      - Class: `Conv3DNoTemporal(in_planes: int, out_planes: int, midplanes: Optional[int] = None, stride: int = 1, padding: int = 1) -> None`
        - Description: Applies a 3D convolution over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size :math:`(N, C_{in}, D, H, W)` and output :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` can be precisely described as: .. math:: out(N_i, C_{out_j}) = bias(C_{out_j}) + \sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \star input(N_i, k) where :math:`\star` is the valid 3D `cross-correlation`_ operator This module supports :ref:`TensorFloat32<tf32_on_ampere>`. On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward. * :attr:`stride` controls the stride for the cross-correlation. * :attr:`padding` controls the amount of padding applied to the input. It can be either a string {'valid', 'same'} or a tuple of ints giving the amount of implicit padding applied on both sides. * :attr:`dilation` controls the spacing between the kernel points; also known as the  trous algorithm. It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does. * :attr:`groups` controls the connections between inputs and outputs. :attr:`in_channels` and :attr:`out_channels` must both be divisible by :attr:`groups`. For example, * At groups=1, all inputs are convolved to all outputs. * At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated. * At groups= :attr:`in_channels`, each input channel is convolved with its own set of filters (of size :math:`\frac{\text{out\_channels}}{\text{in\_channels}}`). The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be: - a single ``int`` -- in which case the same value is used for the depth, height and width dimension - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension, the second `int` for the height dimension and the third `int` for the width dimension Note: When `groups == in_channels` and `out_channels == K * in_channels`, where `K` is a positive integer, this operation is also known as a "depthwise convolution". In other words, for an input of size :math:`(N, C_{in}, L_{in})`, a depthwise convolution with a depthwise multiplier `K` can be performed with the arguments :math:`(C_\text{in}=C_\text{in}, C_\text{out}=C_\text{in} \times \text{K}, ..., \text{groups}=C_\text{in})`. Note: In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information. Note: ``padding='valid'`` is the same as no padding. ``padding='same'`` pads the input so the output has the shape as the input. However, this mode doesn't support any stride values other than 1. Note: This module supports complex data types i.e. ``complex32, complex64, complex128``. Args: in_channels (int): Number of channels in the input image out_channels (int): Number of channels produced by the convolution kernel_size (int or tuple): Size of the convolving kernel stride (int or tuple, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to all six sides of the input. Default: 0 padding_mode (str, optional): ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``. Default: ``'zeros'`` dilation (int or tuple, optional): Spacing between kernel elements. Default: 1 groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True`` Shape: - Input: :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})` or :math:`(C_{in}, D_{in}, H_{in}, W_{in})` - Output: :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` or :math:`(C_{out}, D_{out}, H_{out}, W_{out})`, where .. math:: D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor .. math:: H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor .. math:: W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2] \times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor Attributes: weight (Tensor): the learnable weights of the module of shape :math:`(\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}},` :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})`. The values of these weights are sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}` bias (Tensor): the learnable bias of the module of shape (out_channels). If :attr:`bias` is ``True``, then the values of these weights are sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}` Examples:: >>> # With square kernels and equal stride >>> m = nn.Conv3d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0)) >>> input = torch.randn(20, 16, 10, 50, 100) >>> output = m(input) .. _cross-correlation: https://en.wikipedia.org/wiki/Cross-correlation .. _link: https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
      - Class: `Conv3DSimple(in_planes: int, out_planes: int, midplanes: Optional[int] = None, stride: int = 1, padding: int = 1) -> None`
        - Description: Applies a 3D convolution over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size :math:`(N, C_{in}, D, H, W)` and output :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` can be precisely described as: .. math:: out(N_i, C_{out_j}) = bias(C_{out_j}) + \sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \star input(N_i, k) where :math:`\star` is the valid 3D `cross-correlation`_ operator This module supports :ref:`TensorFloat32<tf32_on_ampere>`. On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward. * :attr:`stride` controls the stride for the cross-correlation. * :attr:`padding` controls the amount of padding applied to the input. It can be either a string {'valid', 'same'} or a tuple of ints giving the amount of implicit padding applied on both sides. * :attr:`dilation` controls the spacing between the kernel points; also known as the  trous algorithm. It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does. * :attr:`groups` controls the connections between inputs and outputs. :attr:`in_channels` and :attr:`out_channels` must both be divisible by :attr:`groups`. For example, * At groups=1, all inputs are convolved to all outputs. * At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated. * At groups= :attr:`in_channels`, each input channel is convolved with its own set of filters (of size :math:`\frac{\text{out\_channels}}{\text{in\_channels}}`). The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be: - a single ``int`` -- in which case the same value is used for the depth, height and width dimension - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension, the second `int` for the height dimension and the third `int` for the width dimension Note: When `groups == in_channels` and `out_channels == K * in_channels`, where `K` is a positive integer, this operation is also known as a "depthwise convolution". In other words, for an input of size :math:`(N, C_{in}, L_{in})`, a depthwise convolution with a depthwise multiplier `K` can be performed with the arguments :math:`(C_\text{in}=C_\text{in}, C_\text{out}=C_\text{in} \times \text{K}, ..., \text{groups}=C_\text{in})`. Note: In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information. Note: ``padding='valid'`` is the same as no padding. ``padding='same'`` pads the input so the output has the shape as the input. However, this mode doesn't support any stride values other than 1. Note: This module supports complex data types i.e. ``complex32, complex64, complex128``. Args: in_channels (int): Number of channels in the input image out_channels (int): Number of channels produced by the convolution kernel_size (int or tuple): Size of the convolving kernel stride (int or tuple, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to all six sides of the input. Default: 0 padding_mode (str, optional): ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``. Default: ``'zeros'`` dilation (int or tuple, optional): Spacing between kernel elements. Default: 1 groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True`` Shape: - Input: :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})` or :math:`(C_{in}, D_{in}, H_{in}, W_{in})` - Output: :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` or :math:`(C_{out}, D_{out}, H_{out}, W_{out})`, where .. math:: D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor .. math:: H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor .. math:: W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2] \times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor Attributes: weight (Tensor): the learnable weights of the module of shape :math:`(\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}},` :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})`. The values of these weights are sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}` bias (Tensor): the learnable bias of the module of shape (out_channels). If :attr:`bias` is ``True``, then the values of these weights are sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}` Examples:: >>> # With square kernels and equal stride >>> m = nn.Conv3d(16, 33, 3, stride=2) >>> # non-square kernels and unequal stride and with padding >>> m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0)) >>> input = torch.randn(20, 16, 10, 50, 100) >>> output = m(input) .. _cross-correlation: https://en.wikipedia.org/wiki/Cross-correlation .. _link: https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
      - Class: `MC3_18_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `R2Plus1D_18_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `R2Plus1dStem() -> None`
        - Description: R(2+1)D stem is different than the default one as it uses separated 3D convolution
      - Class: `R3D_18_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Class: `VideoClassification(*, crop_size: Tuple[int, int], resize_size: Tuple[int, int], mean: Tuple[float, ...] = (0.43216, 0.394666, 0.37645), std: Tuple[float, ...] = (0.22803, 0.22145, 0.216989), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `VideoResNet(block: Type[Union[torchvision.models.video.resnet.BasicBlock, torchvision.models.video.resnet.Bottleneck]], conv_makers: Sequence[Type[Union[torchvision.models.video.resnet.Conv3DSimple, torchvision.models.video.resnet.Conv3DNoTemporal, torchvision.models.video.resnet.Conv2Plus1D]]], layers: List[int], stem: Callable[..., torch.nn.modules.module.Module], num_classes: int = 400, zero_init_residual: bool = False) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `_ModelURLs(Unable to retrieve signature)`
        - Description: dict() -> new empty dictionary dict(mapping) -> new dictionary initialized from a mapping object's (key, value) pairs dict(iterable) -> new dictionary initialized as if via: d = {} for k, v in iterable: d[k] = v dict(**kwargs) -> new dictionary initialized with the name=value pairs in the keyword argument list. For example: dict(one=1, two=2)
      - Function: `_log_api_usage_once(obj: Any) -> None`
        - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
      - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
        - Description: No docstring available
      - Function: `_video_resnet(block: Type[Union[torchvision.models.video.resnet.BasicBlock, torchvision.models.video.resnet.Bottleneck]], conv_makers: Sequence[Type[Union[torchvision.models.video.resnet.Conv3DSimple, torchvision.models.video.resnet.Conv3DNoTemporal, torchvision.models.video.resnet.Conv2Plus1D]]], layers: List[int], stem: Callable[..., torch.nn.modules.module.Module], weights: Optional[torchvision.models._api.WeightsEnum], progress: bool, **kwargs: Any) -> torchvision.models.video.resnet.VideoResNet`
        - Description: No docstring available
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Function: `mc3_18(*, weights: Optional[torchvision.models.video.resnet.MC3_18_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.video.resnet.VideoResNet`
        - Description: Construct 18 layer Mixed Convolution network as in .. betastatus:: video module Reference: `A Closer Look at Spatiotemporal Convolutions for Action Recognition <https://arxiv.org/abs/1711.11248>`__. Args: weights (:class:`~torchvision.models.video.MC3_18_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.video.MC3_18_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.video.resnet.VideoResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/video/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.video.MC3_18_Weights :members:
      - Class: `partial(Unable to retrieve signature)`
        - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
      - Function: `r2plus1d_18(*, weights: Optional[torchvision.models.video.resnet.R2Plus1D_18_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.video.resnet.VideoResNet`
        - Description: Construct 18 layer deep R(2+1)D network as in .. betastatus:: video module Reference: `A Closer Look at Spatiotemporal Convolutions for Action Recognition <https://arxiv.org/abs/1711.11248>`__. Args: weights (:class:`~torchvision.models.video.R2Plus1D_18_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.video.R2Plus1D_18_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.video.resnet.VideoResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/video/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.video.R2Plus1D_18_Weights :members:
      - Function: `r3d_18(*, weights: Optional[torchvision.models.video.resnet.R3D_18_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.video.resnet.VideoResNet`
        - Description: Construct 18 layer Resnet3D model. .. betastatus:: video module Reference: `A Closer Look at Spatiotemporal Convolutions for Action Recognition <https://arxiv.org/abs/1711.11248>`__. Args: weights (:class:`~torchvision.models.video.R3D_18_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.video.R3D_18_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.video.resnet.VideoResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/video/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.video.R3D_18_Weights :members:
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
    - s3d.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `Conv3dNormActivation(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int, int]] = 3, stride: Union[int, Tuple[int, int, int]] = 1, padding: Union[int, Tuple[int, int, int], str, NoneType] = None, groups: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.batchnorm.BatchNorm3d'>, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, dilation: Union[int, Tuple[int, int, int]] = 1, inplace: Optional[bool] = True, bias: Optional[bool] = None) -> None`
        - Description: Configurable block used for Convolution3d-Normalization-Activation blocks. Args: in_channels (int): Number of channels in the input video. out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block kernel_size: (int, optional): Size of the convolving kernel. Default: 3 stride (int, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation`` groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm3d`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` dilation (int): Spacing between kernel elements. Default: 1 inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True`` bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.
      - Class: `S3D(num_classes: int = 400, dropout: float = 0.2, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
        - Description: S3D main class. Args: num_class (int): number of classes for the classification task. dropout (float): dropout probability. norm_layer (Optional[Callable]): Module specifying the normalization layer to use. Inputs: x (Tensor): batch of videos with dimensions (batch, channel, time, height, width)
      - Class: `S3D_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `SepInceptionBlock3D(in_planes: int, b0_out: int, b1_mid: int, b1_out: int, b2_mid: int, b2_out: int, b3_out: int, norm_layer: Callable[..., torch.nn.modules.module.Module])`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `TemporalSeparableConv(in_planes: int, out_planes: int, kernel_size: int, stride: int, padding: int, norm_layer: Callable[..., torch.nn.modules.module.Module])`
        - Description: A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ``OrderedDict`` of modules can be passed in. The ``forward()`` method of ``Sequential`` accepts any input and forwards it to the first module it contains. It then "chains" outputs to inputs sequentially for each subsequent module, finally returning the output of the last module. The value a ``Sequential`` provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the ``Sequential`` applies to each of the modules it stores (which are each a registered submodule of the ``Sequential``). What's the difference between a ``Sequential`` and a :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it sounds like--a list for storing ``Module`` s! On the other hand, the layers in a ``Sequential`` are connected in a cascading way. Example:: # Using Sequential to create a small model. When `model` is run, # input will first be passed to `Conv2d(1,20,5)`. The output of # `Conv2d(1,20,5)` will be used as the input to the first # `ReLU`; the output of the first `ReLU` will become the input # for `Conv2d(20,64,5)`. Finally, the output of # `Conv2d(20,64,5)` will be used as input to the second `ReLU` model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # Using Sequential with OrderedDict. This is functionally the # same as the above code model = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ]))
      - Class: `VideoClassification(*, crop_size: Tuple[int, int], resize_size: Tuple[int, int], mean: Tuple[float, ...] = (0.43216, 0.394666, 0.37645), std: Tuple[float, ...] = (0.22803, 0.22145, 0.216989), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Function: `_log_api_usage_once(obj: Any) -> None`
        - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
      - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
        - Description: No docstring available
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Class: `partial(Unable to retrieve signature)`
        - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
      - Function: `s3d(*, weights: Optional[torchvision.models.video.s3d.S3D_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.video.s3d.S3D`
        - Description: Construct Separable 3D CNN model. Reference: `Rethinking Spatiotemporal Feature Learning <https://arxiv.org/abs/1712.04851>`__. .. betastatus:: video module Args: weights (:class:`~torchvision.models.video.S3D_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.video.S3D_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.video.S3D`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/video/s3d.py>`_ for more details about this class. .. autoclass:: torchvision.models.video.S3D_Weights :members:
    - swin_transformer.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `PatchEmbed3d(patch_size: List[int], in_channels: int = 3, embed_dim: int = 96, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
        - Description: Video to Patch Embedding. Args: patch_size (List[int]): Patch token size. in_channels (int): Number of input channels. Default: 3 embed_dim (int): Number of linear projection output channels. Default: 96. norm_layer (nn.Module, optional): Normalization layer. Default: None
      - Class: `PatchMerging(dim: int, norm_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.normalization.LayerNorm'>)`
        - Description: Patch Merging Layer. Args: dim (int): Number of input channels. norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
      - Class: `ShiftedWindowAttention3d(dim: int, window_size: List[int], shift_size: List[int], num_heads: int, qkv_bias: bool = True, proj_bias: bool = True, attention_dropout: float = 0.0, dropout: float = 0.0) -> None`
        - Description: See :func:`shifted_window_attention_3d`.
      - Class: `Swin3D_B_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `Swin3D_S_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `Swin3D_T_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Class: `SwinTransformer3d(patch_size: List[int], embed_dim: int, depths: List[int], num_heads: List[int], window_size: List[int], mlp_ratio: float = 4.0, dropout: float = 0.0, attention_dropout: float = 0.0, stochastic_depth_prob: float = 0.1, num_classes: int = 400, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None, block: Optional[Callable[..., torch.nn.modules.module.Module]] = None, downsample_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torchvision.models.swin_transformer.PatchMerging'>, patch_embed: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
        - Description: Implements 3D Swin Transformer from the `"Video Swin Transformer" <https://arxiv.org/abs/2106.13230>`_ paper. Args: patch_size (List[int]): Patch size. embed_dim (int): Patch embedding dimension. depths (List(int)): Depth of each Swin Transformer layer. num_heads (List(int)): Number of attention heads in different layers. window_size (List[int]): Window size. mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.0. dropout (float): Dropout rate. Default: 0.0. attention_dropout (float): Attention dropout rate. Default: 0.0. stochastic_depth_prob (float): Stochastic depth rate. Default: 0.1. num_classes (int): Number of classes for classification head. Default: 400. norm_layer (nn.Module, optional): Normalization layer. Default: None. block (nn.Module, optional): SwinTransformer Block. Default: None. downsample_layer (nn.Module): Downsample layer (patch merging). Default: PatchMerging. patch_embed (nn.Module, optional): Patch Embedding layer. Default: None.
      - Class: `SwinTransformerBlock(dim: int, num_heads: int, window_size: List[int], shift_size: List[int], mlp_ratio: float = 4.0, dropout: float = 0.0, attention_dropout: float = 0.0, stochastic_depth_prob: float = 0.0, norm_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.normalization.LayerNorm'>, attn_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torchvision.models.swin_transformer.ShiftedWindowAttention'>)`
        - Description: Swin Transformer Block. Args: dim (int): Number of input channels. num_heads (int): Number of attention heads. window_size (List[int]): Window size. shift_size (List[int]): Shift size for shifted window attention. mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.0. dropout (float): Dropout rate. Default: 0.0. attention_dropout (float): Attention dropout rate. Default: 0.0. stochastic_depth_prob: (float): Stochastic depth rate. Default: 0.0. norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm. attn_layer (nn.Module): Attention layer. Default: ShiftedWindowAttention
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Class: `VideoClassification(*, crop_size: Tuple[int, int], resize_size: Tuple[int, int], mean: Tuple[float, ...] = (0.43216, 0.394666, 0.37645), std: Tuple[float, ...] = (0.22803, 0.22145, 0.216989), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
        - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
      - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
      - Function: `_compute_attention_mask_3d(x: torch.Tensor, size_dhw: Tuple[int, int, int], window_size: Tuple[int, int, int], shift_size: Tuple[int, int, int]) -> torch.Tensor`
        - Description: No docstring available
      - Function: `_compute_pad_size_3d(size_dhw: Tuple[int, int, int], patch_size: Tuple[int, int, int]) -> Tuple[int, int, int]`
        - Description: No docstring available
      - Function: `_get_relative_position_bias(relative_position_bias_table: torch.Tensor, relative_position_index: torch.Tensor, window_size: List[int]) -> torch.Tensor`
        - Description: No docstring available
      - Function: `_get_window_and_shift_size(shift_size: List[int], size_dhw: List[int], window_size: List[int]) -> Tuple[List[int], List[int]]`
        - Description: No docstring available
      - Function: `_log_api_usage_once(obj: Any) -> None`
        - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
      - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
        - Description: No docstring available
      - Function: `_swin_transformer3d(patch_size: List[int], embed_dim: int, depths: List[int], num_heads: List[int], window_size: List[int], stochastic_depth_prob: float, weights: Optional[torchvision.models._api.WeightsEnum], progress: bool, **kwargs: Any) -> torchvision.models.video.swin_transformer.SwinTransformer3d`
        - Description: No docstring available
      - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
        - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
      - Class: `partial(Unable to retrieve signature)`
        - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
      - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
        - Description: No docstring available
      - Function: `shifted_window_attention_3d(input: torch.Tensor, qkv_weight: torch.Tensor, proj_weight: torch.Tensor, relative_position_bias: torch.Tensor, window_size: List[int], num_heads: int, shift_size: List[int], attention_dropout: float = 0.0, dropout: float = 0.0, qkv_bias: Optional[torch.Tensor] = None, proj_bias: Optional[torch.Tensor] = None, training: bool = True) -> torch.Tensor`
        - Description: Window based multi-head self attention (W-MSA) module with relative position bias. It supports both of shifted and non-shifted window. Args: input (Tensor[B, T, H, W, C]): The input tensor, 5-dimensions. qkv_weight (Tensor[in_dim, out_dim]): The weight tensor of query, key, value. proj_weight (Tensor[out_dim, out_dim]): The weight tensor of projection. relative_position_bias (Tensor): The learned relative position bias added to attention. window_size (List[int]): 3-dimensions window size, T, H, W . num_heads (int): Number of attention heads. shift_size (List[int]): Shift size for shifted window attention (T, H, W). attention_dropout (float): Dropout ratio of attention weight. Default: 0.0. dropout (float): Dropout ratio of output. Default: 0.0. qkv_bias (Tensor[out_dim], optional): The bias tensor of query, key, value. Default: None. proj_bias (Tensor[out_dim], optional): The bias tensor of projection. Default: None. training (bool, optional): Training flag used by the dropout parameters. Default: True. Returns: Tensor[B, T, H, W, C]: The output tensor after shifted window attention.
      - Function: `swin3d_b(*, weights: Optional[torchvision.models.video.swin_transformer.Swin3D_B_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.video.swin_transformer.SwinTransformer3d`
        - Description: Constructs a swin_base architecture from `Video Swin Transformer <https://arxiv.org/abs/2106.13230>`_. Args: weights (:class:`~torchvision.models.video.Swin3D_B_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.video.Swin3D_B_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.video.swin_transformer.SwinTransformer`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/video/swin_transformer.py>`_ for more details about this class. .. autoclass:: torchvision.models.video.Swin3D_B_Weights :members:
      - Function: `swin3d_s(*, weights: Optional[torchvision.models.video.swin_transformer.Swin3D_S_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.video.swin_transformer.SwinTransformer3d`
        - Description: Constructs a swin_small architecture from `Video Swin Transformer <https://arxiv.org/abs/2106.13230>`_. Args: weights (:class:`~torchvision.models.video.Swin3D_S_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.video.Swin3D_S_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.video.swin_transformer.SwinTransformer`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/video/swin_transformer.py>`_ for more details about this class. .. autoclass:: torchvision.models.video.Swin3D_S_Weights :members:
      - Function: `swin3d_t(*, weights: Optional[torchvision.models.video.swin_transformer.Swin3D_T_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.video.swin_transformer.SwinTransformer3d`
        - Description: Constructs a swin_tiny architecture from `Video Swin Transformer <https://arxiv.org/abs/2106.13230>`_. Args: weights (:class:`~torchvision.models.video.Swin3D_T_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.video.Swin3D_T_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.video.swin_transformer.SwinTransformer`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/video/swin_transformer.py>`_ for more details about this class. .. autoclass:: torchvision.models.video.Swin3D_T_Weights :members:
  - __pycache__/
  - alexnet.py
    - Class: `AlexNet(num_classes: int = 1000, dropout: float = 0.5) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `AlexNet_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
      - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
    - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
      - Description: No docstring available
    - Function: `alexnet(*, weights: Optional[torchvision.models.alexnet.AlexNet_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.alexnet.AlexNet`
      - Description: AlexNet model architecture from `One weird trick for parallelizing convolutional neural networks <https://arxiv.org/abs/1404.5997>`__. .. note:: AlexNet was originally introduced in the `ImageNet Classification with Deep Convolutional Neural Networks <https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html>`__ paper. Our implementation is based instead on the "One weird trick" paper above. Args: weights (:class:`~torchvision.models.AlexNet_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.AlexNet_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.squeezenet.AlexNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/alexnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.AlexNet_Weights :members:
    - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
      - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
    - Class: `partial(Unable to retrieve signature)`
      - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
    - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
      - Description: No docstring available
  - convnext.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `CNBlock(dim, layer_scale: float, stochastic_depth_prob: float, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `CNBlockConfig(input_channels: int, out_channels: Optional[int], num_layers: int) -> None`
      - Description: No docstring available
    - Class: `Conv2dNormActivation(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]] = 3, stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int], str, NoneType] = None, groups: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, dilation: Union[int, Tuple[int, int]] = 1, inplace: Optional[bool] = True, bias: Optional[bool] = None) -> None`
      - Description: Configurable block used for Convolution2d-Normalization-Activation blocks. Args: in_channels (int): Number of channels in the input image out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block kernel_size: (int, optional): Size of the convolving kernel. Default: 3 stride (int, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation`` groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm2d`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` dilation (int): Spacing between kernel elements. Default: 1 inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True`` bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.
    - Class: `ConvNeXt(block_setting: List[torchvision.models.convnext.CNBlockConfig], stochastic_depth_prob: float = 0.0, layer_scale: float = 1e-06, num_classes: int = 1000, block: Optional[Callable[..., torch.nn.modules.module.Module]] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None, **kwargs: Any) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `ConvNeXt_Base_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ConvNeXt_Large_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ConvNeXt_Small_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ConvNeXt_Tiny_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `LayerNorm2d(normalized_shape: Union[int, List[int], torch.Size], eps: float = 1e-05, elementwise_affine: bool = True, bias: bool = True, device=None, dtype=None) -> None`
      - Description: Applies Layer Normalization over a mini-batch of inputs as described in the paper `Layer Normalization <https://arxiv.org/abs/1607.06450>`__ .. math:: y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta The mean and standard-deviation are calculated over the last `D` dimensions, where `D` is the dimension of :attr:`normalized_shape`. For example, if :attr:`normalized_shape` is ``(3, 5)`` (a 2-dimensional shape), the mean and standard-deviation are computed over the last 2 dimensions of the input (i.e. ``input.mean((-2, -1))``). :math:`\gamma` and :math:`\beta` are learnable affine transform parameters of :attr:`normalized_shape` if :attr:`elementwise_affine` is ``True``. The standard-deviation is calculated via the biased estimator, equivalent to `torch.var(input, unbiased=False)`. .. note:: Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the :attr:`affine` option, Layer Normalization applies per-element scale and bias with :attr:`elementwise_affine`. This layer uses statistics computed from input data in both training and evaluation modes. Args: normalized_shape (int or list or torch.Size): input shape from an expected input of size .. math:: [* \times \text{normalized\_shape}[0] \times \text{normalized\_shape}[1] \times \ldots \times \text{normalized\_shape}[-1]] If a single integer is used, it is treated as a singleton list, and this module will normalize over the last dimension which is expected to be of that specific size. eps: a value added to the denominator for numerical stability. Default: 1e-5 elementwise_affine: a boolean value that when set to ``True``, this module has learnable per-element affine parameters initialized to ones (for weights) and zeros (for biases). Default: ``True``. bias: If set to ``False``, the layer will not learn an additive bias (only relevant if :attr:`elementwise_affine` is ``True``). Default: ``True``. Attributes: weight: the learnable weights of the module of shape :math:`\text{normalized\_shape}` when :attr:`elementwise_affine` is set to ``True``. The values are initialized to 1. bias: the learnable bias of the module of shape :math:`\text{normalized\_shape}` when :attr:`elementwise_affine` is set to ``True``. The values are initialized to 0. Shape: - Input: :math:`(N, *)` - Output: :math:`(N, *)` (same shape as input) Examples:: >>> # NLP Example >>> batch, sentence_length, embedding_dim = 20, 5, 10 >>> embedding = torch.randn(batch, sentence_length, embedding_dim) >>> layer_norm = nn.LayerNorm(embedding_dim) >>> # Activate module >>> layer_norm(embedding) >>> >>> # Image Example >>> N, C, H, W = 20, 5, 10, 10 >>> input = torch.randn(N, C, H, W) >>> # Normalize over the last three dimensions (i.e. the channel and spatial dimensions) >>> # as shown in the image below >>> layer_norm = nn.LayerNorm([C, H, W]) >>> output = layer_norm(input) .. image:: ../_static/img/nn/layer_norm.jpg :scale: 50 %
    - Class: `Permute(dims: List[int])`
      - Description: This module returns a view of the tensor input with its dimensions permuted. Args: dims (List[int]): The desired ordering of dimensions
    - Class: `StochasticDepth(p: float, mode: str) -> None`
      - Description: See :func:`stochastic_depth`.
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
      - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
    - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Function: `_convnext(block_setting: List[torchvision.models.convnext.CNBlockConfig], stochastic_depth_prob: float, weights: Optional[torchvision.models._api.WeightsEnum], progress: bool, **kwargs: Any) -> torchvision.models.convnext.ConvNeXt`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
      - Description: No docstring available
    - Function: `convnext_base(*, weights: Optional[torchvision.models.convnext.ConvNeXt_Base_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.convnext.ConvNeXt`
      - Description: ConvNeXt Base model architecture from the `A ConvNet for the 2020s <https://arxiv.org/abs/2201.03545>`_ paper. Args: weights (:class:`~torchvision.models.convnext.ConvNeXt_Base_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.convnext.ConvNeXt_Base_Weights` below for more details and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.convnext.ConvNext`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/convnext.py>`_ for more details about this class. .. autoclass:: torchvision.models.ConvNeXt_Base_Weights :members:
    - Function: `convnext_large(*, weights: Optional[torchvision.models.convnext.ConvNeXt_Large_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.convnext.ConvNeXt`
      - Description: ConvNeXt Large model architecture from the `A ConvNet for the 2020s <https://arxiv.org/abs/2201.03545>`_ paper. Args: weights (:class:`~torchvision.models.convnext.ConvNeXt_Large_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.convnext.ConvNeXt_Large_Weights` below for more details and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.convnext.ConvNext`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/convnext.py>`_ for more details about this class. .. autoclass:: torchvision.models.ConvNeXt_Large_Weights :members:
    - Function: `convnext_small(*, weights: Optional[torchvision.models.convnext.ConvNeXt_Small_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.convnext.ConvNeXt`
      - Description: ConvNeXt Small model architecture from the `A ConvNet for the 2020s <https://arxiv.org/abs/2201.03545>`_ paper. Args: weights (:class:`~torchvision.models.convnext.ConvNeXt_Small_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.convnext.ConvNeXt_Small_Weights` below for more details and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.convnext.ConvNext`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/convnext.py>`_ for more details about this class. .. autoclass:: torchvision.models.ConvNeXt_Small_Weights :members:
    - Function: `convnext_tiny(*, weights: Optional[torchvision.models.convnext.ConvNeXt_Tiny_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.convnext.ConvNeXt`
      - Description: ConvNeXt Tiny model architecture from the `A ConvNet for the 2020s <https://arxiv.org/abs/2201.03545>`_ paper. Args: weights (:class:`~torchvision.models.convnext.ConvNeXt_Tiny_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.convnext.ConvNeXt_Tiny_Weights` below for more details and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.convnext.ConvNext`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/convnext.py>`_ for more details about this class. .. autoclass:: torchvision.models.ConvNeXt_Tiny_Weights :members:
    - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
      - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
    - Class: `partial(Unable to retrieve signature)`
      - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
    - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
      - Description: No docstring available
  - densenet.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `DenseNet(growth_rate: int = 32, block_config: Tuple[int, int, int, int] = (6, 12, 24, 16), num_init_features: int = 64, bn_size: int = 4, drop_rate: float = 0, num_classes: int = 1000, memory_efficient: bool = False) -> None`
      - Description: Densenet-BC model class, based on `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`_. Args: growth_rate (int) - how many filters to add each layer (`k` in paper) block_config (list of 4 ints) - how many layers in each pooling block num_init_features (int) - the number of filters to learn in the first convolution layer bn_size (int) - multiplicative factor for number of bottle neck layers (i.e. bn_size * k features in the bottleneck layer) drop_rate (float) - dropout rate after each dense layer num_classes (int) - number of classification classes memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient, but slower. Default: *False*. See `"paper" <https://arxiv.org/pdf/1707.06990.pdf>`_.
    - Class: `DenseNet121_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `DenseNet161_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `DenseNet169_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `DenseNet201_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `OrderedDict(Unable to retrieve signature)`
      - Description: Dictionary that remembers insertion order
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
      - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
    - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `_DenseBlock(num_layers: int, num_input_features: int, bn_size: int, growth_rate: int, drop_rate: float, memory_efficient: bool = False) -> None`
      - Description: Holds submodules in a dictionary. :class:`~torch.nn.ModuleDict` can be indexed like a regular Python dictionary, but modules it contains are properly registered, and will be visible by all :class:`~torch.nn.Module` methods. :class:`~torch.nn.ModuleDict` is an **ordered** dictionary that respects * the order of insertion, and * in :meth:`~torch.nn.ModuleDict.update`, the order of the merged ``OrderedDict``, ``dict`` (started from Python 3.6) or another :class:`~torch.nn.ModuleDict` (the argument to :meth:`~torch.nn.ModuleDict.update`). Note that :meth:`~torch.nn.ModuleDict.update` with other unordered mapping types (e.g., Python's plain ``dict`` before Python version 3.6) does not preserve the order of the merged mapping. Args: modules (iterable, optional): a mapping (dictionary) of (string: module) or an iterable of key-value pairs of type (string, module) Example:: class MyModule(nn.Module): def __init__(self): super().__init__() self.choices = nn.ModuleDict({ 'conv': nn.Conv2d(10, 10, 3), 'pool': nn.MaxPool2d(3) }) self.activations = nn.ModuleDict([ ['lrelu', nn.LeakyReLU()], ['prelu', nn.PReLU()] ]) def forward(self, x, choice, act): x = self.choices[choice](x) x = self.activations[act](x) return x
    - Class: `_DenseLayer(num_input_features: int, growth_rate: int, bn_size: int, drop_rate: float, memory_efficient: bool = False) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `_Transition(num_input_features: int, num_output_features: int) -> None`
      - Description: A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ``OrderedDict`` of modules can be passed in. The ``forward()`` method of ``Sequential`` accepts any input and forwards it to the first module it contains. It then "chains" outputs to inputs sequentially for each subsequent module, finally returning the output of the last module. The value a ``Sequential`` provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the ``Sequential`` applies to each of the modules it stores (which are each a registered submodule of the ``Sequential``). What's the difference between a ``Sequential`` and a :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it sounds like--a list for storing ``Module`` s! On the other hand, the layers in a ``Sequential`` are connected in a cascading way. Example:: # Using Sequential to create a small model. When `model` is run, # input will first be passed to `Conv2d(1,20,5)`. The output of # `Conv2d(1,20,5)` will be used as the input to the first # `ReLU`; the output of the first `ReLU` will become the input # for `Conv2d(20,64,5)`. Finally, the output of # `Conv2d(20,64,5)` will be used as input to the second `ReLU` model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # Using Sequential with OrderedDict. This is functionally the # same as the above code model = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ]))
    - Function: `_densenet(growth_rate: int, block_config: Tuple[int, int, int, int], num_init_features: int, weights: Optional[torchvision.models._api.WeightsEnum], progress: bool, **kwargs: Any) -> torchvision.models.densenet.DenseNet`
      - Description: No docstring available
    - Function: `_load_state_dict(model: torch.nn.modules.module.Module, weights: torchvision.models._api.WeightsEnum, progress: bool) -> None`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
      - Description: No docstring available
    - Function: `densenet121(*, weights: Optional[torchvision.models.densenet.DenseNet121_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.densenet.DenseNet`
      - Description: Densenet-121 model from `Densely Connected Convolutional Networks <https://arxiv.org/abs/1608.06993>`_. Args: weights (:class:`~torchvision.models.DenseNet121_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.DenseNet121_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.densenet.DenseNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/densenet.py>`_ for more details about this class. .. autoclass:: torchvision.models.DenseNet121_Weights :members:
    - Function: `densenet161(*, weights: Optional[torchvision.models.densenet.DenseNet161_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.densenet.DenseNet`
      - Description: Densenet-161 model from `Densely Connected Convolutional Networks <https://arxiv.org/abs/1608.06993>`_. Args: weights (:class:`~torchvision.models.DenseNet161_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.DenseNet161_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.densenet.DenseNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/densenet.py>`_ for more details about this class. .. autoclass:: torchvision.models.DenseNet161_Weights :members:
    - Function: `densenet169(*, weights: Optional[torchvision.models.densenet.DenseNet169_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.densenet.DenseNet`
      - Description: Densenet-169 model from `Densely Connected Convolutional Networks <https://arxiv.org/abs/1608.06993>`_. Args: weights (:class:`~torchvision.models.DenseNet169_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.DenseNet169_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.densenet.DenseNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/densenet.py>`_ for more details about this class. .. autoclass:: torchvision.models.DenseNet169_Weights :members:
    - Function: `densenet201(*, weights: Optional[torchvision.models.densenet.DenseNet201_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.densenet.DenseNet`
      - Description: Densenet-201 model from `Densely Connected Convolutional Networks <https://arxiv.org/abs/1608.06993>`_. Args: weights (:class:`~torchvision.models.DenseNet201_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.DenseNet201_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.densenet.DenseNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/densenet.py>`_ for more details about this class. .. autoclass:: torchvision.models.DenseNet201_Weights :members:
    - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
      - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
    - Class: `partial(Unable to retrieve signature)`
      - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
    - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
      - Description: No docstring available
  - efficientnet.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Conv2dNormActivation(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]] = 3, stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int], str, NoneType] = None, groups: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, dilation: Union[int, Tuple[int, int]] = 1, inplace: Optional[bool] = True, bias: Optional[bool] = None) -> None`
      - Description: Configurable block used for Convolution2d-Normalization-Activation blocks. Args: in_channels (int): Number of channels in the input image out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block kernel_size: (int, optional): Size of the convolving kernel. Default: 3 stride (int, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation`` groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm2d`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` dilation (int): Spacing between kernel elements. Default: 1 inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True`` bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.
    - Class: `EfficientNet(inverted_residual_setting: Sequence[Union[torchvision.models.efficientnet.MBConvConfig, torchvision.models.efficientnet.FusedMBConvConfig]], dropout: float, stochastic_depth_prob: float = 0.2, num_classes: int = 1000, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None, last_channel: Optional[int] = None) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `EfficientNet_B0_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `EfficientNet_B1_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `EfficientNet_B2_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `EfficientNet_B3_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `EfficientNet_B4_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `EfficientNet_B5_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `EfficientNet_B6_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `EfficientNet_B7_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `EfficientNet_V2_L_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `EfficientNet_V2_M_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `EfficientNet_V2_S_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `FusedMBConv(cnf: torchvision.models.efficientnet.FusedMBConvConfig, stochastic_depth_prob: float, norm_layer: Callable[..., torch.nn.modules.module.Module]) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `FusedMBConvConfig(expand_ratio: float, kernel: int, stride: int, input_channels: int, out_channels: int, num_layers: int, block: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
      - Description: _MBConvConfig(expand_ratio: float, kernel: int, stride: int, input_channels: int, out_channels: int, num_layers: int, block: Callable[..., torch.nn.modules.module.Module])
    - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `InterpolationMode(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: Interpolation modes Available interpolation methods are ``nearest``, ``nearest-exact``, ``bilinear``, ``bicubic``, ``box``, ``hamming``, and ``lanczos``.
    - Class: `MBConv(cnf: torchvision.models.efficientnet.MBConvConfig, stochastic_depth_prob: float, norm_layer: Callable[..., torch.nn.modules.module.Module], se_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torchvision.ops.misc.SqueezeExcitation'>) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `MBConvConfig(expand_ratio: float, kernel: int, stride: int, input_channels: int, out_channels: int, num_layers: int, width_mult: float = 1.0, depth_mult: float = 1.0, block: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
      - Description: _MBConvConfig(expand_ratio: float, kernel: int, stride: int, input_channels: int, out_channels: int, num_layers: int, block: Callable[..., torch.nn.modules.module.Module])
    - Class: `SqueezeExcitation(input_channels: int, squeeze_channels: int, activation: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.activation.ReLU'>, scale_activation: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.activation.Sigmoid'>) -> None`
      - Description: This block implements the Squeeze-and-Excitation block from https://arxiv.org/abs/1709.01507 (see Fig. 1). Parameters ``activation``, and ``scale_activation`` correspond to ``delta`` and ``sigma`` in eq. 3. Args: input_channels (int): Number of channels in the input image squeeze_channels (int): Number of squeeze channels activation (Callable[..., torch.nn.Module], optional): ``delta`` activation. Default: ``torch.nn.ReLU`` scale_activation (Callable[..., torch.nn.Module]): ``sigma`` activation. Default: ``torch.nn.Sigmoid``
    - Class: `StochasticDepth(p: float, mode: str) -> None`
      - Description: See :func:`stochastic_depth`.
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
      - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
    - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `_MBConvConfig(expand_ratio: float, kernel: int, stride: int, input_channels: int, out_channels: int, num_layers: int, block: Callable[..., torch.nn.modules.module.Module]) -> None`
      - Description: _MBConvConfig(expand_ratio: float, kernel: int, stride: int, input_channels: int, out_channels: int, num_layers: int, block: Callable[..., torch.nn.modules.module.Module])
    - Function: `_efficientnet(inverted_residual_setting: Sequence[Union[torchvision.models.efficientnet.MBConvConfig, torchvision.models.efficientnet.FusedMBConvConfig]], dropout: float, last_channel: Optional[int], weights: Optional[torchvision.models._api.WeightsEnum], progress: bool, **kwargs: Any) -> torchvision.models.efficientnet.EfficientNet`
      - Description: No docstring available
    - Function: `_efficientnet_conf(arch: str, **kwargs: Any) -> Tuple[Sequence[Union[torchvision.models.efficientnet.MBConvConfig, torchvision.models.efficientnet.FusedMBConvConfig]], Optional[int]]`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int`
      - Description: This function is taken from the original tf repo. It ensures that all layers have a channel number that is divisible by 8 It can be seen here: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
    - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
      - Description: No docstring available
    - Function: `dataclass(cls=None, /, *, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False, match_args=True, kw_only=False, slots=False, weakref_slot=False)`
      - Description: Add dunder methods based on the fields defined in the class. Examines PEP 526 __annotations__ to determine fields. If init is true, an __init__() method is added to the class. If repr is true, a __repr__() method is added. If order is true, rich comparison dunder methods are added. If unsafe_hash is true, a __hash__() method is added. If frozen is true, fields may not be assigned to after instance creation. If match_args is true, the __match_args__ tuple is added. If kw_only is true, then by default all fields are keyword-only. If slots is true, a new class with a __slots__ attribute is returned.
    - Function: `efficientnet_b0(*, weights: Optional[torchvision.models.efficientnet.EfficientNet_B0_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.efficientnet.EfficientNet`
      - Description: EfficientNet B0 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper. Args: weights (:class:`~torchvision.models.EfficientNet_B0_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.EfficientNet_B0_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.EfficientNet_B0_Weights :members:
    - Function: `efficientnet_b1(*, weights: Optional[torchvision.models.efficientnet.EfficientNet_B1_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.efficientnet.EfficientNet`
      - Description: EfficientNet B1 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper. Args: weights (:class:`~torchvision.models.EfficientNet_B1_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.EfficientNet_B1_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.EfficientNet_B1_Weights :members:
    - Function: `efficientnet_b2(*, weights: Optional[torchvision.models.efficientnet.EfficientNet_B2_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.efficientnet.EfficientNet`
      - Description: EfficientNet B2 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper. Args: weights (:class:`~torchvision.models.EfficientNet_B2_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.EfficientNet_B2_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.EfficientNet_B2_Weights :members:
    - Function: `efficientnet_b3(*, weights: Optional[torchvision.models.efficientnet.EfficientNet_B3_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.efficientnet.EfficientNet`
      - Description: EfficientNet B3 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper. Args: weights (:class:`~torchvision.models.EfficientNet_B3_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.EfficientNet_B3_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.EfficientNet_B3_Weights :members:
    - Function: `efficientnet_b4(*, weights: Optional[torchvision.models.efficientnet.EfficientNet_B4_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.efficientnet.EfficientNet`
      - Description: EfficientNet B4 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper. Args: weights (:class:`~torchvision.models.EfficientNet_B4_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.EfficientNet_B4_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.EfficientNet_B4_Weights :members:
    - Function: `efficientnet_b5(*, weights: Optional[torchvision.models.efficientnet.EfficientNet_B5_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.efficientnet.EfficientNet`
      - Description: EfficientNet B5 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper. Args: weights (:class:`~torchvision.models.EfficientNet_B5_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.EfficientNet_B5_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.EfficientNet_B5_Weights :members:
    - Function: `efficientnet_b6(*, weights: Optional[torchvision.models.efficientnet.EfficientNet_B6_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.efficientnet.EfficientNet`
      - Description: EfficientNet B6 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper. Args: weights (:class:`~torchvision.models.EfficientNet_B6_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.EfficientNet_B6_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.EfficientNet_B6_Weights :members:
    - Function: `efficientnet_b7(*, weights: Optional[torchvision.models.efficientnet.EfficientNet_B7_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.efficientnet.EfficientNet`
      - Description: EfficientNet B7 model architecture from the `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks <https://arxiv.org/abs/1905.11946>`_ paper. Args: weights (:class:`~torchvision.models.EfficientNet_B7_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.EfficientNet_B7_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.EfficientNet_B7_Weights :members:
    - Function: `efficientnet_v2_l(*, weights: Optional[torchvision.models.efficientnet.EfficientNet_V2_L_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.efficientnet.EfficientNet`
      - Description: Constructs an EfficientNetV2-L architecture from `EfficientNetV2: Smaller Models and Faster Training <https://arxiv.org/abs/2104.00298>`_. Args: weights (:class:`~torchvision.models.EfficientNet_V2_L_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.EfficientNet_V2_L_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.EfficientNet_V2_L_Weights :members:
    - Function: `efficientnet_v2_m(*, weights: Optional[torchvision.models.efficientnet.EfficientNet_V2_M_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.efficientnet.EfficientNet`
      - Description: Constructs an EfficientNetV2-M architecture from `EfficientNetV2: Smaller Models and Faster Training <https://arxiv.org/abs/2104.00298>`_. Args: weights (:class:`~torchvision.models.EfficientNet_V2_M_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.EfficientNet_V2_M_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.EfficientNet_V2_M_Weights :members:
    - Function: `efficientnet_v2_s(*, weights: Optional[torchvision.models.efficientnet.EfficientNet_V2_S_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.efficientnet.EfficientNet`
      - Description: Constructs an EfficientNetV2-S architecture from `EfficientNetV2: Smaller Models and Faster Training <https://arxiv.org/abs/2104.00298>`_. Args: weights (:class:`~torchvision.models.EfficientNet_V2_S_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.EfficientNet_V2_S_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.efficientnet.EfficientNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.EfficientNet_V2_S_Weights :members:
    - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
      - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
    - Class: `partial(Unable to retrieve signature)`
      - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
    - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
      - Description: No docstring available
  - feature_extraction.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `DualGraphModule(root: torch.nn.modules.module.Module, train_graph: torch.fx.graph.Graph, eval_graph: torch.fx.graph.Graph, class_name: str = 'GraphModule')`
      - Description: A derivative of `fx.GraphModule`. Differs in the following ways: - Requires a train and eval version of the underlying graph - Copies submodules according to the nodes of both train and eval graphs. - Calling train(mode) switches between train graph and eval graph.
    - Class: `LeafModuleAwareTracer(*args, **kwargs)`
      - Description: An fx.Tracer that allows the user to specify a set of leaf modules, i.e. modules that are not to be traced through. The resulting graph ends up having single nodes referencing calls to the leaf modules' forward methods.
    - Class: `NodePathTracer(*args, **kwargs)`
      - Description: NodePathTracer is an FX tracer that, for each operation, also records the name of the Node from which the operation originated. A node name here is a `.` separated path walking the hierarchy from top level module down to leaf operation or leaf module. The name of the top level module is not included as part of the node name. For example, if we trace a module whose forward method applies a ReLU module, the name for that node will simply be 'relu'. Some notes on the specifics: - Nodes are recorded to `self.node_to_qualname` which is a dictionary mapping a given Node object to its node name. - Nodes are recorded in the order which they are executed during tracing. - When a duplicate node name is encountered, a suffix of the form _{int} is added. The counter starts from 1.
    - Class: `OrderedDict(Unable to retrieve signature)`
      - Description: Dictionary that remembers insertion order
    - Function: `_copy_attr(from_module: torch.nn.modules.module.Module, to_module: torch.nn.modules.module.Module, target: str)`
      - Description: No docstring available
    - Function: `_get_leaf_modules_for_ops() -> List[type]`
      - Description: No docstring available
    - Function: `_is_subseq(x, y)`
      - Description: Check if y is a subsequence of x https://stackoverflow.com/a/24017747/4391249
    - Function: `_set_default_tracer_kwargs(original_tr_kwargs: Optional[Dict[str, Any]]) -> Dict[str, Any]`
      - Description: No docstring available
    - Function: `_warn_graph_differences(train_tracer: torchvision.models.feature_extraction.NodePathTracer, eval_tracer: torchvision.models.feature_extraction.NodePathTracer)`
      - Description: Utility function for warning the user if there are differences between the train graph nodes and the eval graph nodes.
    - Class: `chain(Unable to retrieve signature)`
      - Description: chain(*iterables) --> chain object Return a chain object whose .__next__() method returns elements from the first iterable until it is exhausted, then elements from the next iterable, until all of the iterables are exhausted.
    - Function: `create_feature_extractor(model: torch.nn.modules.module.Module, return_nodes: Union[List[str], Dict[str, str], NoneType] = None, train_return_nodes: Union[List[str], Dict[str, str], NoneType] = None, eval_return_nodes: Union[List[str], Dict[str, str], NoneType] = None, tracer_kwargs: Optional[Dict[str, Any]] = None, suppress_diff_warning: bool = False) -> torch.fx.graph_module.GraphModule`
      - Description: Creates a new graph module that returns intermediate nodes from a given model as dictionary with user specified keys as strings, and the requested outputs as values. This is achieved by re-writing the computation graph of the model via FX to return the desired nodes as outputs. All unused nodes are removed, together with their corresponding parameters. Desired output nodes must be specified as a ``.`` separated path walking the module hierarchy from top level module down to leaf operation or leaf module. For more details on the node naming conventions used here, please see the :ref:`relevant subheading <about-node-names>` in the `documentation <https://pytorch.org/vision/stable/feature_extraction.html>`_. Not all models will be FX traceable, although with some massaging they can be made to cooperate. Here's a (not exhaustive) list of tips: - If you don't need to trace through a particular, problematic sub-module, turn it into a "leaf module" by passing a list of ``leaf_modules`` as one of the ``tracer_kwargs`` (see example below). It will not be traced through, but rather, the resulting graph will hold a reference to that module's forward method. - Likewise, you may turn functions into leaf functions by passing a list of ``autowrap_functions`` as one of the ``tracer_kwargs`` (see example below). - Some inbuilt Python functions can be problematic. For instance, ``int`` will raise an error during tracing. You may wrap them in your own function and then pass that in ``autowrap_functions`` as one of the ``tracer_kwargs``. For further information on FX see the `torch.fx documentation <https://pytorch.org/docs/stable/fx.html>`_. Args: model (nn.Module): model on which we will extract the features return_nodes (list or dict, optional): either a ``List`` or a ``Dict`` containing the names (or partial names - see note above) of the nodes for which the activations will be returned. If it is a ``Dict``, the keys are the node names, and the values are the user-specified keys for the graph module's returned dictionary. If it is a ``List``, it is treated as a ``Dict`` mapping node specification strings directly to output names. In the case that ``train_return_nodes`` and ``eval_return_nodes`` are specified, this should not be specified. train_return_nodes (list or dict, optional): similar to ``return_nodes``. This can be used if the return nodes for train mode are different than those from eval mode. If this is specified, ``eval_return_nodes`` must also be specified, and ``return_nodes`` should not be specified. eval_return_nodes (list or dict, optional): similar to ``return_nodes``. This can be used if the return nodes for train mode are different than those from eval mode. If this is specified, ``train_return_nodes`` must also be specified, and `return_nodes` should not be specified. tracer_kwargs (dict, optional): a dictionary of keyword arguments for ``NodePathTracer`` (which passes them onto it's parent class `torch.fx.Tracer <https://pytorch.org/docs/stable/fx.html#torch.fx.Tracer>`_). By default, it will be set to wrap and make leaf nodes all torchvision ops: {"autowrap_modules": (math, torchvision.ops,),"leaf_modules": _get_leaf_modules_for_ops(),} WARNING: In case the user provides tracer_kwargs, above default arguments will be appended to the user provided dictionary. suppress_diff_warning (bool, optional): whether to suppress a warning when there are discrepancies between the train and eval version of the graph. Defaults to False. Examples:: >>> # Feature extraction with resnet >>> model = torchvision.models.resnet18() >>> # extract layer1 and layer3, giving as names `feat1` and feat2` >>> model = create_feature_extractor( >>> model, {'layer1': 'feat1', 'layer3': 'feat2'}) >>> out = model(torch.rand(1, 3, 224, 224)) >>> print([(k, v.shape) for k, v in out.items()]) >>> [('feat1', torch.Size([1, 64, 56, 56])), >>> ('feat2', torch.Size([1, 256, 14, 14]))] >>> # Specifying leaf modules and leaf functions >>> def leaf_function(x): >>> # This would raise a TypeError if traced through >>> return int(x) >>> >>> class LeafModule(torch.nn.Module): >>> def forward(self, x): >>> # This would raise a TypeError if traced through >>> int(x.shape[0]) >>> return torch.nn.functional.relu(x + 4) >>> >>> class MyModule(torch.nn.Module): >>> def __init__(self): >>> super().__init__() >>> self.conv = torch.nn.Conv2d(3, 1, 3) >>> self.leaf_module = LeafModule() >>> >>> def forward(self, x): >>> leaf_function(x.shape[0]) >>> x = self.conv(x) >>> return self.leaf_module(x) >>> >>> model = create_feature_extractor( >>> MyModule(), return_nodes=['leaf_module'], >>> tracer_kwargs={'leaf_modules': [LeafModule], >>> 'autowrap_functions': [leaf_function]})
    - Function: `deepcopy(x, memo=None, _nil=[])`
      - Description: Deep copy operation on arbitrary Python objects. See the module's __doc__ string for more info.
    - Function: `get_graph_node_names(model: torch.nn.modules.module.Module, tracer_kwargs: Optional[Dict[str, Any]] = None, suppress_diff_warning: bool = False) -> Tuple[List[str], List[str]]`
      - Description: Dev utility to return node names in order of execution. See note on node names under :func:`create_feature_extractor`. Useful for seeing which node names are available for feature extraction. There are two reasons that node names can't easily be read directly from the code for a model: 1. Not all submodules are traced through. Modules from ``torch.nn`` all fall within this category. 2. Nodes representing the repeated application of the same operation or leaf module get a ``_{counter}`` postfix. The model is traced twice: once in train mode, and once in eval mode. Both sets of node names are returned. For more details on the node naming conventions used here, please see the :ref:`relevant subheading <about-node-names>` in the `documentation <https://pytorch.org/vision/stable/feature_extraction.html>`_. Args: model (nn.Module): model for which we'd like to print node names tracer_kwargs (dict, optional): a dictionary of keyword arguments for ``NodePathTracer`` (they are eventually passed onto `torch.fx.Tracer <https://pytorch.org/docs/stable/fx.html#torch.fx.Tracer>`_). By default, it will be set to wrap and make leaf nodes all torchvision ops: {"autowrap_modules": (math, torchvision.ops,),"leaf_modules": _get_leaf_modules_for_ops(),} WARNING: In case the user provides tracer_kwargs, above default arguments will be appended to the user provided dictionary. suppress_diff_warning (bool, optional): whether to suppress a warning when there are discrepancies between the train and eval version of the graph. Defaults to False. Returns: tuple(list, list): a list of node names from tracing the model in train mode, and another from tracing the model in eval mode. Examples:: >>> model = torchvision.models.resnet18() >>> train_nodes, eval_nodes = get_graph_node_names(model)
  - googlenet.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `BasicConv2d(in_channels: int, out_channels: int, **kwargs: Any) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `GoogLeNet(num_classes: int = 1000, aux_logits: bool = True, transform_input: bool = False, init_weights: Optional[bool] = None, blocks: Optional[List[Callable[..., torch.nn.modules.module.Module]]] = None, dropout: float = 0.2, dropout_aux: float = 0.7) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `GoogLeNetOutputs(logits, aux_logits2, aux_logits1)`
      - Description: GoogLeNetOutputs(logits, aux_logits2, aux_logits1)
    - Class: `GoogLeNet_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `Inception(in_channels: int, ch1x1: int, ch3x3red: int, ch3x3: int, ch5x5red: int, ch5x5: int, pool_proj: int, conv_block: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `InceptionAux(in_channels: int, num_classes: int, conv_block: Optional[Callable[..., torch.nn.modules.module.Module]] = None, dropout: float = 0.7) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
      - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
    - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `_GoogLeNetOutputs(logits, aux_logits2, aux_logits1)`
      - Description: GoogLeNetOutputs(logits, aux_logits2, aux_logits1)
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
      - Description: No docstring available
    - Function: `googlenet(*, weights: Optional[torchvision.models.googlenet.GoogLeNet_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.googlenet.GoogLeNet`
      - Description: GoogLeNet (Inception v1) model architecture from `Going Deeper with Convolutions <http://arxiv.org/abs/1409.4842>`_. Args: weights (:class:`~torchvision.models.GoogLeNet_Weights`, optional): The pretrained weights for the model. See :class:`~torchvision.models.GoogLeNet_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.GoogLeNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/googlenet.py>`_ for more details about this class. .. autoclass:: torchvision.models.GoogLeNet_Weights :members:
    - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
      - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
    - Function: `namedtuple(typename, field_names, *, rename=False, defaults=None, module=None)`
      - Description: Returns a new subclass of tuple with named fields. >>> Point = namedtuple('Point', ['x', 'y']) >>> Point.__doc__ # docstring for the new class 'Point(x, y)' >>> p = Point(11, y=22) # instantiate with positional args or keywords >>> p[0] + p[1] # indexable like a plain tuple 33 >>> x, y = p # unpack like a regular tuple >>> x, y (11, 22) >>> p.x + p.y # fields also accessible by name 33 >>> d = p._asdict() # convert to a dictionary >>> d['x'] 11 >>> Point(**d) # convert from a dictionary Point(x=11, y=22) >>> p._replace(x=100) # _replace() is like str.replace() but targets named fields Point(x=100, y=22)
    - Class: `partial(Unable to retrieve signature)`
      - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
    - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
      - Description: No docstring available
  - inception.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `BasicConv2d(in_channels: int, out_channels: int, **kwargs: Any) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `Inception3(num_classes: int = 1000, aux_logits: bool = True, transform_input: bool = False, inception_blocks: Optional[List[Callable[..., torch.nn.modules.module.Module]]] = None, init_weights: Optional[bool] = None, dropout: float = 0.5) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `InceptionA(in_channels: int, pool_features: int, conv_block: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `InceptionAux(in_channels: int, num_classes: int, conv_block: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `InceptionB(in_channels: int, conv_block: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `InceptionC(in_channels: int, channels_7x7: int, conv_block: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `InceptionD(in_channels: int, conv_block: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `InceptionE(in_channels: int, conv_block: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `InceptionOutputs(logits, aux_logits)`
      - Description: InceptionOutputs(logits, aux_logits)
    - Class: `Inception_V3_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
      - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
    - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `_InceptionOutputs(logits, aux_logits)`
      - Description: InceptionOutputs(logits, aux_logits)
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
      - Description: No docstring available
    - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
      - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
    - Function: `inception_v3(*, weights: Optional[torchvision.models.inception.Inception_V3_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.inception.Inception3`
      - Description: Inception v3 model architecture from `Rethinking the Inception Architecture for Computer Vision <http://arxiv.org/abs/1512.00567>`_. .. note:: **Important**: In contrast to the other models the inception_v3 expects tensors with a size of N x 3 x 299 x 299, so ensure your images are sized accordingly. Args: weights (:class:`~torchvision.models.Inception_V3_Weights`, optional): The pretrained weights for the model. See :class:`~torchvision.models.Inception_V3_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.Inception3`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/inception.py>`_ for more details about this class. .. autoclass:: torchvision.models.Inception_V3_Weights :members:
    - Function: `namedtuple(typename, field_names, *, rename=False, defaults=None, module=None)`
      - Description: Returns a new subclass of tuple with named fields. >>> Point = namedtuple('Point', ['x', 'y']) >>> Point.__doc__ # docstring for the new class 'Point(x, y)' >>> p = Point(11, y=22) # instantiate with positional args or keywords >>> p[0] + p[1] # indexable like a plain tuple 33 >>> x, y = p # unpack like a regular tuple >>> x, y (11, 22) >>> p.x + p.y # fields also accessible by name 33 >>> d = p._asdict() # convert to a dictionary >>> d['x'] 11 >>> Point(**d) # convert from a dictionary Point(x=11, y=22) >>> p._replace(x=100) # _replace() is like str.replace() but targets named fields Point(x=100, y=22)
    - Class: `partial(Unable to retrieve signature)`
      - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
    - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
      - Description: No docstring available
  - maxvit.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Conv2dNormActivation(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]] = 3, stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int], str, NoneType] = None, groups: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, dilation: Union[int, Tuple[int, int]] = 1, inplace: Optional[bool] = True, bias: Optional[bool] = None) -> None`
      - Description: Configurable block used for Convolution2d-Normalization-Activation blocks. Args: in_channels (int): Number of channels in the input image out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block kernel_size: (int, optional): Size of the convolving kernel. Default: 3 stride (int, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation`` groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm2d`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` dilation (int): Spacing between kernel elements. Default: 1 inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True`` bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.
    - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `InterpolationMode(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: Interpolation modes Available interpolation methods are ``nearest``, ``nearest-exact``, ``bilinear``, ``bicubic``, ``box``, ``hamming``, and ``lanczos``.
    - Class: `MBConv(in_channels: int, out_channels: int, expansion_ratio: float, squeeze_ratio: float, stride: int, activation_layer: Callable[..., torch.nn.modules.module.Module], norm_layer: Callable[..., torch.nn.modules.module.Module], p_stochastic_dropout: float = 0.0) -> None`
      - Description: MBConv: Mobile Inverted Residual Bottleneck. Args: in_channels (int): Number of input channels. out_channels (int): Number of output channels. expansion_ratio (float): Expansion ratio in the bottleneck. squeeze_ratio (float): Squeeze ratio in the SE Layer. stride (int): Stride of the depthwise convolution. activation_layer (Callable[..., nn.Module]): Activation function. norm_layer (Callable[..., nn.Module]): Normalization function. p_stochastic_dropout (float): Probability of stochastic depth.
    - Class: `MaxVit(input_size: Tuple[int, int], stem_channels: int, partition_size: int, block_channels: List[int], block_layers: List[int], head_dim: int, stochastic_depth_prob: float, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None, activation_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.activation.GELU'>, squeeze_ratio: float = 0.25, expansion_ratio: float = 4, mlp_ratio: int = 4, mlp_dropout: float = 0.0, attention_dropout: float = 0.0, num_classes: int = 1000) -> None`
      - Description: Implements MaxVit Transformer from the `MaxViT: Multi-Axis Vision Transformer <https://arxiv.org/abs/2204.01697>`_ paper. Args: input_size (Tuple[int, int]): Size of the input image. stem_channels (int): Number of channels in the stem. partition_size (int): Size of the partitions. block_channels (List[int]): Number of channels in each block. block_layers (List[int]): Number of layers in each block. stochastic_depth_prob (float): Probability of stochastic depth. Expands to a list of probabilities for each layer that scales linearly to the specified value. squeeze_ratio (float): Squeeze ratio in the SE Layer. Default: 0.25. expansion_ratio (float): Expansion ratio in the MBConv bottleneck. Default: 4. norm_layer (Callable[..., nn.Module]): Normalization function. Default: None (setting to None will produce a `BatchNorm2d(eps=1e-3, momentum=0.99)`). activation_layer (Callable[..., nn.Module]): Activation function Default: nn.GELU. head_dim (int): Dimension of the attention heads. mlp_ratio (int): Expansion ratio of the MLP layer. Default: 4. mlp_dropout (float): Dropout probability for the MLP layer. Default: 0.0. attention_dropout (float): Dropout probability for the attention layer. Default: 0.0. num_classes (int): Number of classes. Default: 1000.
    - Class: `MaxVitBlock(in_channels: int, out_channels: int, squeeze_ratio: float, expansion_ratio: float, norm_layer: Callable[..., torch.nn.modules.module.Module], activation_layer: Callable[..., torch.nn.modules.module.Module], head_dim: int, mlp_ratio: int, mlp_dropout: float, attention_dropout: float, partition_size: int, input_grid_size: Tuple[int, int], n_layers: int, p_stochastic: List[float]) -> None`
      - Description: A MaxVit block consisting of `n_layers` MaxVit layers. Args: in_channels (int): Number of input channels. out_channels (int): Number of output channels. expansion_ratio (float): Expansion ratio in the bottleneck. squeeze_ratio (float): Squeeze ratio in the SE Layer. activation_layer (Callable[..., nn.Module]): Activation function. norm_layer (Callable[..., nn.Module]): Normalization function. head_dim (int): Dimension of the attention heads. mlp_ratio (int): Ratio of the MLP layer. mlp_dropout (float): Dropout probability for the MLP layer. attention_dropout (float): Dropout probability for the attention layer. p_stochastic_dropout (float): Probability of stochastic depth. partition_size (int): Size of the partitions. input_grid_size (Tuple[int, int]): Size of the input feature grid. n_layers (int): Number of layers in the block. p_stochastic (List[float]): List of probabilities for stochastic depth for each layer.
    - Class: `MaxVitLayer(in_channels: int, out_channels: int, squeeze_ratio: float, expansion_ratio: float, stride: int, norm_layer: Callable[..., torch.nn.modules.module.Module], activation_layer: Callable[..., torch.nn.modules.module.Module], head_dim: int, mlp_ratio: int, mlp_dropout: float, attention_dropout: float, p_stochastic_dropout: float, partition_size: int, grid_size: Tuple[int, int]) -> None`
      - Description: MaxVit layer consisting of a MBConv layer followed by a PartitionAttentionLayer with `window` and a PartitionAttentionLayer with `grid`. Args: in_channels (int): Number of input channels. out_channels (int): Number of output channels. expansion_ratio (float): Expansion ratio in the bottleneck. squeeze_ratio (float): Squeeze ratio in the SE Layer. stride (int): Stride of the depthwise convolution. activation_layer (Callable[..., nn.Module]): Activation function. norm_layer (Callable[..., nn.Module]): Normalization function. head_dim (int): Dimension of the attention heads. mlp_ratio (int): Ratio of the MLP layer. mlp_dropout (float): Dropout probability for the MLP layer. attention_dropout (float): Dropout probability for the attention layer. p_stochastic_dropout (float): Probability of stochastic depth. partition_size (int): Size of the partitions. grid_size (Tuple[int, int]): Size of the input feature grid.
    - Class: `MaxVit_T_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `OrderedDict(Unable to retrieve signature)`
      - Description: Dictionary that remembers insertion order
    - Class: `PartitionAttentionLayer(in_channels: int, head_dim: int, partition_size: int, partition_type: str, grid_size: Tuple[int, int], mlp_ratio: int, activation_layer: Callable[..., torch.nn.modules.module.Module], norm_layer: Callable[..., torch.nn.modules.module.Module], attention_dropout: float, mlp_dropout: float, p_stochastic_dropout: float) -> None`
      - Description: Layer for partitioning the input tensor into non-overlapping windows and applying attention to each window. Args: in_channels (int): Number of input channels. head_dim (int): Dimension of each attention head. partition_size (int): Size of the partitions. partition_type (str): Type of partitioning to use. Can be either "grid" or "window". grid_size (Tuple[int, int]): Size of the grid to partition the input tensor into. mlp_ratio (int): Ratio of the feature size expansion in the MLP layer. activation_layer (Callable[..., nn.Module]): Activation function to use. norm_layer (Callable[..., nn.Module]): Normalization function to use. attention_dropout (float): Dropout probability for the attention layer. mlp_dropout (float): Dropout probability for the MLP layer. p_stochastic_dropout (float): Probability of dropping out a partition.
    - Class: `RelativePositionalMultiHeadAttention(feat_dim: int, head_dim: int, max_seq_len: int) -> None`
      - Description: Relative Positional Multi-Head Attention. Args: feat_dim (int): Number of input features. head_dim (int): Number of features per head. max_seq_len (int): Maximum sequence length.
    - Class: `SqueezeExcitation(input_channels: int, squeeze_channels: int, activation: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.activation.ReLU'>, scale_activation: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.activation.Sigmoid'>) -> None`
      - Description: This block implements the Squeeze-and-Excitation block from https://arxiv.org/abs/1709.01507 (see Fig. 1). Parameters ``activation``, and ``scale_activation`` correspond to ``delta`` and ``sigma`` in eq. 3. Args: input_channels (int): Number of channels in the input image squeeze_channels (int): Number of squeeze channels activation (Callable[..., torch.nn.Module], optional): ``delta`` activation. Default: ``torch.nn.ReLU`` scale_activation (Callable[..., torch.nn.Module]): ``sigma`` activation. Default: ``torch.nn.Sigmoid``
    - Class: `StochasticDepth(p: float, mode: str) -> None`
      - Description: See :func:`stochastic_depth`.
    - Class: `SwapAxes(a: int, b: int) -> None`
      - Description: Permute the axes of a tensor.
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
      - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
    - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `WindowDepartition() -> None`
      - Description: Departition the input tensor of non-overlapping windows into a feature volume of layout [B, C, H, W].
    - Class: `WindowPartition() -> None`
      - Description: Partition the input tensor into non-overlapping windows.
    - Function: `_get_conv_output_shape(input_size: Tuple[int, int], kernel_size: int, stride: int, padding: int) -> Tuple[int, int]`
      - Description: No docstring available
    - Function: `_get_relative_position_index(height: int, width: int) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_make_block_input_shapes(input_size: Tuple[int, int], n_blocks: int) -> List[Tuple[int, int]]`
      - Description: Util function to check that the input size is correct for a MaxVit configuration.
    - Function: `_maxvit(stem_channels: int, block_channels: List[int], block_layers: List[int], stochastic_depth_prob: float, partition_size: int, head_dim: int, weights: Optional[torchvision.models._api.WeightsEnum] = None, progress: bool = False, **kwargs: Any) -> torchvision.models.maxvit.MaxVit`
      - Description: No docstring available
    - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
      - Description: No docstring available
    - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
      - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
    - Function: `maxvit_t(*, weights: Optional[torchvision.models.maxvit.MaxVit_T_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.maxvit.MaxVit`
      - Description: Constructs a maxvit_t architecture from `MaxViT: Multi-Axis Vision Transformer <https://arxiv.org/abs/2204.01697>`_. Args: weights (:class:`~torchvision.models.MaxVit_T_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.MaxVit_T_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.maxvit.MaxVit`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/maxvit.py>`_ for more details about this class. .. autoclass:: torchvision.models.MaxVit_T_Weights :members:
    - Class: `partial(Unable to retrieve signature)`
      - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
    - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
      - Description: No docstring available
  - mnasnet.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `MNASNet(alpha: float, num_classes: int = 1000, dropout: float = 0.2) -> None`
      - Description: MNASNet, as described in https://arxiv.org/abs/1807.11626. This implements the B1 variant of the model. >>> model = MNASNet(1.0, num_classes=1000) >>> x = torch.rand(1, 3, 224, 224) >>> y = model(x) >>> y.dim() 2 >>> y.nelement() 1000
    - Class: `MNASNet0_5_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `MNASNet0_75_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `MNASNet1_0_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `MNASNet1_3_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
      - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
    - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `_InvertedResidual(in_ch: int, out_ch: int, kernel_size: int, stride: int, expansion_factor: int, bn_momentum: float = 0.1) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Function: `_get_depths(alpha: float) -> List[int]`
      - Description: Scales tensor depths as in reference MobileNet code, prefers rounding up rather than down.
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_mnasnet(alpha: float, weights: Optional[torchvision.models._api.WeightsEnum], progress: bool, **kwargs: Any) -> torchvision.models.mnasnet.MNASNet`
      - Description: No docstring available
    - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
      - Description: No docstring available
    - Function: `_round_to_multiple_of(val: float, divisor: int, round_up_bias: float = 0.9) -> int`
      - Description: Asymmetric rounding to make `val` divisible by `divisor`. With default bias, will round up, unless the number is no more than 10% greater than the smaller divisible value, i.e. (83, 8) -> 80, but (84, 8) -> 88.
    - Function: `_stack(in_ch: int, out_ch: int, kernel_size: int, stride: int, exp_factor: int, repeats: int, bn_momentum: float) -> torch.nn.modules.container.Sequential`
      - Description: Creates a stack of inverted residuals.
    - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
      - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
    - Function: `mnasnet0_5(*, weights: Optional[torchvision.models.mnasnet.MNASNet0_5_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.mnasnet.MNASNet`
      - Description: MNASNet with depth multiplier of 0.5 from `MnasNet: Platform-Aware Neural Architecture Search for Mobile <https://arxiv.org/abs/1807.11626>`_ paper. Args: weights (:class:`~torchvision.models.MNASNet0_5_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.MNASNet0_5_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.mnasnet.MNASNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/mnasnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.MNASNet0_5_Weights :members:
    - Function: `mnasnet0_75(*, weights: Optional[torchvision.models.mnasnet.MNASNet0_75_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.mnasnet.MNASNet`
      - Description: MNASNet with depth multiplier of 0.75 from `MnasNet: Platform-Aware Neural Architecture Search for Mobile <https://arxiv.org/abs/1807.11626>`_ paper. Args: weights (:class:`~torchvision.models.MNASNet0_75_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.MNASNet0_75_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.mnasnet.MNASNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/mnasnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.MNASNet0_75_Weights :members:
    - Function: `mnasnet1_0(*, weights: Optional[torchvision.models.mnasnet.MNASNet1_0_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.mnasnet.MNASNet`
      - Description: MNASNet with depth multiplier of 1.0 from `MnasNet: Platform-Aware Neural Architecture Search for Mobile <https://arxiv.org/abs/1807.11626>`_ paper. Args: weights (:class:`~torchvision.models.MNASNet1_0_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.MNASNet1_0_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.mnasnet.MNASNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/mnasnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.MNASNet1_0_Weights :members:
    - Function: `mnasnet1_3(*, weights: Optional[torchvision.models.mnasnet.MNASNet1_3_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.mnasnet.MNASNet`
      - Description: MNASNet with depth multiplier of 1.3 from `MnasNet: Platform-Aware Neural Architecture Search for Mobile <https://arxiv.org/abs/1807.11626>`_ paper. Args: weights (:class:`~torchvision.models.MNASNet1_3_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.MNASNet1_3_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.mnasnet.MNASNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/mnasnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.MNASNet1_3_Weights :members:
    - Class: `partial(Unable to retrieve signature)`
      - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
    - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
      - Description: No docstring available
  - mobilenet.py
    - Class: `MobileNetV2(num_classes: int = 1000, width_mult: float = 1.0, inverted_residual_setting: Optional[List[List[int]]] = None, round_nearest: int = 8, block: Optional[Callable[..., torch.nn.modules.module.Module]] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None, dropout: float = 0.2) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `MobileNetV3(inverted_residual_setting: List[torchvision.models.mobilenetv3.InvertedResidualConfig], last_channel: int, num_classes: int = 1000, block: Optional[Callable[..., torch.nn.modules.module.Module]] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None, dropout: float = 0.2, **kwargs: Any) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `MobileNet_V2_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `MobileNet_V3_Large_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `MobileNet_V3_Small_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Function: `mobilenet_v2(*, weights: Optional[torchvision.models.mobilenetv2.MobileNet_V2_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.mobilenetv2.MobileNetV2`
      - Description: MobileNetV2 architecture from the `MobileNetV2: Inverted Residuals and Linear Bottlenecks <https://arxiv.org/abs/1801.04381>`_ paper. Args: weights (:class:`~torchvision.models.MobileNet_V2_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.MobileNet_V2_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.mobilenetv2.MobileNetV2`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py>`_ for more details about this class. .. autoclass:: torchvision.models.MobileNet_V2_Weights :members:
    - Function: `mobilenet_v3_large(*, weights: Optional[torchvision.models.mobilenetv3.MobileNet_V3_Large_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.mobilenetv3.MobileNetV3`
      - Description: Constructs a large MobileNetV3 architecture from `Searching for MobileNetV3 <https://arxiv.org/abs/1905.02244>`__. Args: weights (:class:`~torchvision.models.MobileNet_V3_Large_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.MobileNet_V3_Large_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.mobilenet.MobileNetV3`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv3.py>`_ for more details about this class. .. autoclass:: torchvision.models.MobileNet_V3_Large_Weights :members:
    - Function: `mobilenet_v3_small(*, weights: Optional[torchvision.models.mobilenetv3.MobileNet_V3_Small_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.mobilenetv3.MobileNetV3`
      - Description: Constructs a small MobileNetV3 architecture from `Searching for MobileNetV3 <https://arxiv.org/abs/1905.02244>`__. Args: weights (:class:`~torchvision.models.MobileNet_V3_Small_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.MobileNet_V3_Small_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.mobilenet.MobileNetV3`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv3.py>`_ for more details about this class. .. autoclass:: torchvision.models.MobileNet_V3_Small_Weights :members:
  - mobilenetv2.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Conv2dNormActivation(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]] = 3, stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int], str, NoneType] = None, groups: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, dilation: Union[int, Tuple[int, int]] = 1, inplace: Optional[bool] = True, bias: Optional[bool] = None) -> None`
      - Description: Configurable block used for Convolution2d-Normalization-Activation blocks. Args: in_channels (int): Number of channels in the input image out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block kernel_size: (int, optional): Size of the convolving kernel. Default: 3 stride (int, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation`` groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm2d`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` dilation (int): Spacing between kernel elements. Default: 1 inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True`` bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.
    - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `InvertedResidual(inp: int, oup: int, stride: int, expand_ratio: int, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `MobileNetV2(num_classes: int = 1000, width_mult: float = 1.0, inverted_residual_setting: Optional[List[List[int]]] = None, round_nearest: int = 8, block: Optional[Callable[..., torch.nn.modules.module.Module]] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None, dropout: float = 0.2) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `MobileNet_V2_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
      - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
    - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int`
      - Description: This function is taken from the original tf repo. It ensures that all layers have a channel number that is divisible by 8 It can be seen here: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
    - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
      - Description: No docstring available
    - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
      - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
    - Function: `mobilenet_v2(*, weights: Optional[torchvision.models.mobilenetv2.MobileNet_V2_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.mobilenetv2.MobileNetV2`
      - Description: MobileNetV2 architecture from the `MobileNetV2: Inverted Residuals and Linear Bottlenecks <https://arxiv.org/abs/1801.04381>`_ paper. Args: weights (:class:`~torchvision.models.MobileNet_V2_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.MobileNet_V2_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.mobilenetv2.MobileNetV2`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py>`_ for more details about this class. .. autoclass:: torchvision.models.MobileNet_V2_Weights :members:
    - Class: `partial(Unable to retrieve signature)`
      - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
    - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
      - Description: No docstring available
  - mobilenetv3.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Conv2dNormActivation(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]] = 3, stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int], str, NoneType] = None, groups: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, dilation: Union[int, Tuple[int, int]] = 1, inplace: Optional[bool] = True, bias: Optional[bool] = None) -> None`
      - Description: Configurable block used for Convolution2d-Normalization-Activation blocks. Args: in_channels (int): Number of channels in the input image out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block kernel_size: (int, optional): Size of the convolving kernel. Default: 3 stride (int, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation`` groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm2d`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` dilation (int): Spacing between kernel elements. Default: 1 inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True`` bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.
    - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `InvertedResidual(cnf: torchvision.models.mobilenetv3.InvertedResidualConfig, norm_layer: Callable[..., torch.nn.modules.module.Module], se_layer: Callable[..., torch.nn.modules.module.Module] = functools.partial(<class 'torchvision.ops.misc.SqueezeExcitation'>, scale_activation=<class 'torch.nn.modules.activation.Hardsigmoid'>))`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `InvertedResidualConfig(input_channels: int, kernel: int, expanded_channels: int, out_channels: int, use_se: bool, activation: str, stride: int, dilation: int, width_mult: float)`
      - Description: No docstring available
    - Class: `MobileNetV3(inverted_residual_setting: List[torchvision.models.mobilenetv3.InvertedResidualConfig], last_channel: int, num_classes: int = 1000, block: Optional[Callable[..., torch.nn.modules.module.Module]] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None, dropout: float = 0.2, **kwargs: Any) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `MobileNet_V3_Large_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `MobileNet_V3_Small_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `SElayer(input_channels: int, squeeze_channels: int, activation: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.activation.ReLU'>, scale_activation: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.activation.Sigmoid'>) -> None`
      - Description: This block implements the Squeeze-and-Excitation block from https://arxiv.org/abs/1709.01507 (see Fig. 1). Parameters ``activation``, and ``scale_activation`` correspond to ``delta`` and ``sigma`` in eq. 3. Args: input_channels (int): Number of channels in the input image squeeze_channels (int): Number of squeeze channels activation (Callable[..., torch.nn.Module], optional): ``delta`` activation. Default: ``torch.nn.ReLU`` scale_activation (Callable[..., torch.nn.Module]): ``sigma`` activation. Default: ``torch.nn.Sigmoid``
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
      - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
    - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int`
      - Description: This function is taken from the original tf repo. It ensures that all layers have a channel number that is divisible by 8 It can be seen here: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
    - Function: `_mobilenet_v3(inverted_residual_setting: List[torchvision.models.mobilenetv3.InvertedResidualConfig], last_channel: int, weights: Optional[torchvision.models._api.WeightsEnum], progress: bool, **kwargs: Any) -> torchvision.models.mobilenetv3.MobileNetV3`
      - Description: No docstring available
    - Function: `_mobilenet_v3_conf(arch: str, width_mult: float = 1.0, reduced_tail: bool = False, dilated: bool = False, **kwargs: Any)`
      - Description: No docstring available
    - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
      - Description: No docstring available
    - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
      - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
    - Function: `mobilenet_v3_large(*, weights: Optional[torchvision.models.mobilenetv3.MobileNet_V3_Large_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.mobilenetv3.MobileNetV3`
      - Description: Constructs a large MobileNetV3 architecture from `Searching for MobileNetV3 <https://arxiv.org/abs/1905.02244>`__. Args: weights (:class:`~torchvision.models.MobileNet_V3_Large_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.MobileNet_V3_Large_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.mobilenet.MobileNetV3`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv3.py>`_ for more details about this class. .. autoclass:: torchvision.models.MobileNet_V3_Large_Weights :members:
    - Function: `mobilenet_v3_small(*, weights: Optional[torchvision.models.mobilenetv3.MobileNet_V3_Small_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.mobilenetv3.MobileNetV3`
      - Description: Constructs a small MobileNetV3 architecture from `Searching for MobileNetV3 <https://arxiv.org/abs/1905.02244>`__. Args: weights (:class:`~torchvision.models.MobileNet_V3_Small_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.MobileNet_V3_Small_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.mobilenet.MobileNetV3`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv3.py>`_ for more details about this class. .. autoclass:: torchvision.models.MobileNet_V3_Small_Weights :members:
    - Class: `partial(Unable to retrieve signature)`
      - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
    - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
      - Description: No docstring available
  - regnet.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `AnyStage(width_in: int, width_out: int, stride: int, depth: int, block_constructor: Callable[..., torch.nn.modules.module.Module], norm_layer: Callable[..., torch.nn.modules.module.Module], activation_layer: Callable[..., torch.nn.modules.module.Module], group_width: int, bottleneck_multiplier: float, se_ratio: Optional[float] = None, stage_index: int = 0) -> None`
      - Description: AnyNet stage (sequence of blocks w/ the same output shape).
    - Class: `BlockParams(depths: List[int], widths: List[int], group_widths: List[int], bottleneck_multipliers: List[float], strides: List[int], se_ratio: Optional[float] = None) -> None`
      - Description: No docstring available
    - Class: `BottleneckTransform(width_in: int, width_out: int, stride: int, norm_layer: Callable[..., torch.nn.modules.module.Module], activation_layer: Callable[..., torch.nn.modules.module.Module], group_width: int, bottleneck_multiplier: float, se_ratio: Optional[float]) -> None`
      - Description: Bottleneck transformation: 1x1, 3x3 [+SE], 1x1.
    - Class: `Conv2dNormActivation(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]] = 3, stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int], str, NoneType] = None, groups: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, dilation: Union[int, Tuple[int, int]] = 1, inplace: Optional[bool] = True, bias: Optional[bool] = None) -> None`
      - Description: Configurable block used for Convolution2d-Normalization-Activation blocks. Args: in_channels (int): Number of channels in the input image out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block kernel_size: (int, optional): Size of the convolving kernel. Default: 3 stride (int, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation`` groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm2d`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` dilation (int): Spacing between kernel elements. Default: 1 inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True`` bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.
    - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `InterpolationMode(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: Interpolation modes Available interpolation methods are ``nearest``, ``nearest-exact``, ``bilinear``, ``bicubic``, ``box``, ``hamming``, and ``lanczos``.
    - Class: `OrderedDict(Unable to retrieve signature)`
      - Description: Dictionary that remembers insertion order
    - Class: `RegNet(block_params: torchvision.models.regnet.BlockParams, num_classes: int = 1000, stem_width: int = 32, stem_type: Optional[Callable[..., torch.nn.modules.module.Module]] = None, block_type: Optional[Callable[..., torch.nn.modules.module.Module]] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None, activation: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `RegNet_X_16GF_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `RegNet_X_1_6GF_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `RegNet_X_32GF_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `RegNet_X_3_2GF_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `RegNet_X_400MF_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `RegNet_X_800MF_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `RegNet_X_8GF_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `RegNet_Y_128GF_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `RegNet_Y_16GF_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `RegNet_Y_1_6GF_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `RegNet_Y_32GF_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `RegNet_Y_3_2GF_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `RegNet_Y_400MF_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `RegNet_Y_800MF_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `RegNet_Y_8GF_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ResBottleneckBlock(width_in: int, width_out: int, stride: int, norm_layer: Callable[..., torch.nn.modules.module.Module], activation_layer: Callable[..., torch.nn.modules.module.Module], group_width: int = 1, bottleneck_multiplier: float = 1.0, se_ratio: Optional[float] = None) -> None`
      - Description: Residual bottleneck block: x + F(x), F = bottleneck transform.
    - Class: `SimpleStemIN(width_in: int, width_out: int, norm_layer: Callable[..., torch.nn.modules.module.Module], activation_layer: Callable[..., torch.nn.modules.module.Module]) -> None`
      - Description: Simple stem for ImageNet: 3x3, BN, ReLU.
    - Class: `SqueezeExcitation(input_channels: int, squeeze_channels: int, activation: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.activation.ReLU'>, scale_activation: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.activation.Sigmoid'>) -> None`
      - Description: This block implements the Squeeze-and-Excitation block from https://arxiv.org/abs/1709.01507 (see Fig. 1). Parameters ``activation``, and ``scale_activation`` correspond to ``delta`` and ``sigma`` in eq. 3. Args: input_channels (int): Number of channels in the input image squeeze_channels (int): Number of squeeze channels activation (Callable[..., torch.nn.Module], optional): ``delta`` activation. Default: ``torch.nn.ReLU`` scale_activation (Callable[..., torch.nn.Module]): ``sigma`` activation. Default: ``torch.nn.Sigmoid``
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
      - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
    - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int`
      - Description: This function is taken from the original tf repo. It ensures that all layers have a channel number that is divisible by 8 It can be seen here: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
    - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
      - Description: No docstring available
    - Function: `_regnet(block_params: torchvision.models.regnet.BlockParams, weights: Optional[torchvision.models._api.WeightsEnum], progress: bool, **kwargs: Any) -> torchvision.models.regnet.RegNet`
      - Description: No docstring available
    - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
      - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
    - Class: `partial(Unable to retrieve signature)`
      - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
    - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
      - Description: No docstring available
    - Function: `regnet_x_16gf(*, weights: Optional[torchvision.models.regnet.RegNet_X_16GF_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.regnet.RegNet`
      - Description: Constructs a RegNetX_16GF architecture from `Designing Network Design Spaces <https://arxiv.org/abs/2003.13678>`_. Args: weights (:class:`~torchvision.models.RegNet_X_16GF_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.RegNet_X_16GF_Weights` below for more details and possible values. By default, no pretrained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to either ``torchvision.models.regnet.RegNet`` or ``torchvision.models.regnet.BlockParams`` class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/regnet.py>`_ for more detail about the classes. .. autoclass:: torchvision.models.RegNet_X_16GF_Weights :members:
    - Function: `regnet_x_1_6gf(*, weights: Optional[torchvision.models.regnet.RegNet_X_1_6GF_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.regnet.RegNet`
      - Description: Constructs a RegNetX_1.6GF architecture from `Designing Network Design Spaces <https://arxiv.org/abs/2003.13678>`_. Args: weights (:class:`~torchvision.models.RegNet_X_1_6GF_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.RegNet_X_1_6GF_Weights` below for more details and possible values. By default, no pretrained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to either ``torchvision.models.regnet.RegNet`` or ``torchvision.models.regnet.BlockParams`` class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/regnet.py>`_ for more detail about the classes. .. autoclass:: torchvision.models.RegNet_X_1_6GF_Weights :members:
    - Function: `regnet_x_32gf(*, weights: Optional[torchvision.models.regnet.RegNet_X_32GF_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.regnet.RegNet`
      - Description: Constructs a RegNetX_32GF architecture from `Designing Network Design Spaces <https://arxiv.org/abs/2003.13678>`_. Args: weights (:class:`~torchvision.models.RegNet_X_32GF_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.RegNet_X_32GF_Weights` below for more details and possible values. By default, no pretrained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to either ``torchvision.models.regnet.RegNet`` or ``torchvision.models.regnet.BlockParams`` class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/regnet.py>`_ for more detail about the classes. .. autoclass:: torchvision.models.RegNet_X_32GF_Weights :members:
    - Function: `regnet_x_3_2gf(*, weights: Optional[torchvision.models.regnet.RegNet_X_3_2GF_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.regnet.RegNet`
      - Description: Constructs a RegNetX_3.2GF architecture from `Designing Network Design Spaces <https://arxiv.org/abs/2003.13678>`_. Args: weights (:class:`~torchvision.models.RegNet_X_3_2GF_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.RegNet_X_3_2GF_Weights` below for more details and possible values. By default, no pretrained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to either ``torchvision.models.regnet.RegNet`` or ``torchvision.models.regnet.BlockParams`` class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/regnet.py>`_ for more detail about the classes. .. autoclass:: torchvision.models.RegNet_X_3_2GF_Weights :members:
    - Function: `regnet_x_400mf(*, weights: Optional[torchvision.models.regnet.RegNet_X_400MF_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.regnet.RegNet`
      - Description: Constructs a RegNetX_400MF architecture from `Designing Network Design Spaces <https://arxiv.org/abs/2003.13678>`_. Args: weights (:class:`~torchvision.models.RegNet_X_400MF_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.RegNet_X_400MF_Weights` below for more details and possible values. By default, no pretrained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to either ``torchvision.models.regnet.RegNet`` or ``torchvision.models.regnet.BlockParams`` class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/regnet.py>`_ for more detail about the classes. .. autoclass:: torchvision.models.RegNet_X_400MF_Weights :members:
    - Function: `regnet_x_800mf(*, weights: Optional[torchvision.models.regnet.RegNet_X_800MF_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.regnet.RegNet`
      - Description: Constructs a RegNetX_800MF architecture from `Designing Network Design Spaces <https://arxiv.org/abs/2003.13678>`_. Args: weights (:class:`~torchvision.models.RegNet_X_800MF_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.RegNet_X_800MF_Weights` below for more details and possible values. By default, no pretrained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to either ``torchvision.models.regnet.RegNet`` or ``torchvision.models.regnet.BlockParams`` class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/regnet.py>`_ for more detail about the classes. .. autoclass:: torchvision.models.RegNet_X_800MF_Weights :members:
    - Function: `regnet_x_8gf(*, weights: Optional[torchvision.models.regnet.RegNet_X_8GF_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.regnet.RegNet`
      - Description: Constructs a RegNetX_8GF architecture from `Designing Network Design Spaces <https://arxiv.org/abs/2003.13678>`_. Args: weights (:class:`~torchvision.models.RegNet_X_8GF_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.RegNet_X_8GF_Weights` below for more details and possible values. By default, no pretrained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to either ``torchvision.models.regnet.RegNet`` or ``torchvision.models.regnet.BlockParams`` class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/regnet.py>`_ for more detail about the classes. .. autoclass:: torchvision.models.RegNet_X_8GF_Weights :members:
    - Function: `regnet_y_128gf(*, weights: Optional[torchvision.models.regnet.RegNet_Y_128GF_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.regnet.RegNet`
      - Description: Constructs a RegNetY_128GF architecture from `Designing Network Design Spaces <https://arxiv.org/abs/2003.13678>`_. Args: weights (:class:`~torchvision.models.RegNet_Y_128GF_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.RegNet_Y_128GF_Weights` below for more details and possible values. By default, no pretrained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to either ``torchvision.models.regnet.RegNet`` or ``torchvision.models.regnet.BlockParams`` class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/regnet.py>`_ for more detail about the classes. .. autoclass:: torchvision.models.RegNet_Y_128GF_Weights :members:
    - Function: `regnet_y_16gf(*, weights: Optional[torchvision.models.regnet.RegNet_Y_16GF_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.regnet.RegNet`
      - Description: Constructs a RegNetY_16GF architecture from `Designing Network Design Spaces <https://arxiv.org/abs/2003.13678>`_. Args: weights (:class:`~torchvision.models.RegNet_Y_16GF_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.RegNet_Y_16GF_Weights` below for more details and possible values. By default, no pretrained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to either ``torchvision.models.regnet.RegNet`` or ``torchvision.models.regnet.BlockParams`` class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/regnet.py>`_ for more detail about the classes. .. autoclass:: torchvision.models.RegNet_Y_16GF_Weights :members:
    - Function: `regnet_y_1_6gf(*, weights: Optional[torchvision.models.regnet.RegNet_Y_1_6GF_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.regnet.RegNet`
      - Description: Constructs a RegNetY_1.6GF architecture from `Designing Network Design Spaces <https://arxiv.org/abs/2003.13678>`_. Args: weights (:class:`~torchvision.models.RegNet_Y_1_6GF_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.RegNet_Y_1_6GF_Weights` below for more details and possible values. By default, no pretrained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to either ``torchvision.models.regnet.RegNet`` or ``torchvision.models.regnet.BlockParams`` class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/regnet.py>`_ for more detail about the classes. .. autoclass:: torchvision.models.RegNet_Y_1_6GF_Weights :members:
    - Function: `regnet_y_32gf(*, weights: Optional[torchvision.models.regnet.RegNet_Y_32GF_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.regnet.RegNet`
      - Description: Constructs a RegNetY_32GF architecture from `Designing Network Design Spaces <https://arxiv.org/abs/2003.13678>`_. Args: weights (:class:`~torchvision.models.RegNet_Y_32GF_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.RegNet_Y_32GF_Weights` below for more details and possible values. By default, no pretrained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to either ``torchvision.models.regnet.RegNet`` or ``torchvision.models.regnet.BlockParams`` class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/regnet.py>`_ for more detail about the classes. .. autoclass:: torchvision.models.RegNet_Y_32GF_Weights :members:
    - Function: `regnet_y_3_2gf(*, weights: Optional[torchvision.models.regnet.RegNet_Y_3_2GF_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.regnet.RegNet`
      - Description: Constructs a RegNetY_3.2GF architecture from `Designing Network Design Spaces <https://arxiv.org/abs/2003.13678>`_. Args: weights (:class:`~torchvision.models.RegNet_Y_3_2GF_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.RegNet_Y_3_2GF_Weights` below for more details and possible values. By default, no pretrained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to either ``torchvision.models.regnet.RegNet`` or ``torchvision.models.regnet.BlockParams`` class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/regnet.py>`_ for more detail about the classes. .. autoclass:: torchvision.models.RegNet_Y_3_2GF_Weights :members:
    - Function: `regnet_y_400mf(*, weights: Optional[torchvision.models.regnet.RegNet_Y_400MF_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.regnet.RegNet`
      - Description: Constructs a RegNetY_400MF architecture from `Designing Network Design Spaces <https://arxiv.org/abs/2003.13678>`_. Args: weights (:class:`~torchvision.models.RegNet_Y_400MF_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.RegNet_Y_400MF_Weights` below for more details and possible values. By default, no pretrained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to either ``torchvision.models.regnet.RegNet`` or ``torchvision.models.regnet.BlockParams`` class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/regnet.py>`_ for more detail about the classes. .. autoclass:: torchvision.models.RegNet_Y_400MF_Weights :members:
    - Function: `regnet_y_800mf(*, weights: Optional[torchvision.models.regnet.RegNet_Y_800MF_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.regnet.RegNet`
      - Description: Constructs a RegNetY_800MF architecture from `Designing Network Design Spaces <https://arxiv.org/abs/2003.13678>`_. Args: weights (:class:`~torchvision.models.RegNet_Y_800MF_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.RegNet_Y_800MF_Weights` below for more details and possible values. By default, no pretrained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to either ``torchvision.models.regnet.RegNet`` or ``torchvision.models.regnet.BlockParams`` class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/regnet.py>`_ for more detail about the classes. .. autoclass:: torchvision.models.RegNet_Y_800MF_Weights :members:
    - Function: `regnet_y_8gf(*, weights: Optional[torchvision.models.regnet.RegNet_Y_8GF_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.regnet.RegNet`
      - Description: Constructs a RegNetY_8GF architecture from `Designing Network Design Spaces <https://arxiv.org/abs/2003.13678>`_. Args: weights (:class:`~torchvision.models.RegNet_Y_8GF_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.RegNet_Y_8GF_Weights` below for more details and possible values. By default, no pretrained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to either ``torchvision.models.regnet.RegNet`` or ``torchvision.models.regnet.BlockParams`` class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/regnet.py>`_ for more detail about the classes. .. autoclass:: torchvision.models.RegNet_Y_8GF_Weights :members:
  - resnet.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `BasicBlock(inplanes: int, planes: int, stride: int = 1, downsample: Optional[torch.nn.modules.module.Module] = None, groups: int = 1, base_width: int = 64, dilation: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `Bottleneck(inplanes: int, planes: int, stride: int = 1, downsample: Optional[torch.nn.modules.module.Module] = None, groups: int = 1, base_width: int = 64, dilation: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `ResNeXt101_32X8D_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ResNeXt101_64X4D_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ResNeXt50_32X4D_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ResNet(block: Type[Union[torchvision.models.resnet.BasicBlock, torchvision.models.resnet.Bottleneck]], layers: List[int], num_classes: int = 1000, zero_init_residual: bool = False, groups: int = 1, width_per_group: int = 64, replace_stride_with_dilation: Optional[List[bool]] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `ResNet101_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ResNet152_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ResNet18_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ResNet34_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ResNet50_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
      - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
    - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `Wide_ResNet101_2_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `Wide_ResNet50_2_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
      - Description: No docstring available
    - Function: `_resnet(block: Type[Union[torchvision.models.resnet.BasicBlock, torchvision.models.resnet.Bottleneck]], layers: List[int], weights: Optional[torchvision.models._api.WeightsEnum], progress: bool, **kwargs: Any) -> torchvision.models.resnet.ResNet`
      - Description: No docstring available
    - Function: `conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> torch.nn.modules.conv.Conv2d`
      - Description: 1x1 convolution
    - Function: `conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> torch.nn.modules.conv.Conv2d`
      - Description: 3x3 convolution with padding
    - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
      - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
    - Class: `partial(Unable to retrieve signature)`
      - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
    - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
      - Description: No docstring available
    - Function: `resnet101(*, weights: Optional[torchvision.models.resnet.ResNet101_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.resnet.ResNet`
      - Description: ResNet-101 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__. .. note:: The bottleneck of TorchVision places the stride for downsampling to the second 3x3 convolution while the original paper places it to the first 1x1 convolution. This variant improves the accuracy and is known as `ResNet V1.5 <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_. Args: weights (:class:`~torchvision.models.ResNet101_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ResNet101_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.ResNet101_Weights :members:
    - Function: `resnet152(*, weights: Optional[torchvision.models.resnet.ResNet152_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.resnet.ResNet`
      - Description: ResNet-152 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__. .. note:: The bottleneck of TorchVision places the stride for downsampling to the second 3x3 convolution while the original paper places it to the first 1x1 convolution. This variant improves the accuracy and is known as `ResNet V1.5 <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_. Args: weights (:class:`~torchvision.models.ResNet152_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ResNet152_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.ResNet152_Weights :members:
    - Function: `resnet18(*, weights: Optional[torchvision.models.resnet.ResNet18_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.resnet.ResNet`
      - Description: ResNet-18 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__. Args: weights (:class:`~torchvision.models.ResNet18_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ResNet18_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.ResNet18_Weights :members:
    - Function: `resnet34(*, weights: Optional[torchvision.models.resnet.ResNet34_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.resnet.ResNet`
      - Description: ResNet-34 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__. Args: weights (:class:`~torchvision.models.ResNet34_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ResNet34_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.ResNet34_Weights :members:
    - Function: `resnet50(*, weights: Optional[torchvision.models.resnet.ResNet50_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.resnet.ResNet`
      - Description: ResNet-50 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__. .. note:: The bottleneck of TorchVision places the stride for downsampling to the second 3x3 convolution while the original paper places it to the first 1x1 convolution. This variant improves the accuracy and is known as `ResNet V1.5 <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_. Args: weights (:class:`~torchvision.models.ResNet50_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ResNet50_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.ResNet50_Weights :members:
    - Function: `resnext101_32x8d(*, weights: Optional[torchvision.models.resnet.ResNeXt101_32X8D_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.resnet.ResNet`
      - Description: ResNeXt-101 32x8d model from `Aggregated Residual Transformation for Deep Neural Networks <https://arxiv.org/abs/1611.05431>`_. Args: weights (:class:`~torchvision.models.ResNeXt101_32X8D_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ResNeXt101_32X8D_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.ResNeXt101_32X8D_Weights :members:
    - Function: `resnext101_64x4d(*, weights: Optional[torchvision.models.resnet.ResNeXt101_64X4D_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.resnet.ResNet`
      - Description: ResNeXt-101 64x4d model from `Aggregated Residual Transformation for Deep Neural Networks <https://arxiv.org/abs/1611.05431>`_. Args: weights (:class:`~torchvision.models.ResNeXt101_64X4D_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ResNeXt101_64X4D_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.ResNeXt101_64X4D_Weights :members:
    - Function: `resnext50_32x4d(*, weights: Optional[torchvision.models.resnet.ResNeXt50_32X4D_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.resnet.ResNet`
      - Description: ResNeXt-50 32x4d model from `Aggregated Residual Transformation for Deep Neural Networks <https://arxiv.org/abs/1611.05431>`_. Args: weights (:class:`~torchvision.models.ResNeXt50_32X4D_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ResNext50_32X4D_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.ResNeXt50_32X4D_Weights :members:
    - Function: `wide_resnet101_2(*, weights: Optional[torchvision.models.resnet.Wide_ResNet101_2_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.resnet.ResNet`
      - Description: Wide ResNet-101-2 model from `Wide Residual Networks <https://arxiv.org/abs/1605.07146>`_. The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-101 has 2048-512-2048 channels, and in Wide ResNet-101-2 has 2048-1024-2048. Args: weights (:class:`~torchvision.models.Wide_ResNet101_2_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.Wide_ResNet101_2_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.Wide_ResNet101_2_Weights :members:
    - Function: `wide_resnet50_2(*, weights: Optional[torchvision.models.resnet.Wide_ResNet50_2_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.resnet.ResNet`
      - Description: Wide ResNet-50-2 model from `Wide Residual Networks <https://arxiv.org/abs/1605.07146>`_. The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048 channels, and in Wide ResNet-50-2 has 2048-1024-2048. Args: weights (:class:`~torchvision.models.Wide_ResNet50_2_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.Wide_ResNet50_2_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_ for more details about this class. .. autoclass:: torchvision.models.Wide_ResNet50_2_Weights :members:
  - shufflenetv2.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `InvertedResidual(inp: int, oup: int, stride: int) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `ShuffleNetV2(stages_repeats: List[int], stages_out_channels: List[int], num_classes: int = 1000, inverted_residual: Callable[..., torch.nn.modules.module.Module] = <class 'torchvision.models.shufflenetv2.InvertedResidual'>) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `ShuffleNet_V2_X0_5_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ShuffleNet_V2_X1_0_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ShuffleNet_V2_X1_5_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ShuffleNet_V2_X2_0_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
      - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
    - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
      - Description: No docstring available
    - Function: `_shufflenetv2(weights: Optional[torchvision.models._api.WeightsEnum], progress: bool, *args: Any, **kwargs: Any) -> torchvision.models.shufflenetv2.ShuffleNetV2`
      - Description: No docstring available
    - Function: `channel_shuffle(x: torch.Tensor, groups: int) -> torch.Tensor`
      - Description: No docstring available
    - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
      - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
    - Class: `partial(Unable to retrieve signature)`
      - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
    - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
      - Description: No docstring available
    - Function: `shufflenet_v2_x0_5(*, weights: Optional[torchvision.models.shufflenetv2.ShuffleNet_V2_X0_5_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.shufflenetv2.ShuffleNetV2`
      - Description: Constructs a ShuffleNetV2 architecture with 0.5x output channels, as described in `ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design <https://arxiv.org/abs/1807.11164>`__. Args: weights (:class:`~torchvision.models.ShuffleNet_V2_X0_5_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ShuffleNet_V2_X0_5_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.shufflenetv2.ShuffleNetV2`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/shufflenetv2.py>`_ for more details about this class. .. autoclass:: torchvision.models.ShuffleNet_V2_X0_5_Weights :members:
    - Function: `shufflenet_v2_x1_0(*, weights: Optional[torchvision.models.shufflenetv2.ShuffleNet_V2_X1_0_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.shufflenetv2.ShuffleNetV2`
      - Description: Constructs a ShuffleNetV2 architecture with 1.0x output channels, as described in `ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design <https://arxiv.org/abs/1807.11164>`__. Args: weights (:class:`~torchvision.models.ShuffleNet_V2_X1_0_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ShuffleNet_V2_X1_0_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.shufflenetv2.ShuffleNetV2`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/shufflenetv2.py>`_ for more details about this class. .. autoclass:: torchvision.models.ShuffleNet_V2_X1_0_Weights :members:
    - Function: `shufflenet_v2_x1_5(*, weights: Optional[torchvision.models.shufflenetv2.ShuffleNet_V2_X1_5_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.shufflenetv2.ShuffleNetV2`
      - Description: Constructs a ShuffleNetV2 architecture with 1.5x output channels, as described in `ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design <https://arxiv.org/abs/1807.11164>`__. Args: weights (:class:`~torchvision.models.ShuffleNet_V2_X1_5_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ShuffleNet_V2_X1_5_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.shufflenetv2.ShuffleNetV2`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/shufflenetv2.py>`_ for more details about this class. .. autoclass:: torchvision.models.ShuffleNet_V2_X1_5_Weights :members:
    - Function: `shufflenet_v2_x2_0(*, weights: Optional[torchvision.models.shufflenetv2.ShuffleNet_V2_X2_0_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.shufflenetv2.ShuffleNetV2`
      - Description: Constructs a ShuffleNetV2 architecture with 2.0x output channels, as described in `ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design <https://arxiv.org/abs/1807.11164>`__. Args: weights (:class:`~torchvision.models.ShuffleNet_V2_X2_0_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ShuffleNet_V2_X2_0_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.shufflenetv2.ShuffleNetV2`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/shufflenetv2.py>`_ for more details about this class. .. autoclass:: torchvision.models.ShuffleNet_V2_X2_0_Weights :members:
  - squeezenet.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Fire(inplanes: int, squeeze_planes: int, expand1x1_planes: int, expand3x3_planes: int) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `SqueezeNet(version: str = '1_0', num_classes: int = 1000, dropout: float = 0.5) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `SqueezeNet1_0_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `SqueezeNet1_1_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
      - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
    - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
      - Description: No docstring available
    - Function: `_squeezenet(version: str, weights: Optional[torchvision.models._api.WeightsEnum], progress: bool, **kwargs: Any) -> torchvision.models.squeezenet.SqueezeNet`
      - Description: No docstring available
    - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
      - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
    - Class: `partial(Unable to retrieve signature)`
      - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
    - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
      - Description: No docstring available
    - Function: `squeezenet1_0(*, weights: Optional[torchvision.models.squeezenet.SqueezeNet1_0_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.squeezenet.SqueezeNet`
      - Description: SqueezeNet model architecture from the `SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size <https://arxiv.org/abs/1602.07360>`_ paper. Args: weights (:class:`~torchvision.models.SqueezeNet1_0_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.SqueezeNet1_0_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.squeezenet.SqueezeNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/squeezenet.py>`_ for more details about this class. .. autoclass:: torchvision.models.SqueezeNet1_0_Weights :members:
    - Function: `squeezenet1_1(*, weights: Optional[torchvision.models.squeezenet.SqueezeNet1_1_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.squeezenet.SqueezeNet`
      - Description: SqueezeNet 1.1 model from the `official SqueezeNet repo <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_. SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters than SqueezeNet 1.0, without sacrificing accuracy. Args: weights (:class:`~torchvision.models.SqueezeNet1_1_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.SqueezeNet1_1_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.squeezenet.SqueezeNet`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/squeezenet.py>`_ for more details about this class. .. autoclass:: torchvision.models.SqueezeNet1_1_Weights :members:
  - swin_transformer.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `InterpolationMode(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: Interpolation modes Available interpolation methods are ``nearest``, ``nearest-exact``, ``bilinear``, ``bicubic``, ``box``, ``hamming``, and ``lanczos``.
    - Class: `MLP(in_channels: int, hidden_channels: List[int], norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, inplace: Optional[bool] = None, bias: bool = True, dropout: float = 0.0)`
      - Description: This block implements the multi-layer perceptron (MLP) module. Args: in_channels (int): Number of channels of the input hidden_channels (List[int]): List of the hidden channel dimensions norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the linear layer. If ``None`` this layer won't be used. Default: ``None`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the linear layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` inplace (bool, optional): Parameter for the activation layer, which can optionally do the operation in-place. Default is ``None``, which uses the respective default values of the ``activation_layer`` and Dropout layer. bias (bool): Whether to use bias in the linear layer. Default ``True`` dropout (float): The probability for the dropout layer. Default: 0.0
    - Class: `PatchMerging(dim: int, norm_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.normalization.LayerNorm'>)`
      - Description: Patch Merging Layer. Args: dim (int): Number of input channels. norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
    - Class: `PatchMergingV2(dim: int, norm_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.normalization.LayerNorm'>)`
      - Description: Patch Merging Layer for Swin Transformer V2. Args: dim (int): Number of input channels. norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
    - Class: `Permute(dims: List[int])`
      - Description: This module returns a view of the tensor input with its dimensions permuted. Args: dims (List[int]): The desired ordering of dimensions
    - Class: `ShiftedWindowAttention(dim: int, window_size: List[int], shift_size: List[int], num_heads: int, qkv_bias: bool = True, proj_bias: bool = True, attention_dropout: float = 0.0, dropout: float = 0.0)`
      - Description: See :func:`shifted_window_attention`.
    - Class: `ShiftedWindowAttentionV2(dim: int, window_size: List[int], shift_size: List[int], num_heads: int, qkv_bias: bool = True, proj_bias: bool = True, attention_dropout: float = 0.0, dropout: float = 0.0)`
      - Description: See :func:`shifted_window_attention_v2`.
    - Class: `StochasticDepth(p: float, mode: str) -> None`
      - Description: See :func:`stochastic_depth`.
    - Class: `SwinTransformer(patch_size: List[int], embed_dim: int, depths: List[int], num_heads: List[int], window_size: List[int], mlp_ratio: float = 4.0, dropout: float = 0.0, attention_dropout: float = 0.0, stochastic_depth_prob: float = 0.1, num_classes: int = 1000, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None, block: Optional[Callable[..., torch.nn.modules.module.Module]] = None, downsample_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torchvision.models.swin_transformer.PatchMerging'>)`
      - Description: Implements Swin Transformer from the `"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows" <https://arxiv.org/abs/2103.14030>`_ paper. Args: patch_size (List[int]): Patch size. embed_dim (int): Patch embedding dimension. depths (List(int)): Depth of each Swin Transformer layer. num_heads (List(int)): Number of attention heads in different layers. window_size (List[int]): Window size. mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.0. dropout (float): Dropout rate. Default: 0.0. attention_dropout (float): Attention dropout rate. Default: 0.0. stochastic_depth_prob (float): Stochastic depth rate. Default: 0.1. num_classes (int): Number of classes for classification head. Default: 1000. block (nn.Module, optional): SwinTransformer Block. Default: None. norm_layer (nn.Module, optional): Normalization layer. Default: None. downsample_layer (nn.Module): Downsample layer (patch merging). Default: PatchMerging.
    - Class: `SwinTransformerBlock(dim: int, num_heads: int, window_size: List[int], shift_size: List[int], mlp_ratio: float = 4.0, dropout: float = 0.0, attention_dropout: float = 0.0, stochastic_depth_prob: float = 0.0, norm_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.normalization.LayerNorm'>, attn_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torchvision.models.swin_transformer.ShiftedWindowAttention'>)`
      - Description: Swin Transformer Block. Args: dim (int): Number of input channels. num_heads (int): Number of attention heads. window_size (List[int]): Window size. shift_size (List[int]): Shift size for shifted window attention. mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.0. dropout (float): Dropout rate. Default: 0.0. attention_dropout (float): Attention dropout rate. Default: 0.0. stochastic_depth_prob: (float): Stochastic depth rate. Default: 0.0. norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm. attn_layer (nn.Module): Attention layer. Default: ShiftedWindowAttention
    - Class: `SwinTransformerBlockV2(dim: int, num_heads: int, window_size: List[int], shift_size: List[int], mlp_ratio: float = 4.0, dropout: float = 0.0, attention_dropout: float = 0.0, stochastic_depth_prob: float = 0.0, norm_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.normalization.LayerNorm'>, attn_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torchvision.models.swin_transformer.ShiftedWindowAttentionV2'>)`
      - Description: Swin Transformer V2 Block. Args: dim (int): Number of input channels. num_heads (int): Number of attention heads. window_size (List[int]): Window size. shift_size (List[int]): Shift size for shifted window attention. mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.0. dropout (float): Dropout rate. Default: 0.0. attention_dropout (float): Attention dropout rate. Default: 0.0. stochastic_depth_prob: (float): Stochastic depth rate. Default: 0.0. norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm. attn_layer (nn.Module): Attention layer. Default: ShiftedWindowAttentionV2.
    - Class: `Swin_B_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `Swin_S_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `Swin_T_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `Swin_V2_B_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `Swin_V2_S_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `Swin_V2_T_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
      - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
    - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Function: `_get_relative_position_bias(relative_position_bias_table: torch.Tensor, relative_position_index: torch.Tensor, window_size: List[int]) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
      - Description: No docstring available
    - Function: `_patch_merging_pad(x: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_swin_transformer(patch_size: List[int], embed_dim: int, depths: List[int], num_heads: List[int], window_size: List[int], stochastic_depth_prob: float, weights: Optional[torchvision.models._api.WeightsEnum], progress: bool, **kwargs: Any) -> torchvision.models.swin_transformer.SwinTransformer`
      - Description: No docstring available
    - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
      - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
    - Class: `partial(Unable to retrieve signature)`
      - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
    - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
      - Description: No docstring available
    - Function: `shifted_window_attention(input: torch.Tensor, qkv_weight: torch.Tensor, proj_weight: torch.Tensor, relative_position_bias: torch.Tensor, window_size: List[int], num_heads: int, shift_size: List[int], attention_dropout: float = 0.0, dropout: float = 0.0, qkv_bias: Optional[torch.Tensor] = None, proj_bias: Optional[torch.Tensor] = None, logit_scale: Optional[torch.Tensor] = None, training: bool = True) -> torch.Tensor`
      - Description: Window based multi-head self attention (W-MSA) module with relative position bias. It supports both of shifted and non-shifted window. Args: input (Tensor[N, H, W, C]): The input tensor or 4-dimensions. qkv_weight (Tensor[in_dim, out_dim]): The weight tensor of query, key, value. proj_weight (Tensor[out_dim, out_dim]): The weight tensor of projection. relative_position_bias (Tensor): The learned relative position bias added to attention. window_size (List[int]): Window size. num_heads (int): Number of attention heads. shift_size (List[int]): Shift size for shifted window attention. attention_dropout (float): Dropout ratio of attention weight. Default: 0.0. dropout (float): Dropout ratio of output. Default: 0.0. qkv_bias (Tensor[out_dim], optional): The bias tensor of query, key, value. Default: None. proj_bias (Tensor[out_dim], optional): The bias tensor of projection. Default: None. logit_scale (Tensor[out_dim], optional): Logit scale of cosine attention for Swin Transformer V2. Default: None. training (bool, optional): Training flag used by the dropout parameters. Default: True. Returns: Tensor[N, H, W, C]: The output tensor after shifted window attention.
    - Function: `swin_b(*, weights: Optional[torchvision.models.swin_transformer.Swin_B_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.swin_transformer.SwinTransformer`
      - Description: Constructs a swin_base architecture from `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows <https://arxiv.org/abs/2103.14030>`_. Args: weights (:class:`~torchvision.models.Swin_B_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.Swin_B_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.swin_transformer.SwinTransformer`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/swin_transformer.py>`_ for more details about this class. .. autoclass:: torchvision.models.Swin_B_Weights :members:
    - Function: `swin_s(*, weights: Optional[torchvision.models.swin_transformer.Swin_S_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.swin_transformer.SwinTransformer`
      - Description: Constructs a swin_small architecture from `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows <https://arxiv.org/abs/2103.14030>`_. Args: weights (:class:`~torchvision.models.Swin_S_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.Swin_S_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.swin_transformer.SwinTransformer`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/swin_transformer.py>`_ for more details about this class. .. autoclass:: torchvision.models.Swin_S_Weights :members:
    - Function: `swin_t(*, weights: Optional[torchvision.models.swin_transformer.Swin_T_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.swin_transformer.SwinTransformer`
      - Description: Constructs a swin_tiny architecture from `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows <https://arxiv.org/abs/2103.14030>`_. Args: weights (:class:`~torchvision.models.Swin_T_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.Swin_T_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.swin_transformer.SwinTransformer`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/swin_transformer.py>`_ for more details about this class. .. autoclass:: torchvision.models.Swin_T_Weights :members:
    - Function: `swin_v2_b(*, weights: Optional[torchvision.models.swin_transformer.Swin_V2_B_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.swin_transformer.SwinTransformer`
      - Description: Constructs a swin_v2_base architecture from `Swin Transformer V2: Scaling Up Capacity and Resolution <https://arxiv.org/abs/2111.09883>`_. Args: weights (:class:`~torchvision.models.Swin_V2_B_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.Swin_V2_B_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.swin_transformer.SwinTransformer`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/swin_transformer.py>`_ for more details about this class. .. autoclass:: torchvision.models.Swin_V2_B_Weights :members:
    - Function: `swin_v2_s(*, weights: Optional[torchvision.models.swin_transformer.Swin_V2_S_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.swin_transformer.SwinTransformer`
      - Description: Constructs a swin_v2_small architecture from `Swin Transformer V2: Scaling Up Capacity and Resolution <https://arxiv.org/abs/2111.09883>`_. Args: weights (:class:`~torchvision.models.Swin_V2_S_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.Swin_V2_S_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.swin_transformer.SwinTransformer`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/swin_transformer.py>`_ for more details about this class. .. autoclass:: torchvision.models.Swin_V2_S_Weights :members:
    - Function: `swin_v2_t(*, weights: Optional[torchvision.models.swin_transformer.Swin_V2_T_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.swin_transformer.SwinTransformer`
      - Description: Constructs a swin_v2_tiny architecture from `Swin Transformer V2: Scaling Up Capacity and Resolution <https://arxiv.org/abs/2111.09883>`_. Args: weights (:class:`~torchvision.models.Swin_V2_T_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.Swin_V2_T_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.swin_transformer.SwinTransformer`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/swin_transformer.py>`_ for more details about this class. .. autoclass:: torchvision.models.Swin_V2_T_Weights :members:
  - vgg.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `VGG(features: torch.nn.modules.module.Module, num_classes: int = 1000, init_weights: bool = True, dropout: float = 0.5) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `VGG11_BN_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `VGG11_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `VGG13_BN_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `VGG13_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `VGG16_BN_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `VGG16_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `VGG19_BN_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `VGG19_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
      - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
    - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
      - Description: No docstring available
    - Function: `_vgg(cfg: str, batch_norm: bool, weights: Optional[torchvision.models._api.WeightsEnum], progress: bool, **kwargs: Any) -> torchvision.models.vgg.VGG`
      - Description: No docstring available
    - Function: `cast(typ, val)`
      - Description: Cast a value to a type. This returns the value unchanged. To the type checker this signals that the return value has the designated type, but at runtime we intentionally don't check anything (we want this to be as fast as possible).
    - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
      - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
    - Function: `make_layers(cfg: List[Union[str, int]], batch_norm: bool = False) -> torch.nn.modules.container.Sequential`
      - Description: No docstring available
    - Class: `partial(Unable to retrieve signature)`
      - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
    - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
      - Description: No docstring available
    - Function: `vgg11(*, weights: Optional[torchvision.models.vgg.VGG11_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.vgg.VGG`
      - Description: VGG-11 from `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/abs/1409.1556>`__. Args: weights (:class:`~torchvision.models.VGG11_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.VGG11_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.vgg.VGG`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py>`_ for more details about this class. .. autoclass:: torchvision.models.VGG11_Weights :members:
    - Function: `vgg11_bn(*, weights: Optional[torchvision.models.vgg.VGG11_BN_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.vgg.VGG`
      - Description: VGG-11-BN from `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/abs/1409.1556>`__. Args: weights (:class:`~torchvision.models.VGG11_BN_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.VGG11_BN_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.vgg.VGG`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py>`_ for more details about this class. .. autoclass:: torchvision.models.VGG11_BN_Weights :members:
    - Function: `vgg13(*, weights: Optional[torchvision.models.vgg.VGG13_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.vgg.VGG`
      - Description: VGG-13 from `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/abs/1409.1556>`__. Args: weights (:class:`~torchvision.models.VGG13_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.VGG13_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.vgg.VGG`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py>`_ for more details about this class. .. autoclass:: torchvision.models.VGG13_Weights :members:
    - Function: `vgg13_bn(*, weights: Optional[torchvision.models.vgg.VGG13_BN_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.vgg.VGG`
      - Description: VGG-13-BN from `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/abs/1409.1556>`__. Args: weights (:class:`~torchvision.models.VGG13_BN_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.VGG13_BN_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.vgg.VGG`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py>`_ for more details about this class. .. autoclass:: torchvision.models.VGG13_BN_Weights :members:
    - Function: `vgg16(*, weights: Optional[torchvision.models.vgg.VGG16_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.vgg.VGG`
      - Description: VGG-16 from `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/abs/1409.1556>`__. Args: weights (:class:`~torchvision.models.VGG16_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.VGG16_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.vgg.VGG`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py>`_ for more details about this class. .. autoclass:: torchvision.models.VGG16_Weights :members:
    - Function: `vgg16_bn(*, weights: Optional[torchvision.models.vgg.VGG16_BN_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.vgg.VGG`
      - Description: VGG-16-BN from `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/abs/1409.1556>`__. Args: weights (:class:`~torchvision.models.VGG16_BN_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.VGG16_BN_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.vgg.VGG`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py>`_ for more details about this class. .. autoclass:: torchvision.models.VGG16_BN_Weights :members:
    - Function: `vgg19(*, weights: Optional[torchvision.models.vgg.VGG19_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.vgg.VGG`
      - Description: VGG-19 from `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/abs/1409.1556>`__. Args: weights (:class:`~torchvision.models.VGG19_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.VGG19_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.vgg.VGG`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py>`_ for more details about this class. .. autoclass:: torchvision.models.VGG19_Weights :members:
    - Function: `vgg19_bn(*, weights: Optional[torchvision.models.vgg.VGG19_BN_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.vgg.VGG`
      - Description: VGG-19_BN from `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/abs/1409.1556>`__. Args: weights (:class:`~torchvision.models.VGG19_BN_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.VGG19_BN_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.vgg.VGG`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py>`_ for more details about this class. .. autoclass:: torchvision.models.VGG19_BN_Weights :members:
  - vision_transformer.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Conv2dNormActivation(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]] = 3, stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int], str, NoneType] = None, groups: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, dilation: Union[int, Tuple[int, int]] = 1, inplace: Optional[bool] = True, bias: Optional[bool] = None) -> None`
      - Description: Configurable block used for Convolution2d-Normalization-Activation blocks. Args: in_channels (int): Number of channels in the input image out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block kernel_size: (int, optional): Size of the convolving kernel. Default: 3 stride (int, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation`` groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm2d`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` dilation (int): Spacing between kernel elements. Default: 1 inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True`` bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.
    - Class: `ConvStemConfig(out_channels: int, kernel_size: int, stride: int, norm_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, activation_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.activation.ReLU'>)`
      - Description: ConvStemConfig(out_channels, kernel_size, stride, norm_layer, activation_layer)
    - Class: `Encoder(seq_length: int, num_layers: int, num_heads: int, hidden_dim: int, mlp_dim: int, dropout: float, attention_dropout: float, norm_layer: Callable[..., torch.nn.modules.module.Module] = functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06))`
      - Description: Transformer Model Encoder for sequence to sequence translation.
    - Class: `EncoderBlock(num_heads: int, hidden_dim: int, mlp_dim: int, dropout: float, attention_dropout: float, norm_layer: Callable[..., torch.nn.modules.module.Module] = functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06))`
      - Description: Transformer encoder block.
    - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `InterpolationMode(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: Interpolation modes Available interpolation methods are ``nearest``, ``nearest-exact``, ``bilinear``, ``bicubic``, ``box``, ``hamming``, and ``lanczos``.
    - Class: `MLP(in_channels: int, hidden_channels: List[int], norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, inplace: Optional[bool] = None, bias: bool = True, dropout: float = 0.0)`
      - Description: This block implements the multi-layer perceptron (MLP) module. Args: in_channels (int): Number of channels of the input hidden_channels (List[int]): List of the hidden channel dimensions norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the linear layer. If ``None`` this layer won't be used. Default: ``None`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the linear layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` inplace (bool, optional): Parameter for the activation layer, which can optionally do the operation in-place. Default is ``None``, which uses the respective default values of the ``activation_layer`` and Dropout layer. bias (bool): Whether to use bias in the linear layer. Default ``True`` dropout (float): The probability for the dropout layer. Default: 0.0
    - Class: `MLPBlock(in_dim: int, mlp_dim: int, dropout: float)`
      - Description: Transformer MLP block.
    - Function: `NamedTuple(typename, fields=None, /, **kwargs)`
      - Description: Typed version of namedtuple. Usage:: class Employee(NamedTuple): name: str id: int This is equivalent to:: Employee = collections.namedtuple('Employee', ['name', 'id']) The resulting class has an extra __annotations__ attribute, giving a dict that maps field names to types. (The field names are also in the _fields attribute, which is part of the namedtuple API.) An alternative equivalent functional syntax is also accepted:: Employee = NamedTuple('Employee', [('name', str), ('id', int)])
    - Class: `OrderedDict(Unable to retrieve signature)`
      - Description: Dictionary that remembers insertion order
    - Class: `ViT_B_16_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ViT_B_32_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ViT_H_14_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ViT_L_16_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `ViT_L_32_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `VisionTransformer(image_size: int, patch_size: int, num_layers: int, num_heads: int, hidden_dim: int, mlp_dim: int, dropout: float = 0.0, attention_dropout: float = 0.0, num_classes: int = 1000, representation_size: Optional[int] = None, norm_layer: Callable[..., torch.nn.modules.module.Module] = functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06), conv_stem_configs: Optional[List[torchvision.models.vision_transformer.ConvStemConfig]] = None)`
      - Description: Vision Transformer as per https://arxiv.org/abs/2010.11929.
    - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
      - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
    - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
      - Description: No docstring available
    - Function: `_vision_transformer(patch_size: int, num_layers: int, num_heads: int, hidden_dim: int, mlp_dim: int, weights: Optional[torchvision.models._api.WeightsEnum], progress: bool, **kwargs: Any) -> torchvision.models.vision_transformer.VisionTransformer`
      - Description: No docstring available
    - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
      - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
    - Function: `interpolate_embeddings(image_size: int, patch_size: int, model_state: 'OrderedDict[str, torch.Tensor]', interpolation_mode: str = 'bicubic', reset_heads: bool = False) -> 'OrderedDict[str, torch.Tensor]'`
      - Description: This function helps interpolate positional embeddings during checkpoint loading, especially when you want to apply a pre-trained model on images with different resolution. Args: image_size (int): Image size of the new model. patch_size (int): Patch size of the new model. model_state (OrderedDict[str, torch.Tensor]): State dict of the pre-trained model. interpolation_mode (str): The algorithm used for upsampling. Default: bicubic. reset_heads (bool): If true, not copying the state of heads. Default: False. Returns: OrderedDict[str, torch.Tensor]: A state dict which can be loaded into the new model.
    - Class: `partial(Unable to retrieve signature)`
      - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
    - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
      - Description: No docstring available
    - Function: `vit_b_16(*, weights: Optional[torchvision.models.vision_transformer.ViT_B_16_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.vision_transformer.VisionTransformer`
      - Description: Constructs a vit_b_16 architecture from `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>`_. Args: weights (:class:`~torchvision.models.ViT_B_16_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ViT_B_16_Weights` below for more details and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.vision_transformer.VisionTransformer`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py>`_ for more details about this class. .. autoclass:: torchvision.models.ViT_B_16_Weights :members:
    - Function: `vit_b_32(*, weights: Optional[torchvision.models.vision_transformer.ViT_B_32_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.vision_transformer.VisionTransformer`
      - Description: Constructs a vit_b_32 architecture from `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>`_. Args: weights (:class:`~torchvision.models.ViT_B_32_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ViT_B_32_Weights` below for more details and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.vision_transformer.VisionTransformer`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py>`_ for more details about this class. .. autoclass:: torchvision.models.ViT_B_32_Weights :members:
    - Function: `vit_h_14(*, weights: Optional[torchvision.models.vision_transformer.ViT_H_14_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.vision_transformer.VisionTransformer`
      - Description: Constructs a vit_h_14 architecture from `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>`_. Args: weights (:class:`~torchvision.models.ViT_H_14_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ViT_H_14_Weights` below for more details and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.vision_transformer.VisionTransformer`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py>`_ for more details about this class. .. autoclass:: torchvision.models.ViT_H_14_Weights :members:
    - Function: `vit_l_16(*, weights: Optional[torchvision.models.vision_transformer.ViT_L_16_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.vision_transformer.VisionTransformer`
      - Description: Constructs a vit_l_16 architecture from `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>`_. Args: weights (:class:`~torchvision.models.ViT_L_16_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ViT_L_16_Weights` below for more details and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.vision_transformer.VisionTransformer`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py>`_ for more details about this class. .. autoclass:: torchvision.models.ViT_L_16_Weights :members:
    - Function: `vit_l_32(*, weights: Optional[torchvision.models.vision_transformer.ViT_L_32_Weights] = None, progress: bool = True, **kwargs: Any) -> torchvision.models.vision_transformer.VisionTransformer`
      - Description: Constructs a vit_l_32 architecture from `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>`_. Args: weights (:class:`~torchvision.models.ViT_L_32_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.models.ViT_L_32_Weights` below for more details and possible values. By default, no pre-trained weights are used. progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.models.vision_transformer.VisionTransformer`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py>`_ for more details about this class. .. autoclass:: torchvision.models.ViT_L_32_Weights :members:
  - _api.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Enum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: Create a collection of name/value pairs. Example enumeration: >>> class Color(Enum): ... RED = 1 ... BLUE = 2 ... GREEN = 3 Access them by: - attribute access:: >>> Color.RED <Color.RED: 1> - value lookup: >>> Color(1) <Color.RED: 1> - name lookup: >>> Color['RED'] <Color.RED: 1> Enumerations can be iterated over, and know how many members they have: >>> len(Color) 3 >>> list(Color) [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>] Methods can be added to enumerations, and members can have their own attributes -- see the documentation for details.
    - Class: `ModuleType(name, doc=None)`
      - Description: Create a module object. The name must be a string; the optional doc argument can have any type.
    - Class: `TypeVar(name, *constraints, bound=None, covariant=False, contravariant=False)`
      - Description: Type variable. Usage:: T = TypeVar('T') # Can be anything A = TypeVar('A', str, bytes) # Must be str or bytes Type variables exist primarily for the benefit of static type checkers. They serve as the parameters for generic types as well as for generic function definitions. See class Generic for more information on generic types. Generic functions work as follows: def repeat(x: T, n: int) -> List[T]: '''Return a list containing n references to x.''' return [x]*n def longest(x: A, y: A) -> A: '''Return the longest of two strings.''' return x if len(x) >= len(y) else y The latter example's signature is essentially the overloading of (str, str) -> str and (bytes, bytes) -> bytes. Also note that if the arguments are instances of some subclass of str, the return type is still plain str. At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError. Type variables defined with covariant=True or contravariant=True can be used to declare covariant or contravariant generic types. See PEP 484 for more details. By default generic types are invariant in all type variables. Type variables can be introspected. e.g.: T.__name__ == 'T' T.__constraints__ == () T.__covariant__ == False T.__contravariant__ = False A.__constraints__ == (str, bytes) Note that only type variables defined in global scope can be pickled.
    - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
      - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
    - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Function: `_get_enum_from_fn(fn: Callable) -> Type[torchvision.models._api.WeightsEnum]`
      - Description: Internal method that gets the weight enum of a specific model builder method. Args: fn (Callable): The builder method used to create the model. Returns: WeightsEnum: The requested weight enum.
    - Function: `dataclass(cls=None, /, *, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False, match_args=True, kw_only=False, slots=False, weakref_slot=False)`
      - Description: Add dunder methods based on the fields defined in the class. Examines PEP 526 __annotations__ to determine fields. If init is true, an __init__() method is added to the class. If repr is true, a __repr__() method is added. If order is true, rich comparison dunder methods are added. If unsafe_hash is true, a __hash__() method is added. If frozen is true, fields may not be assigned to after instance creation. If match_args is true, the __match_args__ tuple is added. If kw_only is true, then by default all fields are keyword-only. If slots is true, a new class with a __slots__ attribute is returned.
    - Function: `get_model(name: str, **config: Any) -> torch.nn.modules.module.Module`
      - Description: Gets the model name and configuration and returns an instantiated model. Args: name (str): The name under which the model is registered. **config (Any): parameters passed to the model builder method. Returns: model (nn.Module): The initialized model.
    - Function: `get_model_builder(name: str) -> Callable[..., torch.nn.modules.module.Module]`
      - Description: Gets the model name and returns the model builder method. Args: name (str): The name under which the model is registered. Returns: fn (Callable): The model builder method.
    - Function: `get_model_weights(name: Union[Callable, str]) -> Type[torchvision.models._api.WeightsEnum]`
      - Description: Returns the weights enum class associated to the given model. Args: name (callable or str): The model builder function or the name under which it is registered. Returns: weights_enum (WeightsEnum): The weights enum class associated with the model.
    - Function: `get_weight(name: str) -> torchvision.models._api.WeightsEnum`
      - Description: Gets the weights enum value by its full name. Example: "ResNet50_Weights.IMAGENET1K_V1" Args: name (str): The name of the weight enum entry. Returns: WeightsEnum: The requested weight enum.
    - Function: `list_models(module: Optional[module] = None, include: Union[Iterable[str], str, NoneType] = None, exclude: Union[Iterable[str], str, NoneType] = None) -> List[str]`
      - Description: Returns a list with the names of registered models. Args: module (ModuleType, optional): The module from which we want to extract the available models. include (str or Iterable[str], optional): Filter(s) for including the models from the set of all models. Filters are passed to `fnmatch <https://docs.python.org/3/library/fnmatch.html>`__ to match Unix shell-style wildcards. In case of many filters, the results is the union of individual filters. exclude (str or Iterable[str], optional): Filter(s) applied after include_filters to remove models. Filter are passed to `fnmatch <https://docs.python.org/3/library/fnmatch.html>`__ to match Unix shell-style wildcards. In case of many filters, the results is removal of all the models that match any individual filter. Returns: models (list): A list with the names of available models.
    - Function: `load_state_dict_from_url(url: str, model_dir: Optional[str] = None, map_location: Union[Callable[[torch.Tensor, str], torch.Tensor], torch.device, str, Dict[str, str], NoneType] = None, progress: bool = True, check_hash: bool = False, file_name: Optional[str] = None, weights_only: bool = False) -> Dict[str, Any]`
      - Description: Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically decompressed. If the object is already present in `model_dir`, it's deserialized and returned. The default value of ``model_dir`` is ``<hub_dir>/checkpoints`` where ``hub_dir`` is the directory returned by :func:`~torch.hub.get_dir`. Args: url (str): URL of the object to download model_dir (str, optional): directory in which to save the object map_location (optional): a function or a dict specifying how to remap storage locations (see torch.load) progress (bool, optional): whether or not to display a progress bar to stderr. Default: True check_hash(bool, optional): If True, the filename part of the URL should follow the naming convention ``filename-<sha256>.ext`` where ``<sha256>`` is the first eight or more digits of the SHA256 hash of the contents of the file. The hash is used to ensure unique names and to verify the contents of the file. Default: False file_name (str, optional): name for the downloaded file. Filename from ``url`` will be used if not set. weights_only(bool, optional): If True, only weights will be loaded and no complex pickled objects. Recommended for untrusted sources. See :func:`~torch.load` for more details. Example: >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_HUB) >>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')
    - Class: `partial(Unable to retrieve signature)`
      - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
    - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
      - Description: No docstring available
    - Function: `signature(obj, *, follow_wrapped=True, globals=None, locals=None, eval_str=False)`
      - Description: Get a signature object for the passed callable.
  - _meta.py
  - _utils.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `IntermediateLayerGetter(model: torch.nn.modules.module.Module, return_layers: Dict[str, str]) -> None`
      - Description: Module wrapper that returns intermediate layers from a model It has a strong assumption that the modules have been registered into the model in the same order as they are used. This means that one should **not** reuse the same nn.Module twice in the forward if you want this to work. Additionally, it is only able to query submodules that are directly assigned to the model. So if `model` is passed, `model.feature1` can be returned, but not `model.feature1.layer2`. Args: model (nn.Module): model on which we will extract the features return_layers (Dict[name, new_name]): a dict containing the names of the modules for which the activations will be returned as the key of the dict, and the value of the dict is the name of the returned activation (which the user can specify). Examples:: >>> m = torchvision.models.resnet18(weights=ResNet18_Weights.DEFAULT) >>> # extract layer1 and layer3, giving as names `feat1` and feat2` >>> new_m = torchvision.models._utils.IntermediateLayerGetter(m, >>> {'layer1': 'feat1', 'layer3': 'feat2'}) >>> out = new_m(torch.rand(1, 3, 224, 224)) >>> print([(k, v.shape) for k, v in out.items()]) >>> [('feat1', torch.Size([1, 64, 56, 56])), >>> ('feat2', torch.Size([1, 256, 14, 14]))]
    - Class: `OrderedDict(Unable to retrieve signature)`
      - Description: Dictionary that remembers insertion order
    - Class: `TypeVar(name, *constraints, bound=None, covariant=False, contravariant=False)`
      - Description: Type variable. Usage:: T = TypeVar('T') # Can be anything A = TypeVar('A', str, bytes) # Must be str or bytes Type variables exist primarily for the benefit of static type checkers. They serve as the parameters for generic types as well as for generic function definitions. See class Generic for more information on generic types. Generic functions work as follows: def repeat(x: T, n: int) -> List[T]: '''Return a list containing n references to x.''' return [x]*n def longest(x: A, y: A) -> A: '''Return the longest of two strings.''' return x if len(x) >= len(y) else y The latter example's signature is essentially the overloading of (str, str) -> str and (bytes, bytes) -> bytes. Also note that if the arguments are instances of some subclass of str, the return type is still plain str. At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError. Type variables defined with covariant=True or contravariant=True can be used to declare covariant or contravariant generic types. See PEP 484 for more details. By default generic types are invariant in all type variables. Type variables can be introspected. e.g.: T.__name__ == 'T' T.__constraints__ == () T.__covariant__ == False T.__contravariant__ = False A.__constraints__ == (str, bytes) Note that only type variables defined in global scope can be pickled.
    - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
    - Class: `_ModelURLs(Unable to retrieve signature)`
      - Description: dict() -> new empty dictionary dict(mapping) -> new dictionary initialized from a mapping object's (key, value) pairs dict(iterable) -> new dictionary initialized as if via: d = {} for k, v in iterable: d[k] = v dict(**kwargs) -> new dictionary initialized with the name=value pairs in the keyword argument list. For example: dict(one=1, two=2)
    - Function: `_make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int`
      - Description: This function is taken from the original tf repo. It ensures that all layers have a channel number that is divisible by 8 It can be seen here: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
    - Function: `_ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: ~V) -> None`
      - Description: No docstring available
    - Function: `_ovewrite_value_param(param: str, actual: Optional[~V], expected: ~V) -> ~V`
      - Description: No docstring available
    - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
      - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
    - Function: `kwonly_to_pos_or_kw(fn: Callable[..., ~D]) -> Callable[..., ~D]`
      - Description: Decorates a function that uses keyword only parameters to also allow them being passed as positionals. For example, consider the use case of changing the signature of ``old_fn`` into the one from ``new_fn``: .. code:: def old_fn(foo, bar, baz=None): ... def new_fn(foo, *, bar, baz=None): ... Calling ``old_fn("foo", "bar, "baz")`` was valid, but the same call is no longer valid with ``new_fn``. To keep BC and at the same time warn the user of the deprecation, this decorator can be used: .. code:: @kwonly_to_pos_or_kw def new_fn(foo, *, bar, baz=None): ... new_fn("foo", "bar, "baz")
    - Function: `sequence_to_str(seq: Sequence, separate_last: str = '') -> str`
      - Description: No docstring available
- ops/
  - __pycache__/
  - boxes.py
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Function: `_assert_has_ops()`
      - Description: No docstring available
    - Function: `_batched_nms_coordinate_trick(boxes: torch.Tensor, scores: torch.Tensor, idxs: torch.Tensor, iou_threshold: float) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_batched_nms_vanilla(boxes: torch.Tensor, scores: torch.Tensor, idxs: torch.Tensor, iou_threshold: float) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_box_cxcywh_to_xyxy(boxes: torch.Tensor) -> torch.Tensor`
      - Description: Converts bounding boxes from (cx, cy, w, h) format to (x1, y1, x2, y2) format. (cx, cy) refers to center of bounding box (w, h) are width and height of bounding box Args: boxes (Tensor[N, 4]): boxes in (cx, cy, w, h) format which will be converted. Returns: boxes (Tensor(N, 4)): boxes in (x1, y1, x2, y2) format.
    - Function: `_box_diou_iou(boxes1: torch.Tensor, boxes2: torch.Tensor, eps: float = 1e-07) -> Tuple[torch.Tensor, torch.Tensor]`
      - Description: No docstring available
    - Function: `_box_inter_union(boxes1: torch.Tensor, boxes2: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]`
      - Description: No docstring available
    - Function: `_box_xywh_to_xyxy(boxes: torch.Tensor) -> torch.Tensor`
      - Description: Converts bounding boxes from (x, y, w, h) format to (x1, y1, x2, y2) format. (x, y) refers to top left of bounding box. (w, h) refers to width and height of box. Args: boxes (Tensor[N, 4]): boxes in (x, y, w, h) which will be converted. Returns: boxes (Tensor[N, 4]): boxes in (x1, y1, x2, y2) format.
    - Function: `_box_xyxy_to_cxcywh(boxes: torch.Tensor) -> torch.Tensor`
      - Description: Converts bounding boxes from (x1, y1, x2, y2) format to (cx, cy, w, h) format. (x1, y1) refer to top left of bounding box (x2, y2) refer to bottom right of bounding box Args: boxes (Tensor[N, 4]): boxes in (x1, y1, x2, y2) format which will be converted. Returns: boxes (Tensor(N, 4)): boxes in (cx, cy, w, h) format.
    - Function: `_box_xyxy_to_xywh(boxes: torch.Tensor) -> torch.Tensor`
      - Description: Converts bounding boxes from (x1, y1, x2, y2) format to (x, y, w, h) format. (x1, y1) refer to top left of bounding box (x2, y2) refer to bottom right of bounding box Args: boxes (Tensor[N, 4]): boxes in (x1, y1, x2, y2) which will be converted. Returns: boxes (Tensor[N, 4]): boxes in (x, y, w, h) format.
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_upcast(t: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `batched_nms(boxes: torch.Tensor, scores: torch.Tensor, idxs: torch.Tensor, iou_threshold: float) -> torch.Tensor`
      - Description: Performs non-maximum suppression in a batched fashion. Each index value correspond to a category, and NMS will not be applied between elements of different categories. Args: boxes (Tensor[N, 4]): boxes where NMS will be performed. They are expected to be in ``(x1, y1, x2, y2)`` format with ``0 <= x1 < x2`` and ``0 <= y1 < y2``. scores (Tensor[N]): scores for each one of the boxes idxs (Tensor[N]): indices of the categories for each one of the boxes. iou_threshold (float): discards all overlapping boxes with IoU > iou_threshold Returns: Tensor: int64 tensor with the indices of the elements that have been kept by NMS, sorted in decreasing order of scores
    - Function: `box_area(boxes: torch.Tensor) -> torch.Tensor`
      - Description: Computes the area of a set of bounding boxes, which are specified by their (x1, y1, x2, y2) coordinates. Args: boxes (Tensor[N, 4]): boxes for which the area will be computed. They are expected to be in (x1, y1, x2, y2) format with ``0 <= x1 < x2`` and ``0 <= y1 < y2``. Returns: Tensor[N]: the area for each box
    - Function: `box_convert(boxes: torch.Tensor, in_fmt: str, out_fmt: str) -> torch.Tensor`
      - Description: Converts boxes from given in_fmt to out_fmt. Supported in_fmt and out_fmt are: 'xyxy': boxes are represented via corners, x1, y1 being top left and x2, y2 being bottom right. This is the format that torchvision utilities expect. 'xywh' : boxes are represented via corner, width and height, x1, y2 being top left, w, h being width and height. 'cxcywh' : boxes are represented via centre, width and height, cx, cy being center of box, w, h being width and height. Args: boxes (Tensor[N, 4]): boxes which will be converted. in_fmt (str): Input format of given boxes. Supported formats are ['xyxy', 'xywh', 'cxcywh']. out_fmt (str): Output format of given boxes. Supported formats are ['xyxy', 'xywh', 'cxcywh'] Returns: Tensor[N, 4]: Boxes into converted format.
    - Function: `box_iou(boxes1: torch.Tensor, boxes2: torch.Tensor) -> torch.Tensor`
      - Description: Return intersection-over-union (Jaccard index) between two sets of boxes. Both sets of boxes are expected to be in ``(x1, y1, x2, y2)`` format with ``0 <= x1 < x2`` and ``0 <= y1 < y2``. Args: boxes1 (Tensor[N, 4]): first set of boxes boxes2 (Tensor[M, 4]): second set of boxes Returns: Tensor[N, M]: the NxM matrix containing the pairwise IoU values for every element in boxes1 and boxes2
    - Function: `clip_boxes_to_image(boxes: torch.Tensor, size: Tuple[int, int]) -> torch.Tensor`
      - Description: Clip boxes so that they lie inside an image of size `size`. Args: boxes (Tensor[N, 4]): boxes in ``(x1, y1, x2, y2)`` format with ``0 <= x1 < x2`` and ``0 <= y1 < y2``. size (Tuple[height, width]): size of the image Returns: Tensor[N, 4]: clipped boxes
    - Function: `complete_box_iou(boxes1: torch.Tensor, boxes2: torch.Tensor, eps: float = 1e-07) -> torch.Tensor`
      - Description: Return complete intersection-over-union (Jaccard index) between two sets of boxes. Both sets of boxes are expected to be in ``(x1, y1, x2, y2)`` format with ``0 <= x1 < x2`` and ``0 <= y1 < y2``. Args: boxes1 (Tensor[N, 4]): first set of boxes boxes2 (Tensor[M, 4]): second set of boxes eps (float, optional): small number to prevent division by zero. Default: 1e-7 Returns: Tensor[N, M]: the NxM matrix containing the pairwise complete IoU values for every element in boxes1 and boxes2
    - Function: `distance_box_iou(boxes1: torch.Tensor, boxes2: torch.Tensor, eps: float = 1e-07) -> torch.Tensor`
      - Description: Return distance intersection-over-union (Jaccard index) between two sets of boxes. Both sets of boxes are expected to be in ``(x1, y1, x2, y2)`` format with ``0 <= x1 < x2`` and ``0 <= y1 < y2``. Args: boxes1 (Tensor[N, 4]): first set of boxes boxes2 (Tensor[M, 4]): second set of boxes eps (float, optional): small number to prevent division by zero. Default: 1e-7 Returns: Tensor[N, M]: the NxM matrix containing the pairwise distance IoU values for every element in boxes1 and boxes2
    - Function: `generalized_box_iou(boxes1: torch.Tensor, boxes2: torch.Tensor) -> torch.Tensor`
      - Description: Return generalized intersection-over-union (Jaccard index) between two sets of boxes. Both sets of boxes are expected to be in ``(x1, y1, x2, y2)`` format with ``0 <= x1 < x2`` and ``0 <= y1 < y2``. Args: boxes1 (Tensor[N, 4]): first set of boxes boxes2 (Tensor[M, 4]): second set of boxes Returns: Tensor[N, M]: the NxM matrix containing the pairwise generalized IoU values for every element in boxes1 and boxes2
    - Function: `masks_to_boxes(masks: torch.Tensor) -> torch.Tensor`
      - Description: Compute the bounding boxes around the provided masks. Returns a [N, 4] tensor containing bounding boxes. The boxes are in ``(x1, y1, x2, y2)`` format with ``0 <= x1 < x2`` and ``0 <= y1 < y2``. Args: masks (Tensor[N, H, W]): masks to transform where N is the number of masks and (H, W) are the spatial dimensions. Returns: Tensor[N, 4]: bounding boxes
    - Function: `nms(boxes: torch.Tensor, scores: torch.Tensor, iou_threshold: float) -> torch.Tensor`
      - Description: Performs non-maximum suppression (NMS) on the boxes according to their intersection-over-union (IoU). NMS iteratively removes lower scoring boxes which have an IoU greater than iou_threshold with another (higher scoring) box. If multiple boxes have the exact same score and satisfy the IoU criterion with respect to a reference box, the selected box is not guaranteed to be the same between CPU and GPU. This is similar to the behavior of argsort in PyTorch when repeated values are present. Args: boxes (Tensor[N, 4])): boxes to perform NMS on. They are expected to be in ``(x1, y1, x2, y2)`` format with ``0 <= x1 < x2`` and ``0 <= y1 < y2``. scores (Tensor[N]): scores for each one of the boxes iou_threshold (float): discards all overlapping boxes with IoU > iou_threshold Returns: Tensor: int64 tensor with the indices of the elements that have been kept by NMS, sorted in decreasing order of scores
    - Function: `remove_small_boxes(boxes: torch.Tensor, min_size: float) -> torch.Tensor`
      - Description: Remove boxes which contains at least one side smaller than min_size. Args: boxes (Tensor[N, 4]): boxes in ``(x1, y1, x2, y2)`` format with ``0 <= x1 < x2`` and ``0 <= y1 < y2``. min_size (float): minimum size Returns: Tensor[K]: indices of the boxes that have both sides larger than min_size
  - ciou_loss.py
    - Function: `_diou_iou_loss(boxes1: torch.Tensor, boxes2: torch.Tensor, eps: float = 1e-07) -> Tuple[torch.Tensor, torch.Tensor]`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_upcast_non_float(t: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `complete_box_iou_loss(boxes1: torch.Tensor, boxes2: torch.Tensor, reduction: str = 'none', eps: float = 1e-07) -> torch.Tensor`
      - Description: Gradient-friendly IoU loss with an additional penalty that is non-zero when the boxes do not overlap. This loss function considers important geometrical factors such as overlap area, normalized central point distance and aspect ratio. This loss is symmetric, so the boxes1 and boxes2 arguments are interchangeable. Both sets of boxes are expected to be in ``(x1, y1, x2, y2)`` format with ``0 <= x1 < x2`` and ``0 <= y1 < y2``, and The two boxes should have the same dimensions. Args: boxes1 : (Tensor[N, 4] or Tensor[4]) first set of boxes boxes2 : (Tensor[N, 4] or Tensor[4]) second set of boxes reduction : (string, optional) Specifies the reduction to apply to the output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: No reduction will be applied to the output. ``'mean'``: The output will be averaged. ``'sum'``: The output will be summed. Default: ``'none'`` eps : (float): small number to prevent division by zero. Default: 1e-7 Returns: Tensor: Loss tensor with the reduction option applied. Reference: Zhaohui Zheng et al.: Complete Intersection over Union Loss: https://arxiv.org/abs/1911.08287
  - deform_conv.py
    - Class: `DeformConv2d(in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = True)`
      - Description: See :func:`deform_conv2d`.
    - Class: `Parameter(data=None, requires_grad=True)`
      - Description: A kind of Tensor that is to be considered a module parameter. Parameters are :class:`~torch.Tensor` subclasses, that have a very special property when used with :class:`Module` s - when they're assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in :meth:`~Module.parameters` iterator. Assigning a Tensor doesn't have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as :class:`Parameter`, these temporaries would get registered too. Args: data (Tensor): parameter tensor. requires_grad (bool, optional): if the parameter requires gradient. Note that the torch.no_grad() context does NOT affect the default behavior of Parameter creation--the Parameter will still have `requires_grad=True` in :class:`~no_grad` mode. See :ref:`locally-disable-grad-doc` for more details. Default: `True`
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Function: `_assert_has_ops()`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_pair(x)`
      - Description: No docstring available
    - Function: `deform_conv2d(input: torch.Tensor, offset: torch.Tensor, weight: torch.Tensor, bias: Optional[torch.Tensor] = None, stride: Tuple[int, int] = (1, 1), padding: Tuple[int, int] = (0, 0), dilation: Tuple[int, int] = (1, 1), mask: Optional[torch.Tensor] = None) -> torch.Tensor`
      - Description: Performs Deformable Convolution v2, described in `Deformable ConvNets v2: More Deformable, Better Results <https://arxiv.org/abs/1811.11168>`__ if :attr:`mask` is not ``None`` and Performs Deformable Convolution, described in `Deformable Convolutional Networks <https://arxiv.org/abs/1703.06211>`__ if :attr:`mask` is ``None``. Args: input (Tensor[batch_size, in_channels, in_height, in_width]): input tensor offset (Tensor[batch_size, 2 * offset_groups * kernel_height * kernel_width, out_height, out_width]): offsets to be applied for each position in the convolution kernel. weight (Tensor[out_channels, in_channels // groups, kernel_height, kernel_width]): convolution weights, split into groups of size (in_channels // groups) bias (Tensor[out_channels]): optional bias of shape (out_channels,). Default: None stride (int or Tuple[int, int]): distance between convolution centers. Default: 1 padding (int or Tuple[int, int]): height/width of padding of zeroes around each image. Default: 0 dilation (int or Tuple[int, int]): the spacing between kernel elements. Default: 1 mask (Tensor[batch_size, offset_groups * kernel_height * kernel_width, out_height, out_width]): masks to be applied for each position in the convolution kernel. Default: None Returns: Tensor[batch_sz, out_channels, out_h, out_w]: result of convolution Examples:: >>> input = torch.rand(4, 3, 10, 10) >>> kh, kw = 3, 3 >>> weight = torch.rand(5, 3, kh, kw) >>> # offset and mask should have the same spatial size as the output >>> # of the convolution. In this case, for an input of 10, stride of 1 >>> # and kernel size of 3, without padding, the output size is 8 >>> offset = torch.rand(4, 2 * kh * kw, 8, 8) >>> mask = torch.rand(4, kh * kw, 8, 8) >>> out = deform_conv2d(input, offset, weight, mask=mask) >>> print(out.shape) >>> # returns >>> torch.Size([4, 5, 8, 8])
  - diou_loss.py
    - Function: `_diou_iou_loss(boxes1: torch.Tensor, boxes2: torch.Tensor, eps: float = 1e-07) -> Tuple[torch.Tensor, torch.Tensor]`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_loss_inter_union(boxes1: torch.Tensor, boxes2: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]`
      - Description: No docstring available
    - Function: `_upcast_non_float(t: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `distance_box_iou_loss(boxes1: torch.Tensor, boxes2: torch.Tensor, reduction: str = 'none', eps: float = 1e-07) -> torch.Tensor`
      - Description: Gradient-friendly IoU loss with an additional penalty that is non-zero when the distance between boxes' centers isn't zero. Indeed, for two exactly overlapping boxes, the distance IoU is the same as the IoU loss. This loss is symmetric, so the boxes1 and boxes2 arguments are interchangeable. Both sets of boxes are expected to be in ``(x1, y1, x2, y2)`` format with ``0 <= x1 < x2`` and ``0 <= y1 < y2``, and The two boxes should have the same dimensions. Args: boxes1 (Tensor[N, 4]): first set of boxes boxes2 (Tensor[N, 4]): second set of boxes reduction (string, optional): Specifies the reduction to apply to the output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: No reduction will be applied to the output. ``'mean'``: The output will be averaged. ``'sum'``: The output will be summed. Default: ``'none'`` eps (float, optional): small number to prevent division by zero. Default: 1e-7 Returns: Tensor: Loss tensor with the reduction option applied. Reference: Zhaohui Zheng et al.: Distance Intersection over Union Loss: https://arxiv.org/abs/1911.08287
  - drop_block.py
    - Class: `DropBlock2d(p: float, block_size: int, inplace: bool = False, eps: float = 1e-06) -> None`
      - Description: See :func:`drop_block2d`.
    - Class: `DropBlock3d(p: float, block_size: int, inplace: bool = False, eps: float = 1e-06) -> None`
      - Description: See :func:`drop_block3d`.
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `drop_block2d(input: torch.Tensor, p: float, block_size: int, inplace: bool = False, eps: float = 1e-06, training: bool = True) -> torch.Tensor`
      - Description: Implements DropBlock2d from `"DropBlock: A regularization method for convolutional networks" <https://arxiv.org/abs/1810.12890>`. Args: input (Tensor[N, C, H, W]): The input tensor or 4-dimensions with the first one being its batch i.e. a batch with ``N`` rows. p (float): Probability of an element to be dropped. block_size (int): Size of the block to drop. inplace (bool): If set to ``True``, will do this operation in-place. Default: ``False``. eps (float): A value added to the denominator for numerical stability. Default: 1e-6. training (bool): apply dropblock if is ``True``. Default: ``True``. Returns: Tensor[N, C, H, W]: The randomly zeroed tensor after dropblock.
    - Function: `drop_block3d(input: torch.Tensor, p: float, block_size: int, inplace: bool = False, eps: float = 1e-06, training: bool = True) -> torch.Tensor`
      - Description: Implements DropBlock3d from `"DropBlock: A regularization method for convolutional networks" <https://arxiv.org/abs/1810.12890>`. Args: input (Tensor[N, C, D, H, W]): The input tensor or 5-dimensions with the first one being its batch i.e. a batch with ``N`` rows. p (float): Probability of an element to be dropped. block_size (int): Size of the block to drop. inplace (bool): If set to ``True``, will do this operation in-place. Default: ``False``. eps (float): A value added to the denominator for numerical stability. Default: 1e-6. training (bool): apply dropblock if is ``True``. Default: ``True``. Returns: Tensor[N, C, D, H, W]: The randomly zeroed tensor after dropblock.
  - feature_pyramid_network.py
    - Class: `Conv2dNormActivation(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]] = 3, stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int], str, NoneType] = None, groups: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, dilation: Union[int, Tuple[int, int]] = 1, inplace: Optional[bool] = True, bias: Optional[bool] = None) -> None`
      - Description: Configurable block used for Convolution2d-Normalization-Activation blocks. Args: in_channels (int): Number of channels in the input image out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block kernel_size: (int, optional): Size of the convolving kernel. Default: 3 stride (int, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation`` groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm2d`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` dilation (int): Spacing between kernel elements. Default: 1 inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True`` bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.
    - Class: `ExtraFPNBlock(*args, **kwargs) -> None`
      - Description: Base class for the extra block in the FPN. Args: results (List[Tensor]): the result of the FPN x (List[Tensor]): the original feature maps names (List[str]): the names for each one of the original feature maps Returns: results (List[Tensor]): the extended set of results of the FPN names (List[str]): the extended set of names for the results
    - Class: `FeaturePyramidNetwork(in_channels_list: List[int], out_channels: int, extra_blocks: Optional[torchvision.ops.feature_pyramid_network.ExtraFPNBlock] = None, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None)`
      - Description: Module that adds a FPN from on top of a set of feature maps. This is based on `"Feature Pyramid Network for Object Detection" <https://arxiv.org/abs/1612.03144>`_. The feature maps are currently supposed to be in increasing depth order. The input to the model is expected to be an OrderedDict[Tensor], containing the feature maps on top of which the FPN will be added. Args: in_channels_list (list[int]): number of channels for each feature map that is passed to the module out_channels (int): number of channels of the FPN representation extra_blocks (ExtraFPNBlock or None): if provided, extra operations will be performed. It is expected to take the fpn features, the original features and the names of the original features as input, and returns a new list of feature maps and their corresponding names norm_layer (callable, optional): Module specifying the normalization layer to use. Default: None Examples:: >>> m = torchvision.ops.FeaturePyramidNetwork([10, 20, 30], 5) >>> # get some dummy data >>> x = OrderedDict() >>> x['feat0'] = torch.rand(1, 10, 64, 64) >>> x['feat2'] = torch.rand(1, 20, 16, 16) >>> x['feat3'] = torch.rand(1, 30, 8, 8) >>> # compute the FPN on top of x >>> output = m(x) >>> print([(k, v.shape) for k, v in output.items()]) >>> # returns >>> [('feat0', torch.Size([1, 5, 64, 64])), >>> ('feat2', torch.Size([1, 5, 16, 16])), >>> ('feat3', torch.Size([1, 5, 8, 8]))]
    - Class: `LastLevelMaxPool(*args, **kwargs) -> None`
      - Description: Applies a max_pool2d (not actual max_pool2d, we just subsample) on top of the last feature map
    - Class: `LastLevelP6P7(in_channels: int, out_channels: int)`
      - Description: This module is used in RetinaNet to generate extra layers, P6 and P7.
    - Class: `OrderedDict(Unable to retrieve signature)`
      - Description: Dictionary that remembers insertion order
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
  - focal_loss.py
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `sigmoid_focal_loss(inputs: torch.Tensor, targets: torch.Tensor, alpha: float = 0.25, gamma: float = 2, reduction: str = 'none') -> torch.Tensor`
      - Description: Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002. Args: inputs (Tensor): A float tensor of arbitrary shape. The predictions for each example. targets (Tensor): A float tensor with the same shape as inputs. Stores the binary classification label for each element in inputs (0 for the negative class and 1 for the positive class). alpha (float): Weighting factor in range (0,1) to balance positive vs negative examples or -1 for ignore. Default: ``0.25``. gamma (float): Exponent of the modulating factor (1 - p_t) to balance easy vs hard examples. Default: ``2``. reduction (string): ``'none'`` | ``'mean'`` | ``'sum'`` ``'none'``: No reduction will be applied to the output. ``'mean'``: The output will be averaged. ``'sum'``: The output will be summed. Default: ``'none'``. Returns: Loss tensor with the reduction option applied.
  - giou_loss.py
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_loss_inter_union(boxes1: torch.Tensor, boxes2: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]`
      - Description: No docstring available
    - Function: `_upcast_non_float(t: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `generalized_box_iou_loss(boxes1: torch.Tensor, boxes2: torch.Tensor, reduction: str = 'none', eps: float = 1e-07) -> torch.Tensor`
      - Description: Gradient-friendly IoU loss with an additional penalty that is non-zero when the boxes do not overlap and scales with the size of their smallest enclosing box. This loss is symmetric, so the boxes1 and boxes2 arguments are interchangeable. Both sets of boxes are expected to be in ``(x1, y1, x2, y2)`` format with ``0 <= x1 < x2`` and ``0 <= y1 < y2``, and The two boxes should have the same dimensions. Args: boxes1 (Tensor[N, 4] or Tensor[4]): first set of boxes boxes2 (Tensor[N, 4] or Tensor[4]): second set of boxes reduction (string, optional): Specifies the reduction to apply to the output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: No reduction will be applied to the output. ``'mean'``: The output will be averaged. ``'sum'``: The output will be summed. Default: ``'none'`` eps (float): small number to prevent division by zero. Default: 1e-7 Returns: Tensor: Loss tensor with the reduction option applied. Reference: Hamid Rezatofighi et al.: Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression: https://arxiv.org/abs/1902.09630
  - misc.py
    - Class: `Conv2dNormActivation(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]] = 3, stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int], str, NoneType] = None, groups: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, dilation: Union[int, Tuple[int, int]] = 1, inplace: Optional[bool] = True, bias: Optional[bool] = None) -> None`
      - Description: Configurable block used for Convolution2d-Normalization-Activation blocks. Args: in_channels (int): Number of channels in the input image out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block kernel_size: (int, optional): Size of the convolving kernel. Default: 3 stride (int, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation`` groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm2d`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` dilation (int): Spacing between kernel elements. Default: 1 inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True`` bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.
    - Class: `Conv3dNormActivation(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int, int]] = 3, stride: Union[int, Tuple[int, int, int]] = 1, padding: Union[int, Tuple[int, int, int], str, NoneType] = None, groups: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.batchnorm.BatchNorm3d'>, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, dilation: Union[int, Tuple[int, int, int]] = 1, inplace: Optional[bool] = True, bias: Optional[bool] = None) -> None`
      - Description: Configurable block used for Convolution3d-Normalization-Activation blocks. Args: in_channels (int): Number of channels in the input video. out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block kernel_size: (int, optional): Size of the convolving kernel. Default: 3 stride (int, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation`` groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm3d`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` dilation (int): Spacing between kernel elements. Default: 1 inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True`` bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.
    - Class: `ConvNormActivation(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, ...]] = 3, stride: Union[int, Tuple[int, ...]] = 1, padding: Union[int, Tuple[int, ...], str, NoneType] = None, groups: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, dilation: Union[int, Tuple[int, ...]] = 1, inplace: Optional[bool] = True, bias: Optional[bool] = None, conv_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.conv.Conv2d'>) -> None`
      - Description: A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ``OrderedDict`` of modules can be passed in. The ``forward()`` method of ``Sequential`` accepts any input and forwards it to the first module it contains. It then "chains" outputs to inputs sequentially for each subsequent module, finally returning the output of the last module. The value a ``Sequential`` provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the ``Sequential`` applies to each of the modules it stores (which are each a registered submodule of the ``Sequential``). What's the difference between a ``Sequential`` and a :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it sounds like--a list for storing ``Module`` s! On the other hand, the layers in a ``Sequential`` are connected in a cascading way. Example:: # Using Sequential to create a small model. When `model` is run, # input will first be passed to `Conv2d(1,20,5)`. The output of # `Conv2d(1,20,5)` will be used as the input to the first # `ReLU`; the output of the first `ReLU` will become the input # for `Conv2d(20,64,5)`. Finally, the output of # `Conv2d(20,64,5)` will be used as input to the second `ReLU` model = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # Using Sequential with OrderedDict. This is functionally the # same as the above code model = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ]))
    - Class: `FrozenBatchNorm2d(num_features: int, eps: float = 1e-05)`
      - Description: BatchNorm2d where the batch statistics and the affine parameters are fixed Args: num_features (int): Number of features ``C`` from an expected input of size ``(N, C, H, W)`` eps (float): a value added to the denominator for numerical stability. Default: 1e-5
    - Class: `MLP(in_channels: int, hidden_channels: List[int], norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = None, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, inplace: Optional[bool] = None, bias: bool = True, dropout: float = 0.0)`
      - Description: This block implements the multi-layer perceptron (MLP) module. Args: in_channels (int): Number of channels of the input hidden_channels (List[int]): List of the hidden channel dimensions norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the linear layer. If ``None`` this layer won't be used. Default: ``None`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the linear layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` inplace (bool, optional): Parameter for the activation layer, which can optionally do the operation in-place. Default is ``None``, which uses the respective default values of the ``activation_layer`` and Dropout layer. bias (bool): Whether to use bias in the linear layer. Default ``True`` dropout (float): The probability for the dropout layer. Default: 0.0
    - Class: `Permute(dims: List[int])`
      - Description: This module returns a view of the tensor input with its dimensions permuted. Args: dims (List[int]): The desired ordering of dimensions
    - Class: `SqueezeExcitation(input_channels: int, squeeze_channels: int, activation: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.activation.ReLU'>, scale_activation: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.activation.Sigmoid'>) -> None`
      - Description: This block implements the Squeeze-and-Excitation block from https://arxiv.org/abs/1709.01507 (see Fig. 1). Parameters ``activation``, and ``scale_activation`` correspond to ``delta`` and ``sigma`` in eq. 3. Args: input_channels (int): Number of channels in the input image squeeze_channels (int): Number of squeeze channels activation (Callable[..., torch.nn.Module], optional): ``delta`` activation. Default: ``torch.nn.ReLU`` scale_activation (Callable[..., torch.nn.Module]): ``sigma`` activation. Default: ``torch.nn.Sigmoid``
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_make_ntuple(x: Any, n: int) -> Tuple[Any, ...]`
      - Description: Make n-tuple from input x. If x is an iterable, then we just convert it to tuple. Otherwise, we will make a tuple of length n, all with value of x. reference: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/utils.py#L8 Args: x (Any): input value n (int): length of the resulting tuple
    - Function: `interpolate(input: torch.Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> torch.Tensor`
      - Description: Down/up samples the input to either the given :attr:`size` or the given :attr:`scale_factor` The algorithm used for interpolation is determined by :attr:`mode`. Currently temporal, spatial and volumetric sampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape. The input dimensions are interpreted in the form: `mini-batch x channels x [optional depth] x [optional height] x width`. The modes available for resizing are: `nearest`, `linear` (3D-only), `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only), `area`, `nearest-exact` Args: input (Tensor): the input tensor size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]): output spatial size. scale_factor (float or Tuple[float]): multiplier for spatial size. If `scale_factor` is a tuple, its length has to match the number of spatial dimensions; `input.dim() - 2`. mode (str): algorithm used for upsampling: ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` | ``'trilinear'`` | ``'area'`` | ``'nearest-exact'``. Default: ``'nearest'`` align_corners (bool, optional): Geometrically, we consider the pixels of the input and output as squares rather than points. If set to ``True``, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to ``False``, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation *independent* of input size when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode` is ``'linear'``, ``'bilinear'``, ``'bicubic'`` or ``'trilinear'``. Default: ``False`` recompute_scale_factor (bool, optional): recompute the scale_factor for use in the interpolation calculation. If `recompute_scale_factor` is ``True``, then `scale_factor` must be passed in and `scale_factor` is used to compute the output `size`. The computed output `size` will be used to infer new scales for the interpolation. Note that when `scale_factor` is floating-point, it may differ from the recomputed `scale_factor` due to rounding and precision issues. If `recompute_scale_factor` is ``False``, then `size` or `scale_factor` will be used directly for interpolation. Default: ``None``. antialias (bool, optional): flag to apply anti-aliasing. Default: ``False``. Using anti-alias option together with ``align_corners=False``, interpolation result would match Pillow result for downsampling operation. Supported modes: ``'bilinear'``, ``'bicubic'``. .. note:: With ``mode='bicubic'``, it's possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call ``result.clamp(min=0, max=255)`` if you want to reduce the overshoot when displaying the image. .. note:: Mode ``mode='nearest-exact'`` matches Scikit-Image and PIL nearest neighbours interpolation algorithms and fixes known issues with ``mode='nearest'``. This mode is introduced to keep backward compatibility. Mode ``mode='nearest'`` matches buggy OpenCV's ``INTER_NEAREST`` interpolation algorithm. .. note:: The gradients for the dtype ``float16`` on CUDA may be inaccurate in the upsample operation when using modes ``['linear', 'bilinear', 'bicubic', 'trilinear', 'area']``. For more details, please refer to the discussion in `issue#104157 <https://github.com/pytorch/pytorch/issues/104157>`_. Note: This operation may produce nondeterministic gradients when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.
  - poolers.py
    - Class: `LevelMapper(k_min: int, k_max: int, canonical_scale: int = 224, canonical_level: int = 4, eps: float = 1e-06)`
      - Description: Determine which FPN level each RoI in a set of RoIs should map to based on the heuristic in the FPN paper. Args: k_min (int) k_max (int) canonical_scale (int) canonical_level (int) eps (float)
    - Class: `MultiScaleRoIAlign(featmap_names: List[str], output_size: Union[int, Tuple[int], List[int]], sampling_ratio: int, *, canonical_scale: int = 224, canonical_level: int = 4)`
      - Description: Multi-scale RoIAlign pooling, which is useful for detection with or without FPN. It infers the scale of the pooling via the heuristics specified in eq. 1 of the `Feature Pyramid Network paper <https://arxiv.org/abs/1612.03144>`_. They keyword-only parameters ``canonical_scale`` and ``canonical_level`` correspond respectively to ``224`` and ``k0=4`` in eq. 1, and have the following meaning: ``canonical_level`` is the target level of the pyramid from which to pool a region of interest with ``w x h = canonical_scale x canonical_scale``. Args: featmap_names (List[str]): the names of the feature maps that will be used for the pooling. output_size (List[Tuple[int, int]] or List[int]): output size for the pooled region sampling_ratio (int): sampling ratio for ROIAlign canonical_scale (int, optional): canonical_scale for LevelMapper canonical_level (int, optional): canonical_level for LevelMapper Examples:: >>> m = torchvision.ops.MultiScaleRoIAlign(['feat1', 'feat3'], 3, 2) >>> i = OrderedDict() >>> i['feat1'] = torch.rand(1, 5, 64, 64) >>> i['feat2'] = torch.rand(1, 5, 32, 32) # this feature won't be used in the pooling >>> i['feat3'] = torch.rand(1, 5, 16, 16) >>> # create some random bounding boxes >>> boxes = torch.rand(6, 4) * 256; boxes[:, 2:] += boxes[:, :2] >>> # original image size, before computing the feature maps >>> image_sizes = [(512, 512)] >>> output = m(i, [boxes], image_sizes) >>> print(output.shape) >>> torch.Size([6, 5, 3, 3])
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Function: `_convert_to_roi_format(boxes: List[torch.Tensor]) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_filter_input(x: Dict[str, torch.Tensor], featmap_names: List[str]) -> List[torch.Tensor]`
      - Description: No docstring available
    - Function: `_infer_scale(feature: torch.Tensor, original_size: List[int]) -> float`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_multiscale_roi_align(x_filtered: List[torch.Tensor], boxes: List[torch.Tensor], output_size: List[int], sampling_ratio: int, scales: Optional[List[float]], mapper: Optional[torchvision.ops.poolers.LevelMapper]) -> torch.Tensor`
      - Description: Args: x_filtered (List[Tensor]): List of input tensors. boxes (List[Tensor[N, 4]]): boxes to be used to perform the pooling operation, in (x1, y1, x2, y2) format and in the image reference size, not the feature map reference. The coordinate must satisfy ``0 <= x1 < x2`` and ``0 <= y1 < y2``. output_size (Union[List[Tuple[int, int]], List[int]]): size of the output sampling_ratio (int): sampling ratio for ROIAlign scales (Optional[List[float]]): If None, scales will be automatically inferred. Default value is None. mapper (Optional[LevelMapper]): If none, mapper will be automatically inferred. Default value is None. Returns: result (Tensor)
    - Function: `_onnx_merge_levels(levels: torch.Tensor, unmerged_results: List[torch.Tensor]) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_setup_scales(features: List[torch.Tensor], image_shapes: List[Tuple[int, int]], canonical_scale: int, canonical_level: int) -> Tuple[List[float], torchvision.ops.poolers.LevelMapper]`
      - Description: No docstring available
    - Function: `box_area(boxes: torch.Tensor) -> torch.Tensor`
      - Description: Computes the area of a set of bounding boxes, which are specified by their (x1, y1, x2, y2) coordinates. Args: boxes (Tensor[N, 4]): boxes for which the area will be computed. They are expected to be in (x1, y1, x2, y2) format with ``0 <= x1 < x2`` and ``0 <= y1 < y2``. Returns: Tensor[N]: the area for each box
    - Function: `initLevelMapper(k_min: int, k_max: int, canonical_scale: int = 224, canonical_level: int = 4, eps: float = 1e-06)`
      - Description: No docstring available
    - Function: `roi_align(input: torch.Tensor, boxes: Union[torch.Tensor, List[torch.Tensor]], output_size: None, spatial_scale: float = 1.0, sampling_ratio: int = -1, aligned: bool = False) -> torch.Tensor`
      - Description: Performs Region of Interest (RoI) Align operator with average pooling, as described in Mask R-CNN. Args: input (Tensor[N, C, H, W]): The input tensor, i.e. a batch with ``N`` elements. Each element contains ``C`` feature maps of dimensions ``H x W``. If the tensor is quantized, we expect a batch size of ``N == 1``. boxes (Tensor[K, 5] or List[Tensor[L, 4]]): the box coordinates in (x1, y1, x2, y2) format where the regions will be taken from. The coordinate must satisfy ``0 <= x1 < x2`` and ``0 <= y1 < y2``. If a single Tensor is passed, then the first column should contain the index of the corresponding element in the batch, i.e. a number in ``[0, N - 1]``. If a list of Tensors is passed, then each Tensor will correspond to the boxes for an element i in the batch. output_size (int or Tuple[int, int]): the size of the output (in bins or pixels) after the pooling is performed, as (height, width). spatial_scale (float): a scaling factor that maps the box coordinates to the input coordinates. For example, if your boxes are defined on the scale of a 224x224 image and your input is a 112x112 feature map (resulting from a 0.5x scaling of the original image), you'll want to set this to 0.5. Default: 1.0 sampling_ratio (int): number of sampling points in the interpolation grid used to compute the output value of each pooled output bin. If > 0, then exactly ``sampling_ratio x sampling_ratio`` sampling points per bin are used. If <= 0, then an adaptive number of grid points are used (computed as ``ceil(roi_width / output_width)``, and likewise for height). Default: -1 aligned (bool): If False, use the legacy implementation. If True, pixel shift the box coordinates it by -0.5 for a better alignment with the two neighboring pixel indices. This version is used in Detectron2 Returns: Tensor[K, C, output_size[0], output_size[1]]: The pooled RoIs.
  - ps_roi_align.py
    - Class: `PSRoIAlign(output_size: int, spatial_scale: float, sampling_ratio: int)`
      - Description: See :func:`ps_roi_align`.
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Function: `_assert_has_ops()`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_pair(x)`
      - Description: No docstring available
    - Function: `check_roi_boxes_shape(boxes: Union[torch.Tensor, List[torch.Tensor]])`
      - Description: No docstring available
    - Function: `convert_boxes_to_roi_format(boxes: List[torch.Tensor]) -> torch.Tensor`
      - Description: No docstring available
    - Function: `ps_roi_align(input: torch.Tensor, boxes: torch.Tensor, output_size: int, spatial_scale: float = 1.0, sampling_ratio: int = -1) -> torch.Tensor`
      - Description: Performs Position-Sensitive Region of Interest (RoI) Align operator mentioned in Light-Head R-CNN. Args: input (Tensor[N, C, H, W]): The input tensor, i.e. a batch with ``N`` elements. Each element contains ``C`` feature maps of dimensions ``H x W``. boxes (Tensor[K, 5] or List[Tensor[L, 4]]): the box coordinates in (x1, y1, x2, y2) format where the regions will be taken from. The coordinate must satisfy ``0 <= x1 < x2`` and ``0 <= y1 < y2``. If a single Tensor is passed, then the first column should contain the index of the corresponding element in the batch, i.e. a number in ``[0, N - 1]``. If a list of Tensors is passed, then each Tensor will correspond to the boxes for an element i in the batch. output_size (int or Tuple[int, int]): the size of the output (in bins or pixels) after the pooling is performed, as (height, width). spatial_scale (float): a scaling factor that maps the box coordinates to the input coordinates. For example, if your boxes are defined on the scale of a 224x224 image and your input is a 112x112 feature map (resulting from a 0.5x scaling of the original image), you'll want to set this to 0.5. Default: 1.0 sampling_ratio (int): number of sampling points in the interpolation grid used to compute the output value of each pooled output bin. If > 0, then exactly ``sampling_ratio x sampling_ratio`` sampling points per bin are used. If <= 0, then an adaptive number of grid points are used (computed as ``ceil(roi_width / output_width)``, and likewise for height). Default: -1 Returns: Tensor[K, C / (output_size[0] * output_size[1]), output_size[0], output_size[1]]: The pooled RoIs
  - ps_roi_pool.py
    - Class: `PSRoIPool(output_size: int, spatial_scale: float)`
      - Description: See :func:`ps_roi_pool`.
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Function: `_assert_has_ops()`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_pair(x)`
      - Description: No docstring available
    - Function: `check_roi_boxes_shape(boxes: Union[torch.Tensor, List[torch.Tensor]])`
      - Description: No docstring available
    - Function: `convert_boxes_to_roi_format(boxes: List[torch.Tensor]) -> torch.Tensor`
      - Description: No docstring available
    - Function: `ps_roi_pool(input: torch.Tensor, boxes: torch.Tensor, output_size: int, spatial_scale: float = 1.0) -> torch.Tensor`
      - Description: Performs Position-Sensitive Region of Interest (RoI) Pool operator described in R-FCN Args: input (Tensor[N, C, H, W]): The input tensor, i.e. a batch with ``N`` elements. Each element contains ``C`` feature maps of dimensions ``H x W``. boxes (Tensor[K, 5] or List[Tensor[L, 4]]): the box coordinates in (x1, y1, x2, y2) format where the regions will be taken from. The coordinate must satisfy ``0 <= x1 < x2`` and ``0 <= y1 < y2``. If a single Tensor is passed, then the first column should contain the index of the corresponding element in the batch, i.e. a number in ``[0, N - 1]``. If a list of Tensors is passed, then each Tensor will correspond to the boxes for an element i in the batch. output_size (int or Tuple[int, int]): the size of the output (in bins or pixels) after the pooling is performed, as (height, width). spatial_scale (float): a scaling factor that maps the box coordinates to the input coordinates. For example, if your boxes are defined on the scale of a 224x224 image and your input is a 112x112 feature map (resulting from a 0.5x scaling of the original image), you'll want to set this to 0.5. Default: 1.0 Returns: Tensor[K, C / (output_size[0] * output_size[1]), output_size[0], output_size[1]]: The pooled RoIs.
  - roi_align.py
    - Class: `RoIAlign(output_size: None, spatial_scale: float, sampling_ratio: int, aligned: bool = False)`
      - Description: See :func:`roi_align`.
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Function: `_assert_has_ops()`
      - Description: No docstring available
    - Function: `_bilinear_interpolate(input, roi_batch_ind, y, x, ymask, xmask)`
      - Description: No docstring available
    - Function: `_has_ops()`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_pair(x)`
      - Description: No docstring available
    - Function: `_roi_align(input, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned)`
      - Description: No docstring available
    - Function: `check_roi_boxes_shape(boxes: Union[torch.Tensor, List[torch.Tensor]])`
      - Description: No docstring available
    - Function: `convert_boxes_to_roi_format(boxes: List[torch.Tensor]) -> torch.Tensor`
      - Description: No docstring available
    - Function: `maybe_cast(tensor)`
      - Description: No docstring available
    - Function: `roi_align(input: torch.Tensor, boxes: Union[torch.Tensor, List[torch.Tensor]], output_size: None, spatial_scale: float = 1.0, sampling_ratio: int = -1, aligned: bool = False) -> torch.Tensor`
      - Description: Performs Region of Interest (RoI) Align operator with average pooling, as described in Mask R-CNN. Args: input (Tensor[N, C, H, W]): The input tensor, i.e. a batch with ``N`` elements. Each element contains ``C`` feature maps of dimensions ``H x W``. If the tensor is quantized, we expect a batch size of ``N == 1``. boxes (Tensor[K, 5] or List[Tensor[L, 4]]): the box coordinates in (x1, y1, x2, y2) format where the regions will be taken from. The coordinate must satisfy ``0 <= x1 < x2`` and ``0 <= y1 < y2``. If a single Tensor is passed, then the first column should contain the index of the corresponding element in the batch, i.e. a number in ``[0, N - 1]``. If a list of Tensors is passed, then each Tensor will correspond to the boxes for an element i in the batch. output_size (int or Tuple[int, int]): the size of the output (in bins or pixels) after the pooling is performed, as (height, width). spatial_scale (float): a scaling factor that maps the box coordinates to the input coordinates. For example, if your boxes are defined on the scale of a 224x224 image and your input is a 112x112 feature map (resulting from a 0.5x scaling of the original image), you'll want to set this to 0.5. Default: 1.0 sampling_ratio (int): number of sampling points in the interpolation grid used to compute the output value of each pooled output bin. If > 0, then exactly ``sampling_ratio x sampling_ratio`` sampling points per bin are used. If <= 0, then an adaptive number of grid points are used (computed as ``ceil(roi_width / output_width)``, and likewise for height). Default: -1 aligned (bool): If False, use the legacy implementation. If True, pixel shift the box coordinates it by -0.5 for a better alignment with the two neighboring pixel indices. This version is used in Detectron2 Returns: Tensor[K, C, output_size[0], output_size[1]]: The pooled RoIs.
  - roi_pool.py
    - Class: `RoIPool(output_size: None, spatial_scale: float)`
      - Description: See :func:`roi_pool`.
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Function: `_assert_has_ops()`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_pair(x)`
      - Description: No docstring available
    - Function: `check_roi_boxes_shape(boxes: Union[torch.Tensor, List[torch.Tensor]])`
      - Description: No docstring available
    - Function: `convert_boxes_to_roi_format(boxes: List[torch.Tensor]) -> torch.Tensor`
      - Description: No docstring available
    - Function: `roi_pool(input: torch.Tensor, boxes: Union[torch.Tensor, List[torch.Tensor]], output_size: None, spatial_scale: float = 1.0) -> torch.Tensor`
      - Description: Performs Region of Interest (RoI) Pool operator described in Fast R-CNN Args: input (Tensor[N, C, H, W]): The input tensor, i.e. a batch with ``N`` elements. Each element contains ``C`` feature maps of dimensions ``H x W``. boxes (Tensor[K, 5] or List[Tensor[L, 4]]): the box coordinates in (x1, y1, x2, y2) format where the regions will be taken from. The coordinate must satisfy ``0 <= x1 < x2`` and ``0 <= y1 < y2``. If a single Tensor is passed, then the first column should contain the index of the corresponding element in the batch, i.e. a number in ``[0, N - 1]``. If a list of Tensors is passed, then each Tensor will correspond to the boxes for an element i in the batch. output_size (int or Tuple[int, int]): the size of the output after the cropping is performed, as (height, width) spatial_scale (float): a scaling factor that maps the box coordinates to the input coordinates. For example, if your boxes are defined on the scale of a 224x224 image and your input is a 112x112 feature map (resulting from a 0.5x scaling of the original image), you'll want to set this to 0.5. Default: 1.0 Returns: Tensor[K, C, output_size[0], output_size[1]]: The pooled RoIs.
  - stochastic_depth.py
    - Class: `StochasticDepth(p: float, mode: str) -> None`
      - Description: See :func:`stochastic_depth`.
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `stochastic_depth(input: torch.Tensor, p: float, mode: str, training: bool = True) -> torch.Tensor`
      - Description: Implements the Stochastic Depth from `"Deep Networks with Stochastic Depth" <https://arxiv.org/abs/1603.09382>`_ used for randomly dropping residual branches of residual architectures. Args: input (Tensor[N, ...]): The input tensor or arbitrary dimensions with the first one being its batch i.e. a batch with ``N`` rows. p (float): probability of the input to be zeroed. mode (str): ``"batch"`` or ``"row"``. ``"batch"`` randomly zeroes the entire input, ``"row"`` zeroes randomly selected rows from the batch. training: apply stochastic depth if is ``True``. Default: ``True`` Returns: Tensor[N, ...]: The randomly zeroed tensor.
  - _box_convert.py
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Function: `_box_cxcywh_to_xyxy(boxes: torch.Tensor) -> torch.Tensor`
      - Description: Converts bounding boxes from (cx, cy, w, h) format to (x1, y1, x2, y2) format. (cx, cy) refers to center of bounding box (w, h) are width and height of bounding box Args: boxes (Tensor[N, 4]): boxes in (cx, cy, w, h) format which will be converted. Returns: boxes (Tensor(N, 4)): boxes in (x1, y1, x2, y2) format.
    - Function: `_box_xywh_to_xyxy(boxes: torch.Tensor) -> torch.Tensor`
      - Description: Converts bounding boxes from (x, y, w, h) format to (x1, y1, x2, y2) format. (x, y) refers to top left of bounding box. (w, h) refers to width and height of box. Args: boxes (Tensor[N, 4]): boxes in (x, y, w, h) which will be converted. Returns: boxes (Tensor[N, 4]): boxes in (x1, y1, x2, y2) format.
    - Function: `_box_xyxy_to_cxcywh(boxes: torch.Tensor) -> torch.Tensor`
      - Description: Converts bounding boxes from (x1, y1, x2, y2) format to (cx, cy, w, h) format. (x1, y1) refer to top left of bounding box (x2, y2) refer to bottom right of bounding box Args: boxes (Tensor[N, 4]): boxes in (x1, y1, x2, y2) format which will be converted. Returns: boxes (Tensor(N, 4)): boxes in (cx, cy, w, h) format.
    - Function: `_box_xyxy_to_xywh(boxes: torch.Tensor) -> torch.Tensor`
      - Description: Converts bounding boxes from (x1, y1, x2, y2) format to (x, y, w, h) format. (x1, y1) refer to top left of bounding box (x2, y2) refer to bottom right of bounding box Args: boxes (Tensor[N, 4]): boxes in (x1, y1, x2, y2) which will be converted. Returns: boxes (Tensor[N, 4]): boxes in (x, y, w, h) format.
  - _register_onnx_ops.py
    - Function: `_process_batch_indices_for_roi_align(g, rois)`
      - Description: No docstring available
    - Function: `_process_rois_for_roi_align(g, rois)`
      - Description: No docstring available
    - Function: `_process_sampling_ratio_for_roi_align(g, sampling_ratio: int)`
      - Description: No docstring available
    - Function: `_register_custom_op()`
      - Description: No docstring available
    - Function: `parse_args(*arg_descriptors: '_ValueDescriptor')`
      - Description: A decorator which converts args from torch._C.Value to built-in types. For example: ``` @parse_args('v', 'i', 'fs') foo(g, a, b, c): assert isinstance(a, torch._C.Value) assert isinstance(b, int) assert isinstance(c, list) assert isinstance(c[0], float) ``` Args: arg_descriptors: list of str, where each element is a string that specifies the type to convert to. Valid descriptors: "v": no conversion, keep torch._C.Value. "i": int "is": list of int "f": float "fs": list of float "b": bool "s": str "t": torch.Tensor "none": the variable is unused
    - Function: `roi_align_opset11(g, input, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned)`
      - Description: No docstring available
    - Function: `roi_align_opset16(g, input, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned)`
      - Description: No docstring available
    - Function: `roi_pool(g, input, rois, spatial_scale, pooled_height, pooled_width)`
      - Description: No docstring available
    - Function: `symbolic_multi_label_nms(g, boxes, scores, iou_threshold)`
      - Description: No docstring available
  - _utils.py
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Function: `_cat(tensors: List[torch.Tensor], dim: int = 0) -> torch.Tensor`
      - Description: Efficient version of torch.cat that avoids a copy if there is only a single element in a list
    - Function: `_loss_inter_union(boxes1: torch.Tensor, boxes2: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]`
      - Description: No docstring available
    - Function: `_upcast(t: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_upcast_non_float(t: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `check_roi_boxes_shape(boxes: Union[torch.Tensor, List[torch.Tensor]])`
      - Description: No docstring available
    - Function: `convert_boxes_to_roi_format(boxes: List[torch.Tensor]) -> torch.Tensor`
      - Description: No docstring available
    - Function: `split_normalization_params(model: torch.nn.modules.module.Module, norm_classes: Optional[List[type]] = None) -> Tuple[List[torch.Tensor], List[torch.Tensor]]`
      - Description: No docstring available
- prototype/
  - datasets/
    - utils/
      - __pycache__/
      - _dataset.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - _encoded.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - _internal.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - _resource.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
    - _builtin/
      - __pycache__/
      - caltech.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - celeba.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - cifar.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - clevr.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - coco.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - country211.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - cub200.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - dtd.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - eurosat.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - fer2013.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - food101.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - gtsrb.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - imagenet.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - mnist.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - oxford_iiit_pet.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - pcam.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - sbd.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - semeion.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - stanford_cars.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - svhn.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - usps.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
      - voc.py
        - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
    - __pycache__/
    - benchmark.py
      - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
    - generate_category_files.py
      - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
    - _api.py
      - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
    - _folder.py
      - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
    - _home.py
      - (Error: `torchvision.prototype.datasets` depends on PyTorch's `torchdata` (https://github.com/pytorch/data). You can install it with `pip install --pre torchdata --extra-index-url https://download.pytorch.org/whl/nightly/cpu)
  - models/
    - depth/
      - stereo/
        - __pycache__/
        - crestereo.py
          - Class: `AdaptiveGroupCorrelationLayer(iterative_correlation_layer: torchvision.prototype.models.depth.stereo.crestereo.IterativeCorrelationLayer, attention_offset_correlation_layer: torchvision.prototype.models.depth.stereo.crestereo.AttentionOffsetCorrelationLayer) -> None`
            - Description: Container for computing various correlation types between a left and right feature map. This module does not contain any optimisable parameters, it's solely a collection of ops. We wrap in a nn.Module for torch.jit.script compatibility Adaptive Group Correlation operations from: https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_With_Adaptive_Correlation_CVPR_2022_paper.pdf Canonical reference implementation: https://github.com/megvii-research/CREStereo/blob/master/nets/corr.py
          - Class: `AttentionOffsetCorrelationLayer(groups: int = 4, attention_module: Optional[torch.nn.modules.module.Module] = None, search_window_1d: Tuple[int, int] = (1, 9), search_dilate_1d: Tuple[int, int] = (1, 1), search_window_2d: Tuple[int, int] = (3, 3), search_dilate_2d: Tuple[int, int] = (1, 1)) -> None`
            - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
          - Class: `CREStereo(*, feature_encoder: torchvision.models.optical_flow.raft.FeatureEncoder, update_block: torchvision.models.optical_flow.raft.UpdateBlock, flow_head: torchvision.models.optical_flow.raft.FlowHead, self_attn_block: torchvision.prototype.models.depth.stereo.crestereo.LocalFeatureTransformer, cross_attn_block: torchvision.prototype.models.depth.stereo.crestereo.LocalFeatureTransformer, feature_downsample_rates: Tuple[int, ...] = (2, 4), correlation_groups: int = 4, search_window_1d: Tuple[int, int] = (1, 9), search_dilate_1d: Tuple[int, int] = (1, 1), search_window_2d: Tuple[int, int] = (3, 3), search_dilate_2d: Tuple[int, int] = (1, 1)) -> None`
            - Description: Implements CREStereo from the `"Practical Stereo Matching via Cascaded Recurrent Network With Adaptive Correlation" <https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_With_Adaptive_Correlation_CVPR_2022_paper.pdf>`_ paper. Args: feature_encoder (raft.FeatureEncoder): Raft-like Feature Encoder module extract low-level features from inputs. update_block (raft.UpdateBlock): Raft-like Update Block which recursively refines a flow-map. flow_head (raft.FlowHead): Raft-like Flow Head which predics a flow-map from some inputs. self_attn_block (LocalFeatureTransformer): A Local Feature Transformer that performs self attention on the two feature maps. cross_attn_block (LocalFeatureTransformer): A Local Feature Transformer that performs cross attention between the two feature maps used in the Adaptive Group Correlation module. feature_downsample_rates (List[int]): The downsample rates used to build a feature pyramid from the outputs of the `feature_encoder`. Default: [2, 4] correlation_groups (int): In how many groups should the features be split when computer per-pixel correlation. Defaults 4. search_window_1d (Tuple[int, int]): The alternate search window size in the x and y directions for the 1D case. Defaults to (1, 9). search_dilate_1d (Tuple[int, int]): The dilation used in the `search_window_1d` when selecting pixels. Similar to `nn.Conv2d` dilate. Defaults to (1, 1). search_window_2d (Tuple[int, int]): The alternate search window size in the x and y directions for the 2D case. Defaults to (3, 3). search_dilate_2d (Tuple[int, int]): The dilation used in the `search_window_2d` when selecting pixels. Similar to `nn.Conv2d` dilate. Defaults to (1, 1).
          - Class: `CREStereo_Base_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
            - Description: The metrics reported here are as follows. ``mae`` is the "mean-average-error" and indicates how far (in pixels) the predicted disparity is from its true value (equivalent to ``epe``). This is averaged over all pixels of all images. ``1px``, ``3px``, ``5px`` and indicate the percentage of pixels that have a lower error than that of the ground truth. ``relepe`` is the "relative-end-point-error" and is the average ``epe`` divided by the average ground truth disparity. ``fl-all`` corresponds to the average of pixels whose epe is either <3px, or whom's ``relepe`` is lower than 0.05 (therefore higher is better).
          - Class: `Conv2dNormActivation(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]] = 3, stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int], str, NoneType] = None, groups: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, dilation: Union[int, Tuple[int, int]] = 1, inplace: Optional[bool] = True, bias: Optional[bool] = None) -> None`
            - Description: Configurable block used for Convolution2d-Normalization-Activation blocks. Args: in_channels (int): Number of channels in the input image out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block kernel_size: (int, optional): Size of the convolving kernel. Default: 3 stride (int, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation`` groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm2d`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` dilation (int): Spacing between kernel elements. Default: 1 inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True`` bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.
          - Class: `ConvexMaskPredictor(*, in_channels: int, hidden_size: int, upsample_factor: int, multiplier: float = 0.25) -> None`
            - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
          - Class: `IterativeCorrelationLayer(groups: int = 4, search_window_1d: Tuple[int, int] = (1, 9), search_dilate_1d: Tuple[int, int] = (1, 1), search_window_2d: Tuple[int, int] = (3, 3), search_dilate_2d: Tuple[int, int] = (1, 1)) -> None`
            - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
          - Class: `LinearAttention(eps: float = 1e-06, feature_map_fn: Callable[[torch.Tensor], torch.Tensor] = <function elu_feature_map at 0x000002080BA69940>) -> None`
            - Description: Linear attention operation from: https://arxiv.org/pdf/2006.16236.pdf Canonical implementation reference: https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/linear_attention.py LoFTR implementation reference: https://github.com/zju3dv/LoFTR/blob/2122156015b61fbb650e28b58a958e4d632b1058/src/loftr/loftr_module/linear_attention.py
          - Class: `LocalFeatureEncoderLayer(*, dim_model: int, num_heads: int, attention_module: Callable[..., torch.nn.modules.module.Module] = <class 'torchvision.prototype.models.depth.stereo.crestereo.LinearAttention'>) -> None`
            - Description: LoFTR transformer module from: https://arxiv.org/pdf/2104.00680.pdf Canonical implementations at: https://github.com/zju3dv/LoFTR/blob/master/src/loftr/loftr_module/transformer.py
          - Class: `LocalFeatureTransformer(*, dim_model: int, num_heads: int, attention_directions: List[str], attention_module: Callable[..., torch.nn.modules.module.Module] = <class 'torchvision.prototype.models.depth.stereo.crestereo.LinearAttention'>) -> None`
            - Description: LoFTR transformer module from: https://arxiv.org/pdf/2104.00680.pdf Canonical implementations at: https://github.com/zju3dv/LoFTR/blob/master/src/loftr/loftr_module/transformer.py
          - Class: `PositionalEncodingSine(dim_model: int, max_size: int = 256) -> None`
            - Description: Sinusoidal positional encodings Using the scaling term from https://github.com/megvii-research/CREStereo/blob/master/nets/attention/position_encoding.py Reference implementation from https://github.com/facebookresearch/detr/blob/8a144f83a287f4d3fece4acdf073f387c5af387d/models/position_encoding.py#L28-L48
          - Class: `PyramidDownsample(factors: Iterable[int]) -> None`
            - Description: A simple wrapper that return and Avg Pool feature pyramid based on the provided scales. Implicitly returns the input as well.
          - Class: `SoftmaxAttention(dropout: float = 0.0) -> None`
            - Description: A simple softmax attention operation LoFTR implementation reference: https://github.com/zju3dv/LoFTR/blob/2122156015b61fbb650e28b58a958e4d632b1058/src/loftr/loftr_module/linear_attention.py
          - Class: `StereoMatching(*, use_gray_scale: bool = False, resize_size: Optional[Tuple[int, ...]], mean: Tuple[float, ...] = (0.5, 0.5, 0.5), std: Tuple[float, ...] = (0.5, 0.5, 0.5), interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>) -> None`
            - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
          - Class: `Tensor(Unable to retrieve signature)`
            - Description: No docstring available
          - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
            - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
          - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
            - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
          - Function: `_check_window_specs(search_window_1d: Tuple[int, int] = (1, 9), search_dilate_1d: Tuple[int, int] = (1, 1), search_window_2d: Tuple[int, int] = (3, 3), search_dilate_2d: Tuple[int, int] = (1, 1)) -> None`
            - Description: No docstring available
          - Function: `_crestereo(*, weights: Optional[torchvision.models._api.WeightsEnum], progress: bool, feature_encoder_layers: Tuple[int, int, int, int, int], feature_encoder_strides: Tuple[int, int, int, int], feature_encoder_block: Callable[..., torch.nn.modules.module.Module], feature_encoder_norm_layer: Callable[..., torch.nn.modules.module.Module], feature_downsample_rates: Tuple[int, ...], corr_groups: int, corr_search_window_2d: Tuple[int, int], corr_search_dilate_2d: Tuple[int, int], corr_search_window_1d: Tuple[int, int], corr_search_dilate_1d: Tuple[int, int], flow_head_hidden_size: int, recurrent_block_hidden_state_size: int, recurrent_block_kernel_size: Tuple[Tuple[int, int], Tuple[int, int]], recurrent_block_padding: Tuple[Tuple[int, int], Tuple[int, int]], motion_encoder_corr_layers: Tuple[int, int], motion_encoder_flow_layers: Tuple[int, int], motion_encoder_out_channels: int, num_attention_heads: int, num_self_attention_layers: int, num_cross_attention_layers: int, self_attention_module: Callable[..., torch.nn.modules.module.Module], cross_attention_module: Callable[..., torch.nn.modules.module.Module], **kwargs) -> torchvision.prototype.models.depth.stereo.crestereo.CREStereo`
            - Description: No docstring available
          - Function: `crestereo_base(*, weights: Optional[torchvision.prototype.models.depth.stereo.crestereo.CREStereo_Base_Weights] = None, progress=True, **kwargs) -> torchvision.prototype.models.depth.stereo.crestereo.CREStereo`
            - Description: CREStereo model from `Practical Stereo Matching via Cascaded Recurrent Network With Adaptive Correlation <https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_With_Adaptive_Correlation_CVPR_2022_paper.pdf>`_. Please see the example below for a tutorial on how to use this model. Args: weights(:class:`~torchvision.prototype.models.depth.stereo.CREStereo_Base_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.prototype.models.depth.stereo.CREStereo_Base_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.prototype.models.depth.stereo.raft_stereo.RaftStereo`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/optical_flow/crestereo.py>`_ for more details about this class. .. autoclass:: torchvision.prototype.models.depth.stereo.CREStereo_Base_Weights :members:
          - Function: `elu_feature_map(x: torch.Tensor) -> torch.Tensor`
            - Description: Elu feature map operation from: https://arxiv.org/pdf/2006.16236.pdf
          - Function: `get_correlation(left_feature: torch.Tensor, right_feature: torch.Tensor, window_size: Tuple[int, int] = (3, 3), dilate: Tuple[int, int] = (1, 1)) -> torch.Tensor`
            - Description: Function that computes a correlation product between the left and right features. The correlation is computed in a sliding window fashion, namely the left features are fixed and for each ``(i, j)`` location we compute the correlation with a sliding window anchored in ``(i, j)`` from the right feature map. The sliding window selects pixels obtained in the range of the sliding window; i.e ``(i - window_size // 2, i + window_size // 2)`` respectively ``(j - window_size // 2, j + window_size // 2)``.
          - Function: `grid_sample(img: torch.Tensor, absolute_grid: torch.Tensor, mode: str = 'bilinear', align_corners: Optional[bool] = None)`
            - Description: Same as torch's grid_sample, with absolute pixel coordinates instead of normalized coordinates.
          - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
            - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
          - Function: `make_coords_grid(batch_size: int, h: int, w: int, device: str = 'cpu')`
            - Description: No docstring available
          - Class: `partial(Unable to retrieve signature)`
            - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
          - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
            - Description: No docstring available
          - Function: `upsample_flow(flow, up_mask: Optional[torch.Tensor] = None, factor: int = 8)`
            - Description: Upsample flow by the input factor (default 8). If up_mask is None we just interpolate. If up_mask is specified, we upsample using a convex combination of its weights. See paper page 8 and appendix B. Note that in appendix B the picture assumes a downsample factor of 4 instead of 8.
        - raft_stereo.py
          - Class: `BaseEncoder(*, block: Callable[..., torch.nn.modules.module.Module] = <class 'torchvision.models.optical_flow.raft.ResidualBlock'>, layers: Tuple[int, int, int, int] = (64, 64, 96, 128), strides: Tuple[int, int, int, int] = (2, 1, 2, 2), norm_layer: Callable[..., torch.nn.modules.module.Module] = <class 'torch.nn.modules.batchnorm.BatchNorm2d'>)`
            - Description: Base encoder for FeatureEncoder and ContextEncoder in which weight may be shared. See the Raft-Stereo paper section 4.6 on backbone part.
          - Class: `Conv2dNormActivation(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]] = 3, stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int], str, NoneType] = None, groups: int = 1, norm_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, activation_layer: Optional[Callable[..., torch.nn.modules.module.Module]] = <class 'torch.nn.modules.activation.ReLU'>, dilation: Union[int, Tuple[int, int]] = 1, inplace: Optional[bool] = True, bias: Optional[bool] = None) -> None`
            - Description: Configurable block used for Convolution2d-Normalization-Activation blocks. Args: in_channels (int): Number of channels in the input image out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block kernel_size: (int, optional): Size of the convolving kernel. Default: 3 stride (int, optional): Stride of the convolution. Default: 1 padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation`` groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm2d`` activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU`` dilation (int): Spacing between kernel elements. Default: 1 inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True`` bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.
          - Class: `ConvGRU(*, input_size, hidden_size, kernel_size, padding)`
            - Description: Convolutional Gru unit.
          - Class: `CorrBlock1d(*, num_levels: int = 4, radius: int = 4)`
            - Description: The row-wise correlation block. Use indexes from correlation pyramid to create correlation features. The "indexing" of a given centroid pixel x' is done by concatenating its surrounding row neighbours within radius
          - Class: `CorrPyramid1d(num_levels: int = 4)`
            - Description: Row-wise correlation pyramid. Create a row-wise correlation pyramid with ``num_levels`` level from the outputs of the feature encoder, this correlation pyramid will later be used as index to create correlation features using CorrBlock1d.
          - Class: `FeatureEncoder(base_encoder: torchvision.prototype.models.depth.stereo.raft_stereo.BaseEncoder, output_dim: int = 256, shared_base: bool = False, block: Callable[..., torch.nn.modules.module.Module] = <class 'torchvision.models.optical_flow.raft.ResidualBlock'>)`
            - Description: Feature Encoder for Raft-Stereo (see paper section 3.1) that may have shared weight with the Context Encoder. The FeatureEncoder takes concatenation of left and right image as input. It produces feature embedding that later will be used to construct correlation volume.
          - Class: `FlowHead(*, in_channels, hidden_size)`
            - Description: Flow head, part of the update block. Takes the hidden state of the recurrent unit as input, and outputs the predicted "delta flow".
          - Class: `MaskPredictor(*, in_channels: int, hidden_size: int, out_channels: int, multiplier: float = 0.25)`
            - Description: Mask predictor to be used when upsampling the predicted disparity.
          - Class: `MotionEncoder(*, in_channels_corr, corr_layers=(256, 192), flow_layers=(128, 64), out_channels=128)`
            - Description: The motion encoder, part of the update block. Takes the current predicted flow and the correlation features as input and returns an encoded version of these.
          - Class: `MultiLevelContextEncoder(base_encoder: torch.nn.modules.module.Module, out_with_blocks: List[bool], output_dim: int = 256, block: Callable[..., torch.nn.modules.module.Module] = <class 'torchvision.models.optical_flow.raft.ResidualBlock'>)`
            - Description: Context Encoder for Raft-Stereo (see paper section 3.1) that may have shared weight with the Feature Encoder. The ContextEncoder takes left image as input, and it outputs concatenated hidden_states and contexts. In Raft-Stereo we have multi level GRUs and this context encoder will also multi outputs (list of Tensor) that correspond to each GRUs. Take note that the length of "out_with_blocks" parameter represent the number of GRU's level. args: base_encoder (nn.Module): The base encoder part that can have a shared weight with feature_encoder's base_encoder because they have same architecture. out_with_blocks (List[bool]): The length represent the number of GRU's level (length of output), and if the element is True then the output layer on that position will have additional block output_dim (int): The dimension of output on each level (default: 256) block (Callable[..., nn.Module]): The type of basic block used for downsampling and output layer (default: ResidualBlock)
          - Class: `MultiLevelUpdateBlock(*, motion_encoder: torchvision.models.optical_flow.raft.MotionEncoder, hidden_dims: List[int])`
            - Description: The update block which contains the motion encoder and grus It must expose a ``hidden_dims`` attribute which is the hidden dimension size of its gru blocks
          - Class: `RaftStereo(*, feature_encoder: torchvision.prototype.models.depth.stereo.raft_stereo.FeatureEncoder, context_encoder: torchvision.prototype.models.depth.stereo.raft_stereo.MultiLevelContextEncoder, corr_pyramid: torchvision.prototype.models.depth.stereo.raft_stereo.CorrPyramid1d, corr_block: torchvision.prototype.models.depth.stereo.raft_stereo.CorrBlock1d, update_block: torchvision.prototype.models.depth.stereo.raft_stereo.MultiLevelUpdateBlock, disparity_head: torch.nn.modules.module.Module, mask_predictor: Optional[torch.nn.modules.module.Module] = None, slow_fast: bool = False)`
            - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
          - Class: `Raft_Stereo_Base_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
            - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
          - Class: `Raft_Stereo_Realtime_Weights(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
            - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
          - Class: `ResidualBlock(in_channels, out_channels, *, norm_layer, stride=1, always_project: bool = False)`
            - Description: Slightly modified Residual block with extra relu and biases.
          - Class: `StereoMatching(*, use_gray_scale: bool = False, resize_size: Optional[Tuple[int, ...]], mean: Tuple[float, ...] = (0.5, 0.5, 0.5), std: Tuple[float, ...] = (0.5, 0.5, 0.5), interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>) -> None`
            - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
          - Class: `Tensor(Unable to retrieve signature)`
            - Description: No docstring available
          - Class: `Weights(url: str, transforms: Callable, meta: Dict[str, Any]) -> None`
            - Description: This class is used to group important attributes associated with the pre-trained weights. Args: url (str): The location where we find the weights. transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms) needed to use the model. The reason we attach a constructor method rather than an already constructed object is because the specific object might have memory and thus we want to delay initialization until needed. meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be informative attributes (for example the number of parameters/flops, recipe link/methods used in training etc), configuration parameters (for example the `num_classes`) needed to construct the model or important meta-data (for example the `classes` of a classification model) needed to use the model.
          - Class: `WeightsEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
            - Description: This class is the parent class of all model weights. Each model building method receives an optional `weights` parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type `Weights`. Args: value (Weights): The data class entry with the weight information.
          - Function: `_log_api_usage_once(obj: Any) -> None`
            - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
          - Function: `_raft_stereo(*, weights: Optional[torchvision.models._api.WeightsEnum], progress: bool, shared_encoder_weight: bool, feature_encoder_layers: Tuple[int, int, int, int, int], feature_encoder_strides: Tuple[int, int, int, int], feature_encoder_block: Callable[..., torch.nn.modules.module.Module], context_encoder_layers: Tuple[int, int, int, int, int], context_encoder_strides: Tuple[int, int, int, int], context_encoder_out_with_blocks: List[bool], context_encoder_block: Callable[..., torch.nn.modules.module.Module], corr_num_levels: int, corr_radius: int, motion_encoder_corr_layers: Tuple[int, int], motion_encoder_flow_layers: Tuple[int, int], motion_encoder_out_channels: int, update_block_hidden_dims: List[int], flow_head_hidden_size: int, mask_predictor_hidden_size: int, use_mask_predictor: bool, slow_fast: bool, **kwargs)`
            - Description: No docstring available
          - Function: `grid_sample(img: torch.Tensor, absolute_grid: torch.Tensor, mode: str = 'bilinear', align_corners: Optional[bool] = None)`
            - Description: Same as torch's grid_sample, with absolute pixel coordinates instead of normalized coordinates.
          - Function: `handle_legacy_interface(**weights: Tuple[str, Union[~W, NoneType, Callable[[Dict[str, Any]], Optional[~W]]]])`
            - Description: Decorates a model builder with the new interface to make it compatible with the old. In particular this handles two things: 1. Allows positional parameters again, but emits a deprecation warning in case they are used. See :func:`torchvision.prototype.utils._internal.kwonly_to_pos_or_kw` for details. 2. Handles the default value change from ``pretrained=False`` to ``weights=None`` and ``pretrained=True`` to ``weights=Weights`` and emits a deprecation warning with instructions for the new interface. Args: **weights (Tuple[str, Union[Optional[W], Callable[[Dict[str, Any]], Optional[W]]]]): Deprecated parameter name and default value for the legacy ``pretrained=True``. The default value can be a callable in which case it will be called with a dictionary of the keyword arguments. The only key that is guaranteed to be in the dictionary is the deprecated parameter name passed as first element in the tuple. All other parameters should be accessed with :meth:`~dict.get`.
          - Function: `make_coords_grid(batch_size: int, h: int, w: int, device: str = 'cpu')`
            - Description: No docstring available
          - Class: `partial(Unable to retrieve signature)`
            - Description: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords.
          - Function: `raft_stereo_base(*, weights: Optional[torchvision.prototype.models.depth.stereo.raft_stereo.Raft_Stereo_Base_Weights] = None, progress=True, **kwargs) -> torchvision.prototype.models.depth.stereo.raft_stereo.RaftStereo`
            - Description: RAFT-Stereo model from `RAFT-Stereo: Multilevel Recurrent Field Transforms for Stereo Matching <https://arxiv.org/abs/2109.07547>`_. Please see the example below for a tutorial on how to use this model. Args: weights(:class:`~torchvision.prototype.models.depth.stereo.Raft_Stereo_Base_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.prototype.models.depth.stereo.Raft_Stereo_Base_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.prototype.models.depth.stereo.raft_stereo.RaftStereo`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/optical_flow/raft.py>`_ for more details about this class. .. autoclass:: torchvision.prototype.models.depth.stereo.Raft_Stereo_Base_Weights :members:
          - Function: `raft_stereo_realtime(*, weights: Optional[torchvision.prototype.models.depth.stereo.raft_stereo.Raft_Stereo_Realtime_Weights] = None, progress=True, **kwargs) -> torchvision.prototype.models.depth.stereo.raft_stereo.RaftStereo`
            - Description: RAFT-Stereo model from `RAFT-Stereo: Multilevel Recurrent Field Transforms for Stereo Matching <https://arxiv.org/abs/2109.07547>`_. This is the realtime variant of the Raft-Stereo model that is described on the paper section 4.7. Please see the example below for a tutorial on how to use this model. Args: weights(:class:`~torchvision.prototype.models.depth.stereo.Raft_Stereo_Realtime_Weights`, optional): The pretrained weights to use. See :class:`~torchvision.prototype.models.depth.stereo.Raft_Stereo_Realtime_Weights` below for more details, and possible values. By default, no pre-trained weights are used. progress (bool): If True, displays a progress bar of the download to stderr. Default is True. **kwargs: parameters passed to the ``torchvision.prototype.models.depth.stereo.raft_stereo.RaftStereo`` base class. Please refer to the `source code <https://github.com/pytorch/vision/blob/main/torchvision/models/optical_flow/raft.py>`_ for more details about this class. .. autoclass:: torchvision.prototype.models.depth.stereo.Raft_Stereo_Realtime_Weights :members:
          - Function: `register_model(name: Optional[str] = None) -> Callable[[Callable[..., ~M]], Callable[..., ~M]]`
            - Description: No docstring available
          - Function: `upsample_flow(flow, up_mask: Optional[torch.Tensor] = None, factor: int = 8)`
            - Description: Upsample flow by the input factor (default 8). If up_mask is None we just interpolate. If up_mask is specified, we upsample using a convex combination of its weights. See paper page 8 and appendix B. Note that in appendix B the picture assumes a downsample factor of 4 instead of 8.
      - __pycache__/
    - __pycache__/
  - transforms/
    - __pycache__/
    - _augment.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `InterpolationMode(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: Interpolation modes Available interpolation methods are ``nearest``, ``nearest-exact``, ``bilinear``, ``bicubic``, ``box``, ``hamming``, and ``lanczos``.
      - Class: `SimpleCopyPaste(blending: bool = True, resize_interpolation: Union[int, torchvision.transforms.functional.InterpolationMode] = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Optional[bool] = None) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Transform() -> 'None'`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Function: `_check_interpolation(interpolation: Union[torchvision.transforms.functional.InterpolationMode, int]) -> torchvision.transforms.functional.InterpolationMode`
        - Description: No docstring available
      - Function: `cast(typ, val)`
        - Description: Cast a value to a type. This returns the value unchanged. To the type checker this signals that the return value has the designated type, but at runtime we intentionally don't check anything (we want this to be as fast as possible).
      - Function: `is_pure_tensor(inpt: Any) -> bool`
        - Description: No docstring available
      - Function: `masks_to_boxes(masks: torch.Tensor) -> torch.Tensor`
        - Description: Compute the bounding boxes around the provided masks. Returns a [N, 4] tensor containing bounding boxes. The boxes are in ``(x1, y1, x2, y2)`` format with ``0 <= x1 < x2`` and ``0 <= y1 < y2``. Args: masks (Tensor[N, H, W]): masks to transform where N is the number of masks and (H, W) are the spatial dimensions. Returns: Tensor[N, 4]: bounding boxes
      - Function: `tree_flatten(pytree: Any) -> Tuple[List[Any], torch.utils._pytree.TreeSpec]`
        - Description: Flattens a pytree into a list of values and a TreeSpec that can be used to reconstruct the pytree.
      - Function: `tree_unflatten(values: List[Any], spec: torch.utils._pytree.TreeSpec) -> Any`
        - Description: Given a list of values and a TreeSpec, builds a pytree. This is the inverse operation of `tree_flatten`.
    - _geometry.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `FixedSizeCrop(size: Union[int, Sequence[int]], fill: Union[int, float, Sequence[int], Sequence[float], NoneType, Dict[Union[Type, str], Union[int, float, Sequence[int], Sequence[float], NoneType]]] = 0, padding_mode: str = 'constant') -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Label(data: 'Any', *, categories: 'Optional[Sequence[str]]' = None, dtype: 'Optional[torch.dtype]' = None, device: 'Optional[Union[torch.device, str, int]]' = None, requires_grad: 'Optional[bool]' = None) -> 'L'`
        - Description: [Beta] Base class for all TVTensors. You probably don't want to use this class unless you're defining your own custom TVTensors. See :ref:`sphx_glr_auto_examples_transforms_plot_custom_tv_tensors.py` for details.
      - Class: `OneHotLabel(data: 'Any', *, categories: 'Optional[Sequence[str]]' = None, dtype: 'Optional[torch.dtype]' = None, device: 'Optional[Union[torch.device, str, int]]' = None, requires_grad: 'bool' = False) -> 'OneHotLabel'`
        - Description: [Beta] Base class for all TVTensors. You probably don't want to use this class unless you're defining your own custom TVTensors. See :ref:`sphx_glr_auto_examples_transforms_plot_custom_tv_tensors.py` for details.
      - Class: `Transform() -> 'None'`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Function: `_get_fill(fill_dict, inpt_type)`
        - Description: No docstring available
      - Function: `_setup_fill_arg(fill: 'Union[_FillType, Dict[Union[Type, str], _FillType]]') -> 'Dict[Union[Type, str], _FillTypeJIT]'`
        - Description: No docstring available
      - Function: `_setup_size(size, error_msg)`
        - Description: No docstring available
      - Function: `get_bounding_boxes(flat_inputs: 'List[Any]') -> 'tv_tensors.BoundingBoxes'`
        - Description: No docstring available
      - Function: `has_any(flat_inputs: 'List[Any]', *types_or_checks: 'Union[Type, Callable[[Any], bool]]') -> 'bool'`
        - Description: No docstring available
      - Function: `is_pure_tensor(inpt: Any) -> bool`
        - Description: No docstring available
      - Function: `query_size(flat_inputs: 'List[Any]') -> 'Tuple[int, int]'`
        - Description: No docstring available
    - _misc.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `PermuteDimensions(dims: Union[Sequence[int], Dict[Type, Optional[Sequence[int]]]]) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Transform() -> 'None'`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `TransposeDimensions(dims: Union[Tuple[int, int], Dict[Type, Optional[Tuple[int, int]]]]) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `TypeVar(name, *constraints, bound=None, covariant=False, contravariant=False)`
        - Description: Type variable. Usage:: T = TypeVar('T') # Can be anything A = TypeVar('A', str, bytes) # Must be str or bytes Type variables exist primarily for the benefit of static type checkers. They serve as the parameters for generic types as well as for generic function definitions. See class Generic for more information on generic types. Generic functions work as follows: def repeat(x: T, n: int) -> List[T]: '''Return a list containing n references to x.''' return [x]*n def longest(x: A, y: A) -> A: '''Return the longest of two strings.''' return x if len(x) >= len(y) else y The latter example's signature is essentially the overloading of (str, str) -> str and (bytes, bytes) -> bytes. Also note that if the arguments are instances of some subclass of str, the return type is still plain str. At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError. Type variables defined with covariant=True or contravariant=True can be used to declare covariant or contravariant generic types. See PEP 484 for more details. By default generic types are invariant in all type variables. Type variables can be introspected. e.g.: T.__name__ == 'T' T.__constraints__ == () T.__covariant__ == False T.__contravariant__ = False A.__constraints__ == (str, bytes) Note that only type variables defined in global scope can be pickled.
      - Function: `_default_arg(value: ~T) -> ~T`
        - Description: No docstring available
      - Function: `_get_defaultdict(default: ~T) -> Dict[Any, ~T]`
        - Description: No docstring available
      - Class: `defaultdict(Unable to retrieve signature)`
        - Description: defaultdict(default_factory=None, /, [...]) --> dict with default factory The default factory is called without arguments to produce a new value when a key is not present, in __getitem__ only. A defaultdict compares equal to a dict with the same items. All remaining arguments are treated the same as if they were passed to the dict constructor, including keyword arguments.
      - Function: `is_pure_tensor(inpt: Any) -> bool`
        - Description: No docstring available
    - _presets.py
      - Class: `InterpolationMode(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: Interpolation modes Available interpolation methods are ``nearest``, ``nearest-exact``, ``bilinear``, ``bicubic``, ``box``, ``hamming``, and ``lanczos``.
      - Class: `StereoMatching(*, use_gray_scale: bool = False, resize_size: Optional[Tuple[int, ...]], mean: Tuple[float, ...] = (0.5, 0.5, 0.5), std: Tuple[float, ...] = (0.5, 0.5, 0.5), interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Tensor(Unable to retrieve signature)`
        - Description: No docstring available
      - Function: `_check_interpolation(interpolation: Union[torchvision.transforms.functional.InterpolationMode, int]) -> torchvision.transforms.functional.InterpolationMode`
        - Description: No docstring available
    - _type_conversion.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `LabelToOneHot(num_categories: int = -1)`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Transform() -> 'None'`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
  - tv_tensors/
    - __pycache__/
    - _label.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `Label(data: 'Any', *, categories: 'Optional[Sequence[str]]' = None, dtype: 'Optional[torch.dtype]' = None, device: 'Optional[Union[torch.device, str, int]]' = None, requires_grad: 'Optional[bool]' = None) -> 'L'`
        - Description: [Beta] Base class for all TVTensors. You probably don't want to use this class unless you're defining your own custom TVTensors. See :ref:`sphx_glr_auto_examples_transforms_plot_custom_tv_tensors.py` for details.
      - Class: `OneHotLabel(data: 'Any', *, categories: 'Optional[Sequence[str]]' = None, dtype: 'Optional[torch.dtype]' = None, device: 'Optional[Union[torch.device, str, int]]' = None, requires_grad: 'bool' = False) -> 'OneHotLabel'`
        - Description: [Beta] Base class for all TVTensors. You probably don't want to use this class unless you're defining your own custom TVTensors. See :ref:`sphx_glr_auto_examples_transforms_plot_custom_tv_tensors.py` for details.
      - Class: `TVTensor(Unable to retrieve signature)`
        - Description: [Beta] Base class for all TVTensors. You probably don't want to use this class unless you're defining your own custom TVTensors. See :ref:`sphx_glr_auto_examples_transforms_plot_custom_tv_tensors.py` for details.
      - Class: `TypeVar(name, *constraints, bound=None, covariant=False, contravariant=False)`
        - Description: Type variable. Usage:: T = TypeVar('T') # Can be anything A = TypeVar('A', str, bytes) # Must be str or bytes Type variables exist primarily for the benefit of static type checkers. They serve as the parameters for generic types as well as for generic function definitions. See class Generic for more information on generic types. Generic functions work as follows: def repeat(x: T, n: int) -> List[T]: '''Return a list containing n references to x.''' return [x]*n def longest(x: A, y: A) -> A: '''Return the longest of two strings.''' return x if len(x) >= len(y) else y The latter example's signature is essentially the overloading of (str, str) -> str and (bytes, bytes) -> bytes. Also note that if the arguments are instances of some subclass of str, the return type is still plain str. At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError. Type variables defined with covariant=True or contravariant=True can be used to declare covariant or contravariant generic types. See PEP 484 for more details. By default generic types are invariant in all type variables. Type variables can be introspected. e.g.: T.__name__ == 'T' T.__constraints__ == () T.__covariant__ == False T.__contravariant__ = False A.__constraints__ == (str, bytes) Note that only type variables defined in global scope can be pickled.
      - Class: `_LabelBase(data: 'Any', *, categories: 'Optional[Sequence[str]]' = None, dtype: 'Optional[torch.dtype]' = None, device: 'Optional[Union[torch.device, str, int]]' = None, requires_grad: 'Optional[bool]' = None) -> 'L'`
        - Description: [Beta] Base class for all TVTensors. You probably don't want to use this class unless you're defining your own custom TVTensors. See :ref:`sphx_glr_auto_examples_transforms_plot_custom_tv_tensors.py` for details.
      - Function: `tree_map(fn: Any, pytree: Any) -> Any`
        - Description: No docstring available
  - utils/
    - __pycache__/
    - _internal.py
      - Class: `BinaryIO()`
        - Description: Typed version of the return of open() in binary mode.
      - Class: `ReadOnlyTensorBuffer(tensor: torch.Tensor) -> None`
        - Description: No docstring available
      - Class: `TypeVar(name, *constraints, bound=None, covariant=False, contravariant=False)`
        - Description: Type variable. Usage:: T = TypeVar('T') # Can be anything A = TypeVar('A', str, bytes) # Must be str or bytes Type variables exist primarily for the benefit of static type checkers. They serve as the parameters for generic types as well as for generic function definitions. See class Generic for more information on generic types. Generic functions work as follows: def repeat(x: T, n: int) -> List[T]: '''Return a list containing n references to x.''' return [x]*n def longest(x: A, y: A) -> A: '''Return the longest of two strings.''' return x if len(x) >= len(y) else y The latter example's signature is essentially the overloading of (str, str) -> str and (bytes, bytes) -> bytes. Also note that if the arguments are instances of some subclass of str, the return type is still plain str. At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError. Type variables defined with covariant=True or contravariant=True can be used to declare covariant or contravariant generic types. See PEP 484 for more details. By default generic types are invariant in all type variables. Type variables can be introspected. e.g.: T.__name__ == 'T' T.__constraints__ == () T.__covariant__ == False T.__contravariant__ = False A.__constraints__ == (str, bytes) Note that only type variables defined in global scope can be pickled.
      - Function: `_read_mutable_buffer_fallback(file: <class 'BinaryIO'>, count: int, item_size: int) -> bytearray`
        - Description: No docstring available
      - Function: `add_suggestion(msg: str, *, word: str, possibilities: Collection[str], close_match_hint: Callable[[str], str] = <function <lambda> at 0x000002080B98AB60>, alternative_hint: Callable[[Sequence[str]], str] = <function <lambda> at 0x000002080B98ACA0>) -> str`
        - Description: No docstring available
      - Function: `fromfile(file: <class 'BinaryIO'>, *, dtype: torch.dtype, byte_order: str, count: int = -1) -> torch.Tensor`
        - Description: Construct a tensor from a binary file. .. note:: This function is similar to :func:`numpy.fromfile` with two notable differences: 1. This function only accepts an open binary file, but not a path to it. 2. This function has an additional ``byte_order`` parameter, since PyTorch's ``dtype``'s do not support that concept. .. note:: If the ``file`` was opened in update mode, i.e. "r+b" or "w+b", reading data is much faster. Be aware that as long as the file is still open, inplace operations on the returned tensor will reflect back to the file. Args: file (IO): Open binary file. dtype (torch.dtype): Data type of the underlying data as well as of the returned tensor. byte_order (str): Byte order of the data. Can be "little" or "big" endian. count (int): Number of values of the returned tensor. If ``-1`` (default), will read the complete file.
      - Function: `sequence_to_str(seq: Sequence, separate_last: str = '') -> str`
        - Description: No docstring available
  - __pycache__/
- transforms/
  - v2/
    - functional/
      - __pycache__/
      - _augment.py
        - Function: `_erase_image_pil(image: PIL.Image.Image, i: int, j: int, h: int, w: int, v: torch.Tensor, inplace: bool = False) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_get_kernel(functional, input_type, *, allow_passthrough=False)`
          - Description: No docstring available
        - Function: `_log_api_usage_once(obj: Any) -> None`
          - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
        - Function: `_register_kernel_internal(functional, input_type, *, tv_tensor_wrapper=True)`
          - Description: No docstring available
        - Function: `erase(inpt: torch.Tensor, i: int, j: int, h: int, w: int, v: torch.Tensor, inplace: bool = False) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.RandomErase` for details.
        - Function: `erase_image(image: torch.Tensor, i: int, j: int, h: int, w: int, v: torch.Tensor, inplace: bool = False) -> torch.Tensor`
          - Description: No docstring available
        - Function: `erase_video(video: torch.Tensor, i: int, j: int, h: int, w: int, v: torch.Tensor, inplace: bool = False) -> torch.Tensor`
          - Description: No docstring available
        - Function: `pil_to_tensor(pic: Any) -> torch.Tensor`
          - Description: Convert a ``PIL Image`` to a tensor of the same type. This function does not support torchscript. See :class:`~torchvision.transforms.PILToTensor` for more details. .. note:: A deep copy of the underlying array is performed. Args: pic (PIL Image): Image to be converted to tensor. Returns: Tensor: Converted image.
        - Function: `to_pil_image(pic, mode=None)`
          - Description: Convert a tensor or an ndarray to PIL Image. This function does not support torchscript. See :class:`~torchvision.transforms.ToPILImage` for more details. Args: pic (Tensor or numpy.ndarray): Image to be converted to PIL Image. mode (`PIL.Image mode`_): color space and pixel depth of input data (optional). .. _PIL.Image mode: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#concept-modes Returns: PIL Image: Image converted to PIL Image.
      - _color.py
        - Function: `_adjust_brightness_image_pil(image: PIL.Image.Image, brightness_factor: float) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_adjust_contrast_image_pil(img: PIL.Image.Image, contrast_factor: float) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_adjust_gamma_image_pil(img: PIL.Image.Image, gamma: float, gain: float = 1.0) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_adjust_hue_image_pil(img: PIL.Image.Image, hue_factor: float) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_adjust_saturation_image_pil(img: PIL.Image.Image, saturation_factor: float) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_adjust_sharpness_image_pil(img: PIL.Image.Image, sharpness_factor: float) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_autocontrast_image_pil(img: PIL.Image.Image) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_blend(image1: torch.Tensor, image2: torch.Tensor, ratio: float) -> torch.Tensor`
          - Description: No docstring available
        - Function: `_equalize_image_pil(img: PIL.Image.Image) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_get_kernel(functional, input_type, *, allow_passthrough=False)`
          - Description: No docstring available
        - Function: `_hsv_to_rgb(img: torch.Tensor) -> torch.Tensor`
          - Description: No docstring available
        - Function: `_invert_image_pil(img: PIL.Image.Image) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_log_api_usage_once(obj: Any) -> None`
          - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
        - Function: `_max_value(dtype: torch.dtype) -> int`
          - Description: No docstring available
        - Function: `_num_value_bits(dtype: torch.dtype) -> int`
          - Description: No docstring available
        - Function: `_permute_channels_image_pil(image: PIL.Image.Image, permutation: List[int]) -> <module 'PIL.Image' from 'C:\\Users\\heman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py'>`
          - Description: No docstring available
        - Function: `_posterize_image_pil(img: PIL.Image.Image, bits: int) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_register_kernel_internal(functional, input_type, *, tv_tensor_wrapper=True)`
          - Description: No docstring available
        - Function: `_rgb_to_grayscale_image(image: torch.Tensor, num_output_channels: int = 1, preserve_dtype: bool = True) -> torch.Tensor`
          - Description: No docstring available
        - Function: `_rgb_to_grayscale_image_pil(image: PIL.Image.Image, num_output_channels: int = 1) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_rgb_to_hsv(image: torch.Tensor) -> torch.Tensor`
          - Description: No docstring available
        - Function: `_solarize_image_pil(img: PIL.Image.Image, threshold: int) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `adjust_brightness(inpt: torch.Tensor, brightness_factor: float) -> torch.Tensor`
          - Description: Adjust brightness.
        - Function: `adjust_brightness_image(image: torch.Tensor, brightness_factor: float) -> torch.Tensor`
          - Description: No docstring available
        - Function: `adjust_brightness_video(video: torch.Tensor, brightness_factor: float) -> torch.Tensor`
          - Description: No docstring available
        - Function: `adjust_contrast(inpt: torch.Tensor, contrast_factor: float) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.RandomAutocontrast`
        - Function: `adjust_contrast_image(image: torch.Tensor, contrast_factor: float) -> torch.Tensor`
          - Description: No docstring available
        - Function: `adjust_contrast_video(video: torch.Tensor, contrast_factor: float) -> torch.Tensor`
          - Description: No docstring available
        - Function: `adjust_gamma(inpt: torch.Tensor, gamma: float, gain: float = 1) -> torch.Tensor`
          - Description: Adjust gamma.
        - Function: `adjust_gamma_image(image: torch.Tensor, gamma: float, gain: float = 1.0) -> torch.Tensor`
          - Description: No docstring available
        - Function: `adjust_gamma_video(video: torch.Tensor, gamma: float, gain: float = 1) -> torch.Tensor`
          - Description: No docstring available
        - Function: `adjust_hue(inpt: torch.Tensor, hue_factor: float) -> torch.Tensor`
          - Description: Adjust hue
        - Function: `adjust_hue_image(image: torch.Tensor, hue_factor: float) -> torch.Tensor`
          - Description: No docstring available
        - Function: `adjust_hue_video(video: torch.Tensor, hue_factor: float) -> torch.Tensor`
          - Description: No docstring available
        - Function: `adjust_saturation(inpt: torch.Tensor, saturation_factor: float) -> torch.Tensor`
          - Description: Adjust saturation.
        - Function: `adjust_saturation_image(image: torch.Tensor, saturation_factor: float) -> torch.Tensor`
          - Description: No docstring available
        - Function: `adjust_saturation_video(video: torch.Tensor, saturation_factor: float) -> torch.Tensor`
          - Description: No docstring available
        - Function: `adjust_sharpness(inpt: torch.Tensor, sharpness_factor: float) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.RandomAdjustSharpness`
        - Function: `adjust_sharpness_image(image: torch.Tensor, sharpness_factor: float) -> torch.Tensor`
          - Description: No docstring available
        - Function: `adjust_sharpness_video(video: torch.Tensor, sharpness_factor: float) -> torch.Tensor`
          - Description: No docstring available
        - Function: `autocontrast(inpt: torch.Tensor) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.RandomAutocontrast` for details.
        - Function: `autocontrast_image(image: torch.Tensor) -> torch.Tensor`
          - Description: No docstring available
        - Function: `autocontrast_video(video: torch.Tensor) -> torch.Tensor`
          - Description: No docstring available
        - Function: `equalize(inpt: torch.Tensor) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.RandomEqualize` for details.
        - Function: `equalize_image(image: torch.Tensor) -> torch.Tensor`
          - Description: No docstring available
        - Function: `equalize_video(video: torch.Tensor) -> torch.Tensor`
          - Description: No docstring available
        - Function: `invert(inpt: torch.Tensor) -> torch.Tensor`
          - Description: [BETA] See :func:`~torchvision.transforms.v2.RandomInvert`.
        - Function: `invert_image(image: torch.Tensor) -> torch.Tensor`
          - Description: No docstring available
        - Function: `invert_video(video: torch.Tensor) -> torch.Tensor`
          - Description: No docstring available
        - Function: `permute_channels(inpt: torch.Tensor, permutation: List[int]) -> torch.Tensor`
          - Description: Permute the channels of the input according to the given permutation. This function supports plain :class:`~torch.Tensor`'s, :class:`PIL.Image.Image`'s, and :class:`torchvision.tv_tensors.Image` and :class:`torchvision.tv_tensors.Video`. Example: >>> rgb_image = torch.rand(3, 256, 256) >>> bgr_image = F.permutate_channels(rgb_image, permutation=[2, 1, 0]) Args: permutation (List[int]): Valid permutation of the input channel indices. The index of the element determines the channel index in the input and the value determines the channel index in the output. For example, ``permutation=[2, 0 , 1]`` - takes ``npt[..., 0, :, :]`` and puts it at ``output[..., 2, :, :]``, - takes ``npt[..., 1, :, :]`` and puts it at ``output[..., 0, :, :]``, and - takes ``npt[..., 2, :, :]`` and puts it at ``output[..., 1, :, :]``. Raises: ValueError: If ``len(permutation)`` doesn't match the number of channels in the input.
        - Function: `permute_channels_image(image: torch.Tensor, permutation: List[int]) -> torch.Tensor`
          - Description: No docstring available
        - Function: `permute_channels_video(video: torch.Tensor, permutation: List[int]) -> torch.Tensor`
          - Description: No docstring available
        - Function: `pil_to_tensor(pic: Any) -> torch.Tensor`
          - Description: Convert a ``PIL Image`` to a tensor of the same type. This function does not support torchscript. See :class:`~torchvision.transforms.PILToTensor` for more details. .. note:: A deep copy of the underlying array is performed. Args: pic (PIL Image): Image to be converted to tensor. Returns: Tensor: Converted image.
        - Function: `posterize(inpt: torch.Tensor, bits: int) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.RandomPosterize` for details.
        - Function: `posterize_image(image: torch.Tensor, bits: int) -> torch.Tensor`
          - Description: No docstring available
        - Function: `posterize_video(video: torch.Tensor, bits: int) -> torch.Tensor`
          - Description: No docstring available
        - Function: `rgb_to_grayscale(inpt: torch.Tensor, num_output_channels: int = 1) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.Grayscale` for details.
        - Function: `rgb_to_grayscale_image(image: torch.Tensor, num_output_channels: int = 1) -> torch.Tensor`
          - Description: No docstring available
        - Function: `solarize(inpt: torch.Tensor, threshold: float) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.RandomSolarize` for details.
        - Function: `solarize_image(image: torch.Tensor, threshold: float) -> torch.Tensor`
          - Description: No docstring available
        - Function: `solarize_video(video: torch.Tensor, threshold: float) -> torch.Tensor`
          - Description: No docstring available
        - Function: `to_dtype_image(image: torch.Tensor, dtype: torch.dtype = torch.float32, scale: bool = False) -> torch.Tensor`
          - Description: No docstring available
        - Function: `to_grayscale(inpt: torch.Tensor, num_output_channels: int = 1) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.Grayscale` for details.
        - Function: `to_pil_image(pic, mode=None)`
          - Description: Convert a tensor or an ndarray to PIL Image. This function does not support torchscript. See :class:`~torchvision.transforms.ToPILImage` for more details. Args: pic (Tensor or numpy.ndarray): Image to be converted to PIL Image. mode (`PIL.Image mode`_): color space and pixel depth of input data (optional). .. _PIL.Image mode: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#concept-modes Returns: PIL Image: Image converted to PIL Image.
      - _deprecated.py
        - Class: `Any(*args, **kwargs)`
          - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
        - Function: `get_image_size(inpt: torch.Tensor) -> List[int]`
          - Description: No docstring available
        - Function: `to_tensor(inpt: Any) -> torch.Tensor`
          - Description: [BETA] [DEPREACTED] Use to_image() and to_dtype() instead.
      - _geometry.py
        - Class: `Any(*args, **kwargs)`
          - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
        - Class: `InterpolationMode(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
          - Description: Interpolation modes Available interpolation methods are ``nearest``, ``nearest-exact``, ``bilinear``, ``bicubic``, ``box``, ``hamming``, and ``lanczos``.
        - Function: `__compute_resized_output_size(image_size: Tuple[int, int], size: List[int], max_size: Optional[int] = None) -> List[int]`
          - Description: No docstring available
        - Function: `__resize_image_pil_dispatch(image: PIL.Image.Image, size: Union[Sequence[int], int], interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, max_size: Optional[int] = None, antialias: Union[str, bool, NoneType] = 'warn') -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_affine_bounding_boxes_dispatch(inpt: torchvision.tv_tensors._bounding_boxes.BoundingBoxes, angle: Union[int, float], translate: List[float], scale: float, shear: List[float], center: Optional[List[float]] = None, **kwargs) -> torchvision.tv_tensors._bounding_boxes.BoundingBoxes`
          - Description: No docstring available
        - Function: `_affine_bounding_boxes_with_expand(bounding_boxes: torch.Tensor, format: torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat, canvas_size: Tuple[int, int], angle: Union[int, float], translate: List[float], scale: float, shear: List[float], center: Optional[List[float]] = None, expand: bool = False) -> Tuple[torch.Tensor, Tuple[int, int]]`
          - Description: No docstring available
        - Function: `_affine_grid(theta: torch.Tensor, w: int, h: int, ow: int, oh: int) -> torch.Tensor`
          - Description: No docstring available
        - Function: `_affine_image_pil(image: PIL.Image.Image, angle: Union[int, float], translate: List[float], scale: float, shear: List[float], interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.NEAREST: 'nearest'>, fill: Optional[List[float]] = None, center: Optional[List[float]] = None) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_affine_mask_dispatch(inpt: torchvision.tv_tensors._mask.Mask, angle: Union[int, float], translate: List[float], scale: float, shear: List[float], fill: Optional[List[float]] = None, center: Optional[List[float]] = None, **kwargs) -> torchvision.tv_tensors._mask.Mask`
          - Description: No docstring available
        - Function: `_affine_parse_args(angle: Union[int, float], translate: List[float], scale: float, shear: List[float], interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.NEAREST: 'nearest'>, center: Optional[List[float]] = None) -> Tuple[float, List[float], List[float], Optional[List[float]]]`
          - Description: No docstring available
        - Function: `_apply_grid_transform(img: torch.Tensor, grid: torch.Tensor, mode: str, fill: Optional[List[float]]) -> torch.Tensor`
          - Description: No docstring available
        - Function: `_assert_grid_transform_inputs(image: torch.Tensor, matrix: Optional[List[float]], interpolation: str, fill: Optional[List[float]], supported_interpolation_modes: List[str], coeffs: Optional[List[float]] = None) -> None`
          - Description: No docstring available
        - Function: `_center_crop_bounding_boxes_dispatch(inpt: torchvision.tv_tensors._bounding_boxes.BoundingBoxes, output_size: List[int]) -> torchvision.tv_tensors._bounding_boxes.BoundingBoxes`
          - Description: No docstring available
        - Function: `_center_crop_compute_crop_anchor(crop_height: int, crop_width: int, image_height: int, image_width: int) -> Tuple[int, int]`
          - Description: No docstring available
        - Function: `_center_crop_compute_padding(crop_height: int, crop_width: int, image_height: int, image_width: int) -> List[int]`
          - Description: No docstring available
        - Function: `_center_crop_image_pil(image: PIL.Image.Image, output_size: List[int]) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_center_crop_parse_output_size(output_size: List[int]) -> List[int]`
          - Description: No docstring available
        - Function: `_check_antialias(img: torch.Tensor, antialias: Union[str, bool, NoneType], interpolation: torchvision.transforms.functional.InterpolationMode) -> Optional[bool]`
          - Description: No docstring available
        - Function: `_check_interpolation(interpolation: Union[torchvision.transforms.functional.InterpolationMode, int]) -> torchvision.transforms.functional.InterpolationMode`
          - Description: No docstring available
        - Function: `_compute_affine_output_size(matrix: List[float], w: int, h: int) -> Tuple[int, int]`
          - Description: No docstring available
        - Function: `_compute_resized_output_size(canvas_size: Tuple[int, int], size: List[int], max_size: Optional[int] = None) -> List[int]`
          - Description: No docstring available
        - Function: `_create_identity_grid(size: Tuple[int, int], device: torch.device, dtype: torch.dtype) -> torch.Tensor`
          - Description: No docstring available
        - Function: `_crop_bounding_boxes_dispatch(inpt: torchvision.tv_tensors._bounding_boxes.BoundingBoxes, top: int, left: int, height: int, width: int) -> torchvision.tv_tensors._bounding_boxes.BoundingBoxes`
          - Description: No docstring available
        - Function: `_crop_image_pil(img: PIL.Image.Image, top: int, left: int, height: int, width: int) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_elastic_bounding_boxes_dispatch(inpt: torchvision.tv_tensors._bounding_boxes.BoundingBoxes, displacement: torch.Tensor, **kwargs) -> torchvision.tv_tensors._bounding_boxes.BoundingBoxes`
          - Description: No docstring available
        - Function: `_elastic_image_pil(image: PIL.Image.Image, displacement: torch.Tensor, interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, fill: Optional[List[float]] = None) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_elastic_mask_dispatch(inpt: torchvision.tv_tensors._mask.Mask, displacement: torch.Tensor, fill: Optional[List[float]] = None, **kwargs) -> torchvision.tv_tensors._mask.Mask`
          - Description: No docstring available
        - Function: `_five_crop_image_pil(image: PIL.Image.Image, size: List[int]) -> Tuple[PIL.Image.Image, PIL.Image.Image, PIL.Image.Image, PIL.Image.Image, PIL.Image.Image]`
          - Description: No docstring available
        - Function: `_get_inverse_affine_matrix(center: List[float], angle: float, translate: List[float], scale: float, shear: List[float], inverted: bool = True) -> List[float]`
          - Description: No docstring available
        - Function: `_get_kernel(functional, input_type, *, allow_passthrough=False)`
          - Description: No docstring available
        - Function: `_get_perspective_coeffs(startpoints: List[List[int]], endpoints: List[List[int]]) -> List[float]`
          - Description: Helper function to get the coefficients (a, b, c, d, e, f, g, h) for the perspective transforms. In Perspective Transform each pixel (x, y) in the original image gets transformed as, (x, y) -> ( (ax + by + c) / (gx + hy + 1), (dx + ey + f) / (gx + hy + 1) ) Args: startpoints (list of list of ints): List containing four lists of two integers corresponding to four corners ``[top-left, top-right, bottom-right, bottom-left]`` of the original image. endpoints (list of list of ints): List containing four lists of two integers corresponding to four corners ``[top-left, top-right, bottom-right, bottom-left]`` of the transformed image. Returns: octuple (a, b, c, d, e, f, g, h) for transforming each pixel.
        - Function: `_get_size_image_pil(image: PIL.Image.Image) -> List[int]`
          - Description: No docstring available
        - Function: `_horizontal_flip_bounding_boxes_dispatch(inpt: torchvision.tv_tensors._bounding_boxes.BoundingBoxes) -> torchvision.tv_tensors._bounding_boxes.BoundingBoxes`
          - Description: No docstring available
        - Function: `_horizontal_flip_image_pil(image: PIL.Image.Image) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_interpolation_modes_from_int(i: int) -> torchvision.transforms.functional.InterpolationMode`
          - Description: No docstring available
        - Function: `_log_api_usage_once(obj: Any) -> None`
          - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
        - Function: `_pad_bounding_boxes_dispatch(inpt: torchvision.tv_tensors._bounding_boxes.BoundingBoxes, padding: List[int], padding_mode: str = 'constant', **kwargs) -> torchvision.tv_tensors._bounding_boxes.BoundingBoxes`
          - Description: No docstring available
        - Function: `_pad_image_pil(img: PIL.Image.Image, padding: Union[int, List[int], Tuple[int, ...]], fill: Union[float, List[float], Tuple[float, ...], NoneType] = 0, padding_mode: Literal['constant', 'edge', 'reflect', 'symmetric'] = 'constant') -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_pad_symmetric(img: torch.Tensor, padding: List[int]) -> torch.Tensor`
          - Description: No docstring available
        - Function: `_pad_with_scalar_fill(image: torch.Tensor, torch_padding: List[int], fill: Union[int, float], padding_mode: str) -> torch.Tensor`
          - Description: No docstring available
        - Function: `_pad_with_vector_fill(image: torch.Tensor, torch_padding: List[int], fill: List[float], padding_mode: str) -> torch.Tensor`
          - Description: No docstring available
        - Function: `_parse_five_crop_size(size: List[int]) -> List[int]`
          - Description: No docstring available
        - Function: `_parse_pad_padding(padding: Union[int, List[int]]) -> List[int]`
          - Description: No docstring available
        - Function: `_perspective_bounding_boxes_dispatch(inpt: torchvision.tv_tensors._bounding_boxes.BoundingBoxes, startpoints: Optional[List[List[int]]], endpoints: Optional[List[List[int]]], coefficients: Optional[List[float]] = None, **kwargs) -> torchvision.tv_tensors._bounding_boxes.BoundingBoxes`
          - Description: No docstring available
        - Function: `_perspective_coefficients(startpoints: Optional[List[List[int]]], endpoints: Optional[List[List[int]]], coefficients: Optional[List[float]]) -> List[float]`
          - Description: No docstring available
        - Function: `_perspective_grid(coeffs: List[float], ow: int, oh: int, dtype: torch.dtype, device: torch.device) -> torch.Tensor`
          - Description: No docstring available
        - Function: `_perspective_image_pil(image: PIL.Image.Image, startpoints: Optional[List[List[int]]], endpoints: Optional[List[List[int]]], interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BICUBIC: 'bicubic'>, fill: Optional[List[float]] = None, coefficients: Optional[List[float]] = None) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_perspective_mask_dispatch(inpt: torchvision.tv_tensors._mask.Mask, startpoints: Optional[List[List[int]]], endpoints: Optional[List[List[int]]], fill: Optional[List[float]] = None, coefficients: Optional[List[float]] = None, **kwargs) -> torchvision.tv_tensors._mask.Mask`
          - Description: No docstring available
        - Function: `_register_five_ten_crop_kernel_internal(functional, input_type)`
          - Description: No docstring available
        - Function: `_register_kernel_internal(functional, input_type, *, tv_tensor_wrapper=True)`
          - Description: No docstring available
        - Function: `_resize_bounding_boxes_dispatch(inpt: torchvision.tv_tensors._bounding_boxes.BoundingBoxes, size: List[int], max_size: Optional[int] = None, **kwargs: Any) -> torchvision.tv_tensors._bounding_boxes.BoundingBoxes`
          - Description: No docstring available
        - Function: `_resize_image_pil(image: PIL.Image.Image, size: Union[Sequence[int], int], interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, max_size: Optional[int] = None) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_resize_mask_dispatch(inpt: torchvision.tv_tensors._mask.Mask, size: List[int], max_size: Optional[int] = None, **kwargs: Any) -> torchvision.tv_tensors._mask.Mask`
          - Description: No docstring available
        - Function: `_resized_crop_bounding_boxes_dispatch(inpt: torchvision.tv_tensors._bounding_boxes.BoundingBoxes, top: int, left: int, height: int, width: int, size: List[int], **kwargs) -> torchvision.tv_tensors._bounding_boxes.BoundingBoxes`
          - Description: No docstring available
        - Function: `_resized_crop_image_pil(image: PIL.Image.Image, top: int, left: int, height: int, width: int, size: List[int], interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_resized_crop_image_pil_dispatch(image: PIL.Image.Image, top: int, left: int, height: int, width: int, size: List[int], interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_resized_crop_mask_dispatch(inpt: torchvision.tv_tensors._mask.Mask, top: int, left: int, height: int, width: int, size: List[int], **kwargs) -> torchvision.tv_tensors._mask.Mask`
          - Description: No docstring available
        - Function: `_rotate_bounding_boxes_dispatch(inpt: torchvision.tv_tensors._bounding_boxes.BoundingBoxes, angle: float, expand: bool = False, center: Optional[List[float]] = None, **kwargs) -> torchvision.tv_tensors._bounding_boxes.BoundingBoxes`
          - Description: No docstring available
        - Function: `_rotate_image_pil(image: PIL.Image.Image, angle: float, interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.NEAREST: 'nearest'>, expand: bool = False, center: Optional[List[float]] = None, fill: Optional[List[float]] = None) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_rotate_mask_dispatch(inpt: torchvision.tv_tensors._mask.Mask, angle: float, expand: bool = False, center: Optional[List[float]] = None, fill: Optional[List[float]] = None, **kwargs) -> torchvision.tv_tensors._mask.Mask`
          - Description: No docstring available
        - Function: `_ten_crop_image_pil(image: PIL.Image.Image, size: List[int], vertical_flip: bool = False) -> Tuple[PIL.Image.Image, PIL.Image.Image, PIL.Image.Image, PIL.Image.Image, PIL.Image.Image, PIL.Image.Image, PIL.Image.Image, PIL.Image.Image, PIL.Image.Image, PIL.Image.Image]`
          - Description: No docstring available
        - Function: `_vertical_flip_bounding_boxes_dispatch(inpt: torchvision.tv_tensors._bounding_boxes.BoundingBoxes) -> torchvision.tv_tensors._bounding_boxes.BoundingBoxes`
          - Description: No docstring available
        - Function: `_vertical_flip_image_pil(image: <module 'PIL.Image' from 'C:\\Users\\heman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py'>) -> <module 'PIL.Image' from 'C:\\Users\\heman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py'>`
          - Description: No docstring available
        - Function: `affine(inpt: torch.Tensor, angle: Union[int, float], translate: List[float], scale: float, shear: List[float], interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.NEAREST: 'nearest'>, fill: Optional[List[float]] = None, center: Optional[List[float]] = None) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.RandomAffine` for details.
        - Function: `affine_bounding_boxes(bounding_boxes: torch.Tensor, format: torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat, canvas_size: Tuple[int, int], angle: Union[int, float], translate: List[float], scale: float, shear: List[float], center: Optional[List[float]] = None) -> torch.Tensor`
          - Description: No docstring available
        - Function: `affine_image(image: torch.Tensor, angle: Union[int, float], translate: List[float], scale: float, shear: List[float], interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.NEAREST: 'nearest'>, fill: Optional[List[float]] = None, center: Optional[List[float]] = None) -> torch.Tensor`
          - Description: No docstring available
        - Function: `affine_mask(mask: torch.Tensor, angle: Union[int, float], translate: List[float], scale: float, shear: List[float], fill: Optional[List[float]] = None, center: Optional[List[float]] = None) -> torch.Tensor`
          - Description: No docstring available
        - Function: `affine_video(video: torch.Tensor, angle: Union[int, float], translate: List[float], scale: float, shear: List[float], interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.NEAREST: 'nearest'>, fill: Optional[List[float]] = None, center: Optional[List[float]] = None) -> torch.Tensor`
          - Description: No docstring available
        - Function: `center_crop(inpt: torch.Tensor, output_size: List[int]) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.RandomCrop` for details.
        - Function: `center_crop_bounding_boxes(bounding_boxes: torch.Tensor, format: torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat, canvas_size: Tuple[int, int], output_size: List[int]) -> Tuple[torch.Tensor, Tuple[int, int]]`
          - Description: No docstring available
        - Function: `center_crop_image(image: torch.Tensor, output_size: List[int]) -> torch.Tensor`
          - Description: No docstring available
        - Function: `center_crop_mask(mask: torch.Tensor, output_size: List[int]) -> torch.Tensor`
          - Description: No docstring available
        - Function: `center_crop_video(video: torch.Tensor, output_size: List[int]) -> torch.Tensor`
          - Description: No docstring available
        - Function: `clamp_bounding_boxes(inpt: torch.Tensor, format: Optional[torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat] = None, canvas_size: Optional[Tuple[int, int]] = None) -> torch.Tensor`
          - Description: [BETA] See :func:`~torchvision.transforms.v2.ClampBoundingBoxes` for details.
        - Function: `convert_bounding_box_format(inpt: torch.Tensor, old_format: Optional[torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat] = None, new_format: Optional[torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat] = None, inplace: bool = False) -> torch.Tensor`
          - Description: [BETA] See :func:`~torchvision.transforms.v2.ConvertBoundingBoxFormat` for details.
        - Function: `crop(inpt: torch.Tensor, top: int, left: int, height: int, width: int) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.RandomCrop` for details.
        - Function: `crop_bounding_boxes(bounding_boxes: torch.Tensor, format: torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat, top: int, left: int, height: int, width: int) -> Tuple[torch.Tensor, Tuple[int, int]]`
          - Description: No docstring available
        - Function: `crop_image(image: torch.Tensor, top: int, left: int, height: int, width: int) -> torch.Tensor`
          - Description: No docstring available
        - Function: `crop_mask(mask: torch.Tensor, top: int, left: int, height: int, width: int) -> torch.Tensor`
          - Description: No docstring available
        - Function: `crop_video(video: torch.Tensor, top: int, left: int, height: int, width: int) -> torch.Tensor`
          - Description: No docstring available
        - Function: `elastic(inpt: torch.Tensor, displacement: torch.Tensor, interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, fill: Optional[List[float]] = None) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.ElasticTransform` for details.
        - Function: `elastic_bounding_boxes(bounding_boxes: torch.Tensor, format: torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat, canvas_size: Tuple[int, int], displacement: torch.Tensor) -> torch.Tensor`
          - Description: No docstring available
        - Function: `elastic_image(image: torch.Tensor, displacement: torch.Tensor, interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, fill: Optional[List[float]] = None) -> torch.Tensor`
          - Description: No docstring available
        - Function: `elastic_mask(mask: torch.Tensor, displacement: torch.Tensor, fill: Optional[List[float]] = None) -> torch.Tensor`
          - Description: No docstring available
        - Function: `elastic_transform(inpt: torch.Tensor, displacement: torch.Tensor, interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, fill: Optional[List[float]] = None) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.ElasticTransform` for details.
        - Function: `elastic_video(video: torch.Tensor, displacement: torch.Tensor, interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, fill: Optional[List[float]] = None) -> torch.Tensor`
          - Description: No docstring available
        - Function: `five_crop(inpt: torch.Tensor, size: List[int]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.FiveCrop` for details.
        - Function: `five_crop_image(image: torch.Tensor, size: List[int]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]`
          - Description: No docstring available
        - Function: `five_crop_video(video: torch.Tensor, size: List[int]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]`
          - Description: No docstring available
        - Function: `grid_sample(input: torch.Tensor, grid: torch.Tensor, mode: str = 'bilinear', padding_mode: str = 'zeros', align_corners: Optional[bool] = None) -> torch.Tensor`
          - Description: Given an :attr:`input` and a flow-field :attr:`grid`, computes the ``output`` using :attr:`input` values and pixel locations from :attr:`grid`. Currently, only spatial (4-D) and volumetric (5-D) :attr:`input` are supported. In the spatial (4-D) case, for :attr:`input` with shape :math:`(N, C, H_\text{in}, W_\text{in})` and :attr:`grid` with shape :math:`(N, H_\text{out}, W_\text{out}, 2)`, the output will have shape :math:`(N, C, H_\text{out}, W_\text{out})`. For each output location ``output[n, :, h, w]``, the size-2 vector ``grid[n, h, w]`` specifies :attr:`input` pixel locations ``x`` and ``y``, which are used to interpolate the output value ``output[n, :, h, w]``. In the case of 5D inputs, ``grid[n, d, h, w]`` specifies the ``x``, ``y``, ``z`` pixel locations for interpolating ``output[n, :, d, h, w]``. :attr:`mode` argument specifies ``nearest`` or ``bilinear`` interpolation method to sample the input pixels. :attr:`grid` specifies the sampling pixel locations normalized by the :attr:`input` spatial dimensions. Therefore, it should have most values in the range of ``[-1, 1]``. For example, values ``x = -1, y = -1`` is the left-top pixel of :attr:`input`, and values ``x = 1, y = 1`` is the right-bottom pixel of :attr:`input`. If :attr:`grid` has values outside the range of ``[-1, 1]``, the corresponding outputs are handled as defined by :attr:`padding_mode`. Options are * ``padding_mode="zeros"``: use ``0`` for out-of-bound grid locations, * ``padding_mode="border"``: use border values for out-of-bound grid locations, * ``padding_mode="reflection"``: use values at locations reflected by the border for out-of-bound grid locations. For location far away from the border, it will keep being reflected until becoming in bound, e.g., (normalized) pixel location ``x = -3.5`` reflects by border ``-1`` and becomes ``x' = 1.5``, then reflects by border ``1`` and becomes ``x'' = -0.5``. Note: This function is often used in conjunction with :func:`affine_grid` to build `Spatial Transformer Networks`_ . Note: When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on :doc:`/notes/randomness` for background. Note: NaN values in :attr:`grid` would be interpreted as ``-1``. Args: input (Tensor): input of shape :math:`(N, C, H_\text{in}, W_\text{in})` (4-D case) or :math:`(N, C, D_\text{in}, H_\text{in}, W_\text{in})` (5-D case) grid (Tensor): flow-field of shape :math:`(N, H_\text{out}, W_\text{out}, 2)` (4-D case) or :math:`(N, D_\text{out}, H_\text{out}, W_\text{out}, 3)` (5-D case) mode (str): interpolation mode to calculate output values ``'bilinear'`` | ``'nearest'`` | ``'bicubic'``. Default: ``'bilinear'`` Note: ``mode='bicubic'`` supports only 4-D input. When ``mode='bilinear'`` and the input is 5-D, the interpolation mode used internally will actually be trilinear. However, when the input is 4-D, the interpolation mode will legitimately be bilinear. padding_mode (str): padding mode for outside grid values ``'zeros'`` | ``'border'`` | ``'reflection'``. Default: ``'zeros'`` align_corners (bool, optional): Geometrically, we consider the pixels of the input as squares rather than points. If set to ``True``, the extrema (``-1`` and ``1``) are considered as referring to the center points of the input's corner pixels. If set to ``False``, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic. This option parallels the ``align_corners`` option in :func:`interpolate`, and so whichever option is used here should also be used there to resize the input image before grid sampling. Default: ``False`` Returns: output (Tensor): output Tensor .. _`Spatial Transformer Networks`: https://arxiv.org/abs/1506.02025 .. warning:: When ``align_corners = True``, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by :func:`grid_sample` will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was ``align_corners = True``. Since then, the default behavior has been changed to ``align_corners = False``, in order to bring it in line with the default for :func:`interpolate`. .. note:: ``mode='bicubic'`` is implemented using the `cubic convolution algorithm`_ with :math:`\alpha=-0.75`. The constant :math:`\alpha` might be different from packages to packages. For example, `PIL`_ and `OpenCV`_ use -0.5 and -0.75 respectively. This algorithm may "overshoot" the range of values it's interpolating. For example, it may produce negative values or values greater than 255 when interpolating input in [0, 255]. Clamp the results with :func:`torch.clamp` to ensure they are within the valid range. .. _`cubic convolution algorithm`: https://en.wikipedia.org/wiki/Bicubic_interpolation .. _`PIL`: https://github.com/python-pillow/Pillow/blob/4634eafe3c695a014267eefdce830b4a825beed7/src/libImaging/Resample.c#L51 .. _`OpenCV`: https://github.com/opencv/opencv/blob/f345ed564a06178670750bad59526cfa4033be55/modules/imgproc/src/resize.cpp#L908
        - Function: `hflip(inpt: torch.Tensor) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.RandomHorizontalFlip` for details.
        - Function: `horizontal_flip(inpt: torch.Tensor) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.RandomHorizontalFlip` for details.
        - Function: `horizontal_flip_bounding_boxes(bounding_boxes: torch.Tensor, format: torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat, canvas_size: Tuple[int, int]) -> torch.Tensor`
          - Description: No docstring available
        - Function: `horizontal_flip_image(image: torch.Tensor) -> torch.Tensor`
          - Description: No docstring available
        - Function: `horizontal_flip_mask(mask: torch.Tensor) -> torch.Tensor`
          - Description: No docstring available
        - Function: `horizontal_flip_video(video: torch.Tensor) -> torch.Tensor`
          - Description: No docstring available
        - Function: `interpolate(input: torch.Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> torch.Tensor`
          - Description: Down/up samples the input to either the given :attr:`size` or the given :attr:`scale_factor` The algorithm used for interpolation is determined by :attr:`mode`. Currently temporal, spatial and volumetric sampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape. The input dimensions are interpreted in the form: `mini-batch x channels x [optional depth] x [optional height] x width`. The modes available for resizing are: `nearest`, `linear` (3D-only), `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only), `area`, `nearest-exact` Args: input (Tensor): the input tensor size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]): output spatial size. scale_factor (float or Tuple[float]): multiplier for spatial size. If `scale_factor` is a tuple, its length has to match the number of spatial dimensions; `input.dim() - 2`. mode (str): algorithm used for upsampling: ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` | ``'trilinear'`` | ``'area'`` | ``'nearest-exact'``. Default: ``'nearest'`` align_corners (bool, optional): Geometrically, we consider the pixels of the input and output as squares rather than points. If set to ``True``, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to ``False``, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation *independent* of input size when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode` is ``'linear'``, ``'bilinear'``, ``'bicubic'`` or ``'trilinear'``. Default: ``False`` recompute_scale_factor (bool, optional): recompute the scale_factor for use in the interpolation calculation. If `recompute_scale_factor` is ``True``, then `scale_factor` must be passed in and `scale_factor` is used to compute the output `size`. The computed output `size` will be used to infer new scales for the interpolation. Note that when `scale_factor` is floating-point, it may differ from the recomputed `scale_factor` due to rounding and precision issues. If `recompute_scale_factor` is ``False``, then `size` or `scale_factor` will be used directly for interpolation. Default: ``None``. antialias (bool, optional): flag to apply anti-aliasing. Default: ``False``. Using anti-alias option together with ``align_corners=False``, interpolation result would match Pillow result for downsampling operation. Supported modes: ``'bilinear'``, ``'bicubic'``. .. note:: With ``mode='bicubic'``, it's possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call ``result.clamp(min=0, max=255)`` if you want to reduce the overshoot when displaying the image. .. note:: Mode ``mode='nearest-exact'`` matches Scikit-Image and PIL nearest neighbours interpolation algorithms and fixes known issues with ``mode='nearest'``. This mode is introduced to keep backward compatibility. Mode ``mode='nearest'`` matches buggy OpenCV's ``INTER_NEAREST`` interpolation algorithm. .. note:: The gradients for the dtype ``float16`` on CUDA may be inaccurate in the upsample operation when using modes ``['linear', 'bilinear', 'bicubic', 'trilinear', 'area']``. For more details, please refer to the discussion in `issue#104157 <https://github.com/pytorch/pytorch/issues/104157>`_. Note: This operation may produce nondeterministic gradients when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.
        - Function: `pad(inpt: torch.Tensor, padding: List[int], fill: Union[int, float, List[float], NoneType] = None, padding_mode: str = 'constant') -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.Pad` for details.
        - Function: `pad_bounding_boxes(bounding_boxes: torch.Tensor, format: torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat, canvas_size: Tuple[int, int], padding: List[int], padding_mode: str = 'constant') -> Tuple[torch.Tensor, Tuple[int, int]]`
          - Description: No docstring available
        - Function: `pad_image(image: torch.Tensor, padding: List[int], fill: Union[int, float, List[float], NoneType] = None, padding_mode: str = 'constant') -> torch.Tensor`
          - Description: No docstring available
        - Function: `pad_mask(mask: torch.Tensor, padding: List[int], fill: Union[int, float, List[float], NoneType] = None, padding_mode: str = 'constant') -> torch.Tensor`
          - Description: No docstring available
        - Function: `pad_video(video: torch.Tensor, padding: List[int], fill: Union[int, float, List[float], NoneType] = None, padding_mode: str = 'constant') -> torch.Tensor`
          - Description: No docstring available
        - Function: `perspective(inpt: torch.Tensor, startpoints: Optional[List[List[int]]], endpoints: Optional[List[List[int]]], interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, fill: Optional[List[float]] = None, coefficients: Optional[List[float]] = None) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.RandomPerspective` for details.
        - Function: `perspective_bounding_boxes(bounding_boxes: torch.Tensor, format: torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat, canvas_size: Tuple[int, int], startpoints: Optional[List[List[int]]], endpoints: Optional[List[List[int]]], coefficients: Optional[List[float]] = None) -> torch.Tensor`
          - Description: No docstring available
        - Function: `perspective_image(image: torch.Tensor, startpoints: Optional[List[List[int]]], endpoints: Optional[List[List[int]]], interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, fill: Optional[List[float]] = None, coefficients: Optional[List[float]] = None) -> torch.Tensor`
          - Description: No docstring available
        - Function: `perspective_mask(mask: torch.Tensor, startpoints: Optional[List[List[int]]], endpoints: Optional[List[List[int]]], fill: Optional[List[float]] = None, coefficients: Optional[List[float]] = None) -> torch.Tensor`
          - Description: No docstring available
        - Function: `perspective_video(video: torch.Tensor, startpoints: Optional[List[List[int]]], endpoints: Optional[List[List[int]]], interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, fill: Optional[List[float]] = None, coefficients: Optional[List[float]] = None) -> torch.Tensor`
          - Description: No docstring available
        - Function: `pil_to_tensor(pic: Any) -> torch.Tensor`
          - Description: Convert a ``PIL Image`` to a tensor of the same type. This function does not support torchscript. See :class:`~torchvision.transforms.PILToTensor` for more details. .. note:: A deep copy of the underlying array is performed. Args: pic (PIL Image): Image to be converted to tensor. Returns: Tensor: Converted image.
        - Function: `resize(inpt: torch.Tensor, size: List[int], interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, max_size: Optional[int] = None, antialias: Union[str, bool, NoneType] = 'warn') -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.Resize` for details.
        - Function: `resize_bounding_boxes(bounding_boxes: torch.Tensor, canvas_size: Tuple[int, int], size: List[int], max_size: Optional[int] = None) -> Tuple[torch.Tensor, Tuple[int, int]]`
          - Description: No docstring available
        - Function: `resize_image(image: torch.Tensor, size: List[int], interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, max_size: Optional[int] = None, antialias: Union[str, bool, NoneType] = 'warn') -> torch.Tensor`
          - Description: No docstring available
        - Function: `resize_mask(mask: torch.Tensor, size: List[int], max_size: Optional[int] = None) -> torch.Tensor`
          - Description: No docstring available
        - Function: `resize_video(video: torch.Tensor, size: List[int], interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, max_size: Optional[int] = None, antialias: Union[str, bool, NoneType] = 'warn') -> torch.Tensor`
          - Description: No docstring available
        - Function: `resized_crop(inpt: torch.Tensor, top: int, left: int, height: int, width: int, size: List[int], interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.RandomResizedCrop` for details.
        - Function: `resized_crop_bounding_boxes(bounding_boxes: torch.Tensor, format: torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat, top: int, left: int, height: int, width: int, size: List[int]) -> Tuple[torch.Tensor, Tuple[int, int]]`
          - Description: No docstring available
        - Function: `resized_crop_image(image: torch.Tensor, top: int, left: int, height: int, width: int, size: List[int], interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> torch.Tensor`
          - Description: No docstring available
        - Function: `resized_crop_mask(mask: torch.Tensor, top: int, left: int, height: int, width: int, size: List[int]) -> torch.Tensor`
          - Description: No docstring available
        - Function: `resized_crop_video(video: torch.Tensor, top: int, left: int, height: int, width: int, size: List[int], interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> torch.Tensor`
          - Description: No docstring available
        - Function: `rotate(inpt: torch.Tensor, angle: float, interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.NEAREST: 'nearest'>, expand: bool = False, center: Optional[List[float]] = None, fill: Optional[List[float]] = None) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.RandomRotation` for details.
        - Function: `rotate_bounding_boxes(bounding_boxes: torch.Tensor, format: torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat, canvas_size: Tuple[int, int], angle: float, expand: bool = False, center: Optional[List[float]] = None) -> Tuple[torch.Tensor, Tuple[int, int]]`
          - Description: No docstring available
        - Function: `rotate_image(image: torch.Tensor, angle: float, interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.NEAREST: 'nearest'>, expand: bool = False, center: Optional[List[float]] = None, fill: Optional[List[float]] = None) -> torch.Tensor`
          - Description: No docstring available
        - Function: `rotate_mask(mask: torch.Tensor, angle: float, expand: bool = False, center: Optional[List[float]] = None, fill: Optional[List[float]] = None) -> torch.Tensor`
          - Description: No docstring available
        - Function: `rotate_video(video: torch.Tensor, angle: float, interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.NEAREST: 'nearest'>, expand: bool = False, center: Optional[List[float]] = None, fill: Optional[List[float]] = None) -> torch.Tensor`
          - Description: No docstring available
        - Function: `ten_crop(inpt: torch.Tensor, size: List[int], vertical_flip: bool = False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.TenCrop` for details.
        - Function: `ten_crop_image(image: torch.Tensor, size: List[int], vertical_flip: bool = False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]`
          - Description: No docstring available
        - Function: `ten_crop_video(video: torch.Tensor, size: List[int], vertical_flip: bool = False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]`
          - Description: No docstring available
        - Function: `to_pil_image(pic, mode=None)`
          - Description: Convert a tensor or an ndarray to PIL Image. This function does not support torchscript. See :class:`~torchvision.transforms.ToPILImage` for more details. Args: pic (Tensor or numpy.ndarray): Image to be converted to PIL Image. mode (`PIL.Image mode`_): color space and pixel depth of input data (optional). .. _PIL.Image mode: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#concept-modes Returns: PIL Image: Image converted to PIL Image.
        - Function: `vertical_flip(inpt: torch.Tensor) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.RandomVerticalFlip` for details.
        - Function: `vertical_flip_bounding_boxes(bounding_boxes: torch.Tensor, format: torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat, canvas_size: Tuple[int, int]) -> torch.Tensor`
          - Description: No docstring available
        - Function: `vertical_flip_image(image: torch.Tensor) -> torch.Tensor`
          - Description: No docstring available
        - Function: `vertical_flip_mask(mask: torch.Tensor) -> torch.Tensor`
          - Description: No docstring available
        - Function: `vertical_flip_video(video: torch.Tensor) -> torch.Tensor`
          - Description: No docstring available
        - Function: `vflip(inpt: torch.Tensor) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.RandomVerticalFlip` for details.
      - _meta.py
        - Class: `BoundingBoxFormat(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
          - Description: [BETA] Coordinate format of a bounding box. Available formats are * ``XYXY`` * ``XYWH`` * ``CXCYWH``
        - Function: `_clamp_bounding_boxes(bounding_boxes: torch.Tensor, format: torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat, canvas_size: Tuple[int, int]) -> torch.Tensor`
          - Description: No docstring available
        - Function: `_convert_bounding_box_format(bounding_boxes: torch.Tensor, old_format: torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat, new_format: torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat, inplace: bool = False) -> torch.Tensor`
          - Description: No docstring available
        - Function: `_cxcywh_to_xyxy(cxcywh: torch.Tensor, inplace: bool) -> torch.Tensor`
          - Description: No docstring available
        - Function: `_get_dimensions_image_pil(img: Any) -> List[int]`
          - Description: No docstring available
        - Function: `_get_kernel(functional, input_type, *, allow_passthrough=False)`
          - Description: No docstring available
        - Function: `_get_num_channels_image_pil(img: Any) -> int`
          - Description: No docstring available
        - Function: `_get_size_image_pil(image: PIL.Image.Image) -> List[int]`
          - Description: No docstring available
        - Function: `_log_api_usage_once(obj: Any) -> None`
          - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
        - Function: `_register_kernel_internal(functional, input_type, *, tv_tensor_wrapper=True)`
          - Description: No docstring available
        - Function: `_xywh_to_xyxy(xywh: torch.Tensor, inplace: bool) -> torch.Tensor`
          - Description: No docstring available
        - Function: `_xyxy_to_cxcywh(xyxy: torch.Tensor, inplace: bool) -> torch.Tensor`
          - Description: No docstring available
        - Function: `_xyxy_to_xywh(xyxy: torch.Tensor, inplace: bool) -> torch.Tensor`
          - Description: No docstring available
        - Function: `clamp_bounding_boxes(inpt: torch.Tensor, format: Optional[torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat] = None, canvas_size: Optional[Tuple[int, int]] = None) -> torch.Tensor`
          - Description: [BETA] See :func:`~torchvision.transforms.v2.ClampBoundingBoxes` for details.
        - Function: `convert_bounding_box_format(inpt: torch.Tensor, old_format: Optional[torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat] = None, new_format: Optional[torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat] = None, inplace: bool = False) -> torch.Tensor`
          - Description: [BETA] See :func:`~torchvision.transforms.v2.ConvertBoundingBoxFormat` for details.
        - Function: `get_dimensions(inpt: torch.Tensor) -> List[int]`
          - Description: No docstring available
        - Function: `get_dimensions_image(image: torch.Tensor) -> List[int]`
          - Description: No docstring available
        - Function: `get_dimensions_video(video: torch.Tensor) -> List[int]`
          - Description: No docstring available
        - Function: `get_image_num_channels(inpt: torch.Tensor) -> int`
          - Description: No docstring available
        - Function: `get_num_channels(inpt: torch.Tensor) -> int`
          - Description: No docstring available
        - Function: `get_num_channels_image(image: torch.Tensor) -> int`
          - Description: No docstring available
        - Function: `get_num_channels_video(video: torch.Tensor) -> int`
          - Description: No docstring available
        - Function: `get_num_frames(inpt: torch.Tensor) -> int`
          - Description: No docstring available
        - Function: `get_num_frames_video(video: torch.Tensor) -> int`
          - Description: No docstring available
        - Function: `get_size(inpt: torch.Tensor) -> List[int]`
          - Description: No docstring available
        - Function: `get_size_bounding_boxes(bounding_box: torchvision.tv_tensors._bounding_boxes.BoundingBoxes) -> List[int]`
          - Description: No docstring available
        - Function: `get_size_image(image: torch.Tensor) -> List[int]`
          - Description: No docstring available
        - Function: `get_size_mask(mask: torch.Tensor) -> List[int]`
          - Description: No docstring available
        - Function: `get_size_video(video: torch.Tensor) -> List[int]`
          - Description: No docstring available
        - Function: `is_pure_tensor(inpt: Any) -> bool`
          - Description: No docstring available
      - _misc.py
        - Function: `_gaussian_blur_image_pil(image: PIL.Image.Image, kernel_size: List[int], sigma: Optional[List[float]] = None) -> PIL.Image.Image`
          - Description: No docstring available
        - Function: `_get_gaussian_kernel1d(kernel_size: int, sigma: float, dtype: torch.dtype, device: torch.device) -> torch.Tensor`
          - Description: No docstring available
        - Function: `_get_gaussian_kernel2d(kernel_size: List[int], sigma: List[float], dtype: torch.dtype, device: torch.device) -> torch.Tensor`
          - Description: No docstring available
        - Function: `_get_kernel(functional, input_type, *, allow_passthrough=False)`
          - Description: No docstring available
        - Function: `_log_api_usage_once(obj: Any) -> None`
          - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
        - Function: `_max_value(dtype: torch.dtype) -> int`
          - Description: No docstring available
        - Function: `_num_value_bits(dtype: torch.dtype) -> int`
          - Description: No docstring available
        - Function: `_register_kernel_internal(functional, input_type, *, tv_tensor_wrapper=True)`
          - Description: No docstring available
        - Function: `_to_dtype_tensor_dispatch(inpt: torch.Tensor, dtype: torch.dtype, scale: bool = False) -> torch.Tensor`
          - Description: No docstring available
        - Function: `convert_image_dtype(image: torch.Tensor, dtype: torch.dtype = torch.float32) -> torch.Tensor`
          - Description: [BETA] [DEPRECATED] Use to_dtype() instead.
        - Function: `gaussian_blur(inpt: torch.Tensor, kernel_size: List[int], sigma: Optional[List[float]] = None) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.GaussianBlur` for details.
        - Function: `gaussian_blur_image(image: torch.Tensor, kernel_size: List[int], sigma: Optional[List[float]] = None) -> torch.Tensor`
          - Description: No docstring available
        - Function: `gaussian_blur_video(video: torch.Tensor, kernel_size: List[int], sigma: Optional[List[float]] = None) -> torch.Tensor`
          - Description: No docstring available
        - Function: `normalize(inpt: torch.Tensor, mean: List[float], std: List[float], inplace: bool = False) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.Normalize` for details.
        - Function: `normalize_image(image: torch.Tensor, mean: List[float], std: List[float], inplace: bool = False) -> torch.Tensor`
          - Description: No docstring available
        - Function: `normalize_video(video: torch.Tensor, mean: List[float], std: List[float], inplace: bool = False) -> torch.Tensor`
          - Description: No docstring available
        - Function: `pil_to_tensor(pic: Any) -> torch.Tensor`
          - Description: Convert a ``PIL Image`` to a tensor of the same type. This function does not support torchscript. See :class:`~torchvision.transforms.PILToTensor` for more details. .. note:: A deep copy of the underlying array is performed. Args: pic (PIL Image): Image to be converted to tensor. Returns: Tensor: Converted image.
        - Function: `to_dtype(inpt: torch.Tensor, dtype: torch.dtype = torch.float32, scale: bool = False) -> torch.Tensor`
          - Description: [BETA] See :func:`~torchvision.transforms.v2.ToDtype` for details.
        - Function: `to_dtype_image(image: torch.Tensor, dtype: torch.dtype = torch.float32, scale: bool = False) -> torch.Tensor`
          - Description: No docstring available
        - Function: `to_dtype_video(video: torch.Tensor, dtype: torch.dtype = torch.float32, scale: bool = False) -> torch.Tensor`
          - Description: No docstring available
        - Function: `to_pil_image(pic, mode=None)`
          - Description: Convert a tensor or an ndarray to PIL Image. This function does not support torchscript. See :class:`~torchvision.transforms.ToPILImage` for more details. Args: pic (Tensor or numpy.ndarray): Image to be converted to PIL Image. mode (`PIL.Image mode`_): color space and pixel depth of input data (optional). .. _PIL.Image mode: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#concept-modes Returns: PIL Image: Image converted to PIL Image.
      - _temporal.py
        - Function: `_get_kernel(functional, input_type, *, allow_passthrough=False)`
          - Description: No docstring available
        - Function: `_log_api_usage_once(obj: Any) -> None`
          - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
        - Function: `_register_kernel_internal(functional, input_type, *, tv_tensor_wrapper=True)`
          - Description: No docstring available
        - Function: `uniform_temporal_subsample(inpt: torch.Tensor, num_samples: int) -> torch.Tensor`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.UniformTemporalSubsample` for details.
        - Function: `uniform_temporal_subsample_video(video: torch.Tensor, num_samples: int) -> torch.Tensor`
          - Description: No docstring available
      - _type_conversion.py
        - Function: `pil_to_tensor(pic: Any) -> torch.Tensor`
          - Description: Convert a ``PIL Image`` to a tensor of the same type. This function does not support torchscript. See :class:`~torchvision.transforms.PILToTensor` for more details. .. note:: A deep copy of the underlying array is performed. Args: pic (PIL Image): Image to be converted to tensor. Returns: Tensor: Converted image.
        - Function: `to_image(inpt: Union[torch.Tensor, PIL.Image.Image, numpy.ndarray]) -> torchvision.tv_tensors._image.Image`
          - Description: [BETA] See :class:`~torchvision.transforms.v2.ToImage` for details.
        - Function: `to_pil_image(pic, mode=None)`
          - Description: Convert a tensor or an ndarray to PIL Image. This function does not support torchscript. See :class:`~torchvision.transforms.ToPILImage` for more details. Args: pic (Tensor or numpy.ndarray): Image to be converted to PIL Image. mode (`PIL.Image mode`_): color space and pixel depth of input data (optional). .. _PIL.Image mode: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#concept-modes Returns: PIL Image: Image converted to PIL Image.
      - _utils.py
        - Class: `Any(*args, **kwargs)`
          - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
        - Function: `_get_kernel(functional, input_type, *, allow_passthrough=False)`
          - Description: No docstring available
        - Function: `_kernel_tv_tensor_wrapper(kernel)`
          - Description: No docstring available
        - Function: `_name_to_functional(name)`
          - Description: No docstring available
        - Function: `_register_five_ten_crop_kernel_internal(functional, input_type)`
          - Description: No docstring available
        - Function: `_register_kernel_internal(functional, input_type, *, tv_tensor_wrapper=True)`
          - Description: No docstring available
        - Function: `is_pure_tensor(inpt: Any) -> bool`
          - Description: No docstring available
        - Function: `register_kernel(functional, tv_tensor_cls)`
          - Description: [BETA] Decorate a kernel to register it for a functional and a (custom) tv_tensor type. See :ref:`sphx_glr_auto_examples_transforms_plot_custom_tv_tensors.py` for usage details.
    - __pycache__/
    - _augment.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `CutMix(*, alpha: float = 1.0, num_classes: int, labels_getter='default') -> None`
        - Description: [BETA] Apply CutMix to the provided batch of images and labels. .. v2betastatus:: CutMix transform Paper: `CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features <https://arxiv.org/abs/1905.04899>`_. .. note:: This transform is meant to be used on **batches** of samples, not individual images. See :ref:`sphx_glr_auto_examples_transforms_plot_cutmix_mixup.py` for detailed usage examples. The sample pairing is deterministic and done by matching consecutive samples in the batch, so the batch needs to be shuffled (this is an implementation detail, not a guaranteed convention.) In the input, the labels are expected to be a tensor of shape ``(batch_size,)``. They will be transformed into a tensor of shape ``(batch_size, num_classes)``. Args: alpha (float, optional): hyperparameter of the Beta distribution used for mixup. Default is 1. num_classes (int): number of classes in the batch. Used for one-hot-encoding. labels_getter (callable or "default", optional): indicates how to identify the labels in the input. By default, this will pick the second parameter as the labels if it's a tensor. This covers the most common scenario where this transform is called as ``CutMix()(imgs_batch, labels_batch)``. It can also be a callable that takes the same input as the transform, and returns the labels.
      - Class: `MixUp(*, alpha: float = 1.0, num_classes: int, labels_getter='default') -> None`
        - Description: [BETA] Apply MixUp to the provided batch of images and labels. .. v2betastatus:: MixUp transform Paper: `mixup: Beyond Empirical Risk Minimization <https://arxiv.org/abs/1710.09412>`_. .. note:: This transform is meant to be used on **batches** of samples, not individual images. See :ref:`sphx_glr_auto_examples_transforms_plot_cutmix_mixup.py` for detailed usage examples. The sample pairing is deterministic and done by matching consecutive samples in the batch, so the batch needs to be shuffled (this is an implementation detail, not a guaranteed convention.) In the input, the labels are expected to be a tensor of shape ``(batch_size,)``. They will be transformed into a tensor of shape ``(batch_size, num_classes)``. Args: alpha (float, optional): hyperparameter of the Beta distribution used for mixup. Default is 1. num_classes (int): number of classes in the batch. Used for one-hot-encoding. labels_getter (callable or "default", optional): indicates how to identify the labels in the input. By default, this will pick the second parameter as the labels if it's a tensor. This covers the most common scenario where this transform is called as ``MixUp()(imgs_batch, labels_batch)``. It can also be a callable that takes the same input as the transform, and returns the labels.
      - Class: `RandomErasing(p: float = 0.5, scale: Tuple[float, float] = (0.02, 0.33), ratio: Tuple[float, float] = (0.3, 3.3), value: float = 0.0, inplace: bool = False)`
        - Description: [BETA] Randomly select a rectangle region in the input image or video and erase its pixels. .. v2betastatus:: RandomErasing transform This transform does not support PIL Image. 'Random Erasing Data Augmentation' by Zhong et al. See https://arxiv.org/abs/1708.04896 Args: p (float, optional): probability that the random erasing operation will be performed. scale (tuple of float, optional): range of proportion of erased area against input image. ratio (tuple of float, optional): range of aspect ratio of erased area. value (number or tuple of numbers): erasing value. Default is 0. If a single int, it is used to erase all pixels. If a tuple of length 3, it is used to erase R, G, B channels respectively. If a str of 'random', erasing each pixel with random values. inplace (bool, optional): boolean to make this transform inplace. Default set to False. Returns: Erased input. Example: >>> from torchvision.transforms import v2 as transforms >>> >>> transform = transforms.Compose([ >>> transforms.RandomHorizontalFlip(), >>> transforms.PILToTensor(), >>> transforms.ConvertImageDtype(torch.float), >>> transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), >>> transforms.RandomErasing(), >>> ])
      - Class: `Transform() -> 'None'`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `_BaseMixUpCutMix(*, alpha: float = 1.0, num_classes: int, labels_getter='default') -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `_RandomApplyTransform(p: 'float' = 0.5) -> 'None'`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Function: `_parse_labels_getter(labels_getter: 'Union[str, Callable[[Any], Optional[torch.Tensor]], None]') -> 'Callable[[Any], Optional[torch.Tensor]]'`
        - Description: No docstring available
      - Function: `has_any(flat_inputs: 'List[Any]', *types_or_checks: 'Union[Type, Callable[[Any], bool]]') -> 'bool'`
        - Description: No docstring available
      - Function: `is_pure_tensor(inpt: Any) -> bool`
        - Description: No docstring available
      - Function: `query_chw(flat_inputs: 'List[Any]') -> 'Tuple[int, int, int]'`
        - Description: No docstring available
      - Function: `query_size(flat_inputs: 'List[Any]') -> 'Tuple[int, int]'`
        - Description: No docstring available
      - Function: `tree_flatten(pytree: Any) -> Tuple[List[Any], torch.utils._pytree.TreeSpec]`
        - Description: Flattens a pytree into a list of values and a TreeSpec that can be used to reconstruct the pytree.
      - Function: `tree_unflatten(values: List[Any], spec: torch.utils._pytree.TreeSpec) -> Any`
        - Description: Given a list of values and a TreeSpec, builds a pytree. This is the inverse operation of `tree_flatten`.
    - _auto_augment.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `AugMix(severity: int = 3, mixture_width: int = 3, chain_depth: int = -1, alpha: float = 1.0, all_ops: bool = True, interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, fill: Union[int, float, Sequence[int], Sequence[float], NoneType, Dict[Union[Type, str], Union[int, float, Sequence[int], Sequence[float], NoneType]]] = None) -> None`
        - Description: [BETA] AugMix data augmentation method based on `"AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty" <https://arxiv.org/abs/1912.02781>`_. .. v2betastatus:: AugMix transform This transformation works on images and videos only. If the input is :class:`torch.Tensor`, it should be of type ``torch.uint8``, and it is expected to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "L" or "RGB". Args: severity (int, optional): The severity of base augmentation operators. Default is ``3``. mixture_width (int, optional): The number of augmentation chains. Default is ``3``. chain_depth (int, optional): The depth of augmentation chains. A negative value denotes stochastic depth sampled from the interval [1, 3]. Default is ``-1``. alpha (float, optional): The hyperparameter for the probability distributions. Default is ``1.0``. all_ops (bool, optional): Use all operations (including brightness, contrast, color and sharpness). Default is ``True``. interpolation (InterpolationMode, optional): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported. fill (sequence or number, optional): Pixel fill value for the area outside the transformed image. If given a number, the value is used for all bands respectively.
      - Class: `AutoAugment(policy: torchvision.transforms.autoaugment.AutoAugmentPolicy = <AutoAugmentPolicy.IMAGENET: 'imagenet'>, interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.NEAREST: 'nearest'>, fill: Union[int, float, Sequence[int], Sequence[float], NoneType, Dict[Union[Type, str], Union[int, float, Sequence[int], Sequence[float], NoneType]]] = None) -> None`
        - Description: [BETA] AutoAugment data augmentation method based on `"AutoAugment: Learning Augmentation Strategies from Data" <https://arxiv.org/pdf/1805.09501.pdf>`_. .. v2betastatus:: AutoAugment transform This transformation works on images and videos only. If the input is :class:`torch.Tensor`, it should be of type ``torch.uint8``, and it is expected to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "L" or "RGB". Args: policy (AutoAugmentPolicy, optional): Desired policy enum defined by :class:`torchvision.transforms.autoaugment.AutoAugmentPolicy`. Default is ``AutoAugmentPolicy.IMAGENET``. interpolation (InterpolationMode, optional): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported. fill (sequence or number, optional): Pixel fill value for the area outside the transformed image. If given a number, the value is used for all bands respectively.
      - Class: `AutoAugmentPolicy(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: AutoAugment policies learned on different datasets. Available policies are IMAGENET, CIFAR10 and SVHN.
      - Class: `InterpolationMode(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: Interpolation modes Available interpolation methods are ``nearest``, ``nearest-exact``, ``bilinear``, ``bicubic``, ``box``, ``hamming``, and ``lanczos``.
      - Class: `RandAugment(num_ops: int = 2, magnitude: int = 9, num_magnitude_bins: int = 31, interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.NEAREST: 'nearest'>, fill: Union[int, float, Sequence[int], Sequence[float], NoneType, Dict[Union[Type, str], Union[int, float, Sequence[int], Sequence[float], NoneType]]] = None) -> None`
        - Description: [BETA] RandAugment data augmentation method based on `"RandAugment: Practical automated data augmentation with a reduced search space" <https://arxiv.org/abs/1909.13719>`_. .. v2betastatus:: RandAugment transform This transformation works on images and videos only. If the input is :class:`torch.Tensor`, it should be of type ``torch.uint8``, and it is expected to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "L" or "RGB". Args: num_ops (int, optional): Number of augmentation transformations to apply sequentially. magnitude (int, optional): Magnitude for all the transformations. num_magnitude_bins (int, optional): The number of different magnitude values. interpolation (InterpolationMode, optional): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported. fill (sequence or number, optional): Pixel fill value for the area outside the transformed image. If given a number, the value is used for all bands respectively.
      - Class: `Transform() -> 'None'`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `TreeSpec(type: Any, context: Any, children_specs: List[ForwardRef('TreeSpec')]) -> None`
        - Description: TreeSpec(type: Any, context: Any, children_specs: List[ForwardRef('TreeSpec')])
      - Class: `TrivialAugmentWide(num_magnitude_bins: int = 31, interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.NEAREST: 'nearest'>, fill: Union[int, float, Sequence[int], Sequence[float], NoneType, Dict[Union[Type, str], Union[int, float, Sequence[int], Sequence[float], NoneType]]] = None)`
        - Description: [BETA] Dataset-independent data-augmentation with TrivialAugment Wide, as described in `"TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation" <https://arxiv.org/abs/2103.10158>`_. .. v2betastatus:: TrivialAugmentWide transform This transformation works on images and videos only. If the input is :class:`torch.Tensor`, it should be of type ``torch.uint8``, and it is expected to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "L" or "RGB". Args: num_magnitude_bins (int, optional): The number of different magnitude values. interpolation (InterpolationMode, optional): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported. fill (sequence or number, optional): Pixel fill value for the area outside the transformed image. If given a number, the value is used for all bands respectively.
      - Class: `_AutoAugmentBase(*, interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.NEAREST: 'nearest'>, fill: Union[int, float, Sequence[int], Sequence[float], NoneType, Dict[Union[Type, str], Union[int, float, Sequence[int], Sequence[float], NoneType]]] = None) -> None`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Function: `_check_interpolation(interpolation: Union[torchvision.transforms.functional.InterpolationMode, int]) -> torchvision.transforms.functional.InterpolationMode`
        - Description: No docstring available
      - Function: `_get_fill(fill_dict, inpt_type)`
        - Description: No docstring available
      - Function: `_setup_fill_arg(fill: 'Union[_FillType, Dict[Union[Type, str], _FillType]]') -> 'Dict[Union[Type, str], _FillTypeJIT]'`
        - Description: No docstring available
      - Function: `check_type(obj: 'Any', types_or_checks: 'Tuple[Union[Type, Callable[[Any], bool]], ...]') -> 'bool'`
        - Description: No docstring available
      - Function: `get_size(inpt: torch.Tensor) -> List[int]`
        - Description: No docstring available
      - Function: `is_pure_tensor(inpt: Any) -> bool`
        - Description: No docstring available
      - Function: `tree_flatten(pytree: Any) -> Tuple[List[Any], torch.utils._pytree.TreeSpec]`
        - Description: Flattens a pytree into a list of values and a TreeSpec that can be used to reconstruct the pytree.
      - Function: `tree_unflatten(values: List[Any], spec: torch.utils._pytree.TreeSpec) -> Any`
        - Description: Given a list of values and a TreeSpec, builds a pytree. This is the inverse operation of `tree_flatten`.
    - _color.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `ColorJitter(brightness: Union[float, Sequence[float], NoneType] = None, contrast: Union[float, Sequence[float], NoneType] = None, saturation: Union[float, Sequence[float], NoneType] = None, hue: Union[float, Sequence[float], NoneType] = None) -> None`
        - Description: [BETA] Randomly change the brightness, contrast, saturation and hue of an image or video. .. v2betastatus:: ColorJitter transform If the input is a :class:`torch.Tensor`, it is expected to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions. If img is PIL Image, mode "1", "I", "F" and modes with transparency (alpha channel) are not supported. Args: brightness (float or tuple of float (min, max)): How much to jitter brightness. brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] or the given [min, max]. Should be non negative numbers. contrast (float or tuple of float (min, max)): How much to jitter contrast. contrast_factor is chosen uniformly from [max(0, 1 - contrast), 1 + contrast] or the given [min, max]. Should be non-negative numbers. saturation (float or tuple of float (min, max)): How much to jitter saturation. saturation_factor is chosen uniformly from [max(0, 1 - saturation), 1 + saturation] or the given [min, max]. Should be non negative numbers. hue (float or tuple of float (min, max)): How much to jitter hue. hue_factor is chosen uniformly from [-hue, hue] or the given [min, max]. Should have 0<= hue <= 0.5 or -0.5 <= min <= max <= 0.5. To jitter hue, the pixel values of the input image has to be non-negative for conversion to HSV space; thus it does not work if you normalize your image to an interval with negative values, or use an interpolation that generates negative values before using this function.
      - Class: `Grayscale(num_output_channels: int = 1)`
        - Description: [BETA] Convert images or videos to grayscale. .. v2betastatus:: Grayscale transform If the input is a :class:`torch.Tensor`, it is expected to have [..., 3 or 1, H, W] shape, where ... means an arbitrary number of leading dimensions Args: num_output_channels (int): (1 or 3) number of channels desired for output image
      - Class: `RandomAdjustSharpness(sharpness_factor: float, p: float = 0.5) -> None`
        - Description: [BETA] Adjust the sharpness of the image or video with a given probability. .. v2betastatus:: RandomAdjustSharpness transform If the input is a :class:`torch.Tensor`, it is expected to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions. Args: sharpness_factor (float): How much to adjust the sharpness. Can be any non-negative number. 0 gives a blurred image, 1 gives the original image while 2 increases the sharpness by a factor of 2. p (float): probability of the image being sharpened. Default value is 0.5
      - Class: `RandomAutocontrast(p: 'float' = 0.5) -> 'None'`
        - Description: [BETA] Autocontrast the pixels of the given image or video with a given probability. .. v2betastatus:: RandomAutocontrast transform If the input is a :class:`torch.Tensor`, it is expected to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "L" or "RGB". Args: p (float): probability of the image being autocontrasted. Default value is 0.5
      - Class: `RandomChannelPermutation() -> 'None'`
        - Description: [BETA] Randomly permute the channels of an image or video .. v2betastatus:: RandomChannelPermutation transform
      - Class: `RandomEqualize(p: 'float' = 0.5) -> 'None'`
        - Description: [BETA] Equalize the histogram of the given image or video with a given probability. .. v2betastatus:: RandomEqualize transform If the input is a :class:`torch.Tensor`, it is expected to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "P", "L" or "RGB". Args: p (float): probability of the image being equalized. Default value is 0.5
      - Class: `RandomGrayscale(p: float = 0.1) -> None`
        - Description: [BETA] Randomly convert image or videos to grayscale with a probability of p (default 0.1). .. v2betastatus:: RandomGrayscale transform If the input is a :class:`torch.Tensor`, it is expected to have [..., 3 or 1, H, W] shape, where ... means an arbitrary number of leading dimensions The output has the same number of channels as the input. Args: p (float): probability that image should be converted to grayscale.
      - Class: `RandomInvert(p: 'float' = 0.5) -> 'None'`
        - Description: [BETA] Inverts the colors of the given image or video with a given probability. .. v2betastatus:: RandomInvert transform If img is a Tensor, it is expected to be in [..., 1 or 3, H, W] format, where ... means it can have an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "L" or "RGB". Args: p (float): probability of the image being color inverted. Default value is 0.5
      - Class: `RandomPhotometricDistort(brightness: Tuple[float, float] = (0.875, 1.125), contrast: Tuple[float, float] = (0.5, 1.5), saturation: Tuple[float, float] = (0.5, 1.5), hue: Tuple[float, float] = (-0.05, 0.05), p: float = 0.5)`
        - Description: [BETA] Randomly distorts the image or video as used in `SSD: Single Shot MultiBox Detector <https://arxiv.org/abs/1512.02325>`_. .. v2betastatus:: RandomPhotometricDistort transform This transform relies on :class:`~torchvision.transforms.v2.ColorJitter` under the hood to adjust the contrast, saturation, hue, brightness, and also randomly permutes channels. Args: brightness (tuple of float (min, max), optional): How much to jitter brightness. brightness_factor is chosen uniformly from [min, max]. Should be non negative numbers. contrast tuple of float (min, max), optional): How much to jitter contrast. contrast_factor is chosen uniformly from [min, max]. Should be non-negative numbers. saturation (tuple of float (min, max), optional): How much to jitter saturation. saturation_factor is chosen uniformly from [min, max]. Should be non negative numbers. hue (tuple of float (min, max), optional): How much to jitter hue. hue_factor is chosen uniformly from [min, max]. Should have -0.5 <= min <= max <= 0.5. To jitter hue, the pixel values of the input image has to be non-negative for conversion to HSV space; thus it does not work if you normalize your image to an interval with negative values, or use an interpolation that generates negative values before using this function. p (float, optional) probability each distortion operation (contrast, saturation, ...) to be applied. Default is 0.5.
      - Class: `RandomPosterize(bits: int, p: float = 0.5) -> None`
        - Description: [BETA] Posterize the image or video with a given probability by reducing the number of bits for each color channel. .. v2betastatus:: RandomPosterize transform If the input is a :class:`torch.Tensor`, it should be of type torch.uint8, and it is expected to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "L" or "RGB". Args: bits (int): number of bits to keep for each channel (0-8) p (float): probability of the image being posterized. Default value is 0.5
      - Class: `RandomSolarize(threshold: float, p: float = 0.5) -> None`
        - Description: [BETA] Solarize the image or video with a given probability by inverting all pixel values above a threshold. .. v2betastatus:: RandomSolarize transform If img is a Tensor, it is expected to be in [..., 1 or 3, H, W] format, where ... means it can have an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "L" or "RGB". Args: threshold (float): all pixels equal or above this value are inverted. p (float): probability of the image being solarized. Default value is 0.5
      - Class: `Transform() -> 'None'`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `_RandomApplyTransform(p: 'float' = 0.5) -> 'None'`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Function: `query_chw(flat_inputs: 'List[Any]') -> 'Tuple[int, int, int]'`
        - Description: No docstring available
    - _container.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `Compose(transforms: Sequence[Callable]) -> None`
        - Description: [BETA] Composes several transforms together. .. v2betastatus:: Compose transform This transform does not support torchscript. Please, see the note below. Args: transforms (list of ``Transform`` objects): list of transforms to compose. Example: >>> transforms.Compose([ >>> transforms.CenterCrop(10), >>> transforms.PILToTensor(), >>> transforms.ConvertImageDtype(torch.float), >>> ]) .. note:: In order to script the transformations, please use ``torch.nn.Sequential`` as below. >>> transforms = torch.nn.Sequential( >>> transforms.CenterCrop(10), >>> transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), >>> ) >>> scripted_transforms = torch.jit.script(transforms) Make sure to use only scriptable transformations, i.e. that work with ``torch.Tensor``, does not require `lambda` functions or ``PIL.Image``.
      - Class: `RandomApply(transforms: Union[Sequence[Callable], torch.nn.modules.container.ModuleList], p: float = 0.5) -> None`
        - Description: [BETA] Apply randomly a list of transformations with a given probability. .. v2betastatus:: RandomApply transform .. note:: In order to script the transformation, please use ``torch.nn.ModuleList`` as input instead of list/tuple of transforms as shown below: >>> transforms = transforms.RandomApply(torch.nn.ModuleList([ >>> transforms.ColorJitter(), >>> ]), p=0.3) >>> scripted_transforms = torch.jit.script(transforms) Make sure to use only scriptable transformations, i.e. that work with ``torch.Tensor``, does not require `lambda` functions or ``PIL.Image``. Args: transforms (sequence or torch.nn.Module): list of transformations p (float): probability of applying the list of transforms
      - Class: `RandomChoice(transforms: Sequence[Callable], p: Optional[List[float]] = None) -> None`
        - Description: [BETA] Apply single transformation randomly picked from a list. .. v2betastatus:: RandomChoice transform This transform does not support torchscript. Args: transforms (sequence or torch.nn.Module): list of transformations p (list of floats or None, optional): probability of each transform being picked. If ``p`` doesn't sum to 1, it is automatically normalized. If ``None`` (default), all transforms have the same probability.
      - Class: `RandomOrder(transforms: Sequence[Callable]) -> None`
        - Description: [BETA] Apply a list of transformations in a random order. .. v2betastatus:: RandomOrder transform This transform does not support torchscript. Args: transforms (sequence or torch.nn.Module): list of transformations
      - Class: `Transform() -> 'None'`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - _deprecated.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `ToTensor() -> None`
        - Description: [BETA] [DEPRECATED] Use ``v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`` instead. Convert a PIL Image or ndarray to tensor and scale the values accordingly. .. v2betastatus:: ToTensor transform .. warning:: :class:`v2.ToTensor` is deprecated and will be removed in a future release. Please use instead ``v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])``. This transform does not support torchscript. Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8 In the other cases, tensors are returned without scaling. .. note:: Because the input image is scaled to [0.0, 1.0], this transformation should not be used when transforming target image masks. See the `references`_ for implementing the transforms for image masks. .. _references: https://github.com/pytorch/vision/tree/main/references/segmentation
      - Class: `Transform() -> 'None'`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - _geometry.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `CenterCrop(size: Union[int, Sequence[int]])`
        - Description: [BETA] Crop the input at the center. .. v2betastatus:: CenterCrop transform If the input is a :class:`torch.Tensor` or a ``TVTensor`` (e.g. :class:`~torchvision.tv_tensors.Image`, :class:`~torchvision.tv_tensors.Video`, :class:`~torchvision.tv_tensors.BoundingBoxes` etc.) it can have arbitrary number of leading batch dimensions. For example, the image can have ``[..., C, H, W]`` shape. A bounding box can have ``[..., 4]`` shape. If image size is smaller than output size along any edge, image is padded with 0 and then center cropped. Args: size (sequence or int): Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).
      - Class: `ElasticTransform(alpha: Union[float, Sequence[float]] = 50.0, sigma: Union[float, Sequence[float]] = 5.0, interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, fill: Union[int, float, Sequence[int], Sequence[float], NoneType, Dict[Union[Type, str], Union[int, float, Sequence[int], Sequence[float], NoneType]]] = 0) -> None`
        - Description: [BETA] Transform the input with elastic transformations. .. v2betastatus:: RandomPerspective transform If the input is a :class:`torch.Tensor` or a ``TVTensor`` (e.g. :class:`~torchvision.tv_tensors.Image`, :class:`~torchvision.tv_tensors.Video`, :class:`~torchvision.tv_tensors.BoundingBoxes` etc.) it can have arbitrary number of leading batch dimensions. For example, the image can have ``[..., C, H, W]`` shape. A bounding box can have ``[..., 4]`` shape. Given alpha and sigma, it will generate displacement vectors for all pixels based on random offsets. Alpha controls the strength and sigma controls the smoothness of the displacements. The displacements are added to an identity grid and the resulting grid is used to transform the input. .. note:: Implementation to transform bounding boxes is approximative (not exact). We construct an approximation of the inverse grid as ``inverse_grid = identity - displacement``. This is not an exact inverse of the grid used to transform images, i.e. ``grid = identity + displacement``. Our assumption is that ``displacement * displacement`` is small and can be ignored. Large displacements would lead to large errors in the approximation. Applications: Randomly transforms the morphology of objects in images and produces a see-through-water-like effect. Args: alpha (float or sequence of floats, optional): Magnitude of displacements. Default is 50.0. sigma (float or sequence of floats, optional): Smoothness of displacements. Default is 5.0. interpolation (InterpolationMode, optional): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. fill (number or tuple or dict, optional): Pixel fill value used when the ``padding_mode`` is constant. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. Fill value can be also a dictionary mapping data type to the fill value, e.g. ``fill={tv_tensors.Image: 127, tv_tensors.Mask: 0}`` where ``Image`` will be filled with 127 and ``Mask`` will be filled with 0.
      - Class: `FiveCrop(size: Union[int, Sequence[int]]) -> None`
        - Description: [BETA] Crop the image or video into four corners and the central crop. .. v2betastatus:: FiveCrop transform If the input is a :class:`torch.Tensor` or a :class:`~torchvision.tv_tensors.Image` or a :class:`~torchvision.tv_tensors.Video` it can have arbitrary number of leading batch dimensions. For example, the image can have ``[..., C, H, W]`` shape. .. Note:: This transform returns a tuple of images and there may be a mismatch in the number of inputs and targets your Dataset returns. See below for an example of how to deal with this. Args: size (sequence or int): Desired output size of the crop. If size is an ``int`` instead of sequence like (h, w), a square crop of size (size, size) is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]). Example: >>> class BatchMultiCrop(transforms.Transform): ... def forward(self, sample: Tuple[Tuple[Union[tv_tensors.Image, tv_tensors.Video], ...], int]): ... images_or_videos, labels = sample ... batch_size = len(images_or_videos) ... image_or_video = images_or_videos[0] ... images_or_videos = tv_tensors.wrap(torch.stack(images_or_videos), like=image_or_video) ... labels = torch.full((batch_size,), label, device=images_or_videos.device) ... return images_or_videos, labels ... >>> image = tv_tensors.Image(torch.rand(3, 256, 256)) >>> label = 3 >>> transform = transforms.Compose([transforms.FiveCrop(224), BatchMultiCrop()]) >>> images, labels = transform(image, label) >>> images.shape torch.Size([5, 3, 224, 224]) >>> labels tensor([3, 3, 3, 3, 3])
      - Class: `InterpolationMode(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
        - Description: Interpolation modes Available interpolation methods are ``nearest``, ``nearest-exact``, ``bilinear``, ``bicubic``, ``box``, ``hamming``, and ``lanczos``.
      - Class: `Pad(padding: Union[int, Sequence[int]], fill: Union[int, float, Sequence[int], Sequence[float], NoneType, Dict[Union[Type, str], Union[int, float, Sequence[int], Sequence[float], NoneType]]] = 0, padding_mode: Literal['constant', 'edge', 'reflect', 'symmetric'] = 'constant') -> None`
        - Description: [BETA] Pad the input on all sides with the given "pad" value. .. v2betastatus:: Pad transform If the input is a :class:`torch.Tensor` or a ``TVTensor`` (e.g. :class:`~torchvision.tv_tensors.Image`, :class:`~torchvision.tv_tensors.Video`, :class:`~torchvision.tv_tensors.BoundingBoxes` etc.) it can have arbitrary number of leading batch dimensions. For example, the image can have ``[..., C, H, W]`` shape. A bounding box can have ``[..., 4]`` shape. Args: padding (int or sequence): Padding on each border. If a single int is provided this is used to pad all borders. If sequence of length 2 is provided this is the padding on left/right and top/bottom respectively. If a sequence of length 4 is provided this is the padding for the left, top, right and bottom borders respectively. .. note:: In torchscript mode padding as single int is not supported, use a sequence of length 1: ``[padding, ]``. fill (number or tuple or dict, optional): Pixel fill value used when the ``padding_mode`` is constant. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. Fill value can be also a dictionary mapping data type to the fill value, e.g. ``fill={tv_tensors.Image: 127, tv_tensors.Mask: 0}`` where ``Image`` will be filled with 127 and ``Mask`` will be filled with 0. padding_mode (str, optional): Type of padding. Should be: constant, edge, reflect or symmetric. Default is "constant". - constant: pads with a constant value, this value is specified with fill - edge: pads with the last value at the edge of the image. - reflect: pads with reflection of image without repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2] - symmetric: pads with reflection of image repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3]
      - Class: `RandomAffine(degrees: Union[numbers.Number, Sequence], translate: Optional[Sequence[float]] = None, scale: Optional[Sequence[float]] = None, shear: Union[int, float, Sequence[float], NoneType] = None, interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.NEAREST: 'nearest'>, fill: Union[int, float, Sequence[int], Sequence[float], NoneType, Dict[Union[Type, str], Union[int, float, Sequence[int], Sequence[float], NoneType]]] = 0, center: Optional[List[float]] = None) -> None`
        - Description: [BETA] Random affine transformation the input keeping center invariant. .. v2betastatus:: RandomAffine transform If the input is a :class:`torch.Tensor` or a ``TVTensor`` (e.g. :class:`~torchvision.tv_tensors.Image`, :class:`~torchvision.tv_tensors.Video`, :class:`~torchvision.tv_tensors.BoundingBoxes` etc.) it can have arbitrary number of leading batch dimensions. For example, the image can have ``[..., C, H, W]`` shape. A bounding box can have ``[..., 4]`` shape. Args: degrees (sequence or number): Range of degrees to select from. If degrees is a number instead of sequence like (min, max), the range of degrees will be (-degrees, +degrees). Set to 0 to deactivate rotations. translate (tuple, optional): tuple of maximum absolute fraction for horizontal and vertical translations. For example translate=(a, b), then horizontal shift is randomly sampled in the range -img_width * a < dx < img_width * a and vertical shift is randomly sampled in the range -img_height * b < dy < img_height * b. Will not translate by default. scale (tuple, optional): scaling factor interval, e.g (a, b), then scale is randomly sampled from the range a <= scale <= b. Will keep original scale by default. shear (sequence or number, optional): Range of degrees to select from. If shear is a number, a shear parallel to the x-axis in the range (-shear, +shear) will be applied. Else if shear is a sequence of 2 values a shear parallel to the x-axis in the range (shear[0], shear[1]) will be applied. Else if shear is a sequence of 4 values, an x-axis shear in (shear[0], shear[1]) and y-axis shear in (shear[2], shear[3]) will be applied. Will not apply shear by default. interpolation (InterpolationMode, optional): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. fill (number or tuple or dict, optional): Pixel fill value used when the ``padding_mode`` is constant. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. Fill value can be also a dictionary mapping data type to the fill value, e.g. ``fill={tv_tensors.Image: 127, tv_tensors.Mask: 0}`` where ``Image`` will be filled with 127 and ``Mask`` will be filled with 0. center (sequence, optional): Optional center of rotation, (x, y). Origin is the upper left corner. Default is the center of the image. .. _filters: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#filters
      - Class: `RandomCrop(size: Union[int, Sequence[int]], padding: Union[int, Sequence[int], NoneType] = None, pad_if_needed: bool = False, fill: Union[int, float, Sequence[int], Sequence[float], NoneType, Dict[Union[Type, str], Union[int, float, Sequence[int], Sequence[float], NoneType]]] = 0, padding_mode: Literal['constant', 'edge', 'reflect', 'symmetric'] = 'constant') -> None`
        - Description: [BETA] Crop the input at a random location. .. v2betastatus:: RandomCrop transform If the input is a :class:`torch.Tensor` or a ``TVTensor`` (e.g. :class:`~torchvision.tv_tensors.Image`, :class:`~torchvision.tv_tensors.Video`, :class:`~torchvision.tv_tensors.BoundingBoxes` etc.) it can have arbitrary number of leading batch dimensions. For example, the image can have ``[..., C, H, W]`` shape. A bounding box can have ``[..., 4]`` shape. Args: size (sequence or int): Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]). padding (int or sequence, optional): Optional padding on each border of the image. Default is None. If a single int is provided this is used to pad all borders. If sequence of length 2 is provided this is the padding on left/right and top/bottom respectively. If a sequence of length 4 is provided this is the padding for the left, top, right and bottom borders respectively. .. note:: In torchscript mode padding as single int is not supported, use a sequence of length 1: ``[padding, ]``. pad_if_needed (boolean, optional): It will pad the image if smaller than the desired size to avoid raising an exception. Since cropping is done after padding, the padding seems to be done at a random offset. fill (number or tuple or dict, optional): Pixel fill value used when the ``padding_mode`` is constant. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. Fill value can be also a dictionary mapping data type to the fill value, e.g. ``fill={tv_tensors.Image: 127, tv_tensors.Mask: 0}`` where ``Image`` will be filled with 127 and ``Mask`` will be filled with 0. padding_mode (str, optional): Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant. - constant: pads with a constant value, this value is specified with fill - edge: pads with the last value at the edge of the image. - reflect: pads with reflection of image without repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2] - symmetric: pads with reflection of image repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3]
      - Class: `RandomHorizontalFlip(p: 'float' = 0.5) -> 'None'`
        - Description: [BETA] Horizontally flip the input with a given probability. .. v2betastatus:: RandomHorizontalFlip transform If the input is a :class:`torch.Tensor` or a ``TVTensor`` (e.g. :class:`~torchvision.tv_tensors.Image`, :class:`~torchvision.tv_tensors.Video`, :class:`~torchvision.tv_tensors.BoundingBoxes` etc.) it can have arbitrary number of leading batch dimensions. For example, the image can have ``[..., C, H, W]`` shape. A bounding box can have ``[..., 4]`` shape. Args: p (float, optional): probability of the input being flipped. Default value is 0.5
      - Class: `RandomIoUCrop(min_scale: float = 0.3, max_scale: float = 1.0, min_aspect_ratio: float = 0.5, max_aspect_ratio: float = 2.0, sampler_options: Optional[List[float]] = None, trials: int = 40)`
        - Description: [BETA] Random IoU crop transformation from `"SSD: Single Shot MultiBox Detector" <https://arxiv.org/abs/1512.02325>`_. .. v2betastatus:: RandomIoUCrop transform This transformation requires an image or video data and ``tv_tensors.BoundingBoxes`` in the input. .. warning:: In order to properly remove the bounding boxes below the IoU threshold, `RandomIoUCrop` must be followed by :class:`~torchvision.transforms.v2.SanitizeBoundingBoxes`, either immediately after or later in the transforms pipeline. If the input is a :class:`torch.Tensor` or a ``TVTensor`` (e.g. :class:`~torchvision.tv_tensors.Image`, :class:`~torchvision.tv_tensors.Video`, :class:`~torchvision.tv_tensors.BoundingBoxes` etc.) it can have arbitrary number of leading batch dimensions. For example, the image can have ``[..., C, H, W]`` shape. A bounding box can have ``[..., 4]`` shape. Args: min_scale (float, optional): Minimum factors to scale the input size. max_scale (float, optional): Maximum factors to scale the input size. min_aspect_ratio (float, optional): Minimum aspect ratio for the cropped image or video. max_aspect_ratio (float, optional): Maximum aspect ratio for the cropped image or video. sampler_options (list of float, optional): List of minimal IoU (Jaccard) overlap between all the boxes and a cropped image or video. Default, ``None`` which corresponds to ``[0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]`` trials (int, optional): Number of trials to find a crop for a given value of minimal IoU (Jaccard) overlap. Default, 40.
      - Class: `RandomPerspective(distortion_scale: float = 0.5, p: float = 0.5, interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, fill: Union[int, float, Sequence[int], Sequence[float], NoneType, Dict[Union[Type, str], Union[int, float, Sequence[int], Sequence[float], NoneType]]] = 0) -> None`
        - Description: [BETA] Perform a random perspective transformation of the input with a given probability. .. v2betastatus:: RandomPerspective transform If the input is a :class:`torch.Tensor` or a ``TVTensor`` (e.g. :class:`~torchvision.tv_tensors.Image`, :class:`~torchvision.tv_tensors.Video`, :class:`~torchvision.tv_tensors.BoundingBoxes` etc.) it can have arbitrary number of leading batch dimensions. For example, the image can have ``[..., C, H, W]`` shape. A bounding box can have ``[..., 4]`` shape. Args: distortion_scale (float, optional): argument to control the degree of distortion and ranges from 0 to 1. Default is 0.5. p (float, optional): probability of the input being transformed. Default is 0.5. interpolation (InterpolationMode, optional): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. fill (number or tuple or dict, optional): Pixel fill value used when the ``padding_mode`` is constant. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. Fill value can be also a dictionary mapping data type to the fill value, e.g. ``fill={tv_tensors.Image: 127, tv_tensors.Mask: 0}`` where ``Image`` will be filled with 127 and ``Mask`` will be filled with 0.
      - Class: `RandomResize(min_size: int, max_size: int, interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
        - Description: [BETA] Randomly resize the input. .. v2betastatus:: RandomResize transform This transformation can be used together with ``RandomCrop`` as data augmentations to train models on image segmentation task. Output spatial size is randomly sampled from the interval ``[min_size, max_size]``: .. code-block:: python size = uniform_sample(min_size, max_size) output_width = size output_height = size If the input is a :class:`torch.Tensor` or a ``TVTensor`` (e.g. :class:`~torchvision.tv_tensors.Image`, :class:`~torchvision.tv_tensors.Video`, :class:`~torchvision.tv_tensors.BoundingBoxes` etc.) it can have arbitrary number of leading batch dimensions. For example, the image can have ``[..., C, H, W]`` shape. A bounding box can have ``[..., 4]`` shape. Args: min_size (int): Minimum output size for random sampling max_size (int): Maximum output size for random sampling interpolation (InterpolationMode, optional): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.NEAREST_EXACT``, ``InterpolationMode.BILINEAR`` and ``InterpolationMode.BICUBIC`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. antialias (bool, optional): Whether to apply antialiasing. It only affects **tensors** with bilinear or bicubic modes and it is ignored otherwise: on PIL images, antialiasing is always applied on bilinear or bicubic modes; on other modes (for PIL images and tensors), antialiasing makes no sense and this parameter is ignored. Possible values are: - ``True``: will apply antialiasing for bilinear or bicubic modes. Other mode aren't affected. This is probably what you want to use. - ``False``: will not apply antialiasing for tensors on any mode. PIL images are still antialiased on bilinear or bicubic modes, because PIL doesn't support no antialias. - ``None``: equivalent to ``False`` for tensors and ``True`` for PIL images. This value exists for legacy reasons and you probably don't want to use it unless you really know what you are doing. The current default is ``None`` **but will change to** ``True`` **in v0.17** for the PIL and Tensor backends to be consistent.
      - Class: `RandomResizedCrop(size: Union[int, Sequence[int]], scale: Tuple[float, float] = (0.08, 1.0), ratio: Tuple[float, float] = (0.75, 1.3333333333333333), interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
        - Description: [BETA] Crop a random portion of the input and resize it to a given size. .. v2betastatus:: RandomResizedCrop transform If the input is a :class:`torch.Tensor` or a ``TVTensor`` (e.g. :class:`~torchvision.tv_tensors.Image`, :class:`~torchvision.tv_tensors.Video`, :class:`~torchvision.tv_tensors.BoundingBoxes` etc.) it can have arbitrary number of leading batch dimensions. For example, the image can have ``[..., C, H, W]`` shape. A bounding box can have ``[..., 4]`` shape. A crop of the original input is made: the crop has a random area (H * W) and a random aspect ratio. This crop is finally resized to the given size. This is popularly used to train the Inception networks. Args: size (int or sequence): expected output size of the crop, for each edge. If size is an int instead of sequence like (h, w), a square output size ``(size, size)`` is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]). .. note:: In torchscript mode size as single int is not supported, use a sequence of length 1: ``[size, ]``. scale (tuple of float, optional): Specifies the lower and upper bounds for the random area of the crop, before resizing. The scale is defined with respect to the area of the original image. ratio (tuple of float, optional): lower and upper bounds for the random aspect ratio of the crop, before resizing. interpolation (InterpolationMode, optional): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.NEAREST_EXACT``, ``InterpolationMode.BILINEAR`` and ``InterpolationMode.BICUBIC`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. antialias (bool, optional): Whether to apply antialiasing. It only affects **tensors** with bilinear or bicubic modes and it is ignored otherwise: on PIL images, antialiasing is always applied on bilinear or bicubic modes; on other modes (for PIL images and tensors), antialiasing makes no sense and this parameter is ignored. Possible values are: - ``True``: will apply antialiasing for bilinear or bicubic modes. Other mode aren't affected. This is probably what you want to use. - ``False``: will not apply antialiasing for tensors on any mode. PIL images are still antialiased on bilinear or bicubic modes, because PIL doesn't support no antialias. - ``None``: equivalent to ``False`` for tensors and ``True`` for PIL images. This value exists for legacy reasons and you probably don't want to use it unless you really know what you are doing. The current default is ``None`` **but will change to** ``True`` **in v0.17** for the PIL and Tensor backends to be consistent.
      - Class: `RandomRotation(degrees: Union[numbers.Number, Sequence], interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.NEAREST: 'nearest'>, expand: bool = False, center: Optional[List[float]] = None, fill: Union[int, float, Sequence[int], Sequence[float], NoneType, Dict[Union[Type, str], Union[int, float, Sequence[int], Sequence[float], NoneType]]] = 0) -> None`
        - Description: [BETA] Rotate the input by angle. .. v2betastatus:: RandomRotation transform If the input is a :class:`torch.Tensor` or a ``TVTensor`` (e.g. :class:`~torchvision.tv_tensors.Image`, :class:`~torchvision.tv_tensors.Video`, :class:`~torchvision.tv_tensors.BoundingBoxes` etc.) it can have arbitrary number of leading batch dimensions. For example, the image can have ``[..., C, H, W]`` shape. A bounding box can have ``[..., 4]`` shape. Args: degrees (sequence or number): Range of degrees to select from. If degrees is a number instead of sequence like (min, max), the range of degrees will be (-degrees, +degrees). interpolation (InterpolationMode, optional): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. expand (bool, optional): Optional expansion flag. If true, expands the output to make it large enough to hold the entire rotated image. If false or omitted, make the output image the same size as the input image. Note that the expand flag assumes rotation around the center and no translation. center (sequence, optional): Optional center of rotation, (x, y). Origin is the upper left corner. Default is the center of the image. fill (number or tuple or dict, optional): Pixel fill value used when the ``padding_mode`` is constant. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. Fill value can be also a dictionary mapping data type to the fill value, e.g. ``fill={tv_tensors.Image: 127, tv_tensors.Mask: 0}`` where ``Image`` will be filled with 127 and ``Mask`` will be filled with 0. .. _filters: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#filters
      - Class: `RandomShortestSize(min_size: Union[List[int], Tuple[int], int], max_size: Optional[int] = None, interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn')`
        - Description: [BETA] Randomly resize the input. .. v2betastatus:: RandomShortestSize transform If the input is a :class:`torch.Tensor` or a ``TVTensor`` (e.g. :class:`~torchvision.tv_tensors.Image`, :class:`~torchvision.tv_tensors.Video`, :class:`~torchvision.tv_tensors.BoundingBoxes` etc.) it can have arbitrary number of leading batch dimensions. For example, the image can have ``[..., C, H, W]`` shape. A bounding box can have ``[..., 4]`` shape. Args: min_size (int or sequence of int): Minimum spatial size. Single integer value or a sequence of integer values. max_size (int, optional): Maximum spatial size. Default, None. interpolation (InterpolationMode, optional): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.NEAREST_EXACT``, ``InterpolationMode.BILINEAR`` and ``InterpolationMode.BICUBIC`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. antialias (bool, optional): Whether to apply antialiasing. It only affects **tensors** with bilinear or bicubic modes and it is ignored otherwise: on PIL images, antialiasing is always applied on bilinear or bicubic modes; on other modes (for PIL images and tensors), antialiasing makes no sense and this parameter is ignored. Possible values are: - ``True``: will apply antialiasing for bilinear or bicubic modes. Other mode aren't affected. This is probably what you want to use. - ``False``: will not apply antialiasing for tensors on any mode. PIL images are still antialiased on bilinear or bicubic modes, because PIL doesn't support no antialias. - ``None``: equivalent to ``False`` for tensors and ``True`` for PIL images. This value exists for legacy reasons and you probably don't want to use it unless you really know what you are doing. The current default is ``None`` **but will change to** ``True`` **in v0.17** for the PIL and Tensor backends to be consistent.
      - Class: `RandomVerticalFlip(p: 'float' = 0.5) -> 'None'`
        - Description: [BETA] Vertically flip the input with a given probability. .. v2betastatus:: RandomVerticalFlip transform If the input is a :class:`torch.Tensor` or a ``TVTensor`` (e.g. :class:`~torchvision.tv_tensors.Image`, :class:`~torchvision.tv_tensors.Video`, :class:`~torchvision.tv_tensors.BoundingBoxes` etc.) it can have arbitrary number of leading batch dimensions. For example, the image can have ``[..., C, H, W]`` shape. A bounding box can have ``[..., 4]`` shape. Args: p (float, optional): probability of the input being flipped. Default value is 0.5
      - Class: `RandomZoomOut(fill: Union[int, float, Sequence[int], Sequence[float], NoneType, Dict[Union[Type, str], Union[int, float, Sequence[int], Sequence[float], NoneType]]] = 0, side_range: Sequence[float] = (1.0, 4.0), p: float = 0.5) -> None`
        - Description: [BETA] "Zoom out" transformation from `"SSD: Single Shot MultiBox Detector" <https://arxiv.org/abs/1512.02325>`_. .. v2betastatus:: RandomZoomOut transform This transformation randomly pads images, videos, bounding boxes and masks creating a zoom out effect. Output spatial size is randomly sampled from original size up to a maximum size configured with ``side_range`` parameter: .. code-block:: python r = uniform_sample(side_range[0], side_range[1]) output_width = input_width * r output_height = input_height * r If the input is a :class:`torch.Tensor` or a ``TVTensor`` (e.g. :class:`~torchvision.tv_tensors.Image`, :class:`~torchvision.tv_tensors.Video`, :class:`~torchvision.tv_tensors.BoundingBoxes` etc.) it can have arbitrary number of leading batch dimensions. For example, the image can have ``[..., C, H, W]`` shape. A bounding box can have ``[..., 4]`` shape. Args: fill (number or tuple or dict, optional): Pixel fill value used when the ``padding_mode`` is constant. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. Fill value can be also a dictionary mapping data type to the fill value, e.g. ``fill={tv_tensors.Image: 127, tv_tensors.Mask: 0}`` where ``Image`` will be filled with 127 and ``Mask`` will be filled with 0. side_range (sequence of floats, optional): tuple of two floats defines minimum and maximum factors to scale the input size. p (float, optional): probability that the zoom operation will be performed.
      - Class: `Resize(size: Union[int, Sequence[int]], interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, max_size: Optional[int] = None, antialias: Union[str, bool, NoneType] = 'warn') -> None`
        - Description: [BETA] Resize the input to the given size. .. v2betastatus:: Resize transform If the input is a :class:`torch.Tensor` or a ``TVTensor`` (e.g. :class:`~torchvision.tv_tensors.Image`, :class:`~torchvision.tv_tensors.Video`, :class:`~torchvision.tv_tensors.BoundingBoxes` etc.) it can have arbitrary number of leading batch dimensions. For example, the image can have ``[..., C, H, W]`` shape. A bounding box can have ``[..., 4]`` shape. .. warning:: The output image might be different depending on its type: when downsampling, the interpolation of PIL images and tensors is slightly different, because PIL applies antialiasing. This may lead to significant differences in the performance of a network. Therefore, it is preferable to train and serve a model with the same input types. See also below the ``antialias`` parameter, which can help making the output of PIL images and tensors closer. Args: size (sequence or int): Desired output size. If size is a sequence like (h, w), output size will be matched to this. If size is an int, smaller edge of the image will be matched to this number. i.e, if height > width, then image will be rescaled to (size * height / width, size). .. note:: In torchscript mode size as single int is not supported, use a sequence of length 1: ``[size, ]``. interpolation (InterpolationMode, optional): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.NEAREST_EXACT``, ``InterpolationMode.BILINEAR`` and ``InterpolationMode.BICUBIC`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. max_size (int, optional): The maximum allowed for the longer edge of the resized image. If the longer edge of the image is greater than ``max_size`` after being resized according to ``size``, ``size`` will be overruled so that the longer edge is equal to ``max_size``. As a result, the smaller edge may be shorter than ``size``. This is only supported if ``size`` is an int (or a sequence of length 1 in torchscript mode). antialias (bool, optional): Whether to apply antialiasing. It only affects **tensors** with bilinear or bicubic modes and it is ignored otherwise: on PIL images, antialiasing is always applied on bilinear or bicubic modes; on other modes (for PIL images and tensors), antialiasing makes no sense and this parameter is ignored. Possible values are: - ``True``: will apply antialiasing for bilinear or bicubic modes. Other mode aren't affected. This is probably what you want to use. - ``False``: will not apply antialiasing for tensors on any mode. PIL images are still antialiased on bilinear or bicubic modes, because PIL doesn't support no antialias. - ``None``: equivalent to ``False`` for tensors and ``True`` for PIL images. This value exists for legacy reasons and you probably don't want to use it unless you really know what you are doing. The current default is ``None`` **but will change to** ``True`` **in v0.17** for the PIL and Tensor backends to be consistent.
      - Class: `ScaleJitter(target_size: Tuple[int, int], scale_range: Tuple[float, float] = (0.1, 2.0), interpolation: Union[torchvision.transforms.functional.InterpolationMode, int] = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn')`
        - Description: [BETA] Perform Large Scale Jitter on the input according to `"Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation" <https://arxiv.org/abs/2012.07177>`_. .. v2betastatus:: ScaleJitter transform If the input is a :class:`torch.Tensor` or a ``TVTensor`` (e.g. :class:`~torchvision.tv_tensors.Image`, :class:`~torchvision.tv_tensors.Video`, :class:`~torchvision.tv_tensors.BoundingBoxes` etc.) it can have arbitrary number of leading batch dimensions. For example, the image can have ``[..., C, H, W]`` shape. A bounding box can have ``[..., 4]`` shape. Args: target_size (tuple of int): Target size. This parameter defines base scale for jittering, e.g. ``min(target_size[0] / width, target_size[1] / height)``. scale_range (tuple of float, optional): Minimum and maximum of the scale range. Default, ``(0.1, 2.0)``. interpolation (InterpolationMode, optional): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.NEAREST_EXACT``, ``InterpolationMode.BILINEAR`` and ``InterpolationMode.BICUBIC`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. antialias (bool, optional): Whether to apply antialiasing. It only affects **tensors** with bilinear or bicubic modes and it is ignored otherwise: on PIL images, antialiasing is always applied on bilinear or bicubic modes; on other modes (for PIL images and tensors), antialiasing makes no sense and this parameter is ignored. Possible values are: - ``True``: will apply antialiasing for bilinear or bicubic modes. Other mode aren't affected. This is probably what you want to use. - ``False``: will not apply antialiasing for tensors on any mode. PIL images are still antialiased on bilinear or bicubic modes, because PIL doesn't support no antialias. - ``None``: equivalent to ``False`` for tensors and ``True`` for PIL images. This value exists for legacy reasons and you probably don't want to use it unless you really know what you are doing. The current default is ``None`` **but will change to** ``True`` **in v0.17** for the PIL and Tensor backends to be consistent.
      - Class: `TenCrop(size: Union[int, Sequence[int]], vertical_flip: bool = False) -> None`
        - Description: [BETA] Crop the image or video into four corners and the central crop plus the flipped version of these (horizontal flipping is used by default). .. v2betastatus:: TenCrop transform If the input is a :class:`torch.Tensor` or a :class:`~torchvision.tv_tensors.Image` or a :class:`~torchvision.tv_tensors.Video` it can have arbitrary number of leading batch dimensions. For example, the image can have ``[..., C, H, W]`` shape. See :class:`~torchvision.transforms.v2.FiveCrop` for an example. .. Note:: This transform returns a tuple of images and there may be a mismatch in the number of inputs and targets your Dataset returns. See below for an example of how to deal with this. Args: size (sequence or int): Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]). vertical_flip (bool, optional): Use vertical flipping instead of horizontal
      - Class: `Transform() -> 'None'`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `_RandomApplyTransform(p: 'float' = 0.5) -> 'None'`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Function: `_check_interpolation(interpolation: Union[torchvision.transforms.functional.InterpolationMode, int]) -> torchvision.transforms.functional.InterpolationMode`
        - Description: No docstring available
      - Function: `_check_padding_arg(padding: 'Union[int, Sequence[int]]') -> 'None'`
        - Description: No docstring available
      - Function: `_check_padding_mode_arg(padding_mode: "Literal['constant', 'edge', 'reflect', 'symmetric']") -> 'None'`
        - Description: No docstring available
      - Function: `_check_sequence_input(x, name, req_sizes)`
        - Description: No docstring available
      - Function: `_get_fill(fill_dict, inpt_type)`
        - Description: No docstring available
      - Function: `_get_perspective_coeffs(startpoints: List[List[int]], endpoints: List[List[int]]) -> List[float]`
        - Description: Helper function to get the coefficients (a, b, c, d, e, f, g, h) for the perspective transforms. In Perspective Transform each pixel (x, y) in the original image gets transformed as, (x, y) -> ( (ax + by + c) / (gx + hy + 1), (dx + ey + f) / (gx + hy + 1) ) Args: startpoints (list of list of ints): List containing four lists of two integers corresponding to four corners ``[top-left, top-right, bottom-right, bottom-left]`` of the original image. endpoints (list of list of ints): List containing four lists of two integers corresponding to four corners ``[top-left, top-right, bottom-right, bottom-left]`` of the transformed image. Returns: octuple (a, b, c, d, e, f, g, h) for transforming each pixel.
      - Function: `_setup_angle(x, name, req_sizes=(2,))`
        - Description: No docstring available
      - Function: `_setup_fill_arg(fill: 'Union[_FillType, Dict[Union[Type, str], _FillType]]') -> 'Dict[Union[Type, str], _FillTypeJIT]'`
        - Description: No docstring available
      - Function: `_setup_number_or_seq(arg: 'Union[int, float, Sequence[Union[int, float]]]', name: 'str') -> 'Sequence[float]'`
        - Description: No docstring available
      - Function: `_setup_size(size, error_msg)`
        - Description: No docstring available
      - Function: `box_iou(boxes1: torch.Tensor, boxes2: torch.Tensor) -> torch.Tensor`
        - Description: Return intersection-over-union (Jaccard index) between two sets of boxes. Both sets of boxes are expected to be in ``(x1, y1, x2, y2)`` format with ``0 <= x1 < x2`` and ``0 <= y1 < y2``. Args: boxes1 (Tensor[N, 4]): first set of boxes boxes2 (Tensor[M, 4]): second set of boxes Returns: Tensor[N, M]: the NxM matrix containing the pairwise IoU values for every element in boxes1 and boxes2
      - Function: `cast(typ, val)`
        - Description: Cast a value to a type. This returns the value unchanged. To the type checker this signals that the return value has the designated type, but at runtime we intentionally don't check anything (we want this to be as fast as possible).
      - Function: `get_bounding_boxes(flat_inputs: 'List[Any]') -> 'tv_tensors.BoundingBoxes'`
        - Description: No docstring available
      - Function: `has_all(flat_inputs: 'List[Any]', *types_or_checks: 'Union[Type, Callable[[Any], bool]]') -> 'bool'`
        - Description: No docstring available
      - Function: `has_any(flat_inputs: 'List[Any]', *types_or_checks: 'Union[Type, Callable[[Any], bool]]') -> 'bool'`
        - Description: No docstring available
      - Function: `is_pure_tensor(inpt: Any) -> bool`
        - Description: No docstring available
      - Function: `query_size(flat_inputs: 'List[Any]') -> 'Tuple[int, int]'`
        - Description: No docstring available
    - _meta.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `ClampBoundingBoxes() -> 'None'`
        - Description: [BETA] Clamp bounding boxes to their corresponding image dimensions. The clamping is done according to the bounding boxes' ``canvas_size`` meta-data. .. v2betastatus:: ClampBoundingBoxes transform
      - Class: `ConvertBoundingBoxFormat(format: Union[str, torchvision.tv_tensors._bounding_boxes.BoundingBoxFormat]) -> None`
        - Description: [BETA] Convert bounding box coordinates to the given ``format``, eg from "CXCYWH" to "XYXY". .. v2betastatus:: ConvertBoundingBoxFormat transform Args: format (str or tv_tensors.BoundingBoxFormat): output bounding box format. Possible values are defined by :class:`~torchvision.tv_tensors.BoundingBoxFormat` and string values match the enums, e.g. "XYXY" or "XYWH" etc.
      - Class: `Transform() -> 'None'`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - _misc.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `ConvertImageDtype(dtype: torch.dtype = torch.float32) -> None`
        - Description: [BETA] [DEPRECATED] Use ``v2.ToDtype(dtype, scale=True)`` instead. Convert input image to the given ``dtype`` and scale the values accordingly. .. v2betastatus:: ConvertImageDtype transform .. warning:: Consider using ``ToDtype(dtype, scale=True)`` instead. See :class:`~torchvision.transforms.v2.ToDtype`. This function does not support PIL Image. Args: dtype (torch.dtype): Desired data type of the output .. note:: When converting from a smaller to a larger integer ``dtype`` the maximum values are **not** mapped exactly. If converted back and forth, this mismatch has no effect. Raises: RuntimeError: When trying to cast :class:`torch.float32` to :class:`torch.int32` or :class:`torch.int64` as well as for trying to cast :class:`torch.float64` to :class:`torch.int64`. These conversions might lead to overflow errors since the floating point ``dtype`` cannot store consecutive integers over the whole range of the integer ``dtype``.
      - Class: `GaussianBlur(kernel_size: Union[int, Sequence[int]], sigma: Union[int, float, Sequence[float]] = (0.1, 2.0)) -> None`
        - Description: [BETA] Blurs image with randomly chosen Gaussian blur. .. v2betastatus:: GausssianBlur transform If the input is a Tensor, it is expected to have [..., C, H, W] shape, where ... means an arbitrary number of leading dimensions. Args: kernel_size (int or sequence): Size of the Gaussian kernel. sigma (float or tuple of float (min, max)): Standard deviation to be used for creating kernel to perform blurring. If float, sigma is fixed. If it is tuple of float (min, max), sigma is chosen uniformly at random to lie in the given range.
      - Class: `Identity() -> 'None'`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `Lambda(lambd: Callable[[Any], Any], *types: Type)`
        - Description: [BETA] Apply a user-defined function as a transform. .. v2betastatus:: Lambda transform This transform does not support torchscript. Args: lambd (function): Lambda/function to be used for transform.
      - Class: `LinearTransformation(transformation_matrix: torch.Tensor, mean_vector: torch.Tensor)`
        - Description: [BETA] Transform a tensor image or video with a square transformation matrix and a mean_vector computed offline. .. v2betastatus:: LinearTransformation transform This transform does not support PIL Image. Given transformation_matrix and mean_vector, will flatten the torch.*Tensor and subtract mean_vector from it which is then followed by computing the dot product with the transformation matrix and then reshaping the tensor to its original shape. Applications: whitening transformation: Suppose X is a column vector zero-centered data. Then compute the data covariance matrix [D x D] with torch.mm(X.t(), X), perform SVD on this matrix and pass it as transformation_matrix. Args: transformation_matrix (Tensor): tensor [D x D], D = C x H x W mean_vector (Tensor): tensor [D], D = C x H x W
      - Class: `Normalize(mean: Sequence[float], std: Sequence[float], inplace: bool = False)`
        - Description: [BETA] Normalize a tensor image or video with mean and standard deviation. .. v2betastatus:: Normalize transform This transform does not support PIL Image. Given mean: ``(mean[1],...,mean[n])`` and std: ``(std[1],..,std[n])`` for ``n`` channels, this transform will normalize each channel of the input ``torch.*Tensor`` i.e., ``output[channel] = (input[channel] - mean[channel]) / std[channel]`` .. note:: This transform acts out of place, i.e., it does not mutate the input tensor. Args: mean (sequence): Sequence of means for each channel. std (sequence): Sequence of standard deviations for each channel. inplace(bool,optional): Bool to make this operation in-place.
      - Class: `SanitizeBoundingBoxes(min_size: float = 1.0, labels_getter: Union[Callable[[Any], Optional[torch.Tensor]], str, NoneType] = 'default') -> None`
        - Description: [BETA] Remove degenerate/invalid bounding boxes and their corresponding labels and masks. .. v2betastatus:: SanitizeBoundingBoxes transform This transform removes bounding boxes and their associated labels/masks that: - are below a given ``min_size``: by default this also removes degenerate boxes that have e.g. X2 <= X1. - have any coordinate outside of their corresponding image. You may want to call :class:`~torchvision.transforms.v2.ClampBoundingBoxes` first to avoid undesired removals. It is recommended to call it at the end of a pipeline, before passing the input to the models. It is critical to call this transform if :class:`~torchvision.transforms.v2.RandomIoUCrop` was called. If you want to be extra careful, you may call it after all transforms that may modify bounding boxes but once at the end should be enough in most cases. Args: min_size (float, optional) The size below which bounding boxes are removed. Default is 1. labels_getter (callable or str or None, optional): indicates how to identify the labels in the input. By default, this will try to find a "labels" key in the input (case-insensitive), if the input is a dict or it is a tuple whose second element is a dict. This heuristic should work well with a lot of datasets, including the built-in torchvision datasets. It can also be a callable that takes the same input as the transform, and returns the labels.
      - Class: `ToDtype(dtype: Union[torch.dtype, Dict[Union[Type, str], Optional[torch.dtype]]], scale: bool = False) -> None`
        - Description: [BETA] Converts the input to a specific dtype, optionally scaling the values for images or videos. .. v2betastatus:: ToDtype transform .. note:: ``ToDtype(dtype, scale=True)`` is the recommended replacement for ``ConvertImageDtype(dtype)``. Args: dtype (``torch.dtype`` or dict of ``TVTensor`` -> ``torch.dtype``): The dtype to convert to. If a ``torch.dtype`` is passed, e.g. ``torch.float32``, only images and videos will be converted to that dtype: this is for compatibility with :class:`~torchvision.transforms.v2.ConvertImageDtype`. A dict can be passed to specify per-tv_tensor conversions, e.g. ``dtype={tv_tensors.Image: torch.float32, tv_tensors.Mask: torch.int64, "others":None}``. The "others" key can be used as a catch-all for any other tv_tensor type, and ``None`` means no conversion. scale (bool, optional): Whether to scale the values for images or videos. See :ref:`range_and_dtype`. Default: ``False``.
      - Class: `Transform() -> 'None'`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Function: `_parse_labels_getter(labels_getter: 'Union[str, Callable[[Any], Optional[torch.Tensor]], None]') -> 'Callable[[Any], Optional[torch.Tensor]]'`
        - Description: No docstring available
      - Function: `_setup_number_or_seq(arg: 'Union[int, float, Sequence[Union[int, float]]]', name: 'str') -> 'Sequence[float]'`
        - Description: No docstring available
      - Function: `_setup_size(size, error_msg)`
        - Description: No docstring available
      - Function: `cast(typ, val)`
        - Description: Cast a value to a type. This returns the value unchanged. To the type checker this signals that the return value has the designated type, but at runtime we intentionally don't check anything (we want this to be as fast as possible).
      - Function: `get_bounding_boxes(flat_inputs: 'List[Any]') -> 'tv_tensors.BoundingBoxes'`
        - Description: No docstring available
      - Function: `has_any(flat_inputs: 'List[Any]', *types_or_checks: 'Union[Type, Callable[[Any], bool]]') -> 'bool'`
        - Description: No docstring available
      - Function: `is_pure_tensor(inpt: Any) -> bool`
        - Description: No docstring available
      - Function: `tree_flatten(pytree: Any) -> Tuple[List[Any], torch.utils._pytree.TreeSpec]`
        - Description: Flattens a pytree into a list of values and a TreeSpec that can be used to reconstruct the pytree.
      - Function: `tree_unflatten(values: List[Any], spec: torch.utils._pytree.TreeSpec) -> Any`
        - Description: Given a list of values and a TreeSpec, builds a pytree. This is the inverse operation of `tree_flatten`.
    - _temporal.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `Transform() -> 'None'`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `UniformTemporalSubsample(num_samples: int)`
        - Description: [BETA] Uniformly subsample ``num_samples`` indices from the temporal dimension of the video. .. v2betastatus:: UniformTemporalSubsample transform Videos are expected to be of shape ``[..., T, C, H, W]`` where ``T`` denotes the temporal dimension. When ``num_samples`` is larger than the size of temporal dimension of the video, it will sample frames based on nearest neighbor interpolation. Args: num_samples (int): The number of equispaced samples to be selected
    - _transform.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `Transform() -> 'None'`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Class: `_RandomApplyTransform(p: 'float' = 0.5) -> 'None'`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Function: `_get_kernel(functional, input_type, *, allow_passthrough=False)`
        - Description: No docstring available
      - Function: `_log_api_usage_once(obj: Any) -> None`
        - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
      - Function: `check_type(obj: 'Any', types_or_checks: 'Tuple[Union[Type, Callable[[Any], bool]], ...]') -> 'bool'`
        - Description: No docstring available
      - Function: `has_any(flat_inputs: 'List[Any]', *types_or_checks: 'Union[Type, Callable[[Any], bool]]') -> 'bool'`
        - Description: No docstring available
      - Function: `is_pure_tensor(inpt: Any) -> bool`
        - Description: No docstring available
      - Function: `tree_flatten(pytree: Any) -> Tuple[List[Any], torch.utils._pytree.TreeSpec]`
        - Description: Flattens a pytree into a list of values and a TreeSpec that can be used to reconstruct the pytree.
      - Function: `tree_unflatten(values: List[Any], spec: torch.utils._pytree.TreeSpec) -> Any`
        - Description: Given a list of values and a TreeSpec, builds a pytree. This is the inverse operation of `tree_flatten`.
    - _type_conversion.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Class: `PILToTensor() -> 'None'`
        - Description: [BETA] Convert a PIL Image to a tensor of the same type - this does not scale values. .. v2betastatus:: PILToTensor transform This transform does not support torchscript. Converts a PIL Image (H x W x C) to a Tensor of shape (C x H x W).
      - Class: `ToImage() -> 'None'`
        - Description: [BETA] Convert a tensor, ndarray, or PIL Image to :class:`~torchvision.tv_tensors.Image` ; this does not scale values. .. v2betastatus:: ToImage transform This transform does not support torchscript.
      - Class: `ToPILImage(mode: Optional[str] = None) -> None`
        - Description: [BETA] Convert a tensor or an ndarray to PIL Image .. v2betastatus:: ToPILImage transform This transform does not support torchscript. Converts a torch.*Tensor of shape C x H x W or a numpy ndarray of shape H x W x C to a PIL Image while adjusting the value range depending on the ``mode``. Args: mode (`PIL.Image mode`_): color space and pixel depth of input data (optional). If ``mode`` is ``None`` (default) there are some assumptions made about the input data: - If the input has 4 channels, the ``mode`` is assumed to be ``RGBA``. - If the input has 3 channels, the ``mode`` is assumed to be ``RGB``. - If the input has 2 channels, the ``mode`` is assumed to be ``LA``. - If the input has 1 channel, the ``mode`` is determined by the data type (i.e ``int``, ``float``, ``short``). .. _PIL.Image mode: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#concept-modes
      - Class: `ToPureTensor() -> 'None'`
        - Description: [BETA] Convert all tv_tensors to pure tensors, removing associated metadata (if any). .. v2betastatus:: ToPureTensor transform This doesn't scale or change the values, only the type.
      - Class: `Transform() -> 'None'`
        - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
      - Function: `is_pure_tensor(inpt: Any) -> bool`
        - Description: No docstring available
    - _utils.py
      - Class: `Any(*args, **kwargs)`
        - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
      - Function: `_check_fill_arg(fill: 'Union[_FillType, Dict[Union[Type, str], _FillType]]') -> 'None'`
        - Description: No docstring available
      - Function: `_check_padding_arg(padding: 'Union[int, Sequence[int]]') -> 'None'`
        - Description: No docstring available
      - Function: `_check_padding_mode_arg(padding_mode: "Literal['constant', 'edge', 'reflect', 'symmetric']") -> 'None'`
        - Description: No docstring available
      - Function: `_check_sequence_input(x, name, req_sizes)`
        - Description: No docstring available
      - Function: `_convert_fill_arg(fill: '_FillType') -> '_FillTypeJIT'`
        - Description: No docstring available
      - Function: `_find_labels_default_heuristic(inputs: 'Any') -> 'torch.Tensor'`
        - Description: This heuristic covers three cases: 1. The input is tuple or list whose second item is a labels tensor. This happens for already batched classification inputs for MixUp and CutMix (typically after the Dataloder). 2. The input is a tuple or list whose second item is a dictionary that contains the labels tensor under a label-like (see below) key. This happens for the inputs of detection models. 3. The input is a dictionary that is structured as the one from 2. What is "label-like" key? We first search for an case-insensitive match of 'labels' inside the keys of the dictionary. This is the name our detection models expect. If we can't find that, we look for a case-insensitive match of the term 'label' anywhere inside the key, i.e. 'FooLaBeLBar'. If we can't find that either, the dictionary contains no "label-like" key.
      - Function: `_get_fill(fill_dict, inpt_type)`
        - Description: No docstring available
      - Function: `_parse_labels_getter(labels_getter: 'Union[str, Callable[[Any], Optional[torch.Tensor]], None]') -> 'Callable[[Any], Optional[torch.Tensor]]'`
        - Description: No docstring available
      - Function: `_setup_angle(x, name, req_sizes=(2,))`
        - Description: No docstring available
      - Function: `_setup_fill_arg(fill: 'Union[_FillType, Dict[Union[Type, str], _FillType]]') -> 'Dict[Union[Type, str], _FillTypeJIT]'`
        - Description: No docstring available
      - Function: `_setup_number_or_seq(arg: 'Union[int, float, Sequence[Union[int, float]]]', name: 'str') -> 'Sequence[float]'`
        - Description: No docstring available
      - Function: `_setup_size(size, error_msg)`
        - Description: No docstring available
      - Function: `check_type(obj: 'Any', types_or_checks: 'Tuple[Union[Type, Callable[[Any], bool]], ...]') -> 'bool'`
        - Description: No docstring available
      - Function: `get_bounding_boxes(flat_inputs: 'List[Any]') -> 'tv_tensors.BoundingBoxes'`
        - Description: No docstring available
      - Function: `get_dimensions(inpt: torch.Tensor) -> List[int]`
        - Description: No docstring available
      - Function: `get_size(inpt: torch.Tensor) -> List[int]`
        - Description: No docstring available
      - Function: `has_all(flat_inputs: 'List[Any]', *types_or_checks: 'Union[Type, Callable[[Any], bool]]') -> 'bool'`
        - Description: No docstring available
      - Function: `has_any(flat_inputs: 'List[Any]', *types_or_checks: 'Union[Type, Callable[[Any], bool]]') -> 'bool'`
        - Description: No docstring available
      - Function: `is_pure_tensor(inpt: Any) -> bool`
        - Description: No docstring available
      - Function: `query_chw(flat_inputs: 'List[Any]') -> 'Tuple[int, int, int]'`
        - Description: No docstring available
      - Function: `query_size(flat_inputs: 'List[Any]') -> 'Tuple[int, int]'`
        - Description: No docstring available
      - Function: `sequence_to_str(seq: Sequence, separate_last: str = '') -> str`
        - Description: No docstring available
      - Class: `suppress(*exceptions)`
        - Description: Context manager to suppress specified exceptions After the exception is suppressed, execution proceeds with the next statement following the with statement. with suppress(FileNotFoundError): os.remove(somefile) # Execution still resumes here if the file was already removed
  - __pycache__/
  - autoaugment.py
    - Class: `AugMix(severity: int = 3, mixture_width: int = 3, chain_depth: int = -1, alpha: float = 1.0, all_ops: bool = True, interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, fill: Optional[List[float]] = None) -> None`
      - Description: AugMix data augmentation method based on `"AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty" <https://arxiv.org/abs/1912.02781>`_. If the image is torch Tensor, it should be of type torch.uint8, and it is expected to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "L" or "RGB". Args: severity (int): The severity of base augmentation operators. Default is ``3``. mixture_width (int): The number of augmentation chains. Default is ``3``. chain_depth (int): The depth of augmentation chains. A negative value denotes stochastic depth sampled from the interval [1, 3]. Default is ``-1``. alpha (float): The hyperparameter for the probability distributions. Default is ``1.0``. all_ops (bool): Use all operations (including brightness, contrast, color and sharpness). Default is ``True``. interpolation (InterpolationMode): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported. fill (sequence or number, optional): Pixel fill value for the area outside the transformed image. If given a number, the value is used for all bands respectively.
    - Class: `AutoAugment(policy: torchvision.transforms.autoaugment.AutoAugmentPolicy = <AutoAugmentPolicy.IMAGENET: 'imagenet'>, interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.NEAREST: 'nearest'>, fill: Optional[List[float]] = None) -> None`
      - Description: AutoAugment data augmentation method based on `"AutoAugment: Learning Augmentation Strategies from Data" <https://arxiv.org/pdf/1805.09501.pdf>`_. If the image is torch Tensor, it should be of type torch.uint8, and it is expected to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "L" or "RGB". Args: policy (AutoAugmentPolicy): Desired policy enum defined by :class:`torchvision.transforms.autoaugment.AutoAugmentPolicy`. Default is ``AutoAugmentPolicy.IMAGENET``. interpolation (InterpolationMode): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported. fill (sequence or number, optional): Pixel fill value for the area outside the transformed image. If given a number, the value is used for all bands respectively.
    - Class: `AutoAugmentPolicy(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: AutoAugment policies learned on different datasets. Available policies are IMAGENET, CIFAR10 and SVHN.
    - Class: `Enum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: Create a collection of name/value pairs. Example enumeration: >>> class Color(Enum): ... RED = 1 ... BLUE = 2 ... GREEN = 3 Access them by: - attribute access:: >>> Color.RED <Color.RED: 1> - value lookup: >>> Color(1) <Color.RED: 1> - name lookup: >>> Color['RED'] <Color.RED: 1> Enumerations can be iterated over, and know how many members they have: >>> len(Color) 3 >>> list(Color) [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>] Methods can be added to enumerations, and members can have their own attributes -- see the documentation for details.
    - Class: `InterpolationMode(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: Interpolation modes Available interpolation methods are ``nearest``, ``nearest-exact``, ``bilinear``, ``bicubic``, ``box``, ``hamming``, and ``lanczos``.
    - Class: `RandAugment(num_ops: int = 2, magnitude: int = 9, num_magnitude_bins: int = 31, interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.NEAREST: 'nearest'>, fill: Optional[List[float]] = None) -> None`
      - Description: RandAugment data augmentation method based on `"RandAugment: Practical automated data augmentation with a reduced search space" <https://arxiv.org/abs/1909.13719>`_. If the image is torch Tensor, it should be of type torch.uint8, and it is expected to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "L" or "RGB". Args: num_ops (int): Number of augmentation transformations to apply sequentially. magnitude (int): Magnitude for all the transformations. num_magnitude_bins (int): The number of different magnitude values. interpolation (InterpolationMode): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported. fill (sequence or number, optional): Pixel fill value for the area outside the transformed image. If given a number, the value is used for all bands respectively.
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `TrivialAugmentWide(num_magnitude_bins: int = 31, interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.NEAREST: 'nearest'>, fill: Optional[List[float]] = None) -> None`
      - Description: Dataset-independent data-augmentation with TrivialAugment Wide, as described in `"TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation" <https://arxiv.org/abs/2103.10158>`_. If the image is torch Tensor, it should be of type torch.uint8, and it is expected to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "L" or "RGB". Args: num_magnitude_bins (int): The number of different magnitude values. interpolation (InterpolationMode): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported. fill (sequence or number, optional): Pixel fill value for the area outside the transformed image. If given a number, the value is used for all bands respectively.
    - Function: `_apply_op(img: torch.Tensor, op_name: str, magnitude: float, interpolation: torchvision.transforms.functional.InterpolationMode, fill: Optional[List[float]])`
      - Description: No docstring available
  - functional.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Enum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: Create a collection of name/value pairs. Example enumeration: >>> class Color(Enum): ... RED = 1 ... BLUE = 2 ... GREEN = 3 Access them by: - attribute access:: >>> Color.RED <Color.RED: 1> - value lookup: >>> Color(1) <Color.RED: 1> - name lookup: >>> Color['RED'] <Color.RED: 1> Enumerations can be iterated over, and know how many members they have: >>> len(Color) 3 >>> list(Color) [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>] Methods can be added to enumerations, and members can have their own attributes -- see the documentation for details.
    - Class: `InterpolationMode(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: Interpolation modes Available interpolation methods are ``nearest``, ``nearest-exact``, ``bilinear``, ``bicubic``, ``box``, ``hamming``, and ``lanczos``.
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Function: `_check_antialias(img: torch.Tensor, antialias: Union[str, bool, NoneType], interpolation: torchvision.transforms.functional.InterpolationMode) -> Optional[bool]`
      - Description: No docstring available
    - Function: `_compute_resized_output_size(image_size: Tuple[int, int], size: List[int], max_size: Optional[int] = None) -> List[int]`
      - Description: No docstring available
    - Function: `_get_inverse_affine_matrix(center: List[float], angle: float, translate: List[float], scale: float, shear: List[float], inverted: bool = True) -> List[float]`
      - Description: No docstring available
    - Function: `_get_perspective_coeffs(startpoints: List[List[int]], endpoints: List[List[int]]) -> List[float]`
      - Description: Helper function to get the coefficients (a, b, c, d, e, f, g, h) for the perspective transforms. In Perspective Transform each pixel (x, y) in the original image gets transformed as, (x, y) -> ( (ax + by + c) / (gx + hy + 1), (dx + ey + f) / (gx + hy + 1) ) Args: startpoints (list of list of ints): List containing four lists of two integers corresponding to four corners ``[top-left, top-right, bottom-right, bottom-left]`` of the original image. endpoints (list of list of ints): List containing four lists of two integers corresponding to four corners ``[top-left, top-right, bottom-right, bottom-left]`` of the transformed image. Returns: octuple (a, b, c, d, e, f, g, h) for transforming each pixel.
    - Function: `_interpolation_modes_from_int(i: int) -> torchvision.transforms.functional.InterpolationMode`
      - Description: No docstring available
    - Function: `_is_numpy(img: Any) -> bool`
      - Description: No docstring available
    - Function: `_is_numpy_image(img: Any) -> bool`
      - Description: No docstring available
    - Function: `_is_pil_image(img: Any) -> bool`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `adjust_brightness(img: torch.Tensor, brightness_factor: float) -> torch.Tensor`
      - Description: Adjust brightness of an image. Args: img (PIL Image or Tensor): Image to be adjusted. If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format, where ... means it can have an arbitrary number of leading dimensions. brightness_factor (float): How much to adjust the brightness. Can be any non-negative number. 0 gives a black image, 1 gives the original image while 2 increases the brightness by a factor of 2. Returns: PIL Image or Tensor: Brightness adjusted image.
    - Function: `adjust_contrast(img: torch.Tensor, contrast_factor: float) -> torch.Tensor`
      - Description: Adjust contrast of an image. Args: img (PIL Image or Tensor): Image to be adjusted. If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format, where ... means it can have an arbitrary number of leading dimensions. contrast_factor (float): How much to adjust the contrast. Can be any non-negative number. 0 gives a solid gray image, 1 gives the original image while 2 increases the contrast by a factor of 2. Returns: PIL Image or Tensor: Contrast adjusted image.
    - Function: `adjust_gamma(img: torch.Tensor, gamma: float, gain: float = 1) -> torch.Tensor`
      - Description: Perform gamma correction on an image. Also known as Power Law Transform. Intensities in RGB mode are adjusted based on the following equation: .. math:: I_{\text{out}} = 255 \times \text{gain} \times \left(\frac{I_{\text{in}}}{255}\right)^{\gamma} See `Gamma Correction`_ for more details. .. _Gamma Correction: https://en.wikipedia.org/wiki/Gamma_correction Args: img (PIL Image or Tensor): PIL Image to be adjusted. If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format, where ... means it can have an arbitrary number of leading dimensions. If img is PIL Image, modes with transparency (alpha channel) are not supported. gamma (float): Non negative real number, same as :math:`\gamma` in the equation. gamma larger than 1 make the shadows darker, while gamma smaller than 1 make dark regions lighter. gain (float): The constant multiplier. Returns: PIL Image or Tensor: Gamma correction adjusted image.
    - Function: `adjust_hue(img: torch.Tensor, hue_factor: float) -> torch.Tensor`
      - Description: Adjust hue of an image. The image hue is adjusted by converting the image to HSV and cyclically shifting the intensities in the hue channel (H). The image is then converted back to original image mode. `hue_factor` is the amount of shift in H channel and must be in the interval `[-0.5, 0.5]`. See `Hue`_ for more details. .. _Hue: https://en.wikipedia.org/wiki/Hue Args: img (PIL Image or Tensor): Image to be adjusted. If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format, where ... means it can have an arbitrary number of leading dimensions. If img is PIL Image mode "1", "I", "F" and modes with transparency (alpha channel) are not supported. Note: the pixel values of the input image has to be non-negative for conversion to HSV space; thus it does not work if you normalize your image to an interval with negative values, or use an interpolation that generates negative values before using this function. hue_factor (float): How much to shift the hue channel. Should be in [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in HSV space in positive and negative direction respectively. 0 means no shift. Therefore, both -0.5 and 0.5 will give an image with complementary colors while 0 gives the original image. Returns: PIL Image or Tensor: Hue adjusted image.
    - Function: `adjust_saturation(img: torch.Tensor, saturation_factor: float) -> torch.Tensor`
      - Description: Adjust color saturation of an image. Args: img (PIL Image or Tensor): Image to be adjusted. If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format, where ... means it can have an arbitrary number of leading dimensions. saturation_factor (float): How much to adjust the saturation. 0 will give a black and white image, 1 will give the original image while 2 will enhance the saturation by a factor of 2. Returns: PIL Image or Tensor: Saturation adjusted image.
    - Function: `adjust_sharpness(img: torch.Tensor, sharpness_factor: float) -> torch.Tensor`
      - Description: Adjust the sharpness of an image. Args: img (PIL Image or Tensor): Image to be adjusted. If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format, where ... means it can have an arbitrary number of leading dimensions. sharpness_factor (float): How much to adjust the sharpness. Can be any non-negative number. 0 gives a blurred image, 1 gives the original image while 2 increases the sharpness by a factor of 2. Returns: PIL Image or Tensor: Sharpness adjusted image.
    - Function: `affine(img: torch.Tensor, angle: float, translate: List[int], scale: float, shear: List[float], interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.NEAREST: 'nearest'>, fill: Optional[List[float]] = None, center: Optional[List[int]] = None) -> torch.Tensor`
      - Description: Apply affine transformation on the image keeping image center invariant. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions. Args: img (PIL Image or Tensor): image to transform. angle (number): rotation angle in degrees between -180 and 180, clockwise direction. translate (sequence of integers): horizontal and vertical translations (post-rotation translation) scale (float): overall scale shear (float or sequence): shear angle value in degrees between -180 to 180, clockwise direction. If a sequence is specified, the first value corresponds to a shear parallel to the x-axis, while the second value corresponds to a shear parallel to the y-axis. interpolation (InterpolationMode): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. fill (sequence or number, optional): Pixel fill value for the area outside the transformed image. If given a number, the value is used for all bands respectively. .. note:: In torchscript mode single int/float value is not supported, please use a sequence of length 1: ``[value, ]``. center (sequence, optional): Optional center of rotation. Origin is the upper left corner. Default is the center of the image. Returns: PIL Image or Tensor: Transformed image.
    - Function: `autocontrast(img: torch.Tensor) -> torch.Tensor`
      - Description: Maximize contrast of an image by remapping its pixels per channel so that the lowest becomes black and the lightest becomes white. Args: img (PIL Image or Tensor): Image on which autocontrast is applied. If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format, where ... means it can have an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "L" or "RGB". Returns: PIL Image or Tensor: An image that was autocontrasted.
    - Function: `center_crop(img: torch.Tensor, output_size: List[int]) -> torch.Tensor`
      - Description: Crops the given image at the center. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions. If image size is smaller than output size along any edge, image is padded with 0 and then center cropped. Args: img (PIL Image or Tensor): Image to be cropped. output_size (sequence or int): (height, width) of the crop box. If int or sequence with single int, it is used for both directions. Returns: PIL Image or Tensor: Cropped image.
    - Function: `convert_image_dtype(image: torch.Tensor, dtype: torch.dtype = torch.float32) -> torch.Tensor`
      - Description: Convert a tensor image to the given ``dtype`` and scale the values accordingly This function does not support PIL Image. Args: image (torch.Tensor): Image to be converted dtype (torch.dtype): Desired data type of the output Returns: Tensor: Converted image .. note:: When converting from a smaller to a larger integer ``dtype`` the maximum values are **not** mapped exactly. If converted back and forth, this mismatch has no effect. Raises: RuntimeError: When trying to cast :class:`torch.float32` to :class:`torch.int32` or :class:`torch.int64` as well as for trying to cast :class:`torch.float64` to :class:`torch.int64`. These conversions might lead to overflow errors since the floating point ``dtype`` cannot store consecutive integers over the whole range of the integer ``dtype``.
    - Function: `crop(img: torch.Tensor, top: int, left: int, height: int, width: int) -> torch.Tensor`
      - Description: Crop the given image at specified location and output size. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions. If image size is smaller than output size along any edge, image is padded with 0 and then cropped. Args: img (PIL Image or Tensor): Image to be cropped. (0,0) denotes the top left corner of the image. top (int): Vertical component of the top left corner of the crop box. left (int): Horizontal component of the top left corner of the crop box. height (int): Height of the crop box. width (int): Width of the crop box. Returns: PIL Image or Tensor: Cropped image.
    - Function: `elastic_transform(img: torch.Tensor, displacement: torch.Tensor, interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, fill: Optional[List[float]] = None) -> torch.Tensor`
      - Description: Transform a tensor image with elastic transformations. Given alpha and sigma, it will generate displacement vectors for all pixels based on random offsets. Alpha controls the strength and sigma controls the smoothness of the displacements. The displacements are added to an identity grid and the resulting grid is used to grid_sample from the image. Applications: Randomly transforms the morphology of objects in images and produces a see-through-water-like effect. Args: img (PIL Image or Tensor): Image on which elastic_transform is applied. If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format, where ... means it can have an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "P", "L" or "RGB". displacement (Tensor): The displacement field. Expected shape is [1, H, W, 2]. interpolation (InterpolationMode): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. fill (number or str or tuple): Pixel fill value for constant fill. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. This value is only used when the padding_mode is constant.
    - Function: `equalize(img: torch.Tensor) -> torch.Tensor`
      - Description: Equalize the histogram of an image by applying a non-linear mapping to the input in order to create a uniform distribution of grayscale values in the output. Args: img (PIL Image or Tensor): Image on which equalize is applied. If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format, where ... means it can have an arbitrary number of leading dimensions. The tensor dtype must be ``torch.uint8`` and values are expected to be in ``[0, 255]``. If img is PIL Image, it is expected to be in mode "P", "L" or "RGB". Returns: PIL Image or Tensor: An image that was equalized.
    - Function: `erase(img: torch.Tensor, i: int, j: int, h: int, w: int, v: torch.Tensor, inplace: bool = False) -> torch.Tensor`
      - Description: Erase the input Tensor Image with given value. This transform does not support PIL Image. Args: img (Tensor Image): Tensor image of size (C, H, W) to be erased i (int): i in (i,j) i.e coordinates of the upper left corner. j (int): j in (i,j) i.e coordinates of the upper left corner. h (int): Height of the erased region. w (int): Width of the erased region. v: Erasing value. inplace(bool, optional): For in-place operations. By default, is set False. Returns: Tensor Image: Erased image.
    - Function: `five_crop(img: torch.Tensor, size: List[int]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]`
      - Description: Crop the given image into four corners and the central crop. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions .. Note:: This transform returns a tuple of images and there may be a mismatch in the number of inputs and targets your ``Dataset`` returns. Args: img (PIL Image or Tensor): Image to be cropped. size (sequence or int): Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]). Returns: tuple: tuple (tl, tr, bl, br, center) Corresponding top left, top right, bottom left, bottom right and center crop.
    - Function: `gaussian_blur(img: torch.Tensor, kernel_size: List[int], sigma: Optional[List[float]] = None) -> torch.Tensor`
      - Description: Performs Gaussian blurring on the image by given kernel. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions. Args: img (PIL Image or Tensor): Image to be blurred kernel_size (sequence of ints or int): Gaussian kernel size. Can be a sequence of integers like ``(kx, ky)`` or a single integer for square kernels. .. note:: In torchscript mode kernel_size as single int is not supported, use a sequence of length 1: ``[ksize, ]``. sigma (sequence of floats or float, optional): Gaussian kernel standard deviation. Can be a sequence of floats like ``(sigma_x, sigma_y)`` or a single float to define the same sigma in both X/Y directions. If None, then it is computed using ``kernel_size`` as ``sigma = 0.3 * ((kernel_size - 1) * 0.5 - 1) + 0.8``. Default, None. .. note:: In torchscript mode sigma as single float is not supported, use a sequence of length 1: ``[sigma, ]``. Returns: PIL Image or Tensor: Gaussian Blurred version of the image.
    - Function: `get_dimensions(img: torch.Tensor) -> List[int]`
      - Description: Returns the dimensions of an image as [channels, height, width]. Args: img (PIL Image or Tensor): The image to be checked. Returns: List[int]: The image dimensions.
    - Function: `get_image_num_channels(img: torch.Tensor) -> int`
      - Description: Returns the number of channels of an image. Args: img (PIL Image or Tensor): The image to be checked. Returns: int: The number of channels.
    - Function: `get_image_size(img: torch.Tensor) -> List[int]`
      - Description: Returns the size of an image as [width, height]. Args: img (PIL Image or Tensor): The image to be checked. Returns: List[int]: The image size.
    - Function: `hflip(img: torch.Tensor) -> torch.Tensor`
      - Description: Horizontally flip the given image. Args: img (PIL Image or Tensor): Image to be flipped. If img is a Tensor, it is expected to be in [..., H, W] format, where ... means it can have an arbitrary number of leading dimensions. Returns: PIL Image or Tensor: Horizontally flipped image.
    - Function: `invert(img: torch.Tensor) -> torch.Tensor`
      - Description: Invert the colors of an RGB/grayscale image. Args: img (PIL Image or Tensor): Image to have its colors inverted. If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format, where ... means it can have an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "L" or "RGB". Returns: PIL Image or Tensor: Color inverted image.
    - Function: `normalize(tensor: torch.Tensor, mean: List[float], std: List[float], inplace: bool = False) -> torch.Tensor`
      - Description: Normalize a float tensor image with mean and standard deviation. This transform does not support PIL Image. .. note:: This transform acts out of place by default, i.e., it does not mutates the input tensor. See :class:`~torchvision.transforms.Normalize` for more details. Args: tensor (Tensor): Float tensor image of size (C, H, W) or (B, C, H, W) to be normalized. mean (sequence): Sequence of means for each channel. std (sequence): Sequence of standard deviations for each channel. inplace(bool,optional): Bool to make this operation inplace. Returns: Tensor: Normalized Tensor image.
    - Function: `pad(img: torch.Tensor, padding: List[int], fill: Union[int, float] = 0, padding_mode: str = 'constant') -> torch.Tensor`
      - Description: Pad the given image on all sides with the given "pad" value. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means at most 2 leading dimensions for mode reflect and symmetric, at most 3 leading dimensions for mode edge, and an arbitrary number of leading dimensions for mode constant Args: img (PIL Image or Tensor): Image to be padded. padding (int or sequence): Padding on each border. If a single int is provided this is used to pad all borders. If sequence of length 2 is provided this is the padding on left/right and top/bottom respectively. If a sequence of length 4 is provided this is the padding for the left, top, right and bottom borders respectively. .. note:: In torchscript mode padding as single int is not supported, use a sequence of length 1: ``[padding, ]``. fill (number or tuple): Pixel fill value for constant fill. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. This value is only used when the padding_mode is constant. Only number is supported for torch Tensor. Only int or tuple value is supported for PIL Image. padding_mode (str): Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant. - constant: pads with a constant value, this value is specified with fill - edge: pads with the last value at the edge of the image. If input a 5D torch Tensor, the last 3 dimensions will be padded instead of the last 2 - reflect: pads with reflection of image without repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2] - symmetric: pads with reflection of image repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3] Returns: PIL Image or Tensor: Padded image.
    - Function: `perspective(img: torch.Tensor, startpoints: List[List[int]], endpoints: List[List[int]], interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, fill: Optional[List[float]] = None) -> torch.Tensor`
      - Description: Perform perspective transform of the given image. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions. Args: img (PIL Image or Tensor): Image to be transformed. startpoints (list of list of ints): List containing four lists of two integers corresponding to four corners ``[top-left, top-right, bottom-right, bottom-left]`` of the original image. endpoints (list of list of ints): List containing four lists of two integers corresponding to four corners ``[top-left, top-right, bottom-right, bottom-left]`` of the transformed image. interpolation (InterpolationMode): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. fill (sequence or number, optional): Pixel fill value for the area outside the transformed image. If given a number, the value is used for all bands respectively. .. note:: In torchscript mode single int/float value is not supported, please use a sequence of length 1: ``[value, ]``. Returns: PIL Image or Tensor: transformed Image.
    - Function: `pil_to_tensor(pic: Any) -> torch.Tensor`
      - Description: Convert a ``PIL Image`` to a tensor of the same type. This function does not support torchscript. See :class:`~torchvision.transforms.PILToTensor` for more details. .. note:: A deep copy of the underlying array is performed. Args: pic (PIL Image): Image to be converted to tensor. Returns: Tensor: Converted image.
    - Function: `posterize(img: torch.Tensor, bits: int) -> torch.Tensor`
      - Description: Posterize an image by reducing the number of bits for each color channel. Args: img (PIL Image or Tensor): Image to have its colors posterized. If img is torch Tensor, it should be of type torch.uint8, and it is expected to be in [..., 1 or 3, H, W] format, where ... means it can have an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "L" or "RGB". bits (int): The number of bits to keep for each channel (0-8). Returns: PIL Image or Tensor: Posterized image.
    - Function: `resize(img: torch.Tensor, size: List[int], interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, max_size: Optional[int] = None, antialias: Union[str, bool, NoneType] = 'warn') -> torch.Tensor`
      - Description: Resize the input image to the given size. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions .. warning:: The output image might be different depending on its type: when downsampling, the interpolation of PIL images and tensors is slightly different, because PIL applies antialiasing. This may lead to significant differences in the performance of a network. Therefore, it is preferable to train and serve a model with the same input types. See also below the ``antialias`` parameter, which can help making the output of PIL images and tensors closer. Args: img (PIL Image or Tensor): Image to be resized. size (sequence or int): Desired output size. If size is a sequence like (h, w), the output size will be matched to this. If size is an int, the smaller edge of the image will be matched to this number maintaining the aspect ratio. i.e, if height > width, then image will be rescaled to :math:`\left(\text{size} \times \frac{\text{height}}{\text{width}}, \text{size}\right)`. .. note:: In torchscript mode size as single int is not supported, use a sequence of length 1: ``[size, ]``. interpolation (InterpolationMode): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.NEAREST_EXACT``, ``InterpolationMode.BILINEAR`` and ``InterpolationMode.BICUBIC`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. max_size (int, optional): The maximum allowed for the longer edge of the resized image. If the longer edge of the image is greater than ``max_size`` after being resized according to ``size``, ``size`` will be overruled so that the longer edge is equal to ``max_size``. As a result, the smaller edge may be shorter than ``size``. This is only supported if ``size`` is an int (or a sequence of length 1 in torchscript mode). antialias (bool, optional): Whether to apply antialiasing. It only affects **tensors** with bilinear or bicubic modes and it is ignored otherwise: on PIL images, antialiasing is always applied on bilinear or bicubic modes; on other modes (for PIL images and tensors), antialiasing makes no sense and this parameter is ignored. Possible values are: - ``True``: will apply antialiasing for bilinear or bicubic modes. Other mode aren't affected. This is probably what you want to use. - ``False``: will not apply antialiasing for tensors on any mode. PIL images are still antialiased on bilinear or bicubic modes, because PIL doesn't support no antialias. - ``None``: equivalent to ``False`` for tensors and ``True`` for PIL images. This value exists for legacy reasons and you probably don't want to use it unless you really know what you are doing. The current default is ``None`` **but will change to** ``True`` **in v0.17** for the PIL and Tensor backends to be consistent. Returns: PIL Image or Tensor: Resized image.
    - Function: `resized_crop(img: torch.Tensor, top: int, left: int, height: int, width: int, size: List[int], interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> torch.Tensor`
      - Description: Crop the given image and resize it to desired size. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions Notably used in :class:`~torchvision.transforms.RandomResizedCrop`. Args: img (PIL Image or Tensor): Image to be cropped. (0,0) denotes the top left corner of the image. top (int): Vertical component of the top left corner of the crop box. left (int): Horizontal component of the top left corner of the crop box. height (int): Height of the crop box. width (int): Width of the crop box. size (sequence or int): Desired output size. Same semantics as ``resize``. interpolation (InterpolationMode): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.NEAREST_EXACT``, ``InterpolationMode.BILINEAR`` and ``InterpolationMode.BICUBIC`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. antialias (bool, optional): Whether to apply antialiasing. It only affects **tensors** with bilinear or bicubic modes and it is ignored otherwise: on PIL images, antialiasing is always applied on bilinear or bicubic modes; on other modes (for PIL images and tensors), antialiasing makes no sense and this parameter is ignored. Possible values are: - ``True``: will apply antialiasing for bilinear or bicubic modes. Other mode aren't affected. This is probably what you want to use. - ``False``: will not apply antialiasing for tensors on any mode. PIL images are still antialiased on bilinear or bicubic modes, because PIL doesn't support no antialias. - ``None``: equivalent to ``False`` for tensors and ``True`` for PIL images. This value exists for legacy reasons and you probably don't want to use it unless you really know what you are doing. The current default is ``None`` **but will change to** ``True`` **in v0.17** for the PIL and Tensor backends to be consistent. Returns: PIL Image or Tensor: Cropped image.
    - Function: `rgb_to_grayscale(img: torch.Tensor, num_output_channels: int = 1) -> torch.Tensor`
      - Description: Convert RGB image to grayscale version of image. If the image is torch Tensor, it is expected to have [..., 3, H, W] shape, where ... means an arbitrary number of leading dimensions Note: Please, note that this method supports only RGB images as input. For inputs in other color spaces, please, consider using meth:`~torchvision.transforms.functional.to_grayscale` with PIL Image. Args: img (PIL Image or Tensor): RGB Image to be converted to grayscale. num_output_channels (int): number of channels of the output image. Value can be 1 or 3. Default, 1. Returns: PIL Image or Tensor: Grayscale version of the image. - if num_output_channels = 1 : returned image is single channel - if num_output_channels = 3 : returned image is 3 channel with r = g = b
    - Function: `rotate(img: torch.Tensor, angle: float, interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.NEAREST: 'nearest'>, expand: bool = False, center: Optional[List[int]] = None, fill: Optional[List[float]] = None) -> torch.Tensor`
      - Description: Rotate the image by angle. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions. Args: img (PIL Image or Tensor): image to be rotated. angle (number): rotation angle value in degrees, counter-clockwise. interpolation (InterpolationMode): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. expand (bool, optional): Optional expansion flag. If true, expands the output image to make it large enough to hold the entire rotated image. If false or omitted, make the output image the same size as the input image. Note that the expand flag assumes rotation around the center and no translation. center (sequence, optional): Optional center of rotation. Origin is the upper left corner. Default is the center of the image. fill (sequence or number, optional): Pixel fill value for the area outside the transformed image. If given a number, the value is used for all bands respectively. .. note:: In torchscript mode single int/float value is not supported, please use a sequence of length 1: ``[value, ]``. Returns: PIL Image or Tensor: Rotated image. .. _filters: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#filters
    - Function: `solarize(img: torch.Tensor, threshold: float) -> torch.Tensor`
      - Description: Solarize an RGB/grayscale image by inverting all pixel values above a threshold. Args: img (PIL Image or Tensor): Image to have its colors inverted. If img is torch Tensor, it is expected to be in [..., 1 or 3, H, W] format, where ... means it can have an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "L" or "RGB". threshold (float): All pixels equal or above this value are inverted. Returns: PIL Image or Tensor: Solarized image.
    - Function: `ten_crop(img: torch.Tensor, size: List[int], vertical_flip: bool = False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]`
      - Description: Generate ten cropped images from the given image. Crop the given image into four corners and the central crop plus the flipped version of these (horizontal flipping is used by default). If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions .. Note:: This transform returns a tuple of images and there may be a mismatch in the number of inputs and targets your ``Dataset`` returns. Args: img (PIL Image or Tensor): Image to be cropped. size (sequence or int): Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]). vertical_flip (bool): Use vertical flipping instead of horizontal Returns: tuple: tuple (tl, tr, bl, br, center, tl_flip, tr_flip, bl_flip, br_flip, center_flip) Corresponding top left, top right, bottom left, bottom right and center crop and same for the flipped image.
    - Function: `to_grayscale(img, num_output_channels=1)`
      - Description: Convert PIL image of any mode (RGB, HSV, LAB, etc) to grayscale version of image. This transform does not support torch Tensor. Args: img (PIL Image): PIL Image to be converted to grayscale. num_output_channels (int): number of channels of the output image. Value can be 1 or 3. Default is 1. Returns: PIL Image: Grayscale version of the image. - if num_output_channels = 1 : returned image is single channel - if num_output_channels = 3 : returned image is 3 channel with r = g = b
    - Function: `to_pil_image(pic, mode=None)`
      - Description: Convert a tensor or an ndarray to PIL Image. This function does not support torchscript. See :class:`~torchvision.transforms.ToPILImage` for more details. Args: pic (Tensor or numpy.ndarray): Image to be converted to PIL Image. mode (`PIL.Image mode`_): color space and pixel depth of input data (optional). .. _PIL.Image mode: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#concept-modes Returns: PIL Image: Image converted to PIL Image.
    - Function: `to_tensor(pic) -> torch.Tensor`
      - Description: Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor. This function does not support torchscript. See :class:`~torchvision.transforms.ToTensor` for more details. Args: pic (PIL Image or numpy.ndarray): Image to be converted to tensor. Returns: Tensor: Converted image.
    - Function: `vflip(img: torch.Tensor) -> torch.Tensor`
      - Description: Vertically flip the given image. Args: img (PIL Image or Tensor): Image to be flipped. If img is a Tensor, it is expected to be in [..., H, W] format, where ... means it can have an arbitrary number of leading dimensions. Returns: PIL Image or Tensor: Vertically flipped image.
  - functional_pil.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Function: `adjust_brightness(img: PIL.Image.Image, brightness_factor: float) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `adjust_contrast(img: PIL.Image.Image, contrast_factor: float) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `adjust_gamma(img: PIL.Image.Image, gamma: float, gain: float = 1.0) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `adjust_hue(img: PIL.Image.Image, hue_factor: float) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `adjust_saturation(img: PIL.Image.Image, saturation_factor: float) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `adjust_sharpness(img: PIL.Image.Image, sharpness_factor: float) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `affine(img: PIL.Image.Image, matrix: List[float], interpolation: int = 0, fill: Union[int, float, Sequence[int], Sequence[float], NoneType] = None) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `autocontrast(img: PIL.Image.Image) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `crop(img: PIL.Image.Image, top: int, left: int, height: int, width: int) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `equalize(img: PIL.Image.Image) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `get_dimensions(img: Any) -> List[int]`
      - Description: No docstring available
    - Function: `get_image_num_channels(img: Any) -> int`
      - Description: No docstring available
    - Function: `get_image_size(img: Any) -> List[int]`
      - Description: No docstring available
    - Function: `hflip(img: PIL.Image.Image) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `invert(img: PIL.Image.Image) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `pad(img: PIL.Image.Image, padding: Union[int, List[int], Tuple[int, ...]], fill: Union[float, List[float], Tuple[float, ...], NoneType] = 0, padding_mode: Literal['constant', 'edge', 'reflect', 'symmetric'] = 'constant') -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `perspective(img: PIL.Image.Image, perspective_coeffs: List[float], interpolation: int = 3, fill: Union[int, float, Sequence[int], Sequence[float], NoneType] = None) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `posterize(img: PIL.Image.Image, bits: int) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `resize(img: PIL.Image.Image, size: Union[List[int], int], interpolation: int = 2) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `rotate(img: PIL.Image.Image, angle: float, interpolation: int = 0, expand: bool = False, center: Optional[Tuple[int, int]] = None, fill: Union[int, float, Sequence[int], Sequence[float], NoneType] = None) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `solarize(img: PIL.Image.Image, threshold: int) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `to_grayscale(img: PIL.Image.Image, num_output_channels: int) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `vflip(img: PIL.Image.Image) -> PIL.Image.Image`
      - Description: No docstring available
  - functional_tensor.py
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Function: `adjust_brightness(img: torch.Tensor, brightness_factor: float) -> torch.Tensor`
      - Description: No docstring available
    - Function: `adjust_contrast(img: torch.Tensor, contrast_factor: float) -> torch.Tensor`
      - Description: No docstring available
    - Function: `adjust_gamma(img: torch.Tensor, gamma: float, gain: float = 1) -> torch.Tensor`
      - Description: No docstring available
    - Function: `adjust_hue(img: torch.Tensor, hue_factor: float) -> torch.Tensor`
      - Description: No docstring available
    - Function: `adjust_saturation(img: torch.Tensor, saturation_factor: float) -> torch.Tensor`
      - Description: No docstring available
    - Function: `adjust_sharpness(img: torch.Tensor, sharpness_factor: float) -> torch.Tensor`
      - Description: No docstring available
    - Function: `affine(img: torch.Tensor, matrix: List[float], interpolation: str = 'nearest', fill: Union[int, float, List[float], NoneType] = None) -> torch.Tensor`
      - Description: No docstring available
    - Function: `autocontrast(img: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `convert_image_dtype(image: torch.Tensor, dtype: torch.dtype = torch.float32) -> torch.Tensor`
      - Description: No docstring available
    - Function: `crop(img: torch.Tensor, top: int, left: int, height: int, width: int) -> torch.Tensor`
      - Description: No docstring available
    - Function: `elastic_transform(img: torch.Tensor, displacement: torch.Tensor, interpolation: str = 'bilinear', fill: Union[int, float, List[float], NoneType] = None) -> torch.Tensor`
      - Description: No docstring available
    - Function: `equalize(img: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `erase(img: torch.Tensor, i: int, j: int, h: int, w: int, v: torch.Tensor, inplace: bool = False) -> torch.Tensor`
      - Description: No docstring available
    - Function: `gaussian_blur(img: torch.Tensor, kernel_size: List[int], sigma: List[float]) -> torch.Tensor`
      - Description: No docstring available
    - Function: `get_dimensions(img: torch.Tensor) -> List[int]`
      - Description: No docstring available
    - Function: `get_image_num_channels(img: torch.Tensor) -> int`
      - Description: No docstring available
    - Function: `get_image_size(img: torch.Tensor) -> List[int]`
      - Description: No docstring available
    - Function: `grid_sample(input: torch.Tensor, grid: torch.Tensor, mode: str = 'bilinear', padding_mode: str = 'zeros', align_corners: Optional[bool] = None) -> torch.Tensor`
      - Description: Given an :attr:`input` and a flow-field :attr:`grid`, computes the ``output`` using :attr:`input` values and pixel locations from :attr:`grid`. Currently, only spatial (4-D) and volumetric (5-D) :attr:`input` are supported. In the spatial (4-D) case, for :attr:`input` with shape :math:`(N, C, H_\text{in}, W_\text{in})` and :attr:`grid` with shape :math:`(N, H_\text{out}, W_\text{out}, 2)`, the output will have shape :math:`(N, C, H_\text{out}, W_\text{out})`. For each output location ``output[n, :, h, w]``, the size-2 vector ``grid[n, h, w]`` specifies :attr:`input` pixel locations ``x`` and ``y``, which are used to interpolate the output value ``output[n, :, h, w]``. In the case of 5D inputs, ``grid[n, d, h, w]`` specifies the ``x``, ``y``, ``z`` pixel locations for interpolating ``output[n, :, d, h, w]``. :attr:`mode` argument specifies ``nearest`` or ``bilinear`` interpolation method to sample the input pixels. :attr:`grid` specifies the sampling pixel locations normalized by the :attr:`input` spatial dimensions. Therefore, it should have most values in the range of ``[-1, 1]``. For example, values ``x = -1, y = -1`` is the left-top pixel of :attr:`input`, and values ``x = 1, y = 1`` is the right-bottom pixel of :attr:`input`. If :attr:`grid` has values outside the range of ``[-1, 1]``, the corresponding outputs are handled as defined by :attr:`padding_mode`. Options are * ``padding_mode="zeros"``: use ``0`` for out-of-bound grid locations, * ``padding_mode="border"``: use border values for out-of-bound grid locations, * ``padding_mode="reflection"``: use values at locations reflected by the border for out-of-bound grid locations. For location far away from the border, it will keep being reflected until becoming in bound, e.g., (normalized) pixel location ``x = -3.5`` reflects by border ``-1`` and becomes ``x' = 1.5``, then reflects by border ``1`` and becomes ``x'' = -0.5``. Note: This function is often used in conjunction with :func:`affine_grid` to build `Spatial Transformer Networks`_ . Note: When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on :doc:`/notes/randomness` for background. Note: NaN values in :attr:`grid` would be interpreted as ``-1``. Args: input (Tensor): input of shape :math:`(N, C, H_\text{in}, W_\text{in})` (4-D case) or :math:`(N, C, D_\text{in}, H_\text{in}, W_\text{in})` (5-D case) grid (Tensor): flow-field of shape :math:`(N, H_\text{out}, W_\text{out}, 2)` (4-D case) or :math:`(N, D_\text{out}, H_\text{out}, W_\text{out}, 3)` (5-D case) mode (str): interpolation mode to calculate output values ``'bilinear'`` | ``'nearest'`` | ``'bicubic'``. Default: ``'bilinear'`` Note: ``mode='bicubic'`` supports only 4-D input. When ``mode='bilinear'`` and the input is 5-D, the interpolation mode used internally will actually be trilinear. However, when the input is 4-D, the interpolation mode will legitimately be bilinear. padding_mode (str): padding mode for outside grid values ``'zeros'`` | ``'border'`` | ``'reflection'``. Default: ``'zeros'`` align_corners (bool, optional): Geometrically, we consider the pixels of the input as squares rather than points. If set to ``True``, the extrema (``-1`` and ``1``) are considered as referring to the center points of the input's corner pixels. If set to ``False``, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic. This option parallels the ``align_corners`` option in :func:`interpolate`, and so whichever option is used here should also be used there to resize the input image before grid sampling. Default: ``False`` Returns: output (Tensor): output Tensor .. _`Spatial Transformer Networks`: https://arxiv.org/abs/1506.02025 .. warning:: When ``align_corners = True``, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by :func:`grid_sample` will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was ``align_corners = True``. Since then, the default behavior has been changed to ``align_corners = False``, in order to bring it in line with the default for :func:`interpolate`. .. note:: ``mode='bicubic'`` is implemented using the `cubic convolution algorithm`_ with :math:`\alpha=-0.75`. The constant :math:`\alpha` might be different from packages to packages. For example, `PIL`_ and `OpenCV`_ use -0.5 and -0.75 respectively. This algorithm may "overshoot" the range of values it's interpolating. For example, it may produce negative values or values greater than 255 when interpolating input in [0, 255]. Clamp the results with :func:`torch.clamp` to ensure they are within the valid range. .. _`cubic convolution algorithm`: https://en.wikipedia.org/wiki/Bicubic_interpolation .. _`PIL`: https://github.com/python-pillow/Pillow/blob/4634eafe3c695a014267eefdce830b4a825beed7/src/libImaging/Resample.c#L51 .. _`OpenCV`: https://github.com/opencv/opencv/blob/f345ed564a06178670750bad59526cfa4033be55/modules/imgproc/src/resize.cpp#L908
    - Function: `hflip(img: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `interpolate(input: torch.Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> torch.Tensor`
      - Description: Down/up samples the input to either the given :attr:`size` or the given :attr:`scale_factor` The algorithm used for interpolation is determined by :attr:`mode`. Currently temporal, spatial and volumetric sampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape. The input dimensions are interpreted in the form: `mini-batch x channels x [optional depth] x [optional height] x width`. The modes available for resizing are: `nearest`, `linear` (3D-only), `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only), `area`, `nearest-exact` Args: input (Tensor): the input tensor size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]): output spatial size. scale_factor (float or Tuple[float]): multiplier for spatial size. If `scale_factor` is a tuple, its length has to match the number of spatial dimensions; `input.dim() - 2`. mode (str): algorithm used for upsampling: ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` | ``'trilinear'`` | ``'area'`` | ``'nearest-exact'``. Default: ``'nearest'`` align_corners (bool, optional): Geometrically, we consider the pixels of the input and output as squares rather than points. If set to ``True``, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to ``False``, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation *independent* of input size when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode` is ``'linear'``, ``'bilinear'``, ``'bicubic'`` or ``'trilinear'``. Default: ``False`` recompute_scale_factor (bool, optional): recompute the scale_factor for use in the interpolation calculation. If `recompute_scale_factor` is ``True``, then `scale_factor` must be passed in and `scale_factor` is used to compute the output `size`. The computed output `size` will be used to infer new scales for the interpolation. Note that when `scale_factor` is floating-point, it may differ from the recomputed `scale_factor` due to rounding and precision issues. If `recompute_scale_factor` is ``False``, then `size` or `scale_factor` will be used directly for interpolation. Default: ``None``. antialias (bool, optional): flag to apply anti-aliasing. Default: ``False``. Using anti-alias option together with ``align_corners=False``, interpolation result would match Pillow result for downsampling operation. Supported modes: ``'bilinear'``, ``'bicubic'``. .. note:: With ``mode='bicubic'``, it's possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call ``result.clamp(min=0, max=255)`` if you want to reduce the overshoot when displaying the image. .. note:: Mode ``mode='nearest-exact'`` matches Scikit-Image and PIL nearest neighbours interpolation algorithms and fixes known issues with ``mode='nearest'``. This mode is introduced to keep backward compatibility. Mode ``mode='nearest'`` matches buggy OpenCV's ``INTER_NEAREST`` interpolation algorithm. .. note:: The gradients for the dtype ``float16`` on CUDA may be inaccurate in the upsample operation when using modes ``['linear', 'bilinear', 'bicubic', 'trilinear', 'area']``. For more details, please refer to the discussion in `issue#104157 <https://github.com/pytorch/pytorch/issues/104157>`_. Note: This operation may produce nondeterministic gradients when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.
    - Function: `invert(img: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `normalize(tensor: torch.Tensor, mean: List[float], std: List[float], inplace: bool = False) -> torch.Tensor`
      - Description: No docstring available
    - Function: `pad(img: torch.Tensor, padding: Union[int, List[int]], fill: Union[int, float, NoneType] = 0, padding_mode: str = 'constant') -> torch.Tensor`
      - Description: No docstring available
    - Function: `perspective(img: torch.Tensor, perspective_coeffs: List[float], interpolation: str = 'bilinear', fill: Union[int, float, List[float], NoneType] = None) -> torch.Tensor`
      - Description: No docstring available
    - Function: `posterize(img: torch.Tensor, bits: int) -> torch.Tensor`
      - Description: No docstring available
    - Function: `resize(img: torch.Tensor, size: List[int], interpolation: str = 'bilinear', antialias: Optional[bool] = None) -> torch.Tensor`
      - Description: No docstring available
    - Function: `rgb_to_grayscale(img: torch.Tensor, num_output_channels: int = 1) -> torch.Tensor`
      - Description: No docstring available
    - Function: `rotate(img: torch.Tensor, matrix: List[float], interpolation: str = 'nearest', expand: bool = False, fill: Union[int, float, List[float], NoneType] = None) -> torch.Tensor`
      - Description: No docstring available
    - Function: `solarize(img: torch.Tensor, threshold: float) -> torch.Tensor`
      - Description: No docstring available
    - Function: `vflip(img: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
  - transforms.py
    - Class: `CenterCrop(size)`
      - Description: Crops the given image at the center. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions. If image size is smaller than output size along any edge, image is padded with 0 and then center cropped. Args: size (sequence or int): Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).
    - Class: `ColorJitter(brightness: Union[float, Tuple[float, float]] = 0, contrast: Union[float, Tuple[float, float]] = 0, saturation: Union[float, Tuple[float, float]] = 0, hue: Union[float, Tuple[float, float]] = 0) -> None`
      - Description: Randomly change the brightness, contrast, saturation and hue of an image. If the image is torch Tensor, it is expected to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions. If img is PIL Image, mode "1", "I", "F" and modes with transparency (alpha channel) are not supported. Args: brightness (float or tuple of float (min, max)): How much to jitter brightness. brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] or the given [min, max]. Should be non negative numbers. contrast (float or tuple of float (min, max)): How much to jitter contrast. contrast_factor is chosen uniformly from [max(0, 1 - contrast), 1 + contrast] or the given [min, max]. Should be non-negative numbers. saturation (float or tuple of float (min, max)): How much to jitter saturation. saturation_factor is chosen uniformly from [max(0, 1 - saturation), 1 + saturation] or the given [min, max]. Should be non negative numbers. hue (float or tuple of float (min, max)): How much to jitter hue. hue_factor is chosen uniformly from [-hue, hue] or the given [min, max]. Should have 0<= hue <= 0.5 or -0.5 <= min <= max <= 0.5. To jitter hue, the pixel values of the input image has to be non-negative for conversion to HSV space; thus it does not work if you normalize your image to an interval with negative values, or use an interpolation that generates negative values before using this function.
    - Class: `Compose(transforms)`
      - Description: Composes several transforms together. This transform does not support torchscript. Please, see the note below. Args: transforms (list of ``Transform`` objects): list of transforms to compose. Example: >>> transforms.Compose([ >>> transforms.CenterCrop(10), >>> transforms.PILToTensor(), >>> transforms.ConvertImageDtype(torch.float), >>> ]) .. note:: In order to script the transformations, please use ``torch.nn.Sequential`` as below. >>> transforms = torch.nn.Sequential( >>> transforms.CenterCrop(10), >>> transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), >>> ) >>> scripted_transforms = torch.jit.script(transforms) Make sure to use only scriptable transformations, i.e. that work with ``torch.Tensor``, does not require `lambda` functions or ``PIL.Image``.
    - Class: `ConvertImageDtype(dtype: torch.dtype) -> None`
      - Description: Convert a tensor image to the given ``dtype`` and scale the values accordingly. This function does not support PIL Image. Args: dtype (torch.dtype): Desired data type of the output .. note:: When converting from a smaller to a larger integer ``dtype`` the maximum values are **not** mapped exactly. If converted back and forth, this mismatch has no effect. Raises: RuntimeError: When trying to cast :class:`torch.float32` to :class:`torch.int32` or :class:`torch.int64` as well as for trying to cast :class:`torch.float64` to :class:`torch.int64`. These conversions might lead to overflow errors since the floating point ``dtype`` cannot store consecutive integers over the whole range of the integer ``dtype``.
    - Class: `ElasticTransform(alpha=50.0, sigma=5.0, interpolation=<InterpolationMode.BILINEAR: 'bilinear'>, fill=0)`
      - Description: Transform a tensor image with elastic transformations. Given alpha and sigma, it will generate displacement vectors for all pixels based on random offsets. Alpha controls the strength and sigma controls the smoothness of the displacements. The displacements are added to an identity grid and the resulting grid is used to grid_sample from the image. Applications: Randomly transforms the morphology of objects in images and produces a see-through-water-like effect. Args: alpha (float or sequence of floats): Magnitude of displacements. Default is 50.0. sigma (float or sequence of floats): Smoothness of displacements. Default is 5.0. interpolation (InterpolationMode): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. fill (sequence or number): Pixel fill value for the area outside the transformed image. Default is ``0``. If given a number, the value is used for all bands respectively.
    - Class: `FiveCrop(size)`
      - Description: Crop the given image into four corners and the central crop. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions .. Note:: This transform returns a tuple of images and there may be a mismatch in the number of inputs and targets your Dataset returns. See below for an example of how to deal with this. Args: size (sequence or int): Desired output size of the crop. If size is an ``int`` instead of sequence like (h, w), a square crop of size (size, size) is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]). Example: >>> transform = Compose([ >>> FiveCrop(size), # this is a list of PIL Images >>> Lambda(lambda crops: torch.stack([PILToTensor()(crop) for crop in crops])) # returns a 4D tensor >>> ]) >>> #In your test loop you can do the following: >>> input, target = batch # input is a 5d tensor, target is 2d >>> bs, ncrops, c, h, w = input.size() >>> result = model(input.view(-1, c, h, w)) # fuse batch size and ncrops >>> result_avg = result.view(bs, ncrops, -1).mean(1) # avg over crops
    - Class: `GaussianBlur(kernel_size, sigma=(0.1, 2.0))`
      - Description: Blurs image with randomly chosen Gaussian blur. If the image is torch Tensor, it is expected to have [..., C, H, W] shape, where ... means an arbitrary number of leading dimensions. Args: kernel_size (int or sequence): Size of the Gaussian kernel. sigma (float or tuple of float (min, max)): Standard deviation to be used for creating kernel to perform blurring. If float, sigma is fixed. If it is tuple of float (min, max), sigma is chosen uniformly at random to lie in the given range. Returns: PIL Image or Tensor: Gaussian blurred version of the input image.
    - Class: `Grayscale(num_output_channels=1)`
      - Description: Convert image to grayscale. If the image is torch Tensor, it is expected to have [..., 3, H, W] shape, where ... means an arbitrary number of leading dimensions Args: num_output_channels (int): (1 or 3) number of channels desired for output image Returns: PIL Image: Grayscale version of the input. - If ``num_output_channels == 1`` : returned image is single channel - If ``num_output_channels == 3`` : returned image is 3 channel with r == g == b
    - Class: `InterpolationMode(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: Interpolation modes Available interpolation methods are ``nearest``, ``nearest-exact``, ``bilinear``, ``bicubic``, ``box``, ``hamming``, and ``lanczos``.
    - Class: `Lambda(lambd)`
      - Description: Apply a user-defined lambda as a transform. This transform does not support torchscript. Args: lambd (function): Lambda/function to be used for transform.
    - Class: `LinearTransformation(transformation_matrix, mean_vector)`
      - Description: Transform a tensor image with a square transformation matrix and a mean_vector computed offline. This transform does not support PIL Image. Given transformation_matrix and mean_vector, will flatten the torch.*Tensor and subtract mean_vector from it which is then followed by computing the dot product with the transformation matrix and then reshaping the tensor to its original shape. Applications: whitening transformation: Suppose X is a column vector zero-centered data. Then compute the data covariance matrix [D x D] with torch.mm(X.t(), X), perform SVD on this matrix and pass it as transformation_matrix. Args: transformation_matrix (Tensor): tensor [D x D], D = C x H x W mean_vector (Tensor): tensor [D], D = C x H x W
    - Class: `Normalize(mean, std, inplace=False)`
      - Description: Normalize a tensor image with mean and standard deviation. This transform does not support PIL Image. Given mean: ``(mean[1],...,mean[n])`` and std: ``(std[1],..,std[n])`` for ``n`` channels, this transform will normalize each channel of the input ``torch.*Tensor`` i.e., ``output[channel] = (input[channel] - mean[channel]) / std[channel]`` .. note:: This transform acts out of place, i.e., it does not mutate the input tensor. Args: mean (sequence): Sequence of means for each channel. std (sequence): Sequence of standard deviations for each channel. inplace(bool,optional): Bool to make this operation in-place.
    - Class: `PILToTensor() -> None`
      - Description: Convert a PIL Image to a tensor of the same type - this does not scale values. This transform does not support torchscript. Converts a PIL Image (H x W x C) to a Tensor of shape (C x H x W).
    - Class: `Pad(padding, fill=0, padding_mode='constant')`
      - Description: Pad the given image on all sides with the given "pad" value. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means at most 2 leading dimensions for mode reflect and symmetric, at most 3 leading dimensions for mode edge, and an arbitrary number of leading dimensions for mode constant Args: padding (int or sequence): Padding on each border. If a single int is provided this is used to pad all borders. If sequence of length 2 is provided this is the padding on left/right and top/bottom respectively. If a sequence of length 4 is provided this is the padding for the left, top, right and bottom borders respectively. .. note:: In torchscript mode padding as single int is not supported, use a sequence of length 1: ``[padding, ]``. fill (number or tuple): Pixel fill value for constant fill. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. This value is only used when the padding_mode is constant. Only number is supported for torch Tensor. Only int or tuple value is supported for PIL Image. padding_mode (str): Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant. - constant: pads with a constant value, this value is specified with fill - edge: pads with the last value at the edge of the image. If input a 5D torch Tensor, the last 3 dimensions will be padded instead of the last 2 - reflect: pads with reflection of image without repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2] - symmetric: pads with reflection of image repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3]
    - Class: `RandomAdjustSharpness(sharpness_factor, p=0.5)`
      - Description: Adjust the sharpness of the image randomly with a given probability. If the image is torch Tensor, it is expected to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions. Args: sharpness_factor (float): How much to adjust the sharpness. Can be any non-negative number. 0 gives a blurred image, 1 gives the original image while 2 increases the sharpness by a factor of 2. p (float): probability of the image being sharpened. Default value is 0.5
    - Class: `RandomAffine(degrees, translate=None, scale=None, shear=None, interpolation=<InterpolationMode.NEAREST: 'nearest'>, fill=0, center=None)`
      - Description: Random affine transformation of the image keeping center invariant. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions. Args: degrees (sequence or number): Range of degrees to select from. If degrees is a number instead of sequence like (min, max), the range of degrees will be (-degrees, +degrees). Set to 0 to deactivate rotations. translate (tuple, optional): tuple of maximum absolute fraction for horizontal and vertical translations. For example translate=(a, b), then horizontal shift is randomly sampled in the range -img_width * a < dx < img_width * a and vertical shift is randomly sampled in the range -img_height * b < dy < img_height * b. Will not translate by default. scale (tuple, optional): scaling factor interval, e.g (a, b), then scale is randomly sampled from the range a <= scale <= b. Will keep original scale by default. shear (sequence or number, optional): Range of degrees to select from. If shear is a number, a shear parallel to the x-axis in the range (-shear, +shear) will be applied. Else if shear is a sequence of 2 values a shear parallel to the x-axis in the range (shear[0], shear[1]) will be applied. Else if shear is a sequence of 4 values, an x-axis shear in (shear[0], shear[1]) and y-axis shear in (shear[2], shear[3]) will be applied. Will not apply shear by default. interpolation (InterpolationMode): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. fill (sequence or number): Pixel fill value for the area outside the transformed image. Default is ``0``. If given a number, the value is used for all bands respectively. center (sequence, optional): Optional center of rotation, (x, y). Origin is the upper left corner. Default is the center of the image. .. _filters: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#filters
    - Class: `RandomApply(transforms, p=0.5)`
      - Description: Apply randomly a list of transformations with a given probability. .. note:: In order to script the transformation, please use ``torch.nn.ModuleList`` as input instead of list/tuple of transforms as shown below: >>> transforms = transforms.RandomApply(torch.nn.ModuleList([ >>> transforms.ColorJitter(), >>> ]), p=0.3) >>> scripted_transforms = torch.jit.script(transforms) Make sure to use only scriptable transformations, i.e. that work with ``torch.Tensor``, does not require `lambda` functions or ``PIL.Image``. Args: transforms (sequence or torch.nn.Module): list of transformations p (float): probability
    - Class: `RandomAutocontrast(p=0.5)`
      - Description: Autocontrast the pixels of the given image randomly with a given probability. If the image is torch Tensor, it is expected to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "L" or "RGB". Args: p (float): probability of the image being autocontrasted. Default value is 0.5
    - Class: `RandomChoice(transforms, p=None)`
      - Description: Apply single transformation randomly picked from a list. This transform does not support torchscript.
    - Class: `RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant')`
      - Description: Crop the given image at a random location. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions, but if non-constant padding is used, the input is expected to have at most 2 leading dimensions Args: size (sequence or int): Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]). padding (int or sequence, optional): Optional padding on each border of the image. Default is None. If a single int is provided this is used to pad all borders. If sequence of length 2 is provided this is the padding on left/right and top/bottom respectively. If a sequence of length 4 is provided this is the padding for the left, top, right and bottom borders respectively. .. note:: In torchscript mode padding as single int is not supported, use a sequence of length 1: ``[padding, ]``. pad_if_needed (boolean): It will pad the image if smaller than the desired size to avoid raising an exception. Since cropping is done after padding, the padding seems to be done at a random offset. fill (number or tuple): Pixel fill value for constant fill. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. This value is only used when the padding_mode is constant. Only number is supported for torch Tensor. Only int or tuple value is supported for PIL Image. padding_mode (str): Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant. - constant: pads with a constant value, this value is specified with fill - edge: pads with the last value at the edge of the image. If input a 5D torch Tensor, the last 3 dimensions will be padded instead of the last 2 - reflect: pads with reflection of image without repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2] - symmetric: pads with reflection of image repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3]
    - Class: `RandomEqualize(p=0.5)`
      - Description: Equalize the histogram of the given image randomly with a given probability. If the image is torch Tensor, it is expected to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "P", "L" or "RGB". Args: p (float): probability of the image being equalized. Default value is 0.5
    - Class: `RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)`
      - Description: Randomly selects a rectangle region in a torch.Tensor image and erases its pixels. This transform does not support PIL Image. 'Random Erasing Data Augmentation' by Zhong et al. See https://arxiv.org/abs/1708.04896 Args: p: probability that the random erasing operation will be performed. scale: range of proportion of erased area against input image. ratio: range of aspect ratio of erased area. value: erasing value. Default is 0. If a single int, it is used to erase all pixels. If a tuple of length 3, it is used to erase R, G, B channels respectively. If a str of 'random', erasing each pixel with random values. inplace: boolean to make this transform inplace. Default set to False. Returns: Erased Image. Example: >>> transform = transforms.Compose([ >>> transforms.RandomHorizontalFlip(), >>> transforms.PILToTensor(), >>> transforms.ConvertImageDtype(torch.float), >>> transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), >>> transforms.RandomErasing(), >>> ])
    - Class: `RandomGrayscale(p=0.1)`
      - Description: Randomly convert image to grayscale with a probability of p (default 0.1). If the image is torch Tensor, it is expected to have [..., 3, H, W] shape, where ... means an arbitrary number of leading dimensions Args: p (float): probability that image should be converted to grayscale. Returns: PIL Image or Tensor: Grayscale version of the input image with probability p and unchanged with probability (1-p). - If input image is 1 channel: grayscale version is 1 channel - If input image is 3 channel: grayscale version is 3 channel with r == g == b
    - Class: `RandomHorizontalFlip(p=0.5)`
      - Description: Horizontally flip the given image randomly with a given probability. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions Args: p (float): probability of the image being flipped. Default value is 0.5
    - Class: `RandomInvert(p=0.5)`
      - Description: Inverts the colors of the given image randomly with a given probability. If img is a Tensor, it is expected to be in [..., 1 or 3, H, W] format, where ... means it can have an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "L" or "RGB". Args: p (float): probability of the image being color inverted. Default value is 0.5
    - Class: `RandomOrder(transforms)`
      - Description: Apply a list of transformations in a random order. This transform does not support torchscript.
    - Class: `RandomPerspective(distortion_scale=0.5, p=0.5, interpolation=<InterpolationMode.BILINEAR: 'bilinear'>, fill=0)`
      - Description: Performs a random perspective transformation of the given image with a given probability. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions. Args: distortion_scale (float): argument to control the degree of distortion and ranges from 0 to 1. Default is 0.5. p (float): probability of the image being transformed. Default is 0.5. interpolation (InterpolationMode): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. fill (sequence or number): Pixel fill value for the area outside the transformed image. Default is ``0``. If given a number, the value is used for all bands respectively.
    - Class: `RandomPosterize(bits, p=0.5)`
      - Description: Posterize the image randomly with a given probability by reducing the number of bits for each color channel. If the image is torch Tensor, it should be of type torch.uint8, and it is expected to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "L" or "RGB". Args: bits (int): number of bits to keep for each channel (0-8) p (float): probability of the image being posterized. Default value is 0.5
    - Class: `RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=<InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn')`
      - Description: Crop a random portion of image and resize it to a given size. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions A crop of the original image is made: the crop has a random area (H * W) and a random aspect ratio. This crop is finally resized to the given size. This is popularly used to train the Inception networks. Args: size (int or sequence): expected output size of the crop, for each edge. If size is an int instead of sequence like (h, w), a square output size ``(size, size)`` is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]). .. note:: In torchscript mode size as single int is not supported, use a sequence of length 1: ``[size, ]``. scale (tuple of float): Specifies the lower and upper bounds for the random area of the crop, before resizing. The scale is defined with respect to the area of the original image. ratio (tuple of float): lower and upper bounds for the random aspect ratio of the crop, before resizing. interpolation (InterpolationMode): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.NEAREST_EXACT``, ``InterpolationMode.BILINEAR`` and ``InterpolationMode.BICUBIC`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. antialias (bool, optional): Whether to apply antialiasing. It only affects **tensors** with bilinear or bicubic modes and it is ignored otherwise: on PIL images, antialiasing is always applied on bilinear or bicubic modes; on other modes (for PIL images and tensors), antialiasing makes no sense and this parameter is ignored. Possible values are: - ``True``: will apply antialiasing for bilinear or bicubic modes. Other mode aren't affected. This is probably what you want to use. - ``False``: will not apply antialiasing for tensors on any mode. PIL images are still antialiased on bilinear or bicubic modes, because PIL doesn't support no antialias. - ``None``: equivalent to ``False`` for tensors and ``True`` for PIL images. This value exists for legacy reasons and you probably don't want to use it unless you really know what you are doing. The current default is ``None`` **but will change to** ``True`` **in v0.17** for the PIL and Tensor backends to be consistent.
    - Class: `RandomRotation(degrees, interpolation=<InterpolationMode.NEAREST: 'nearest'>, expand=False, center=None, fill=0)`
      - Description: Rotate the image by angle. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions. Args: degrees (sequence or number): Range of degrees to select from. If degrees is a number instead of sequence like (min, max), the range of degrees will be (-degrees, +degrees). interpolation (InterpolationMode): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. expand (bool, optional): Optional expansion flag. If true, expands the output to make it large enough to hold the entire rotated image. If false or omitted, make the output image the same size as the input image. Note that the expand flag assumes rotation around the center and no translation. center (sequence, optional): Optional center of rotation, (x, y). Origin is the upper left corner. Default is the center of the image. fill (sequence or number): Pixel fill value for the area outside the rotated image. Default is ``0``. If given a number, the value is used for all bands respectively. .. _filters: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#filters
    - Class: `RandomSolarize(threshold, p=0.5)`
      - Description: Solarize the image randomly with a given probability by inverting all pixel values above a threshold. If img is a Tensor, it is expected to be in [..., 1 or 3, H, W] format, where ... means it can have an arbitrary number of leading dimensions. If img is PIL Image, it is expected to be in mode "L" or "RGB". Args: threshold (float): all pixels equal or above this value are inverted. p (float): probability of the image being solarized. Default value is 0.5
    - Class: `RandomTransforms(transforms)`
      - Description: Base class for a list of transformations with randomness Args: transforms (sequence): list of transformations
    - Class: `RandomVerticalFlip(p=0.5)`
      - Description: Vertically flip the given image randomly with a given probability. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions Args: p (float): probability of the image being flipped. Default value is 0.5
    - Class: `Resize(size, interpolation=<InterpolationMode.BILINEAR: 'bilinear'>, max_size=None, antialias='warn')`
      - Description: Resize the input image to the given size. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means a maximum of two leading dimensions .. warning:: The output image might be different depending on its type: when downsampling, the interpolation of PIL images and tensors is slightly different, because PIL applies antialiasing. This may lead to significant differences in the performance of a network. Therefore, it is preferable to train and serve a model with the same input types. See also below the ``antialias`` parameter, which can help making the output of PIL images and tensors closer. Args: size (sequence or int): Desired output size. If size is a sequence like (h, w), output size will be matched to this. If size is an int, smaller edge of the image will be matched to this number. i.e, if height > width, then image will be rescaled to (size * height / width, size). .. note:: In torchscript mode size as single int is not supported, use a sequence of length 1: ``[size, ]``. interpolation (InterpolationMode): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.NEAREST_EXACT``, ``InterpolationMode.BILINEAR`` and ``InterpolationMode.BICUBIC`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. max_size (int, optional): The maximum allowed for the longer edge of the resized image. If the longer edge of the image is greater than ``max_size`` after being resized according to ``size``, ``size`` will be overruled so that the longer edge is equal to ``max_size``. As a result, the smaller edge may be shorter than ``size``. This is only supported if ``size`` is an int (or a sequence of length 1 in torchscript mode). antialias (bool, optional): Whether to apply antialiasing. It only affects **tensors** with bilinear or bicubic modes and it is ignored otherwise: on PIL images, antialiasing is always applied on bilinear or bicubic modes; on other modes (for PIL images and tensors), antialiasing makes no sense and this parameter is ignored. Possible values are: - ``True``: will apply antialiasing for bilinear or bicubic modes. Other mode aren't affected. This is probably what you want to use. - ``False``: will not apply antialiasing for tensors on any mode. PIL images are still antialiased on bilinear or bicubic modes, because PIL doesn't support no antialias. - ``None``: equivalent to ``False`` for tensors and ``True`` for PIL images. This value exists for legacy reasons and you probably don't want to use it unless you really know what you are doing. The current default is ``None`` **but will change to** ``True`` **in v0.17** for the PIL and Tensor backends to be consistent.
    - Class: `Sequence()`
      - Description: All the operations on a read-only sequence. Concrete subclasses must override __new__ or __init__, __getitem__, and __len__.
    - Class: `TenCrop(size, vertical_flip=False)`
      - Description: Crop the given image into four corners and the central crop plus the flipped version of these (horizontal flipping is used by default). If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions .. Note:: This transform returns a tuple of images and there may be a mismatch in the number of inputs and targets your Dataset returns. See below for an example of how to deal with this. Args: size (sequence or int): Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]). vertical_flip (bool): Use vertical flipping instead of horizontal Example: >>> transform = Compose([ >>> TenCrop(size), # this is a tuple of PIL Images >>> Lambda(lambda crops: torch.stack([PILToTensor()(crop) for crop in crops])) # returns a 4D tensor >>> ]) >>> #In your test loop you can do the following: >>> input, target = batch # input is a 5d tensor, target is 2d >>> bs, ncrops, c, h, w = input.size() >>> result = model(input.view(-1, c, h, w)) # fuse batch size and ncrops >>> result_avg = result.view(bs, ncrops, -1).mean(1) # avg over crops
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `ToPILImage(mode=None)`
      - Description: Convert a tensor or an ndarray to PIL Image This transform does not support torchscript. Converts a torch.*Tensor of shape C x H x W or a numpy ndarray of shape H x W x C to a PIL Image while adjusting the value range depending on the ``mode``. Args: mode (`PIL.Image mode`_): color space and pixel depth of input data (optional). If ``mode`` is ``None`` (default) there are some assumptions made about the input data: - If the input has 4 channels, the ``mode`` is assumed to be ``RGBA``. - If the input has 3 channels, the ``mode`` is assumed to be ``RGB``. - If the input has 2 channels, the ``mode`` is assumed to be ``LA``. - If the input has 1 channel, the ``mode`` is determined by the data type (i.e ``int``, ``float``, ``short``). .. _PIL.Image mode: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#concept-modes
    - Class: `ToTensor() -> None`
      - Description: Convert a PIL Image or ndarray to tensor and scale the values accordingly. This transform does not support torchscript. Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8 In the other cases, tensors are returned without scaling. .. note:: Because the input image is scaled to [0.0, 1.0], this transformation should not be used when transforming target image masks. See the `references`_ for implementing the transforms for image masks. .. _references: https://github.com/pytorch/vision/tree/main/references/segmentation
    - Function: `_check_sequence_input(x, name, req_sizes)`
      - Description: No docstring available
    - Function: `_interpolation_modes_from_int(i: int) -> torchvision.transforms.functional.InterpolationMode`
      - Description: No docstring available
    - Function: `_log_api_usage_once(obj: Any) -> None`
      - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
    - Function: `_setup_angle(x, name, req_sizes=(2,))`
      - Description: No docstring available
    - Function: `_setup_size(size, error_msg)`
      - Description: No docstring available
  - _functional_pil.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Function: `_is_pil_image(img: Any) -> bool`
      - Description: No docstring available
    - Function: `_parse_fill(fill: Union[float, List[float], Tuple[float, ...], NoneType], img: PIL.Image.Image, name: str = 'fillcolor') -> Dict[str, Union[float, List[float], Tuple[float, ...], NoneType]]`
      - Description: No docstring available
    - Function: `adjust_brightness(img: PIL.Image.Image, brightness_factor: float) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `adjust_contrast(img: PIL.Image.Image, contrast_factor: float) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `adjust_gamma(img: PIL.Image.Image, gamma: float, gain: float = 1.0) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `adjust_hue(img: PIL.Image.Image, hue_factor: float) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `adjust_saturation(img: PIL.Image.Image, saturation_factor: float) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `adjust_sharpness(img: PIL.Image.Image, sharpness_factor: float) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `affine(img: PIL.Image.Image, matrix: List[float], interpolation: int = 0, fill: Union[int, float, Sequence[int], Sequence[float], NoneType] = None) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `autocontrast(img: PIL.Image.Image) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `crop(img: PIL.Image.Image, top: int, left: int, height: int, width: int) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `equalize(img: PIL.Image.Image) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `get_dimensions(img: Any) -> List[int]`
      - Description: No docstring available
    - Function: `get_image_num_channels(img: Any) -> int`
      - Description: No docstring available
    - Function: `get_image_size(img: Any) -> List[int]`
      - Description: No docstring available
    - Function: `hflip(img: PIL.Image.Image) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `invert(img: PIL.Image.Image) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `pad(img: PIL.Image.Image, padding: Union[int, List[int], Tuple[int, ...]], fill: Union[float, List[float], Tuple[float, ...], NoneType] = 0, padding_mode: Literal['constant', 'edge', 'reflect', 'symmetric'] = 'constant') -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `perspective(img: PIL.Image.Image, perspective_coeffs: List[float], interpolation: int = 3, fill: Union[int, float, Sequence[int], Sequence[float], NoneType] = None) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `posterize(img: PIL.Image.Image, bits: int) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `resize(img: PIL.Image.Image, size: Union[List[int], int], interpolation: int = 2) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `rotate(img: PIL.Image.Image, angle: float, interpolation: int = 0, expand: bool = False, center: Optional[Tuple[int, int]] = None, fill: Union[int, float, Sequence[int], Sequence[float], NoneType] = None) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `solarize(img: PIL.Image.Image, threshold: int) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `to_grayscale(img: PIL.Image.Image, num_output_channels: int) -> PIL.Image.Image`
      - Description: No docstring available
    - Function: `vflip(img: PIL.Image.Image) -> PIL.Image.Image`
      - Description: No docstring available
  - _functional_tensor.py
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Function: `_apply_grid_transform(img: torch.Tensor, grid: torch.Tensor, mode: str, fill: Union[int, float, List[float], NoneType]) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_assert_channels(img: torch.Tensor, permitted: List[int]) -> None`
      - Description: No docstring available
    - Function: `_assert_grid_transform_inputs(img: torch.Tensor, matrix: Optional[List[float]], interpolation: str, fill: Union[int, float, List[float], NoneType], supported_interpolation_modes: List[str], coeffs: Optional[List[float]] = None) -> None`
      - Description: No docstring available
    - Function: `_assert_image_tensor(img: torch.Tensor) -> None`
      - Description: No docstring available
    - Function: `_blend(img1: torch.Tensor, img2: torch.Tensor, ratio: float) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_blurred_degenerate_image(img: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_cast_squeeze_in(img: torch.Tensor, req_dtypes: List[torch.dtype]) -> Tuple[torch.Tensor, bool, bool, torch.dtype]`
      - Description: No docstring available
    - Function: `_cast_squeeze_out(img: torch.Tensor, need_cast: bool, need_squeeze: bool, out_dtype: torch.dtype) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_compute_affine_output_size(matrix: List[float], w: int, h: int) -> Tuple[int, int]`
      - Description: No docstring available
    - Function: `_create_identity_grid(size: List[int]) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_equalize_single_image(img: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_gen_affine_grid(theta: torch.Tensor, w: int, h: int, ow: int, oh: int) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_get_gaussian_kernel1d(kernel_size: int, sigma: float) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_get_gaussian_kernel2d(kernel_size: List[int], sigma: List[float], dtype: torch.dtype, device: torch.device) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_hsv2rgb(img: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_is_tensor_a_torch_image(x: torch.Tensor) -> bool`
      - Description: No docstring available
    - Function: `_max_value(dtype: torch.dtype) -> int`
      - Description: No docstring available
    - Function: `_pad_symmetric(img: torch.Tensor, padding: List[int]) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_parse_pad_padding(padding: Union[int, List[int]]) -> List[int]`
      - Description: No docstring available
    - Function: `_perspective_grid(coeffs: List[float], ow: int, oh: int, dtype: torch.dtype, device: torch.device) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_rgb2hsv(img: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `_scale_channel(img_chan: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `adjust_brightness(img: torch.Tensor, brightness_factor: float) -> torch.Tensor`
      - Description: No docstring available
    - Function: `adjust_contrast(img: torch.Tensor, contrast_factor: float) -> torch.Tensor`
      - Description: No docstring available
    - Function: `adjust_gamma(img: torch.Tensor, gamma: float, gain: float = 1) -> torch.Tensor`
      - Description: No docstring available
    - Function: `adjust_hue(img: torch.Tensor, hue_factor: float) -> torch.Tensor`
      - Description: No docstring available
    - Function: `adjust_saturation(img: torch.Tensor, saturation_factor: float) -> torch.Tensor`
      - Description: No docstring available
    - Function: `adjust_sharpness(img: torch.Tensor, sharpness_factor: float) -> torch.Tensor`
      - Description: No docstring available
    - Function: `affine(img: torch.Tensor, matrix: List[float], interpolation: str = 'nearest', fill: Union[int, float, List[float], NoneType] = None) -> torch.Tensor`
      - Description: No docstring available
    - Function: `autocontrast(img: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `convert_image_dtype(image: torch.Tensor, dtype: torch.dtype = torch.float32) -> torch.Tensor`
      - Description: No docstring available
    - Function: `crop(img: torch.Tensor, top: int, left: int, height: int, width: int) -> torch.Tensor`
      - Description: No docstring available
    - Function: `elastic_transform(img: torch.Tensor, displacement: torch.Tensor, interpolation: str = 'bilinear', fill: Union[int, float, List[float], NoneType] = None) -> torch.Tensor`
      - Description: No docstring available
    - Function: `equalize(img: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `erase(img: torch.Tensor, i: int, j: int, h: int, w: int, v: torch.Tensor, inplace: bool = False) -> torch.Tensor`
      - Description: No docstring available
    - Function: `gaussian_blur(img: torch.Tensor, kernel_size: List[int], sigma: List[float]) -> torch.Tensor`
      - Description: No docstring available
    - Function: `get_dimensions(img: torch.Tensor) -> List[int]`
      - Description: No docstring available
    - Function: `get_image_num_channels(img: torch.Tensor) -> int`
      - Description: No docstring available
    - Function: `get_image_size(img: torch.Tensor) -> List[int]`
      - Description: No docstring available
    - Function: `grid_sample(input: torch.Tensor, grid: torch.Tensor, mode: str = 'bilinear', padding_mode: str = 'zeros', align_corners: Optional[bool] = None) -> torch.Tensor`
      - Description: Given an :attr:`input` and a flow-field :attr:`grid`, computes the ``output`` using :attr:`input` values and pixel locations from :attr:`grid`. Currently, only spatial (4-D) and volumetric (5-D) :attr:`input` are supported. In the spatial (4-D) case, for :attr:`input` with shape :math:`(N, C, H_\text{in}, W_\text{in})` and :attr:`grid` with shape :math:`(N, H_\text{out}, W_\text{out}, 2)`, the output will have shape :math:`(N, C, H_\text{out}, W_\text{out})`. For each output location ``output[n, :, h, w]``, the size-2 vector ``grid[n, h, w]`` specifies :attr:`input` pixel locations ``x`` and ``y``, which are used to interpolate the output value ``output[n, :, h, w]``. In the case of 5D inputs, ``grid[n, d, h, w]`` specifies the ``x``, ``y``, ``z`` pixel locations for interpolating ``output[n, :, d, h, w]``. :attr:`mode` argument specifies ``nearest`` or ``bilinear`` interpolation method to sample the input pixels. :attr:`grid` specifies the sampling pixel locations normalized by the :attr:`input` spatial dimensions. Therefore, it should have most values in the range of ``[-1, 1]``. For example, values ``x = -1, y = -1`` is the left-top pixel of :attr:`input`, and values ``x = 1, y = 1`` is the right-bottom pixel of :attr:`input`. If :attr:`grid` has values outside the range of ``[-1, 1]``, the corresponding outputs are handled as defined by :attr:`padding_mode`. Options are * ``padding_mode="zeros"``: use ``0`` for out-of-bound grid locations, * ``padding_mode="border"``: use border values for out-of-bound grid locations, * ``padding_mode="reflection"``: use values at locations reflected by the border for out-of-bound grid locations. For location far away from the border, it will keep being reflected until becoming in bound, e.g., (normalized) pixel location ``x = -3.5`` reflects by border ``-1`` and becomes ``x' = 1.5``, then reflects by border ``1`` and becomes ``x'' = -0.5``. Note: This function is often used in conjunction with :func:`affine_grid` to build `Spatial Transformer Networks`_ . Note: When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on :doc:`/notes/randomness` for background. Note: NaN values in :attr:`grid` would be interpreted as ``-1``. Args: input (Tensor): input of shape :math:`(N, C, H_\text{in}, W_\text{in})` (4-D case) or :math:`(N, C, D_\text{in}, H_\text{in}, W_\text{in})` (5-D case) grid (Tensor): flow-field of shape :math:`(N, H_\text{out}, W_\text{out}, 2)` (4-D case) or :math:`(N, D_\text{out}, H_\text{out}, W_\text{out}, 3)` (5-D case) mode (str): interpolation mode to calculate output values ``'bilinear'`` | ``'nearest'`` | ``'bicubic'``. Default: ``'bilinear'`` Note: ``mode='bicubic'`` supports only 4-D input. When ``mode='bilinear'`` and the input is 5-D, the interpolation mode used internally will actually be trilinear. However, when the input is 4-D, the interpolation mode will legitimately be bilinear. padding_mode (str): padding mode for outside grid values ``'zeros'`` | ``'border'`` | ``'reflection'``. Default: ``'zeros'`` align_corners (bool, optional): Geometrically, we consider the pixels of the input as squares rather than points. If set to ``True``, the extrema (``-1`` and ``1``) are considered as referring to the center points of the input's corner pixels. If set to ``False``, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic. This option parallels the ``align_corners`` option in :func:`interpolate`, and so whichever option is used here should also be used there to resize the input image before grid sampling. Default: ``False`` Returns: output (Tensor): output Tensor .. _`Spatial Transformer Networks`: https://arxiv.org/abs/1506.02025 .. warning:: When ``align_corners = True``, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by :func:`grid_sample` will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was ``align_corners = True``. Since then, the default behavior has been changed to ``align_corners = False``, in order to bring it in line with the default for :func:`interpolate`. .. note:: ``mode='bicubic'`` is implemented using the `cubic convolution algorithm`_ with :math:`\alpha=-0.75`. The constant :math:`\alpha` might be different from packages to packages. For example, `PIL`_ and `OpenCV`_ use -0.5 and -0.75 respectively. This algorithm may "overshoot" the range of values it's interpolating. For example, it may produce negative values or values greater than 255 when interpolating input in [0, 255]. Clamp the results with :func:`torch.clamp` to ensure they are within the valid range. .. _`cubic convolution algorithm`: https://en.wikipedia.org/wiki/Bicubic_interpolation .. _`PIL`: https://github.com/python-pillow/Pillow/blob/4634eafe3c695a014267eefdce830b4a825beed7/src/libImaging/Resample.c#L51 .. _`OpenCV`: https://github.com/opencv/opencv/blob/f345ed564a06178670750bad59526cfa4033be55/modules/imgproc/src/resize.cpp#L908
    - Function: `hflip(img: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `interpolate(input: torch.Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> torch.Tensor`
      - Description: Down/up samples the input to either the given :attr:`size` or the given :attr:`scale_factor` The algorithm used for interpolation is determined by :attr:`mode`. Currently temporal, spatial and volumetric sampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape. The input dimensions are interpreted in the form: `mini-batch x channels x [optional depth] x [optional height] x width`. The modes available for resizing are: `nearest`, `linear` (3D-only), `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only), `area`, `nearest-exact` Args: input (Tensor): the input tensor size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]): output spatial size. scale_factor (float or Tuple[float]): multiplier for spatial size. If `scale_factor` is a tuple, its length has to match the number of spatial dimensions; `input.dim() - 2`. mode (str): algorithm used for upsampling: ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` | ``'trilinear'`` | ``'area'`` | ``'nearest-exact'``. Default: ``'nearest'`` align_corners (bool, optional): Geometrically, we consider the pixels of the input and output as squares rather than points. If set to ``True``, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to ``False``, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation *independent* of input size when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode` is ``'linear'``, ``'bilinear'``, ``'bicubic'`` or ``'trilinear'``. Default: ``False`` recompute_scale_factor (bool, optional): recompute the scale_factor for use in the interpolation calculation. If `recompute_scale_factor` is ``True``, then `scale_factor` must be passed in and `scale_factor` is used to compute the output `size`. The computed output `size` will be used to infer new scales for the interpolation. Note that when `scale_factor` is floating-point, it may differ from the recomputed `scale_factor` due to rounding and precision issues. If `recompute_scale_factor` is ``False``, then `size` or `scale_factor` will be used directly for interpolation. Default: ``None``. antialias (bool, optional): flag to apply anti-aliasing. Default: ``False``. Using anti-alias option together with ``align_corners=False``, interpolation result would match Pillow result for downsampling operation. Supported modes: ``'bilinear'``, ``'bicubic'``. .. note:: With ``mode='bicubic'``, it's possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call ``result.clamp(min=0, max=255)`` if you want to reduce the overshoot when displaying the image. .. note:: Mode ``mode='nearest-exact'`` matches Scikit-Image and PIL nearest neighbours interpolation algorithms and fixes known issues with ``mode='nearest'``. This mode is introduced to keep backward compatibility. Mode ``mode='nearest'`` matches buggy OpenCV's ``INTER_NEAREST`` interpolation algorithm. .. note:: The gradients for the dtype ``float16`` on CUDA may be inaccurate in the upsample operation when using modes ``['linear', 'bilinear', 'bicubic', 'trilinear', 'area']``. For more details, please refer to the discussion in `issue#104157 <https://github.com/pytorch/pytorch/issues/104157>`_. Note: This operation may produce nondeterministic gradients when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.
    - Function: `invert(img: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
    - Function: `normalize(tensor: torch.Tensor, mean: List[float], std: List[float], inplace: bool = False) -> torch.Tensor`
      - Description: No docstring available
    - Function: `pad(img: torch.Tensor, padding: Union[int, List[int]], fill: Union[int, float, NoneType] = 0, padding_mode: str = 'constant') -> torch.Tensor`
      - Description: No docstring available
    - Function: `perspective(img: torch.Tensor, perspective_coeffs: List[float], interpolation: str = 'bilinear', fill: Union[int, float, List[float], NoneType] = None) -> torch.Tensor`
      - Description: No docstring available
    - Function: `posterize(img: torch.Tensor, bits: int) -> torch.Tensor`
      - Description: No docstring available
    - Function: `resize(img: torch.Tensor, size: List[int], interpolation: str = 'bilinear', antialias: Optional[bool] = None) -> torch.Tensor`
      - Description: No docstring available
    - Function: `rgb_to_grayscale(img: torch.Tensor, num_output_channels: int = 1) -> torch.Tensor`
      - Description: No docstring available
    - Function: `rotate(img: torch.Tensor, matrix: List[float], interpolation: str = 'nearest', expand: bool = False, fill: Union[int, float, List[float], NoneType] = None) -> torch.Tensor`
      - Description: No docstring available
    - Function: `solarize(img: torch.Tensor, threshold: float) -> torch.Tensor`
      - Description: No docstring available
    - Function: `vflip(img: torch.Tensor) -> torch.Tensor`
      - Description: No docstring available
  - _functional_video.py
    - Function: `_is_tensor_video_clip(clip)`
      - Description: No docstring available
    - Function: `center_crop(clip, crop_size)`
      - Description: No docstring available
    - Function: `crop(clip, i, j, h, w)`
      - Description: Args: clip (torch.tensor): Video clip to be cropped. Size is (C, T, H, W)
    - Function: `hflip(clip)`
      - Description: Args: clip (torch.tensor): Video clip to be normalized. Size is (C, T, H, W) Returns: flipped clip (torch.tensor): Size is (C, T, H, W)
    - Function: `normalize(clip, mean, std, inplace=False)`
      - Description: Args: clip (torch.tensor): Video clip to be normalized. Size is (C, T, H, W) mean (tuple): pixel RGB mean. Size is (3) std (tuple): pixel standard deviation. Size is (3) Returns: normalized clip (torch.tensor): Size is (C, T, H, W)
    - Function: `resize(clip, target_size, interpolation_mode)`
      - Description: No docstring available
    - Function: `resized_crop(clip, i, j, h, w, size, interpolation_mode='bilinear')`
      - Description: Do spatial cropping and resizing to the video clip Args: clip (torch.tensor): Video clip to be cropped. Size is (C, T, H, W) i (int): i in (i,j) i.e coordinates of the upper left corner. j (int): j in (i,j) i.e coordinates of the upper left corner. h (int): Height of the cropped region. w (int): Width of the cropped region. size (tuple(int, int)): height and width of resized clip Returns: clip (torch.tensor): Resized and cropped clip. Size is (C, T, H, W)
    - Function: `to_tensor(clip)`
      - Description: Convert tensor data type from uint8 to float, divide value by 255.0 and permute the dimensions of clip tensor Args: clip (torch.tensor, dtype=torch.uint8): Size is (T, H, W, C) Return: clip (torch.tensor, dtype=torch.float): Size is (C, T, H, W)
  - _presets.py
    - Class: `ImageClassification(*, crop_size: int, resize_size: int = 256, mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `InterpolationMode(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: Interpolation modes Available interpolation methods are ``nearest``, ``nearest-exact``, ``bilinear``, ``bicubic``, ``box``, ``hamming``, and ``lanczos``.
    - Class: `ObjectDetection(*args, **kwargs) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `OpticalFlow(*args, **kwargs) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `SemanticSegmentation(*, resize_size: Optional[int], mean: Tuple[float, ...] = (0.485, 0.456, 0.406), std: Tuple[float, ...] = (0.229, 0.224, 0.225), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn') -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
    - Class: `Tensor(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `VideoClassification(*, crop_size: Tuple[int, int], resize_size: Tuple[int, int], mean: Tuple[float, ...] = (0.43216, 0.394666, 0.37645), std: Tuple[float, ...] = (0.22803, 0.22145, 0.216989), interpolation: torchvision.transforms.functional.InterpolationMode = <InterpolationMode.BILINEAR: 'bilinear'>) -> None`
      - Description: Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:`to`, etc. .. note:: As per the example above, an ``__init__()`` call to the parent class must be made before assignment on the child. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool
  - _transforms_video.py
    - Class: `CenterCropVideo(crop_size)`
      - Description: No docstring available
    - Class: `NormalizeVideo(mean, std, inplace=False)`
      - Description: Normalize the video clip by mean subtraction and division by standard deviation Args: mean (3-tuple): pixel RGB mean std (3-tuple): pixel RGB standard deviation inplace (boolean): whether do in-place normalization
    - Class: `RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant')`
      - Description: Crop the given image at a random location. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions, but if non-constant padding is used, the input is expected to have at most 2 leading dimensions Args: size (sequence or int): Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]). padding (int or sequence, optional): Optional padding on each border of the image. Default is None. If a single int is provided this is used to pad all borders. If sequence of length 2 is provided this is the padding on left/right and top/bottom respectively. If a sequence of length 4 is provided this is the padding for the left, top, right and bottom borders respectively. .. note:: In torchscript mode padding as single int is not supported, use a sequence of length 1: ``[padding, ]``. pad_if_needed (boolean): It will pad the image if smaller than the desired size to avoid raising an exception. Since cropping is done after padding, the padding seems to be done at a random offset. fill (number or tuple): Pixel fill value for constant fill. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. This value is only used when the padding_mode is constant. Only number is supported for torch Tensor. Only int or tuple value is supported for PIL Image. padding_mode (str): Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant. - constant: pads with a constant value, this value is specified with fill - edge: pads with the last value at the edge of the image. If input a 5D torch Tensor, the last 3 dimensions will be padded instead of the last 2 - reflect: pads with reflection of image without repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2] - symmetric: pads with reflection of image repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3]
    - Class: `RandomCropVideo(size)`
      - Description: Crop the given image at a random location. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions, but if non-constant padding is used, the input is expected to have at most 2 leading dimensions Args: size (sequence or int): Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]). padding (int or sequence, optional): Optional padding on each border of the image. Default is None. If a single int is provided this is used to pad all borders. If sequence of length 2 is provided this is the padding on left/right and top/bottom respectively. If a sequence of length 4 is provided this is the padding for the left, top, right and bottom borders respectively. .. note:: In torchscript mode padding as single int is not supported, use a sequence of length 1: ``[padding, ]``. pad_if_needed (boolean): It will pad the image if smaller than the desired size to avoid raising an exception. Since cropping is done after padding, the padding seems to be done at a random offset. fill (number or tuple): Pixel fill value for constant fill. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. This value is only used when the padding_mode is constant. Only number is supported for torch Tensor. Only int or tuple value is supported for PIL Image. padding_mode (str): Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant. - constant: pads with a constant value, this value is specified with fill - edge: pads with the last value at the edge of the image. If input a 5D torch Tensor, the last 3 dimensions will be padded instead of the last 2 - reflect: pads with reflection of image without repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2] - symmetric: pads with reflection of image repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3]
    - Class: `RandomHorizontalFlipVideo(p=0.5)`
      - Description: Flip the video clip along the horizontal direction with a given probability Args: p (float): probability of the clip being flipped. Default value is 0.5
    - Class: `RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=<InterpolationMode.BILINEAR: 'bilinear'>, antialias: Union[str, bool, NoneType] = 'warn')`
      - Description: Crop a random portion of image and resize it to a given size. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions A crop of the original image is made: the crop has a random area (H * W) and a random aspect ratio. This crop is finally resized to the given size. This is popularly used to train the Inception networks. Args: size (int or sequence): expected output size of the crop, for each edge. If size is an int instead of sequence like (h, w), a square output size ``(size, size)`` is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]). .. note:: In torchscript mode size as single int is not supported, use a sequence of length 1: ``[size, ]``. scale (tuple of float): Specifies the lower and upper bounds for the random area of the crop, before resizing. The scale is defined with respect to the area of the original image. ratio (tuple of float): lower and upper bounds for the random aspect ratio of the crop, before resizing. interpolation (InterpolationMode): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.NEAREST_EXACT``, ``InterpolationMode.BILINEAR`` and ``InterpolationMode.BICUBIC`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. antialias (bool, optional): Whether to apply antialiasing. It only affects **tensors** with bilinear or bicubic modes and it is ignored otherwise: on PIL images, antialiasing is always applied on bilinear or bicubic modes; on other modes (for PIL images and tensors), antialiasing makes no sense and this parameter is ignored. Possible values are: - ``True``: will apply antialiasing for bilinear or bicubic modes. Other mode aren't affected. This is probably what you want to use. - ``False``: will not apply antialiasing for tensors on any mode. PIL images are still antialiased on bilinear or bicubic modes, because PIL doesn't support no antialias. - ``None``: equivalent to ``False`` for tensors and ``True`` for PIL images. This value exists for legacy reasons and you probably don't want to use it unless you really know what you are doing. The current default is ``None`` **but will change to** ``True`` **in v0.17** for the PIL and Tensor backends to be consistent.
    - Class: `RandomResizedCropVideo(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation_mode='bilinear')`
      - Description: Crop a random portion of image and resize it to a given size. If the image is torch Tensor, it is expected to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions A crop of the original image is made: the crop has a random area (H * W) and a random aspect ratio. This crop is finally resized to the given size. This is popularly used to train the Inception networks. Args: size (int or sequence): expected output size of the crop, for each edge. If size is an int instead of sequence like (h, w), a square output size ``(size, size)`` is made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]). .. note:: In torchscript mode size as single int is not supported, use a sequence of length 1: ``[size, ]``. scale (tuple of float): Specifies the lower and upper bounds for the random area of the crop, before resizing. The scale is defined with respect to the area of the original image. ratio (tuple of float): lower and upper bounds for the random aspect ratio of the crop, before resizing. interpolation (InterpolationMode): Desired interpolation enum defined by :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``. If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.NEAREST_EXACT``, ``InterpolationMode.BILINEAR`` and ``InterpolationMode.BICUBIC`` are supported. The corresponding Pillow integer constants, e.g. ``PIL.Image.BILINEAR`` are accepted as well. antialias (bool, optional): Whether to apply antialiasing. It only affects **tensors** with bilinear or bicubic modes and it is ignored otherwise: on PIL images, antialiasing is always applied on bilinear or bicubic modes; on other modes (for PIL images and tensors), antialiasing makes no sense and this parameter is ignored. Possible values are: - ``True``: will apply antialiasing for bilinear or bicubic modes. Other mode aren't affected. This is probably what you want to use. - ``False``: will not apply antialiasing for tensors on any mode. PIL images are still antialiased on bilinear or bicubic modes, because PIL doesn't support no antialias. - ``None``: equivalent to ``False`` for tensors and ``True`` for PIL images. This value exists for legacy reasons and you probably don't want to use it unless you really know what you are doing. The current default is ``None`` **but will change to** ``True`` **in v0.17** for the PIL and Tensor backends to be consistent.
    - Class: `ToTensorVideo()`
      - Description: Convert tensor data type from uint8 to float, divide value by 255.0 and permute the dimensions of clip tensor
- tv_tensors/
  - __pycache__/
  - _bounding_boxes.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `BoundingBoxFormat(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: [BETA] Coordinate format of a bounding box. Available formats are * ``XYXY`` * ``XYWH`` * ``CXCYWH``
    - Class: `BoundingBoxes(data: 'Any', *, format: 'Union[BoundingBoxFormat, str]', canvas_size: 'Tuple[int, int]', dtype: 'Optional[torch.dtype]' = None, device: 'Optional[Union[torch.device, str, int]]' = None, requires_grad: 'Optional[bool]' = None) -> 'BoundingBoxes'`
      - Description: [BETA] :class:`torch.Tensor` subclass for bounding boxes. .. note:: There should be only one :class:`~torchvision.tv_tensors.BoundingBoxes` instance per sample e.g. ``{"img": img, "bbox": BoundingBoxes(...)}``, although one :class:`~torchvision.tv_tensors.BoundingBoxes` object can contain multiple bounding boxes. Args: data: Any data that can be turned into a tensor with :func:`torch.as_tensor`. format (BoundingBoxFormat, str): Format of the bounding box. canvas_size (two-tuple of ints): Height and width of the corresponding image or video. dtype (torch.dtype, optional): Desired data type of the bounding box. If omitted, will be inferred from ``data``. device (torch.device, optional): Desired device of the bounding box. If omitted and ``data`` is a :class:`torch.Tensor`, the device is taken from it. Otherwise, the bounding box is constructed on the CPU. requires_grad (bool, optional): Whether autograd should record operations on the bounding box. If omitted and ``data`` is a :class:`torch.Tensor`, the value is taken from it. Otherwise, defaults to ``False``.
    - Class: `Enum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
      - Description: Create a collection of name/value pairs. Example enumeration: >>> class Color(Enum): ... RED = 1 ... BLUE = 2 ... GREEN = 3 Access them by: - attribute access:: >>> Color.RED <Color.RED: 1> - value lookup: >>> Color(1) <Color.RED: 1> - name lookup: >>> Color['RED'] <Color.RED: 1> Enumerations can be iterated over, and know how many members they have: >>> len(Color) 3 >>> list(Color) [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>] Methods can be added to enumerations, and members can have their own attributes -- see the documentation for details.
    - Class: `TVTensor(Unable to retrieve signature)`
      - Description: [Beta] Base class for all TVTensors. You probably don't want to use this class unless you're defining your own custom TVTensors. See :ref:`sphx_glr_auto_examples_transforms_plot_custom_tv_tensors.py` for details.
    - Function: `tree_flatten(pytree: Any) -> Tuple[List[Any], torch.utils._pytree.TreeSpec]`
      - Description: Flattens a pytree into a list of values and a TreeSpec that can be used to reconstruct the pytree.
  - _dataset_wrapper.py
    - Class: `VisionDatasetTVTensorWrapper(dataset, target_keys)`
      - Description: No docstring available
    - Class: `WrapperFactories(Unable to retrieve signature)`
      - Description: dict() -> new empty dictionary dict(mapping) -> new dictionary initialized from a mapping object's (key, value) pairs dict(iterable) -> new dictionary initialized as if via: d = {} for k, v in iterable: d[k] = v dict(**kwargs) -> new dictionary initialized with the name=value pairs in the keyword argument list. For example: dict(one=1, two=2)
    - Function: `caltech101_wrapper_factory(dataset, target_keys)`
      - Description: No docstring available
    - Function: `celeba_wrapper_factory(dataset, target_keys)`
      - Description: No docstring available
    - Function: `cityscapes_wrapper_factory(dataset, target_keys)`
      - Description: No docstring available
    - Function: `classification_wrapper_factory(dataset, target_keys)`
      - Description: No docstring available
    - Function: `coco_dectection_wrapper_factory(dataset, target_keys)`
      - Description: No docstring available
    - Class: `dataset_cls(root: str, annotation_path: str, frames_per_clip: int, step_between_clips: int = 1, frame_rate: Optional[int] = None, fold: int = 1, train: bool = True, transform: Optional[Callable] = None, _precomputed_metadata: Optional[Dict[str, Any]] = None, num_workers: int = 1, _video_width: int = 0, _video_height: int = 0, _video_min_dimension: int = 0, _audio_samples: int = 0, output_format: str = 'THWC') -> None`
      - Description: `UCF101 <https://www.crcv.ucf.edu/data/UCF101.php>`_ dataset. UCF101 is an action recognition video dataset. This dataset consider every video as a collection of video clips of fixed size, specified by ``frames_per_clip``, where the step in frames between each clip is given by ``step_between_clips``. The dataset itself can be downloaded from the dataset website; annotations that ``annotation_path`` should be pointing to can be downloaded from `here <https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip>`_. To give an example, for 2 videos with 10 and 15 frames respectively, if ``frames_per_clip=5`` and ``step_between_clips=5``, the dataset size will be (2 + 3) = 5, where the first two elements will come from video 1, and the next three elements from video 2. Note that we drop clips which do not have exactly ``frames_per_clip`` elements, so not all frames in a video might be present. Internally, it uses a VideoClips object to handle clip creation. Args: root (string): Root directory of the UCF101 Dataset. annotation_path (str): path to the folder containing the split files; see docstring above for download instructions of these files frames_per_clip (int): number of frames in a clip. step_between_clips (int, optional): number of frames between each clip. fold (int, optional): which fold to use. Should be between 1 and 3. train (bool, optional): if ``True``, creates a dataset from the train split, otherwise from the ``test`` split. transform (callable, optional): A function/transform that takes in a TxHxWxC video and returns a transformed version. output_format (str, optional): The format of the output video tensors (before transforms). Can be either "THWC" (default) or "TCHW". Returns: tuple: A 3-tuple with the following entries: - video (Tensor[T, H, W, C] or Tensor[T, C, H, W]): The `T` video frames - audio(Tensor[K, L]): the audio frames, where `K` is the number of channels and `L` is the number of points - label (int): class of the video clip
    - Class: `defaultdict(Unable to retrieve signature)`
      - Description: defaultdict(default_factory=None, /, [...]) --> dict with default factory The default factory is called without arguments to produce a new value when a key is not present, in __getitem__ only. A defaultdict compares equal to a dict with the same items. All remaining arguments are treated the same as if they were passed to the dict constructor, including keyword arguments.
    - Function: `identity(item)`
      - Description: No docstring available
    - Function: `identity_wrapper_factory(dataset, target_keys)`
      - Description: No docstring available
    - Function: `kitti_wrapper_factory(dataset, target_keys)`
      - Description: No docstring available
    - Function: `list_of_dicts_to_dict_of_lists(list_of_dicts)`
      - Description: No docstring available
    - Function: `oxford_iiit_pet_wrapper_factor(dataset, target_keys)`
      - Description: No docstring available
    - Function: `parse_target_keys(target_keys, *, available, default)`
      - Description: No docstring available
    - Function: `pil_image_to_mask(pil_image)`
      - Description: No docstring available
    - Function: `raise_not_supported(description)`
      - Description: No docstring available
    - Function: `sbd_wrapper(dataset, target_keys)`
      - Description: No docstring available
    - Function: `segmentation_wrapper_factory(dataset, target_keys)`
      - Description: No docstring available
    - Function: `video_classification_wrapper_factory(dataset, target_keys)`
      - Description: No docstring available
    - Function: `voc_detection_wrapper_factory(dataset, target_keys)`
      - Description: No docstring available
    - Function: `widerface_wrapper(dataset, target_keys)`
      - Description: No docstring available
    - Function: `wrap_dataset_for_transforms_v2(dataset, target_keys=None)`
      - Description: [BETA] Wrap a ``torchvision.dataset`` for usage with :mod:`torchvision.transforms.v2`. .. v2betastatus:: wrap_dataset_for_transforms_v2 function Example: >>> dataset = torchvision.datasets.CocoDetection(...) >>> dataset = wrap_dataset_for_transforms_v2(dataset) .. note:: For now, only the most popular datasets are supported. Furthermore, the wrapper only supports dataset configurations that are fully supported by ``torchvision.transforms.v2``. If you encounter an error prompting you to raise an issue to ``torchvision`` for a dataset or configuration that you need, please do so. The dataset samples are wrapped according to the description below. Special cases: * :class:`~torchvision.datasets.CocoDetection`: Instead of returning the target as list of dicts, the wrapper returns a dict of lists. In addition, the key-value-pairs ``"boxes"`` (in ``XYXY`` coordinate format), ``"masks"`` and ``"labels"`` are added and wrap the data in the corresponding ``torchvision.tv_tensors``. The original keys are preserved. If ``target_keys`` is omitted, returns only the values for the ``"image_id"``, ``"boxes"``, and ``"labels"``. * :class:`~torchvision.datasets.VOCDetection`: The key-value-pairs ``"boxes"`` and ``"labels"`` are added to the target and wrap the data in the corresponding ``torchvision.tv_tensors``. The original keys are preserved. If ``target_keys`` is omitted, returns only the values for the ``"boxes"`` and ``"labels"``. * :class:`~torchvision.datasets.CelebA`: The target for ``target_type="bbox"`` is converted to the ``XYXY`` coordinate format and wrapped into a :class:`~torchvision.tv_tensors.BoundingBoxes` tv_tensor. * :class:`~torchvision.datasets.Kitti`: Instead returning the target as list of dicts, the wrapper returns a dict of lists. In addition, the key-value-pairs ``"boxes"`` and ``"labels"`` are added and wrap the data in the corresponding ``torchvision.tv_tensors``. The original keys are preserved. If ``target_keys`` is omitted, returns only the values for the ``"boxes"`` and ``"labels"``. * :class:`~torchvision.datasets.OxfordIIITPet`: The target for ``target_type="segmentation"`` is wrapped into a :class:`~torchvision.tv_tensors.Mask` tv_tensor. * :class:`~torchvision.datasets.Cityscapes`: The target for ``target_type="semantic"`` is wrapped into a :class:`~torchvision.tv_tensors.Mask` tv_tensor. The target for ``target_type="instance"`` is *replaced* by a dictionary with the key-value-pairs ``"masks"`` (as :class:`~torchvision.tv_tensors.Mask` tv_tensor) and ``"labels"``. * :class:`~torchvision.datasets.WIDERFace`: The value for key ``"bbox"`` in the target is converted to ``XYXY`` coordinate format and wrapped into a :class:`~torchvision.tv_tensors.BoundingBoxes` tv_tensor. Image classification datasets This wrapper is a no-op for image classification datasets, since they were already fully supported by :mod:`torchvision.transforms` and thus no change is needed for :mod:`torchvision.transforms.v2`. Segmentation datasets Segmentation datasets, e.g. :class:`~torchvision.datasets.VOCSegmentation`, return a two-tuple of :class:`PIL.Image.Image`'s. This wrapper leaves the image as is (first item), while wrapping the segmentation mask into a :class:`~torchvision.tv_tensors.Mask` (second item). Video classification datasets Video classification datasets, e.g. :class:`~torchvision.datasets.Kinetics`, return a three-tuple containing a :class:`torch.Tensor` for the video and audio and a :class:`int` as label. This wrapper wraps the video into a :class:`~torchvision.tv_tensors.Video` while leaving the other items as is. .. note:: Only datasets constructed with ``output_format="TCHW"`` are supported, since the alternative ``output_format="THWC"`` is not supported by :mod:`torchvision.transforms.v2`. Args: dataset: the dataset instance to wrap for compatibility with transforms v2. target_keys: Target keys to return in case the target is a dictionary. If ``None`` (default), selected keys are specific to the dataset. If ``"all"``, returns the full target. Can also be a collection of strings for fine grained access. Currently only supported for :class:`~torchvision.datasets.CocoDetection`, :class:`~torchvision.datasets.VOCDetection`, :class:`~torchvision.datasets.Kitti`, and :class:`~torchvision.datasets.WIDERFace`. See above for details.
    - Function: `wrap_target_by_type(target, *, target_types, type_wrappers)`
      - Description: No docstring available
  - _image.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Image(data: 'Any', *, dtype: 'Optional[torch.dtype]' = None, device: 'Optional[Union[torch.device, str, int]]' = None, requires_grad: 'Optional[bool]' = None) -> 'Image'`
      - Description: [BETA] :class:`torch.Tensor` subclass for images. .. note:: In the :ref:`transforms <transforms>`, ``Image`` instances are largely interchangeable with pure :class:`torch.Tensor`. See :ref:`this note <passthrough_heuristic>` for more details. Args: data (tensor-like, PIL.Image.Image): Any data that can be turned into a tensor with :func:`torch.as_tensor` as well as PIL images. dtype (torch.dtype, optional): Desired data type. If omitted, will be inferred from ``data``. device (torch.device, optional): Desired device. If omitted and ``data`` is a :class:`torch.Tensor`, the device is taken from it. Otherwise, the image is constructed on the CPU. requires_grad (bool, optional): Whether autograd should record operations. If omitted and ``data`` is a :class:`torch.Tensor`, the value is taken from it. Otherwise, defaults to ``False``.
    - Class: `TVTensor(Unable to retrieve signature)`
      - Description: [Beta] Base class for all TVTensors. You probably don't want to use this class unless you're defining your own custom TVTensors. See :ref:`sphx_glr_auto_examples_transforms_plot_custom_tv_tensors.py` for details.
  - _mask.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `Mask(data: 'Any', *, dtype: 'Optional[torch.dtype]' = None, device: 'Optional[Union[torch.device, str, int]]' = None, requires_grad: 'Optional[bool]' = None) -> 'Mask'`
      - Description: [BETA] :class:`torch.Tensor` subclass for segmentation and detection masks. Args: data (tensor-like, PIL.Image.Image): Any data that can be turned into a tensor with :func:`torch.as_tensor` as well as PIL images. dtype (torch.dtype, optional): Desired data type. If omitted, will be inferred from ``data``. device (torch.device, optional): Desired device. If omitted and ``data`` is a :class:`torch.Tensor`, the device is taken from it. Otherwise, the mask is constructed on the CPU. requires_grad (bool, optional): Whether autograd should record operations. If omitted and ``data`` is a :class:`torch.Tensor`, the value is taken from it. Otherwise, defaults to ``False``.
    - Class: `TVTensor(Unable to retrieve signature)`
      - Description: [Beta] Base class for all TVTensors. You probably don't want to use this class unless you're defining your own custom TVTensors. See :ref:`sphx_glr_auto_examples_transforms_plot_custom_tv_tensors.py` for details.
  - _torch_function_helpers.py
    - Class: `_ReturnTypeCM(to_restore)`
      - Description: No docstring available
    - Function: `_must_return_subclass()`
      - Description: No docstring available
    - Function: `set_return_type(return_type: str)`
      - Description: [BETA] Set the return type of torch operations on :class:`~torchvision.tv_tensors.TVTensor`. This only affects the behaviour of torch operations. It has no effect on ``torchvision`` transforms or functionals, which will always return as output the same type that was passed as input. .. warning:: We recommend using :class:`~torchvision.transforms.v2.ToPureTensor` at the end of your transform pipelines if you use ``set_return_type("TVTensor")``. This will avoid the ``__torch_function__`` overhead in the models ``forward()``. Can be used as a global flag for the entire program: .. code:: python img = tv_tensors.Image(torch.rand(3, 5, 5)) img + 2 # This is a pure Tensor (default behaviour) set_return_type("TVTensor") img + 2 # This is an Image or as a context manager to restrict the scope: .. code:: python img = tv_tensors.Image(torch.rand(3, 5, 5)) img + 2 # This is a pure Tensor with set_return_type("TVTensor"): img + 2 # This is an Image img + 2 # This is a pure Tensor Args: return_type (str): Can be "TVTensor" or "Tensor" (case-insensitive). Default is "Tensor" (i.e. pure :class:`torch.Tensor`).
  - _tv_tensor.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `DisableTorchFunctionSubclass(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `TVTensor(Unable to retrieve signature)`
      - Description: [Beta] Base class for all TVTensors. You probably don't want to use this class unless you're defining your own custom TVTensors. See :ref:`sphx_glr_auto_examples_transforms_plot_custom_tv_tensors.py` for details.
    - Class: `TypeVar(name, *constraints, bound=None, covariant=False, contravariant=False)`
      - Description: Type variable. Usage:: T = TypeVar('T') # Can be anything A = TypeVar('A', str, bytes) # Must be str or bytes Type variables exist primarily for the benefit of static type checkers. They serve as the parameters for generic types as well as for generic function definitions. See class Generic for more information on generic types. Generic functions work as follows: def repeat(x: T, n: int) -> List[T]: '''Return a list containing n references to x.''' return [x]*n def longest(x: A, y: A) -> A: '''Return the longest of two strings.''' return x if len(x) >= len(y) else y The latter example's signature is essentially the overloading of (str, str) -> str and (bytes, bytes) -> bytes. Also note that if the arguments are instances of some subclass of str, the return type is still plain str. At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError. Type variables defined with covariant=True or contravariant=True can be used to declare covariant or contravariant generic types. See PEP 484 for more details. By default generic types are invariant in all type variables. Type variables can be introspected. e.g.: T.__name__ == 'T' T.__constraints__ == () T.__covariant__ == False T.__contravariant__ = False A.__constraints__ == (str, bytes) Note that only type variables defined in global scope can be pickled.
    - Class: `_device(Unable to retrieve signature)`
      - Description: No docstring available
    - Class: `_dtype()`
      - Description: No docstring available
    - Function: `_must_return_subclass()`
      - Description: No docstring available
  - _video.py
    - Class: `Any(*args, **kwargs)`
      - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
    - Class: `TVTensor(Unable to retrieve signature)`
      - Description: [Beta] Base class for all TVTensors. You probably don't want to use this class unless you're defining your own custom TVTensors. See :ref:`sphx_glr_auto_examples_transforms_plot_custom_tv_tensors.py` for details.
    - Class: `Video(data: 'Any', *, dtype: 'Optional[torch.dtype]' = None, device: 'Optional[Union[torch.device, str, int]]' = None, requires_grad: 'Optional[bool]' = None) -> 'Video'`
      - Description: [BETA] :class:`torch.Tensor` subclass for videos. Args: data (tensor-like): Any data that can be turned into a tensor with :func:`torch.as_tensor`. dtype (torch.dtype, optional): Desired data type. If omitted, will be inferred from ``data``. device (torch.device, optional): Desired device. If omitted and ``data`` is a :class:`torch.Tensor`, the device is taken from it. Otherwise, the video is constructed on the CPU. requires_grad (bool, optional): Whether autograd should record operations. If omitted and ``data`` is a :class:`torch.Tensor`, the value is taken from it. Otherwise, defaults to ``False``.
- __pycache__/
- extension.py
  - Function: `_assert_has_ops()`
    - Description: No docstring available
  - Function: `_check_cuda_version()`
    - Description: Make sure that CUDA versions match between the pytorch install and torchvision install
  - Function: `_get_extension_path(lib_name)`
    - Description: No docstring available
  - Function: `_has_ops()`
    - Description: No docstring available
  - Function: `_load_library(lib_name)`
    - Description: No docstring available
- utils.py
  - Class: `Any(*args, **kwargs)`
    - Description: Special type indicating an unconstrained type. - Any is compatible with every type. - Any assumed to have all methods. - All values assumed to be instances of Any. Note that all the above statements are true from the point of view of static type checkers. At runtime, Any should not be used with instance checks.
  - Class: `BinaryIO()`
    - Description: Typed version of the return of open() in binary mode.
  - Class: `FunctionType(code, globals, name=None, argdefs=None, closure=None)`
    - Description: Create a function object. code a code object globals the globals dictionary name a string that overrides the name from the code object argdefs a tuple that specifies the default argument values closure a tuple that supplies the bindings for free variables
  - Function: `_generate_color_palette(num_objects: int)`
    - Description: No docstring available
  - Function: `_log_api_usage_once(obj: Any) -> None`
    - Description: Logs API usage(module and name) within an organization. In a large ecosystem, it's often useful to track the PyTorch and TorchVision APIs usage. This API provides the similar functionality to the logging module in the Python stdlib. It can be used for debugging purpose to log which methods are used and by default it is inactive, unless the user manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_. Please note it is triggered only once for the same API call within a process. It does not collect any data from open-source users since it is no-op by default. For more information, please refer to * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging; * Logging policy: https://github.com/pytorch/vision/issues/5052; Args: obj (class instance or method): an object to extract info from.
  - Function: `_make_colorwheel() -> torch.Tensor`
    - Description: Generates a color wheel for optical flow visualization as presented in: Baker et al. "A Database and Evaluation Methodology for Optical Flow" (ICCV, 2007) URL: http://vision.middlebury.edu/flow/flowEval-iccv07.pdf. Returns: colorwheel (Tensor[55, 3]): Colorwheel Tensor.
  - Function: `_make_ntuple(x: Any, n: int) -> Tuple[Any, ...]`
    - Description: Make n-tuple from input x. If x is an iterable, then we just convert it to tuple. Otherwise, we will make a tuple of length n, all with value of x. reference: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/utils.py#L8 Args: x (Any): input value n (int): length of the resulting tuple
  - Function: `_normalized_flow_to_image(normalized_flow: torch.Tensor) -> torch.Tensor`
    - Description: Converts a batch of normalized flow to an RGB image. Args: normalized_flow (torch.Tensor): Normalized flow tensor of shape (N, 2, H, W) Returns: img (Tensor(N, 3, H, W)): Flow visualization image of dtype uint8.
  - Function: `_parse_colors(colors: Union[NoneType, str, Tuple[int, int, int], List[Union[str, Tuple[int, int, int]]]], *, num_objects: int) -> List[Tuple[int, int, int]]`
    - Description: Parses a specification of colors for a set of objects. Args: colors: A specification of colors for the objects. This can be one of the following: - None: to generate a color palette automatically. - A list of colors: where each color is either a string (specifying a named color) or an RGB tuple. - A string or an RGB tuple: to use the same color for all objects. If `colors` is a tuple, it should be a 3-tuple specifying the RGB values of the color. If `colors` is a list, it should have at least as many elements as the number of objects to color. num_objects (int): The number of objects to color. Returns: A list of 3-tuples, specifying the RGB values of the colors. Raises: ValueError: If the number of colors in the list is less than the number of objects to color. If `colors` is not a list, tuple, string or None.
  - Function: `draw_bounding_boxes(image: torch.Tensor, boxes: torch.Tensor, labels: Optional[List[str]] = None, colors: Union[List[Union[str, Tuple[int, int, int]]], str, Tuple[int, int, int], NoneType] = None, fill: Optional[bool] = False, width: int = 1, font: Optional[str] = None, font_size: Optional[int] = None) -> torch.Tensor`
    - Description: Draws bounding boxes on given image. The values of the input image should be uint8 between 0 and 255. If fill is True, Resulting Tensor should be saved as PNG image. Args: image (Tensor): Tensor of shape (C x H x W) and dtype uint8. boxes (Tensor): Tensor of size (N, 4) containing bounding boxes in (xmin, ymin, xmax, ymax) format. Note that the boxes are absolute coordinates with respect to the image. In other words: `0 <= xmin < xmax < W` and `0 <= ymin < ymax < H`. labels (List[str]): List containing the labels of bounding boxes. colors (color or list of colors, optional): List containing the colors of the boxes or single color for all boxes. The color can be represented as PIL strings e.g. "red" or "#FF00FF", or as RGB tuples e.g. ``(240, 10, 157)``. By default, random colors are generated for boxes. fill (bool): If `True` fills the bounding box with specified color. width (int): Width of bounding box. font (str): A filename containing a TrueType font. If the file is not found in this filename, the loader may also search in other directories, such as the `fonts/` directory on Windows or `/Library/Fonts/`, `/System/Library/Fonts/` and `~/Library/Fonts/` on macOS. font_size (int): The requested font size in points. Returns: img (Tensor[C, H, W]): Image Tensor of dtype uint8 with bounding boxes plotted.
  - Function: `draw_keypoints(image: torch.Tensor, keypoints: torch.Tensor, connectivity: Optional[List[Tuple[int, int]]] = None, colors: Union[str, Tuple[int, int, int], NoneType] = None, radius: int = 2, width: int = 3) -> torch.Tensor`
    - Description: Draws Keypoints on given RGB image. The values of the input image should be uint8 between 0 and 255. Args: image (Tensor): Tensor of shape (3, H, W) and dtype uint8. keypoints (Tensor): Tensor of shape (num_instances, K, 2) the K keypoints location for each of the N instances, in the format [x, y]. connectivity (List[Tuple[int, int]]]): A List of tuple where, each tuple contains pair of keypoints to be connected. colors (str, Tuple): The color can be represented as PIL strings e.g. "red" or "#FF00FF", or as RGB tuples e.g. ``(240, 10, 157)``. radius (int): Integer denoting radius of keypoint. width (int): Integer denoting width of line connecting keypoints. Returns: img (Tensor[C, H, W]): Image Tensor of dtype uint8 with keypoints drawn.
  - Function: `draw_segmentation_masks(image: torch.Tensor, masks: torch.Tensor, alpha: float = 0.8, colors: Union[List[Union[str, Tuple[int, int, int]]], str, Tuple[int, int, int], NoneType] = None) -> torch.Tensor`
    - Description: Draws segmentation masks on given RGB image. The values of the input image should be uint8 between 0 and 255. Args: image (Tensor): Tensor of shape (3, H, W) and dtype uint8. masks (Tensor): Tensor of shape (num_masks, H, W) or (H, W) and dtype bool. alpha (float): Float number between 0 and 1 denoting the transparency of the masks. 0 means full transparency, 1 means no transparency. colors (color or list of colors, optional): List containing the colors of the masks or single color for all masks. The color can be represented as PIL strings e.g. "red" or "#FF00FF", or as RGB tuples e.g. ``(240, 10, 157)``. By default, random colors are generated for each mask. Returns: img (Tensor[C, H, W]): Image Tensor, with segmentation masks drawn on top.
  - Function: `flow_to_image(flow: torch.Tensor) -> torch.Tensor`
    - Description: Converts a flow to an RGB image. Args: flow (Tensor): Flow of shape (N, 2, H, W) or (2, H, W) and dtype torch.float. Returns: img (Tensor): Image Tensor of dtype uint8 where each color corresponds to a given flow direction. Shape is (N, 3, H, W) or (3, H, W) depending on the input.
  - Function: `make_grid(tensor: Union[torch.Tensor, List[torch.Tensor]], nrow: int = 8, padding: int = 2, normalize: bool = False, value_range: Optional[Tuple[int, int]] = None, scale_each: bool = False, pad_value: float = 0.0) -> torch.Tensor`
    - Description: Make a grid of images. Args: tensor (Tensor or list): 4D mini-batch Tensor of shape (B x C x H x W) or a list of images all of the same size. nrow (int, optional): Number of images displayed in each row of the grid. The final grid size is ``(B / nrow, nrow)``. Default: ``8``. padding (int, optional): amount of padding. Default: ``2``. normalize (bool, optional): If True, shift the image to the range (0, 1), by the min and max values specified by ``value_range``. Default: ``False``. value_range (tuple, optional): tuple (min, max) where min and max are numbers, then these numbers are used to normalize the image. By default, min and max are computed from the tensor. scale_each (bool, optional): If ``True``, scale each image in the batch of images separately rather than the (min, max) over all images. Default: ``False``. pad_value (float, optional): Value for the padded pixels. Default: ``0``. Returns: grid (Tensor): the tensor containing grid of images.
  - Class: `repeat(Unable to retrieve signature)`
    - Description: repeat(object [,times]) -> create an iterator which returns the object for the specified number of times. If not specified, returns the object endlessly.
  - Function: `save_image(tensor: Union[torch.Tensor, List[torch.Tensor]], fp: Union[str, pathlib.Path, BinaryIO], format: Optional[str] = None, **kwargs) -> None`
    - Description: Save a given Tensor into an image file. Args: tensor (Tensor or list): Image to be saved. If given a mini-batch tensor, saves the tensor as a grid of images by calling ``make_grid``. fp (string or file object): A filename or a file object format(Optional): If omitted, the format to use is determined from the filename extension. If a file object was used instead of a filename, this parameter should always be used. **kwargs: Other arguments are documented in ``make_grid``.
- version.py
  - Function: `_check_cuda_version()`
    - Description: Make sure that CUDA versions match between the pytorch install and torchvision install
- _internally_replaced_utils.py
  - Function: `_download_file_from_remote_location(fpath: str, url: str) -> None`
    - Description: No docstring available
  - Function: `_get_extension_path(lib_name)`
    - Description: No docstring available
  - Function: `_get_torch_home()`
    - Description: No docstring available
  - Function: `_is_remote_location_available() -> bool`
    - Description: No docstring available
  - Function: `load_state_dict_from_url(url: str, model_dir: Optional[str] = None, map_location: Union[Callable[[torch.Tensor, str], torch.Tensor], torch.device, str, Dict[str, str], NoneType] = None, progress: bool = True, check_hash: bool = False, file_name: Optional[str] = None, weights_only: bool = False) -> Dict[str, Any]`
    - Description: Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically decompressed. If the object is already present in `model_dir`, it's deserialized and returned. The default value of ``model_dir`` is ``<hub_dir>/checkpoints`` where ``hub_dir`` is the directory returned by :func:`~torch.hub.get_dir`. Args: url (str): URL of the object to download model_dir (str, optional): directory in which to save the object map_location (optional): a function or a dict specifying how to remap storage locations (see torch.load) progress (bool, optional): whether or not to display a progress bar to stderr. Default: True check_hash(bool, optional): If True, the filename part of the URL should follow the naming convention ``filename-<sha256>.ext`` where ``<sha256>`` is the first eight or more digits of the SHA256 hash of the contents of the file. The hash is used to ensure unique names and to verify the contents of the file. Default: False file_name (str, optional): name for the downloaded file. Filename from ``url`` will be used if not set. weights_only(bool, optional): If True, only weights will be loaded and no complex pickled objects. Recommended for untrusted sources. See :func:`~torch.load` for more details. Example: >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_HUB) >>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')
- _meta_registrations.py
  - Function: `meta_roi_align(input, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned)`
    - Description: No docstring available
  - Function: `meta_roi_align_backward(grad, rois, spatial_scale, pooled_height, pooled_width, batch_size, channels, height, width, sampling_ratio, aligned)`
    - Description: No docstring available
  - Function: `register_meta(op_name, overload_name='default')`
    - Description: No docstring available
- _utils.py
  - Class: `StrEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)`
    - Description: Create a collection of name/value pairs. Example enumeration: >>> class Color(Enum): ... RED = 1 ... BLUE = 2 ... GREEN = 3 Access them by: - attribute access:: >>> Color.RED <Color.RED: 1> - value lookup: >>> Color(1) <Color.RED: 1> - name lookup: >>> Color['RED'] <Color.RED: 1> Enumerations can be iterated over, and know how many members they have: >>> len(Color) 3 >>> list(Color) [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>] Methods can be added to enumerations, and members can have their own attributes -- see the documentation for details.
  - Class: `StrEnumMeta(cls, bases, classdict, *, boundary=None, _simple=False, **kwds)`
    - Description: Metaclass for Enum
  - Class: `TypeVar(name, *constraints, bound=None, covariant=False, contravariant=False)`
    - Description: Type variable. Usage:: T = TypeVar('T') # Can be anything A = TypeVar('A', str, bytes) # Must be str or bytes Type variables exist primarily for the benefit of static type checkers. They serve as the parameters for generic types as well as for generic function definitions. See class Generic for more information on generic types. Generic functions work as follows: def repeat(x: T, n: int) -> List[T]: '''Return a list containing n references to x.''' return [x]*n def longest(x: A, y: A) -> A: '''Return the longest of two strings.''' return x if len(x) >= len(y) else y The latter example's signature is essentially the overloading of (str, str) -> str and (bytes, bytes) -> bytes. Also note that if the arguments are instances of some subclass of str, the return type is still plain str. At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError. Type variables defined with covariant=True or contravariant=True can be used to declare covariant or contravariant generic types. See PEP 484 for more details. By default generic types are invariant in all type variables. Type variables can be introspected. e.g.: T.__name__ == 'T' T.__constraints__ == () T.__covariant__ == False T.__contravariant__ = False A.__constraints__ == (str, bytes) Note that only type variables defined in global scope can be pickled.
  - Function: `sequence_to_str(seq: Sequence, separate_last: str = '') -> str`
    - Description: No docstring available

{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":153348,"status":"ok","timestamp":1711272077864,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"UuVhFPeOwUkz","outputId":"67d64e20-d283-4ff4-9e30-900f8ab93bcc"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m810.5/810.5 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.1/163.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.1/269.1 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q PyMuPDF  datasets python-pptx langchain sentence-transformers  accelerate bitsandbytes faiss-gpu openpyxl  pacmap"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22096,"status":"ok","timestamp":1711272134330,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"QVHa2IMutB11","outputId":"7d47b298-e6af-43be-dc35-b7ff97996c4c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m71.7/81.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q arxiv"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":853583,"status":"error","timestamp":1711272987910,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"3FKRaUIZtByV","outputId":"906f1f38-ee50-47cc-9da4-51f6798abab3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Failed to download https://scholar.google.com/scholar_url%3Furl%3Dhttps://proceedings.neurips.cc/paper_files/paper/2023/file/a97b58c4f7551053b0512f92244b0810-Paper-Conference.pdf: 404 Client Error: Not Found for url: https://scholar.google.com/scholar_url%3Furl%3Dhttps://proceedings.neurips.cc/paper_files/paper/2023/file/a97b58c4f7551053b0512f92244b0810-Paper-Conference.pdf\n","Failed to download https://scholar.google.com/scholar_url%3Furl%3Dhttps://arxiv.org/pdf/2309.02705.pdf: 404 Client Error: Not Found for url: https://scholar.google.com/scholar_url%3Furl%3Dhttps://arxiv.org/pdf/2309.02705.pdf\n","Downloaded 3 of 200 files: Xinyun_talk_advllm.pdf\n","Failed to download https://www.utdallas.edu/~muratk/courses/adv-ml-19f/Adversarial%2520Examples%2520that%2520Fool%2520both%2520Computer%2520Vision%2520and%2520Time-Limited%2520Humans.pdf: 404 Client Error: Not Found for url: https://personal.utdallas.edu/~muratk/courses/adv-ml-19f/Adversarial%2520Examples%2520that%2520Fool%2520both%2520Computer%2520Vision%2520and%2520Time-Limited%2520Humans.pdf\n","Failed to download https://www.utdallas.edu/~muratk/courses/adv-ml-19f/Adversarial%2520Examples%2520that%2520Fool%2520both%2520Computer%2520Vision%2520and%2520Time-Limited%2520Humans.pdf: 404 Client Error: Not Found for url: https://personal.utdallas.edu/~muratk/courses/adv-ml-19f/Adversarial%2520Examples%2520that%2520Fool%2520both%2520Computer%2520Vision%2520and%2520Time-Limited%2520Humans.pdf\n","MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?\n","Language Repository for Long Video Understanding\n","Detoxifying Large Language Models via Knowledge Editing\n","Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen Domains by Intrinsic Learning from Redundant LLM Semantics\n","LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding\n","CoMo: Controllable Motion Generation through Language Guided Pose Code Editing\n","Large Language Models meet Network Slicing Management and Orchestration\n","Encoding the Subsurface in 3D with Seismic\n","FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs\n","VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis\n","Improved Baselines for Data-efficient Perceptual Augmentation of LLMs\n","HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models\n","ManiPose: A Comprehensive Benchmark for Pose-aware Object Manipulation in Robotics\n","Flickr30K-CFQ: A Compact and Fragmented Query Dataset for Text-image Retrieval\n","PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns\n","VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning\n","SceneScript: Reconstructing Scenes With An Autoregressive Structured Language Model\n","When Do We Not Need Larger Vision Models?\n","Semantic Layering in Room Segmentation via LLMs\n","Yell At Your Robot: Improving On-the-Fly from Language Corrections\n","mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding\n","HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning\n","RelationVLM: Making Large Vision-Language Models Understand Visual Relations\n","Towards Multimodal In-Context Learning for Vision & Language Models\n","Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs\n","UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All\n","VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation\n","Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification\n","From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models\n","Agent3D-Zero: An Agent for Zero-shot 3D Understanding\n","Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs\n","LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images\n","VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding\n","Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning\n","X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment\n","Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches\n","PhD: A Prompted Visual Hallucination Evaluation Dataset\n","m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks\n","Tokensome: Towards a Genetic Vision-Language GPT for Explainable and Cognitive Karyotyping\n","Large Language Models Powered Context-aware Motion Prediction\n","Neural Erosion: Emulating Controlled Neurodegeneration and Aging in AI Systems\n","Using an LLM to Turn Sign Spottings into Spoken Language Sentences\n","Uni-SMART: Universal Science Multimodal Analysis and Research Transformer\n","HawkEye: Training Video-Text LLMs for Grounding Text in Videos\n","AUTONODE: A Neuro-Graphic Self-Learnable Engine for Cognitive GUI Automation\n","Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs Collaborated Reasoning\n","CrossGLG: LLM Guides One-shot Skeleton-based 3D Action Recognition in a Cross-level Manner\n","TextBlockV2: Towards Precise-Detection-Free Scene Text Spotting with Pre-trained Language Model\n","Language to Map: Topological map generation from natural language path instructions\n","ViTCN: Vision Transformer Contrastive Network For Reasoning\n","Right Place, Right Time! Towards ObjectNav for Non-Stationary Goals\n","3D-VLA: A 3D Vision-Language-Action Generative World Model\n","MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\n","Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation\n","Less is More: Data Value Estimation for Visual Instruction Tuning\n","VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding\n","GiT: Towards Generalist Vision Transformer through Universal Language Interface\n","ChartInstruct: Instruction Tuning for Chart Comprehension and Reasoning\n","VisionGPT: Vision-Language Understanding Agent Using Generalized Multimodal Framework\n","Foundation Models and Information Retrieval in Digital Pathology\n","PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models\n","Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization\n","Masked Generative Story Transformer with Character Guidance and Caption Augmentation\n","A Picture Is Worth a Thousand Words: Exploring Diagram and Video-Based OOP Exercises to Counter LLM Over-Reliance\n","Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation\n","TINA: Think, Interaction, and Action Framework for Zero-Shot Vision Language Navigation\n","LG-Traj: LLM Guided Pedestrian Trajectory Prediction\n","Pix2Pix-OnTheFly: Leveraging LLMs for Instruction-Guided Image Editing\n","Beyond Text: Frozen Large Language Models in Visual Signal Comprehension\n","Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings\n","Multi-modal Auto-regressive Modeling via Visual Words\n","MoAI: Mixture of All Intelligence for Large Language and Vision Models\n","Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The Lengths, Bends, and Dead Ends\n","NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning\n","Let Storytelling Tell Vivid Stories: An Expressive and Fluent Multimodal Storyteller\n","AesopAgent: Agent-driven Evolutionary System on Story-to-Video Production\n","Action Reimagined: Text-to-Pose Video Editing for Dynamic Human Actions\n","SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data\n","Naming, Describing, and Quantifying Visual Objects in Humans and LLMs\n","FocusCLIP: Multimodal Subject-Level Guidance for Zero-Shot Transfer in Human-Centric Tasks\n","DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation\n","Real-Time Multimodal Cognitive Assistant for Emergency Medical Services\n","Can LLMs' Tuning Methods Work in Medical Multimodal Domain?\n","Amharic LLaMA and LLaVA: Multimodal LLMs for Low Resource Languages\n","Low-dose CT Denoising with Language-engaged Dual-space Alignment\n","Reframe Anything: LLM Agent for Open World Video Reframing\n","LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content\n","Are Large Language Models Aligned with People's Social Intuitions for Human-Robot Interactions?\n","SeeGULL Multilingual: a Dataset of Geo-Culturally Situated Stereotypes\n","Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4\n","DeepSeek-VL: Towards Real-World Vision-Language Understanding\n","Beyond Finite Data: Towards Data-free Out-of-distribution Generalization via Extrapolation\n","Will GPT-4 Run DOOM?\n","Debiasing Large Visual Language Models\n","ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment\n","DiffChat: Learning to Chat with Text-to-Image Synthesis Models for Interactive Image Creation\n","Embracing Large Language and Multimodal Models for Prosthetic Technologies\n","$\\text{R}^2$-Bench: Benchmarking the Robustness of Referring Perception Models under Perturbations\n","SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM\n","How Far Are We from Intelligent Visual Deductive Reasoning?\n","Enhancing chest X-ray datasets with privacy-preserving large language models and multi-type annotations: a data-driven approach for improved classification\n","Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning\n","Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection from Remote Sensing Imagery\n","Multimodal Transformer for Comics Text-Cloze\n","CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments\n","Design2Code: How Far Are We From Automating Front-End Engineering?\n","Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception\n","ChatGPT and biometrics: an assessment of face recognition, gender detection, and age estimation capabilities\n","ImgTrojan: Jailbreaking Vision-Language Models with ONE Image\n","Android in the Zoo: Chain-of-Action-Thought for GUI Agents\n","Interactive Continual Learning: Fast and Slow Thinking\n","Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use\n","Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research\n","RegionGPT: Towards Region Understanding Vision Language Model\n","NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n","SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos\n","GuardT2I: Defending Text-to-Image Models from Adversarial Prompts\n","GPTSee: Enhancing Moment Retrieval and Highlight Detection via Description-Based Similarity Features\n","SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code\n","Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning\n","Large Language Models for Simultaneous Named Entity Extraction and Spelling Correction\n","TempCompass: Do Video LLMs Really Understand Videos?\n","LLMs in Political Science: Heralding a New Era of Visual Analysis\n","Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Model\n","A SOUND APPROACH: Using Large Language Models to generate audio descriptions for egocentric text-audio retrieval\n","From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs\n","Chaining text-to-image and large language model: A novel approach for generating personalized e-commerce banners\n","ShapeLLM: Universal 3D Object Understanding for Embodied Interaction\n","Massive Activations in Large Language Models\n","Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual Capabilities Without Richer Cross-Modal Projections\n","A Surprising Failure? Multimodal LLMs and the NLVR Challenge\n","Towards Open-ended Visual Quality Comparison\n","Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models\n","From Large Language Models and Optimization to Decision Optimization CoPilot: A Research Manifesto\n","CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document Visual Question Answering\n","AVI-Talking: Learning Audio-Visual Instructions for Expressive 3D Talking Face Generation\n","InstructEdit: Instruction-based Knowledge Editing for Large Language Models\n","LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding\n","LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper\n","Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models\n","Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics\n","On Trojan Signatures in Large Language Models of Code\n","Selective \"Selective Prediction\": Reducing Unnecessary Abstention in Vision-Language Reasoning\n","AutoMMLab: Automatically Generating Deployable Models from Language Instructions for Computer Vision Tasks\n","Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing\n","Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing\n","Large Multimodal Agents: A Survey\n","Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images\n","Visual Hallucinations of Multi-modal Large Language Models\n","From Keywords to Structured Summaries: Streamlining Scholarly Knowledge Access\n","GenSERP: Large Language Models for Whole Page Presentation\n","Multimodal Healthcare AI: Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology\n","On Large Visual Language Models for Medical Imaging Analysis: An Empirical Study\n","DeiSAM: Segment Anything with Deictic Prompting\n","EXACT-Net:EHR-guided lung tumor auto-segmentation for non-small cell lung cancer radiotherapy\n","Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving\n","A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation\n","Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment\n","LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs\n","How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts\n","OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog\n","Slot-VLM: SlowFast Slots for Video-Language Modeling\n","Model Composition for Multimodal Large Language Models\n","MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion\n","Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering\n","Your Vision-Language Model Itself Is a Strong Filter: Towards High-Quality Instruction Tuning with Data Selection\n","Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents\n","Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data\n","Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships\n","AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\n","Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion\n","LVCHAT: Facilitating Long Video Comprehension\n","LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation\n","NOTE: Notable generation Of patient Text summaries through Efficient approach based on direct preference optimization\n","Modularized Networks for Few-shot Hateful Meme Detection\n","ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs\n","Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning\n","Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning\n","Aligning Modalities in Vision Large Language Models via Preference Fine-tuning\n","CoLLaVO: Crayon Large Language and Vision mOdel\n","VQAttack: Transferable Adversarial Attacks on Visual Question Answering via Pre-trained Models\n","PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter\n","Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering\n","OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models\n","Using Left and Right Brains Together: Towards Vision and Language Planning\n","On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities\n","LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing\n","Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation\n","RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models\n","LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-bebc6eb7edc1>\u001b[0m in \u001b[0;36m<cell line: 96>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0msearch_and_download_pdfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQUERY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0mdownload_papers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQUERY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_FILES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDOWNLOAD_FOLDER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-6-bebc6eb7edc1>\u001b[0m in \u001b[0;36mdownload_papers\u001b[0;34m(query, max_results, save_dir)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Download the paper as a PDF file and save it to the directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/arxiv/__init__.py\u001b[0m in \u001b[0;36mdownload_pdf\u001b[0;34m(self, dirpath, filename)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_default_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mwritten_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwritten_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Standard Library imports\n","import os\n","import re\n","from typing import List\n","from urllib.parse import quote_plus\n","import urllib.request\n","import re\n","from datetime import date, timedelta\n","\n","# Third-party imports\n","import requests\n","import arxiv\n","# Define constants and types\n","QUERY: str = \"ADVERSARICAL PAPERS ON COMPUTER VISION AND LLMS\"\n","MAX_FILES: int = 200\n","SEARCH_ENGINE_URL: str = \"https://www.google.com/search\"\n","DOWNLOAD_FOLDER: str = \"downloaded_files\"\n","\n","# Functions\n","def create_directory(directory_name: str) -> None:\n","    \"\"\"Create a directory if it does not exist.\"\"\"\n","    os.makedirs(directory_name, exist_ok=True)\n","\n","def download_files(urls: List[str], folder: str) -> None:\n","    \"\"\"Download a list of files to a specified folder.\"\"\"\n","    for i, url in enumerate(urls):\n","        if i >= MAX_FILES:\n","            break\n","\n","        try:\n","            file_name = url.split(\"/\")[-1]\n","            file_path = os.path.join(folder, file_name)\n","\n","            with requests.get(url, timeout=30, stream=True) as response:\n","                response.raise_for_status()\n","                with open(file_path, \"wb\") as f:\n","                    for chunk in response.iter_content(chunk_size=8192):\n","                        f.write(chunk)\n","\n","            print(f\"Downloaded {i + 1} of {MAX_FILES} files: {file_name}\")\n","        except requests.RequestException as e:\n","            print(f\"Failed to download {url}: {e}\")\n","\n","def search_and_download_pdfs(query: str) -> None:\n","    \"\"\"Search for PDFs related to a query and download them.\"\"\"\n","    encoded_query = quote_plus(query)\n","    search_url = f\"{SEARCH_ENGINE_URL}?q={encoded_query}\"\n","    response = requests.get(search_url)\n","    response.raise_for_status()\n","    html = response.text\n","\n","    pdf_pattern = r\"https?://\\S+\\.pdf\"\n","    pdf_urls = re.findall(pdf_pattern, html)\n","\n","    folder_name = query.replace(\" \", \"_\")\n","    create_directory(folder_name)\n","    download_files(pdf_urls, folder_name)\n","def download_papers(query, max_results, save_dir):\n","    \"\"\"\n","    This function downloads papers from arXiv based on the provided query.\n","\n","    Parameters:\n","    query (str): The search query for the papers.\n","    max_results (int): The maximum number of results to return.\n","    save_dir (str): The directory where the papers will be saved.\n","    \"\"\"\n","    # Construct the default API client\n","    client = arxiv.Client()\n","\n","    # Create a search object\n","    search = arxiv.Search(\n","        query=query,\n","        max_results=max_results,\n","        sort_by=arxiv.SortCriterion.SubmittedDate\n","    )\n","\n","    # Get the results as a list\n","    results = list(client.results(search))\n","\n","    # Check if the folder exists\n","    if not os.path.exists(save_dir):\n","        # If not, create the folder\n","        os.makedirs(save_dir)\n","\n","    # Loop through the results\n","    for result in results:\n","        # Print the title of the paper\n","        print(result.title)\n","        # Download the paper as a PDF file and save it to the directory\n","        result.download_pdf(save_dir)\n","\n","\n","\n","search_and_download_pdfs(query=QUERY)\n","\n","download_papers(query=QUERY,max_results=MAX_FILES,save_dir=DOWNLOAD_FOLDER)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["4080f47d1882450384f41b1d7efa8c5c","270f052c3b9e4396b5f1a0ce73fdea45","ba1772f8fd0249779ebc11e22bfa7e48","062702ad6c304d1391e2b162fde4e671","9d7a0a8a9c8245379fde62d744c329cd","f60d4c940e43489197b8391d8bdb63fc","8b5cbb9e5b964eb68e8ad017bb13a4a9","2195e0374c3842109cdddc6723316c28","5488068a777a4c9cbb009f369c6b88bb","0657c280a7ef4101af6998701820f9fb","75d7d1d2d84248d3b6e0a34ab57f878b","67bcafe5b34740c7ae202abbd4ea9a23","1033651e716e4151b0d2e04a2247dc63","a46cbe7cabc34f58800c539fe0329726","2d1f6586afba4d8686e865d33758fa50","4ee794ce4e494e73907a932ec4ffdcfb","b489515b602a4ea5bcdce42d1b7dcc45","8b73564e70974a479aac96e432510021","f38c0be4f3a84364bc5f600b86ddea3b","df7462385ee44beb821ba90207eb3a92","310d5621ea4e4bde9ceccb3326dda2ff","76a5af87d6544594bb463317cfe76133"]},"executionInfo":{"elapsed":37311,"status":"ok","timestamp":1711273032721,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"nSmksnRMw2HE","outputId":"090981ed-85cc-4c34-fc0e-b882b0fb593f"},"outputs":[{"name":"stderr","output_type":"stream","text":["MuPDF error: syntax error: could not parse color space (472 0 R)\n","MuPDF error: syntax error: could not parse color space (807 0 R)\n","MuPDF error: syntax error: could not parse color space (833 0 R)\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: unsupported error: cannot create appearance stream for Screen annotations\n","MuPDF error: syntax error: could not parse color space (483 0 R)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4080f47d1882450384f41b1d7efa8c5c","version_major":2,"version_minor":0},"text/plain":["Resolving data files:   0%|          | 0/191 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"67bcafe5b34740c7ae202abbd4ea9a23","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text'],\n","        num_rows: 191\n","    })\n","})"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import csv\n","from typing import Dict, List ,Optional\n","from collections import defaultdict\n","from pathlib import Path\n","import pandas as pd\n","import fitz  # PyMuPDF\n","import json\n","import yaml\n","import re\n","from pptx import Presentation\n","from io import StringIO\n","from datasets import load_dataset,Dataset\n","from tqdm.notebook import tqdm\n","import torch\n","import pandas as pd\n","from typing import Optional, List, Tuple\n","\n","from langchain_community.document_loaders import DirectoryLoader\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from transformers import AutoTokenizer\n","from langchain.vectorstores.faiss import FAISS\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","from langchain_community.vectorstores.utils import DistanceStrategy\n","from transformers import pipeline\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","from langchain.docstore.document import Document as LangchainDocument\n","\n","EXTENSION_READERS = {\n","    '.ipynb': lambda f: json.load(f),\n","    '.md': lambda f: f.read(),\n","    '.py': lambda f: f.read(),\n","    '.csv': lambda f: pd.read_csv(f),\n","    '.json': lambda f: json.load(f),\n","    '.yaml': lambda f: yaml.safe_load(f),\n","    '.txt': lambda f: f.read(),\n","    '.xml': lambda f: f.read(),\n","    '.html': lambda f: f.read(),\n","    '.css': lambda f: f.read(),\n","    '.js': lambda f: f.read(),\n","    '.java': lambda f: f.read(),\n","    '.cpp': lambda f: f.read(),\n","    '.h': lambda f: f.read(),\n","    '.php': lambda f: f.read(),\n","    '.rb': lambda f: f.read(),\n","    '.sql': lambda f: f.read(),\n","    '.xls': lambda f: pd.read_excel(f),\n","    '.xlsx': lambda f: pd.read_excel(f),\n","    '.ppt': lambda f: read_pptx(f),\n","    '.pptx': lambda f: read_pptx(f)\n","}\n","\n","\n","\n","\n","def read_pptx(file):\n","    \"\"\"Custom function to read .pptx files with python-pptx\"\"\"\n","    prs = Presentation(file)\n","    text = []\n","    for slide in prs.slides:\n","        for shape in slide.shapes:\n","            if hasattr(shape, \"text\"):\n","                text.append(shape.text)\n","    return \"\\n\".join(text)\n","\n","# Utilize regular expressions to match any of the file extensions\n","EXTENSION_PATTERN = r\".*\\.(md|py|csv|json|yaml|txt|xml|html|css|js|java|cpp|h|php|rb|sql|xls|xlsx|ppt|pptx|ipynb)$\"\n","\n","\n","def get_files_with_extensions(dir_path: str) -> Dict[str, List[str]]:\n","    ext_files = defaultdict(list)\n","    for root, dirs, files in os.walk(dir_path):\n","        for file in files:\n","            file_path = os.path.join(root, file).replace(\"\\\\\", \"/\")\n","            _, ext = os.path.splitext(file)\n","            ext_files[ext].append(file_path)\n","    return ext_files\n","\n","\n","def write_to_csv(file_path: str, ext_files: Dict[str, List[str]]) -> None:\n","    with open(file_path, 'w', newline='') as csvfile:\n","        writer = csv.writer(csvfile)\n","        max_len = max(len(v) for v in ext_files.values())\n","        writer.writerow(ext_files.keys())\n","        for i in range(max_len):\n","            row = [ext_files[k][i] if i < len(ext_files[k]) else '' for k in ext_files.keys()]\n","            writer.writerow(row)\n","\n","\n","def clean_text(text: str) -> str:\n","    \"\"\"\n","    Clean the extracted text from the PDF.\n","    This function can be customized based on the cleaning requirements.\n","\n","    Parameters:\n","    - text (str): The text extracted from the PDF.\n","\n","    Returns:\n","    - str: The cleaned text.\n","    \"\"\"\n","    cleaned_text = ' '.join(text.split())  # Removing extra whitespaces\n","    # Add more cleaning rules as needed.\n","    return cleaned_text\n","\n","def split_and_save_text(cleaned_text: str, base_output_path: Path, max_size_bytes: int = 50 * 1024 * 1024) -> None:\n","    \"\"\"\n","    Split the cleaned text into multiple files, each smaller than the specified max size, and save them.\n","\n","    Parameters:\n","    - cleaned_text (str): The cleaned text to be split and saved.\n","    - base_output_path (Path): The base path where the text files will be saved.\n","    - max_size_bytes (int): Maximum size of the text file in bytes.\n","    \"\"\"\n","    part_num = 1\n","    text_part = \"\"\n","    for line in cleaned_text.split('\\n'):\n","        if len(text_part.encode('utf-8')) + len(line.encode('utf-8')) < max_size_bytes:\n","            text_part += line + '\\n'\n","        else:\n","            # Save the current part and start a new one\n","            output_path = base_output_path.with_suffix(f'.part{part_num}.txt')\n","            with open(output_path, 'w', encoding='utf-8') as file:\n","                file.write(text_part)\n","            part_num += 1\n","            text_part = line + '\\n' # Start new part with the current line\n","\n","    # Save the last part\n","    if text_part:\n","        output_path = base_output_path.with_suffix(f'.part{part_num}.txt')\n","        with open(output_path, 'w', encoding='utf-8') as file:\n","            file.write(text_part)\n","\n","def convert_pdf_to_text(pdf_path: str, output_folder: str) -> None:\n","    \"\"\"\n","    Convert a PDF file to text files, splitting contents to ensure each resulting file is less than 50 MB.\n","\n","    Parameters:\n","    - pdf_path (str): Path to the PDF file.\n","    - output_folder (str): Path to the folder where the text files will be saved.\n","    \"\"\"\n","    # Ensure the output folder exists, create it if it does not\n","    output_folder_path = Path(output_folder)\n","    output_folder_path.mkdir(parents=True, exist_ok=True)\n","\n","    try:\n","        doc = fitz.open(pdf_path)\n","        text = \"\"\n","        for page in doc:\n","            text += page.get_text()\n","        doc.close()\n","\n","        cleaned_text = clean_text(text)\n","\n","        base_output_path = output_folder_path / Path(pdf_path).stem\n","        split_and_save_text(cleaned_text, base_output_path)\n","    except Exception as e:\n","        print(f\"An error occurred while converting {pdf_path}: {str(e)}\")\n","\n","def process_pdfs_from_csv(csv_path: str, output_folder: str) -> None:\n","    \"\"\"\n","    Process PDFs listed in a CSV file, converting them to text files and ensuring each part is less than 50 MB.\n","\n","    Parameters:\n","    - csv_path (str): Path to the CSV file containing paths to PDF files.\n","    - output_folder (str): Path to the folder where text files will be stored.\n","    \"\"\"\n","    pdf_paths = pd.read_csv(csv_path, encoding='latin1')\n","    # pdf_paths = pd.read_csv(csv_path)\n","    for pdf_path in pdf_paths['.pdf']:\n","        convert_pdf_to_text(pdf_path, output_folder)\n","\n","\n","\n","\n","def list_files_with_extensions(directory_path):\n","    try:\n","        files = os.listdir(directory_path)\n","        return [file for file in files if re.match(EXTENSION_PATTERN, file)]\n","    except FileNotFoundError:\n","        print(f\"The directory {directory_path} was not found.\")\n","        return None\n","\n","def read_file_content(directory_path, filename):\n","    try:\n","        extension = os.path.splitext(filename)[1]\n","        with open(os.path.join(directory_path, filename), 'r',encoding='utf-8') as file:\n","            file_reader = EXTENSION_READERS.get(extension)\n","            return file_reader(file) if file_reader else None\n","    except Exception as e:\n","        print(f\"An error occurred while reading the file {filename}: {e}\")\n","\n","\n","\n","def process_files_txtfile(directory_path: str, user_folder: str, txt_file_counter: int = 1, txt_file_size: int = 0) -> Optional[str]:\n","    \"\"\"\n","    This function recursively processes all files in a given directory and its subdirectories,\n","    and writes their content to a user-specific text file. Each text file is ensured to be less than 50 MB in size.\n","\n","    Args:\n","    directory_path (str): The path to the directory containing the files to be processed.\n","    user_folder (str): The name of the user-specific folder where the text files will be written.\n","    txt_file_counter (int): The counter for the current text file.\n","    txt_file_size (int): The current size of the text file.\n","\n","    Returns:\n","    str: The path to the user-specific folder, or None if an error occurred.\n","    \"\"\"\n","    # Create the user-specific folder if it doesn't exist\n","    user_folder_path = os.path.join(directory_path, user_folder)\n","    os.makedirs(user_folder_path, exist_ok=True)\n","\n","    # Get a list of all files in the directory\n","    files = list_files_with_extensions(directory_path)\n","\n","    if files is None:\n","        return\n","\n","    for filename in files:\n","        content = read_file_content(directory_path, filename)\n","        if content is not None:\n","            # Create a new text file if the size is over 50 MB\n","            if txt_file_size >= 50 * 1024 * 1024:\n","                txt_file_counter += 1\n","                txt_file_size = 0\n","\n","            # Open the text file in append mode\n","            txt_file_path = os.path.join(user_folder_path, f\"{txt_file_counter}.txt\")\n","            with open(txt_file_path, \"a\") as f:\n","                if isinstance(content, pd.DataFrame):\n","                    # Convert DataFrame to CSV string without index and write to file\n","                    content_csv = content.to_csv(index=False)\n","                    f.write(content_csv)\n","                    txt_file_size += len(content_csv)\n","                elif isinstance(content, dict):\n","                    # Convert dict to pretty-printed string using json.dumps and write to file\n","                    content_str = json.dumps(content, indent=4)\n","                    f.write(content_str)\n","                    txt_file_size += len(content_str)\n","                else:\n","                    # If it's not a DataFrame or dict, convert it to a string\n","                    content_str = str(content)\n","                    f.write(content_str)\n","                    txt_file_size += len(content_str)\n","\n","            print(f\"--- File: {filename} ---\")\n","            print(f\"Content written to {txt_file_path}\")\n","            print(\"-------------------------------\\n\")\n","\n","    # Recursively process subdirectories\n","    for subdir in [d for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]:\n","        subdir_path = os.path.join(directory_path, subdir)\n","        txt_file_counter, txt_file_size = process_files_txtfile(subdir_path, user_folder, txt_file_counter, txt_file_size)\n","\n","    return txt_file_counter, txt_file_size\n","def loading_folder_using_datasets(folder_path:str):\n","\n","\n","    dataset = load_dataset('text', data_files=folder_path+'/*.txt')\n","    return dataset\n","\n","##=========================||    Extraction  OF DATA         ||==================================\n","dir_path =f\"/content/{DOWNLOAD_FOLDER}\"\n","dir_output='/content/output' # replace with your directory path\n","csv_file_path = '/content/csvfile.csv'  # replace with your CSV file path\n","ext_files = get_files_with_extensions(dir_path)\n","write_to_csv(csv_file_path, ext_files)\n","process_pdfs_from_csv(csv_path=csv_file_path, output_folder=dir_output)\n","process_files_txtfile(dir_path,  dir_output)\n","dataset=loading_folder_using_datasets(folder_path=dir_output)\n","dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":1760,"status":"ok","timestamp":1711273067847,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"ghGALc8XtBsy","outputId":"91e8a07c-cd1b-4479-fe21-68af27572d57"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"pd\",\n  \"rows\": 191,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 191,\n        \"samples\": [\n          \"PhD: A Prompted Visual Hallucination Evaluation Dataset Jiazhen Liu1,2, Yuhan Fu1,2, Ruobing Xie2, Runquan Xie2, Xingwu Sun2, Fengzong Lian2, Zhanhui Kang2, and Xirong Li1* 1 Key Lab of DEKE, Renmin University of China 2 Machine Learning Platform Department, Tencent {liujiazhen, fuyuhan, xirong}@ruc.edu.cn, xrbsnowing@163.com Abstract The rapid growth of Large Language Models (LLMs) has driven the development of Large Vision-Language Mod- els (LVLMs). The challenge of hallucination, prevalent in LLMs, also emerges in LVLMs. However, most existing efforts mainly focus on object hallucination in LVLM, ig- noring diverse types of LVLM hallucinations. In this study, we delve into the Intrinsic Vision-Language Hallucination (IVL-Hallu) issue, thoroughly analyzing different types of IVL-Hallu on their causes and reflections. Specifically, we propose several novel IVL-Hallu tasks and categorize them into four types: (a) object hallucination, which arises from the misidentification of objects, (b) attribute hallucination, which is caused by the misidentification of attributes, (c) multi-modal conflicting hallucination, which derives from the contradictions between textual and visual information, and (d) counter-common-sense hallucination, which owes to the contradictions between the LVLM knowledge and ac- tual images. Based on these taxonomies, we propose a more challenging benchmark named PhD to evaluate and explore IVL-Hallu. An automated pipeline is proposed for gener- ating different types of IVL-Hallu data. Extensive experi- ments on five SOTA LVLMs reveal their inability to effec- tively tackle our proposed IVL-Hallu tasks, with detailed analyses and insights on the origins and possible solutions of these new challenging IVL-Hallu tasks, facilitating future researches on IVL-Hallu and LVLM. The benchmark can be accessed at this https URL. 1. Introduction The rapid development of LLMs [7, 28, 30] has laid the foundation for LVLMs [1, 3, 4, 25, 35], which convert dif- ferent modalities into a unified text-token-like format and perform generation consistently through LLMs [13]. How- *Corresponding author ever, despite the continuous improvement of LLMs, they still face serious hallucination problems [2, 14, 17], leading to the inevitable occurrence of severe hallucination prob- lems also in LVLMs. Following [14], the hallucinations in LVLM could also be divided into two types: intrinsic hallucinations, which are caused by inconsistencies with the image information, and extrinsic hallucinations, arising from further incorrect inference processes based on the image information. Since images are always viewed as the \\u201cgold standard\\u201d of almost all LVLM tasks, hallucinations reflected by the inconsis- tencies with images are more significant and eager to be solved in practical usages. The Intrinsic Vision-Language Hallucinations (IVL-Hallu) is more fundamental, easy-to- detect, and influential on the practical usage of LVLMs. Intrinsic hallucinations widely exist in LVLMs [5, 20]. The current mainstream LVLM architectures usually con- sist of a relatively lightweight visual module combined with powerful LLMs compared to the SOTA visual models. As a result, the learned textual knowledge encoded in LLMs tends to form a kind of inertia that tempts LVLMs to \\u201cspec- ulate\\u201d about the image content based on their textual knowl- edge of inputs instead of genuinely recognizing and under- standing it. Such speculation may deviate from the actual golden visual content and thus lead to IVL-Hallu. It is recognized that the modal imbalance issue in LVLMs lim- its the capabilities on the visual side, resulting in LVLMs performing less effectively than specialized visual models, where the basic recognition tasks (e.g., identification of small or easily confusable objects) are the most intuitive re- flections. Therefore, several studies have begun to explore the object hallucination issue recently [5, 20, 22]. How- ever, these studies only focus on the coarse-grained IVL- Hallu reflected by objects, and pay less attention to other types of hallucinations in LVLMs. We believe that research on IVL-Hallu should look into the essence of hallucination systematically rather than simply verifying the insufficiency or mismatch of visual capabilities. arXiv:2403.11116v1 [cs.CV] 17 Mar 2024 What are the pedestrians doing in the image? Class1: Object hallucination Class3: Multi-modal conflicting hallucination Class4: Counter-common-sense hallucination Is the object used to eat in the image a fork? Are the spoons on the right of the teapot? Is the teapot in the image made of wood? Which one is bigger in the image? The cat is bigger in the image. Are there forks in the image? utility material position Is there a teapot in the image? Are there spoons in the image? Class2: Attribute hallucination The pedestrians are walking. Figure 1. Different types of IVL-Hallu scenarios in LVLMs. We categorize hallucinations into four types: object, attribute, multi- modal conflicting, and counter-common-sense hallucinations, and use these categories to evaluate contemporary LVLMs. Components that represent hallucinations are indicated in red, while the elements that are actually present in the image are highlighted in green. Based on the origins and reflections of IVL-Hallu, we categorize it into four types (shown in Fig. 1) as follows: \\u2022 Object hallucination. It concentrates on whether LVLM could correctly answer the object in the image, which is a classical, fundamental, and explicit IVL-Hallu widely explored in most LVLM hallucination settings. \\u2022 Attribute hallucination. This IVL-Hallu aims on iden- tifying more fine-grained attribute information of objects from the visual information. Different types of attributes (e.g., utility, material, position) are viewed as judgers on the deeper visual understanding of objects in the image. The misidentification of attributes and the conflicts be- tween prior knowledge in LVLM and actual visual infor- mation make this task more challenging. \\u2022 Multi-modal conflicting hallucination. The multi- modal conflicting hallucination mainly derives from the conflicts between multi-modality information. Some- times the textual and visual inputs are carelessly or in- tentionally contradictory, while the unbalanced LVLM is likely to rely more on the textual instructions (which may contain multi-modal conflicts) rather than the golden in- formation in images. \\u2022 Counter-common-sense hallucination. It is the most implicit and challenging hallucination, which arises from the conflicts between the learned powerful knowledge in LLMs/LVLMs (often dominating the generation of re- sponses) and the truth in images. Therefore, when en- countering images that \\u201cviolate\\u201d common sense (e.g., a bigger mouse with a smaller cat), the model is highly likely to produce hallucinations that appear more reason- able, as assumed by LLM. Based on the proposed taxonomy and new tasks of IVL-Hallu, we design a comprehensive benchmark dataset named Prompted Hallucination Dataset (PhD), utilizing prompts to explore IVL-Hallu in LVLMs. This dataset consists of four types of LVLM hallucinations: object, attribute, multi-modal conflicting, and counter-common- sense hallucinations. (1) For object and attribute types, we provide high-quality hallucination data, including objects, attributes, and their easily confusable items. For example in Fig. 1, a teapot (it\\u2019s actually made of metal) can be made of wood, ceramic, etc., which is all reliable and common in the intrinsic knowledge of LVLMs. (2) For the multi-modal conflicting type, we provide relevant questions that cannot be answered or reasoned based on the image content, such as asking about the location of a fork in a dining table set- ting (in fact, the fork cannot be found in the image). (3) For the counter-common-sense type, we provide conventional questions and answers and point out the aspects of the im- age that are counter-intuitive. Taking the instance in Fig. 1 as an example, the cat is smaller than the mouse, which is a contradiction to the internal knowledge of LVLMs. PhD is carefully designed, ensuring a diverse and abun- dant range of problem types and quantities. It helps to validate different SOTA LVLMs in more challenging IVL- Hallu tasks, enabling a better understanding of the intrinsic causes of IVL-Hallu as well as future solutions. To build PhD, we propose an automated data generation pipeline that generates a large amount of sharp and high-quality IVL- Hallu data. The entire process is efficient with strong prac- tical value. Our contributions are as: \\u2022 Unlike previous work, we start from the intrinsic causes and define four types of IVL-Hallu. These types of hallu- cinations contribute to a broader study and discussion of the nature of hallucinations in LVLMs. \\u2022 We provide a more challenging IVL-Hallu benchmark PhD. The latest open-source LVLMs reveal more hallu- cination issues when tested on our data. \\u2022 We provide an automatic pipeline for building PhD. The pipeline is designed meticulously, focusing on generat- ing data around the context of the image itself, generating more challenging and high-quality data. \\u2022 Numerous experiments have been carried out on the lat- est SOTA LVLMs with in-depth analyses, demonstrating that our challenging PhD can effectively contribute to and facilitate future research on IVL-Hallu. 2. Related Work 2.1. Large Vision-Language Model The development of LLMs has driven the growth of LVLMs [1, 11, 13, 19, 25, 35, 39], resulting in a large number of works. LVLMs generally have a similar architecture, using ViT [6] to encode visual features and employ adapters to combine them with text embeddings. The combined fea- tures are directly fed into the LLMs to perform the genera- tion task. LLMs hold a crucial role in the entire generation process, while the visual component merely supplies visual semantics. In reality, a significant portion of the work is dedicated to training merely a small fraction of the parame- ters in LVLMs, while the LLMs are maintained in a frozen state [24, 25, 39]. Leveraging the powerful capabilities of LLMs, LVLMs indeed exhibit outstanding performance, ef- fectively addressing a variety of multimodal tasks like Vi- sual Question Answering (VQA). However, while taking advantage of LLMs\\u2019 powerful capabilities, LLMs also be- come the inherent factor causing illusions in LVLMs. Currently, the updates of LVLMs are progressing rapidly. Numerous models have launched second-generation ver- sions (LLaVA [25] to LLaVA-1.5 [24], MiniGPT-4 [39] to MiniGPT-V2 [3], and mPLUG-Owl [33] to mPLUG- Owl2 [35]), exhibiting marked performance improvements. These efforts primarily focus on optimizing the training data and using more powerful LLMs, while the overall ad- justments to the model\\u2019s structure remain minimal. Despite the ongoing updates to LVLMs that enhance various visual task metrics [8, 26, 27, 34], the intrinsic hallucination issues caused by LLMs have not been effectively resolved. This is because the method of incorporating visuals into LLMs re- mains unchanged, and the underlying factors contributing to VL-Hallu continue to persist. Therefore, a comprehensive benchmark is needed to evaluate the intrinsic hallucination factors, which will aid in further innovation for LVLMs. Numerous methods have been proposed to detect and mitigate hallucinations in LVLMs[10, 15, 32, 38]. Tech- niques like LRV-Instruction[23] and HalluciDoctor[37] aim to balance data distribution, while Woodpecker[36] and Volcano[18] use external feedback and self-correction, re- spectively, to mitigate hallucinations. Despite these efforts, including OPERA\\u2019s[12] analysis of attention maps, halluci- nations remain a persistent issue for LVLMs, with no defini- tive solution yet in sight. 2.2. Existing Hallucination datasets Existing hallucination datasets have extensively discussed object hallucinations, which refer to instances where the model generates objects that are either inconsistent with or entirely absent from the target image [20, 29]. LVLMs are prone to generating such hallucinations because, within their vast internal knowledge, the coexistence of many ob- jects is reasonable. POPE [20] also finds a similar con- clusion: LVLMs tend to generate frequently appearing or co-occurring objects in the instruction corpora. They con- struct hallucination objects based on the frequency of co- occurring words in the corpus to ask LVLMs object pres- ence question [9, 16], attempting to figure out VL-Hallu. They have taken the first step in exploring VL-Hallu. The questions set by POPE are overly simplistic, which is eas- ily resolved through the enhancement of visual capabilities. With the rapid iteration and updates of LVLMs, the metric on POPE has been breached in a short period. In addition, there are certain issues with obtaining hallucinated objects through word frequency. Without considering the actual im- age context, the generated hallucinated objects can some- times be quite absurd, making it challenging to detect the hallucination issue in the image. Furthermore, object hallucinations represent only a small part of the types of hallucinations in LVLMs. More hal- lucinations, including attribute hallucinations, multi-modal conflicting hallucinations, and counter-common-sense hal- lucinations, require more detailed exploration. Some ex- isting datasets have conducted basic research on these, but the coverage of types is not comprehensive, as shown in Tab. 1. HallusionBench[22] evaluates LVLM reasoning and counterfactual abilities with manually crafted intricate questions, complicating the distinction between model ca- pability and hallucination issues. AMBER[31] assesses ob- ject, attribute, and relationship understanding but lacks a data generation pipeline and has a limited quantity. Com- paratively, PhD provides a data generation pipeline, covers comprehensive hallucination types, and specifies hallucina- tion elements, thereby enhancing the reliability and com- prehensiveness of evaluation. Table 1. Comparison of existing hallucination datasets. PhD possesses the most comprehensive collection of hallucina- tion data of LVLM, encompassing object, attribute, multi-modal conflicting (MMC), and counter-common-sense (CCS) hallucina- tions. Moreover, it is the only dataset that provides the reasons behind the inclusion of hallucinations (i.e., hallucination element, HE) in the data. * symbolizes the amount of data directly provided by the authors, while also facilitating data pipelines that enable the production of further data. Datasets Object Attribute MMC CCS HE #VQA POPE [20] \\\" % % % % 9,000\\u2217 HallusionBench [22] % % % \\\" % 1,129 AMBER [31] \\\" \\\" % % % 15,220 PhD (Ours) \\\" \\\" \\\" \\\" \\\" 53,976\\u2217 3. Dataset Construction Protocols 3.1. Overview of Our PhD As previously introduced, we aim to construct a compre- hensive hallucination benchmark PhD to evaluate LVLM\\u2019s ability to cope with IVL-Hallu. Most of the data is gen- erated through an automated process, which can theoreti- cally produce sufficient data. PhD, comprising over 7,000 images and 53,976 questions, consists of four types of IVL-Hallu largely impacting LVLMs, including (a) object, (b) attribute, (c) multi-modal conflicting, and (d) counter- common-sense hallucinations. The overall statistics and ex- amples of PhD are shown in Tab. 2 and Fig. 2, respectively. PhD is similar in structure to common VQA datasets in that we provide images and corresponding question- answer pairs. However, the most significant difference is that the images or question-answer pairs contain elements that can lead to hallucinations. More specifically, for Ob- ject, Attribute, and Multi-modal conflicting types of hal- lucinatory data, the question-answer pairs are constructed based on confusing items, which means the answers to the constructed questions are either negative or unanswerable. For the Counter-Common-Sense category, the images are counterintuitive. We provide clear definitions for confusing items and counterintuitive images. Confusing items refer to an object or attribute that actually does not exist or does not match in the image but can easily be judged as existing or matching. Counterintuitive images, on the other hand, refers to images that are rare in real situations and contra- dict mainstream knowledge. Such data contains elements that induce hallucinations, hence we refer to our dataset as hallucinatory data. Ultimately, PhD provides images, question-answer pairs, and identifies the hallucinatory ele- ments within the questions. Table 2. The statistical information of PhD. The data qual- ity refers to the proportion of high-quality descriptions measured through manual sampling inspection. The criteria are that (a) the data conform to the expected format, and (b) it is related to the cor- responding hallucination. * represents the total number of images involved for all types of data after de-duplication. Type Images Questions Data Quality Object 2,714 4,413 89.5% Attribute Shape 437 502 91.5% Material 1,198 1,450 90.3% Color 1,309 2,200 92.3% Counting 1,532 2,250 91.9% Position 1,408 1,661 87.3% Sentiment 246 249 89.1% Utility 111 111 87.7% Multi-modal conflicting Absurd 1,075 1,622 90.4% Strong 6,416 13,003 90.8% Weak 6,416 13,003 91.2% Indirect 6,416 13,003 88.4% Conter-common-sense Real 123 132 \\u2013 Generated 250 377 \\u2013 Total 7,020\\u2217 53,976 \\u2013 Object Hallucination Is there a bike in the image? Q: No. A: Bicycles are common in the street scene in the picture and are prone to false detection. HE: Attribute Hallucination Is there a red car in the image? Q: No. A: The red element on the bus in the picture may be believed to be on the car. HE: Multi-modal conflicting Hallucination What material is the glass teapot in the top left corner of the image? (Unanswerable) Q1: The teapot is definitely made of glass. S1: The teapot might be made of glass. S2: The material of the teapot is like the surface of a lake in winter, transparent and easily shattered. S3: The teapot in the upper left corner is not made of glass, and the cups around it are made of glass. HE: Counter-Common-Sense Hallucination Is the maximum number of points on the dice six in the picture? Q: No. A: Normally, the maximum number of sieves is 6, and asking such a common sense question may trigger the model to directly respond based on existing knowledge. HE: Figure 2. Examples of four IVL-Hallu in PhD: images with their corresponding question-answer pairs and hallucination elements (HE). Words annotated in green have correspondences in the image, whereas words annotated in red do not exist or do not match within the image. Q, A, and S stand for Question, Answer, and Statement, respectively. 3.2. Object and Attribute Hallucinations Definition and motivation. Object hallucination refers to the model generating descriptions or captions containing objects inconsistent with or absent from the target image [20]. This is defined on an object level, enabling coarse- grained hallucination detection through the verification of object presence within the image. On the other hand, at- tribute hallucination represents a more detailed aspect of object hallucination, focusing on the attributes or charac- teristics of objects, thereby providing a finer granularity in the detection and understanding of hallucinated content. LVLMs produce these two types of hallucinations because, within the internal knowledge of LLMs, the coexistence of numerous items in specific contexts is deemed plausi- ble. Despite the absence of certain objects or attributes in the image, LVLMs maintain a high level of confidence in generating non-existent or mismatched objects or attributes. Consequently, evaluations of these two types of hallucina- tion data effectively demonstrate the model\\u2019s capability to recognize image content and adhere to the image context in generation. Existing object hallucination datasets are often con- structed based on word frequency relationships rather than image content, which can lead to a significant discrepancy between the constructed object and the content of the im- age. This discrepancy results in such hallucination data being overly simplistic and easily detectable. Addition- ally, present hallucination datasets provide only question- and-answer pairs without elucidating the reasons that might induce the hallucination, thus limiting the utility of this type of data. Consequently, generating object hallucina- tion grounded in image content and identifying the poten- tial causes of this hallucination is imperative. For attribute hallucination, existing datasets heavily rely on manual an- notation, making it difficult to form large-scale datasets. We propose an automated data generation method based on cap- tions, which can also provide factors within the data that may lead to hallucinations. Data format. We begin by utilizing labeled captions of the image, extracting objects and their associated attributes to identify all the prominent elements within the image. Using these prominent elements as a basis, we create confusing items and ultimately construct Yes/No questions based on the confusing items (meaning all answers are no). Through- out this process, we document the rationale behind the se- lection and formulation of these confusing items as the com- ponents contributing to hallucination. Ultimately, we pro- duce the data format in <Question, Answer, Hallucination Elements>, as shown in Fig. 2. The complete data gener- ation process is described in Sec. 3.5. The prompt for the question is \\u201c[Yes/No question] Based on the image, answer yes or no.\\u201d Attribute Selection. We select the 7 most influential at- tributes in our PhD due to the following reasons (Some ex- amples are in Fig. 2): (a) Shape. LVLMs are prone to generating hallucina- tions regarding the shapes of objects due to the prevalence of common shapes among various objects. Concurrently, the multitude of other shapes present in the image can read- ily lead to confusion for LVLMs. (b) Material. LVLMs are prone to generating illusions for materials, as many objects may have multiple material sources. For example, teapots can be made of wood, ce- ramic, and other materials. As the wooden floor serves as the primary visual element in the image, LVLMs may focus on the wood-related information and subsequently associate it with the teapot. (c) Color. Similar to shape, color recognition can be readily influenced by the colors present in surrounding ob- jects. (d) Counting. The counting ability of the model heavily relies on its visual capabilities, but the LLM indeed plays a decisive role in the generation process. Consequently, when counting problems are challenging, the visual component has limited impact, and the LLM tends to consider the de- scriptions related to counting as accurate (since the model is highly confident [20]). (e) Position. When the visual component fails to com- prehend the spatial relationship, the model confidently of- fers its best estimate. (f) Sentiment. Sentiment is a vague concept, that usually requires a combination of context to provide a confident an- swer. If the visual model cannot comprehensively extract environmental information, it may lead the LLM to misin- terpret sentiments. (g) Utility. Since many objects have similar purposes, such as cars and buses both being used for transporting pas- sengers, asking LVLMs to judge objects based on their pur- pose may likely lead them to make guesses based on their internal knowledge. 3.3. Multi-modal conflicting Hallucination Definition and motivation. Multi-modal conflicting Hal- lucination refers to the creation of suggestive questions de- signed to induce the model to mimic or follow the context set by the question in its response. There are primarily two scenarios: (1) asking a question unrelated to the content of the image, which makes these questions unanswerable. Specifically, as described in Fig. 2, the question relates to a glass teapot, an item not present in the image. There- fore, any question centered on it lacks relevance (we also called this absurd question); (2) Directly providing a state- ment that is completely inconsistent with the image and us- ing this description as the context for subsequent questions. The impact of different tones in statement sentences on the model needs to be discussed. Therefore, we also differen- tiate a statement sentence into three different tones: strong, weak, and indirectly related (referring to using a very affir- mative tone, a tentative tone, and rhetorical and descriptive techniques, as shown in Fig. 2 S1, S2, S3, respectively). Essentially, such hallucinations are actually caused by misaligned input modalities (i.e., the textual question does not match the image), and a good model should refuse to an- swer such questions. However, due to LVLMs\\u2019 overreliance on LLMs, the model might disregard the content of the im- age and rely solely on the context constructed by the text for its response, thereby producing unforeseeable hallucination outcomes. Models that perform exceptionally well on this type of data demonstrate superior security and resistance to interference. When constructing this type of hallucination data, we also base it on confusing items. This approach al- lows us not only to avoid excessive discrepancies between the questions and the image content but also to trace the origins of the hallucinations, aiding subsequent analysis. Data format. As shown in Fig. 2, the data also consists of questions, answers, and hallucination elements. The auto- mated pipeline is described in Sec. 3.5. There are two forms of questions. The first type is a VQA question, where the answer is unanswerable. The second type involves provid- ing a descriptive statement and asking whether this descrip- tion is correct. Thus, the prompts are: 1. \\u201c[VQA question] Based on the image, answer in several words\\u201d 2. \\u201c[VQA question] Based on the image, is this question answerable? Answer yes or no.\\u201d 3. \\u201c[Descriptive statement] Based on the image, Is the de- scription right? Answer yes or no.\\u201d 3.4. Counter-Common-Sense Hallucination Definition and motivation. The previous types of data can be interpreted as attempts to explore or capture hallucina- tion information from a textual perspective for assessment purposes. Beyond this, it is necessary to showcase the hal- lucination issue in a more intuitive manner: what happens when the image itself contradicts the common sense em- bedded in LLMs, a scenario we refer to as counterintuitive images. When this situation occurs, according to our pre- vious definitions and explanations, LVLMs tend to rely on their own knowledge rather than the image which violates their knowledge, see Fig. 1. The output performance of these counter-common-sense images can directly verify our conjecture, making them indispensable in our evaluation system. Although it is difficult to construct an automatic method to collect this type of data, we still meticulously col- lected 373 images and categorized them into two categories: real, generated images(see Tab. 2) and generated easy VQA questions (Answer in three words) based on their sources. It is crucial to emphasize that these images should counter common sense, rather than necessitating extensive external knowledge for comprehension. This is because distinguish- ing between inadequate ability and knowledge inertia caus- ing illusions would be challenging if the images demanded a vast amount of external information [22]. Data format. Similarly, each piece of data includes ques- tions, corresponding answers, and the unusual aspects of the image (also regarded as hallucination elements). The prompt is: \\u201c[VQA question] Based on the image, answer in several words.\\u201d 3.5. Automatic Pipeline for Building PhD Studying the nature of hallucinations requires a large amount of data and experimental support. To make our research more meaningful, we have designed a unified pipeline to automatically generate high-quality data corre- sponding to the hallucination data mentioned earlier. To construct a pipeline capable of generating the aforemen- tioned data, we need to adhere to several principles: (a) hal- lucination data must consist of content that does not match the image; (b) the generated text must be reasonable within the context of the image; (c) the generated text must be de- ceptive, making it easy for LVLMs to generate hallucina- tions; and (d) the generation process must be stable and pro- vide a rationale for the creation of hallucination elements. Thus we propose an automated pipeline for object, attribute, and multi-modal conflicting hallucination data construction as shown in Fig. 3. 1. We employ a diverse array of annotations as prelimi- nary inputs, encompassing captions [21], categories, pre- existing VQA (Visual Question Answering) datasets, and more. Initially, we transform these annotations into standardized descriptions. Subsequently, utilizing infor- mation extraction techniques alongside ChatGPT 1, we identify and extract objects along with their pertinent at- tributes from these descriptions. 2. Leveraging the object-attribute information extracted, we utilize ChatGPT to create confusion-inducing items. We have crafted specific prompts that allow ChatGPT to produce confusing items (objects and attributes) that are logically consistent and accompanied by reasoning. The structure of the prompt is as follows: Visual Annotations: <Input> Object O: <Input> Attribute A: <Input> Rules: 1. Based on Visual Annotations. 2. Generate an Object related to O but not existing in the annotations. 3. Generate an Attribute related to A but not existing in the annotations. 1We utilize the API of GPT-3.5 \\u2026 Counting Color Object one red bus one black motorcycle many - car \\u2026 Original Annotation Vision Annotation Prompt for generating confusing items ChatGPT Confusing Object Reason Object Related to the existing objects and common in the street environment. bicycle \\u2026 Confusing Attribute (Color) Reason Attribute Object Red motorcycles are common, and the presence of red on buses is notable. red motorcycle \\u2026 Quality Assurance Object Hallucination: Is there a bicycle in the image? Attribute Hallucination: Is the motorcycle red in the image? Multi-modal conflicting Hallucination: Where is the red bicycle in the image? Extract Objects and Attributes VQA Data: Q: What color is the traffic light? A: Black COCO Categories: Car, Bus, Person COCO caption: \\u2022 A red double decker bus driving down a street. \\u2022 A busy city with a motorcycle and many cars lined up on the road. Descriptions of the image \\u2022 A red double decker bus driving down a street. \\u2022 A busy city with a motorcycle and many cars lined up on the road. \\u2022 There are cars, bus and persons in the image. \\u2022 The color of the traffic light is black. Figure 3. Pipeline for automatically constructing object hallucinations, attribute hallucinations, and multi-modal conflicting hallu- cinations. Words marked in green exist within the image, while those marked in red do not exist or do not match the image. 4. Give the reason why generating the item. Output format: Object: <> Attribute: <> Reason: <> 3. Quality Assurance: We keep a synonym list and con- duct synonym checks to eliminate elements that are nearly identical to the visual annotations, guaranteeing that the confusing items generated are unequivocally in- correct and do not align with the original image. Fur- thermore, we also carry out random manual inspections to evaluate the quality of the content produced. 4. Data Assembly: We meticulously tailor the prompt for ChatGPT to produce hallucination data derived from the items generated. For the three categories of data \\u2014 object, attribute, and multi-modal conflicting \\u2014 the prompts are slightly differentiated. The standard prompt is as follows: Object: <Input> Attribute: <Input> Examples: Object: Bike Attribute: Red Output 1: Is there a bike in the image? Output 2: Is the color of the bicycle in the picture red? Output 3: What is on the red bike in the image? ... Rules: 1. Learn from the Examples. 2. Generate outputs based on the provided object and attribute. Output format: Output: <> 4. Experiments 4.1. Common Setup Selected LVLMs for evaluation. To ensure a thorough and impactful assessment, we chose representative and widely recognized models, compiling a list of 5 LVLMs2 as fol- lows: LLaVA-1.53[25], MiniGPT-v24[3], Qwen-VL5[1], mPLUG-Owl26[35], and InstructBLIP7[4] Data quality evaluation. We take 10% of each gener- ated category in PhD for manual quality assessment. We counted the data rate meeting the criteria, evaluating the stability and quality of our data construction pipeline. The proportion of data meeting the requirements (the content is highly relevant but does not match the images), determined through random sampling and assessment, is presented in Tab. 2. Metric. Considering that most questions result in answers that are either \\u2018yes\\u2019 or \\u2018no\\u2019, or are open-ended, consisting of only a few words, we utilize the simple Accuracy met- ric, a common evaluation measure in VQA tasks [9, 16], to assess our performance. This metric measures the percent- age of questions answered correctly. It\\u2019s important to note that when evaluating answers to open-ended questions, we conduct synonym lookup operations to guarantee that no correct answers are overlooked. 2The versions of models are as of October 1st, 2023 3https://github.com/haotian-liu/LLaVA 4https://github.com/Vision-CAIR/MiniGPT-4 5https://github.com/QwenLM/Qwen-VL 6https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl2 7https://github.com/salesforce/LAVIS/tree/main/projects/instructblip 4.2. Positive Evaluation Given that the answers to questions in PhD are either neg- ative or unanswerable, solely testing on hallucination data could skew the outcomes. To counter this imbalance, a Pos- itive Evaluation is essential, requiring the provision of ques- tions with \\u2018yes\\u2019. We select some from POPE [20] and con- struct others based on COCO [21] captions, ensuring \\u2018Yes\\u2019 as the answer. The outcomes are detailed in the Positive Evaluation in Tab. 3. All models demonstrated remarkable performance, with LLaVA-1.5 standing out by achieving an average accu- racy of 95.9%, though the lead over other models is slight. This positive experiment highlights the LVLMs\\u2019 robust ca- pabilities in conventional scenarios. The format for ques- tions in the following hallucination experiment is closely matched to this experiment, enabling a thorough ablation analysis through the observed variations in performance. 4.3. Object Hallucination This experiment is designed to test whether the model hallu- cinates objects that do not exist in the image. We conducted tests using the previously constructed object hallucination data, with the results presented in Tab. 3. Identifying the presence of a specific object in an image is a fundamental visual task. Yet, LVLMs show markedly lower performance on this task compared to Positive Eval- uation (e.g., Qwen-VL 75.7% in \\u2018No\\u2019 questions versus 95.1% in \\u2018Yes\\u2019 questions). This is attributed to the way PhD designs object hallucination scenarios, taking into ac- count how other elements or similar objects in the image might interfere with object recognition. The fabricated hal- lucination objects are integrated within the image\\u2019s seman- tic context, leading LVLMs to not just recognize image in- formation but also to draw on the LLMs\\u2019 internal knowl- edge for association and reasoning. When an object, though absent in the image, is commonly linked with other ele- ments in the LLMs\\u2019 knowledge, it significantly raises the chances of object hallucination. As a result, LVLMs often fare better on Positive Evaluation\\u2019s \\u2018Yes\\u2019 questions than on Object Hallucination\\u2019s \\u2018No\\u2019 questions, underscoring the im- pact of semantically related or visually similar elements in the image on the recognition task. 4.4. Attribute Hallucination The experimental procedure is similar to the Object Hal- lucination experiment, with results displayed in Tab. 3. Compared with object hallucination, attribute hallucination requires finer-grained vision-language understandings not only on the object itself, but also on its attributes. We will analyze these results from two perspectives: comparison be- tween attributes and comparison with object hallucination. Comparison among different attributes. Different mod- els show similar trends in handling various attributes, with these models performing well on color questions and ex- hibiting weaker capabilities on counting, position, and util- ity questions. The experimental results indicate that the higher the need for understanding and inference in a ques- tion, the greater the likelihood of hallucination. Specifi- cally, for color attributes, given that an object\\u2019s color is a distinct and straightforward attribute, it can be readily iden- tified by the model, leading to high performance across all LVLMs. However, for attributes such as position or util- ity, the model must identify multiple objects and under- stand spatial or functional relationships, relying heavily on the internal knowledge of LLMs for comprehension and in- ference. Therefore, hallucinations are more likely to occur with these attributes. Comparison with object hallucination. LVLMs gener- ally exhibit lower performance on attribute hallucinations compared to object hallucinations. This decrease in per- formance is due to attribute hallucinations being a finer- grained form of object hallucinations. In these cases, LVLMs must not only identify the object but also discern its attributes, demanding a deeper analysis and interpretation of the image\\u2019s semantic content. Consequently, attribute hallucination data offer a more comprehensive insight into the challenges LVLMs face with hallucinations, as well as the influence of LLMs\\u2019 internal knowledge on the process of attribute inference. PhD has crafted high-quality at- tribute hallucination datasets through an automated process, also providing explanations for these hallucinations. Such efforts are crucial for future efforts to analyze and address the issues of hallucinations effectively. 4.5. Multi-modal Conflicting Hallucination Analysis on Absurd Questions. Tab. 3 reveals the out- comes of the multi-modal conflicting experiment concern- ing absurd questions. After framing absurd questions as Yes/No queries (see data format in Sec. 3.3), LVLMs are adept at accurately recognizing these questions as unan- swerable. However, when solely VQA questions are pre- served (structured in an open-ended manner), there is a marked decrease in model performance, particularly for LLaVA, which dropped from 89.3% to 10.9%. Conversely, the decline in performance for Qwen-VL is comparatively minor, which can be attributed to its targeted adversarial training. The reason absurd questions lead models to produce an- swers completely unrelated to the image is due to biases be- tween input modalities. When the content from the visual modality does not match that of the textual modality, cur- rent LVLMs, lack mechanisms to constrain outputs around the image, and \\u201cfill in the blanks\\u201d with internal knowledge when the image fails to answer the question. This results in hallucinations. In real-world scenarios, users might exploit this characteristic of LVLMs to launch attacks, inducing Table 3. Performance on PhD. * represents manually sample 10% of the dataset for validation purposes. The result omits the percentage sign. Overall, all SOTA LVLM models are largely impacted by different types of IVL-Hallu in our PhD. IVL-Hallu Type LLaVA-1.5 MiniGPT4-v2 mPLUG-Owl2 InstructBLIP Qwen-VL Average Positive Evaluation POPE 95.4 91.5 95.3 89.7 94.9 93.4 COCO Caption 96.3 95.3 95.0 97.1 95.3 95.8 Average 95.9 93.4 95.2 93.4 95.1 94.6 Object Hallucination Object 75.0 74.6 78.5 75.2 75.7 75.8 Attribute Hallucination Shape 55.6 71.8 75.7 40.1 76.6 64.0 Material 66.4 63.9 77.2 70.7 63.3 68.3 Color 71.9 67.0 79.6 70.5 84.1 74.6 Counting 38.1 53.8 45.1 27.1 32.5 39.3 Position 43.6 48.0 53.7 45.1 52.2 48.5 Sentiment 66.1 57.7 70.2 61.7 61.7 63.5 Utility 49.1 52.7 50.9 38.2 54.5 49.1 Multi-modal conflicting Hallucination Absurd-Yes/No 89.3 68.8 84.1 12.7 78.3 66.6 Absurd-Open-Ended 10.9 11.8 21.4 4.6 45.8 18.9 Strong-tone 74.6 35.2 70.2 1.5 63.7 49.0 Weak-tone 81.6 46.9 73.9 4.5 70.7 55.5 Indirect-tone 78.4 54.8 75.2 4.0 75.3 57.5 Counter-Common-Sense Real 61.8 63.4 58.0 50.4 67.9 60.3 Generated 69.7 65.4 70.5 70.5 68.4 68.9 Average 62.1 55.7 65.6 38.5 64.7 57.3 the model to produce uncontrollable and unsafe responses. Therefore, conducting multi-modal conflicting experiments on the model holds significant practical importance. Analysis on Different tones. In multi-modal conflict- ing experiments conducted with different tones, the results align with expectations. Current models tend to follow the user\\u2019s queries to a certain extent. The stronger the textual tone, the more likely the model is to rely on textual modal- ity information rather than image information, leading to answers that deviate from the image and result in hallucina- tions. To some extent, the intensity of the tone influences the mismatch between text and image, thereby, this experi- ment can reflect the model\\u2019s ability to resist hallucinations caused by conflicts in modal information. 4.6. Counter-Common-Sense Hallucination In the counter-common-sense experiment, we observed that LVLMs still produced strong hallucinations when inputting simple textual questions related to the images. This ex- periment further validates our explanation for the source of hallucinations: hallucinations occur when LVLMs rely on guessing image information from the internal knowl- edge of LLMs. Thus, the essence of LVLMs\\u2019 hallucina- tions stems from the contradiction between the reasoning of LLMs\\u2019 internal knowledge and the content of the image. This presents a complex challenge; on one hand, the pow- erful capabilities of LVLMs originate from LLMs, yet on the other hand, this contradiction restricts LVLMs from ac- curately reflecting the content of images. According to this explanation, addressing the hallucination issue in PhD can- not solely be approached from the data level; it necessitates enhancing the structure of LVLMs to balance between dif- ferent modalities and between image content and internal knowledge. 5. Conclusions We have noticed a more fundamental issue with the VL- Hallu: LVLMs utilize context semantic information (in- cluding image) and generate results based on the mas- sive knowledge they contain. To validate our idea, we specifically define four major types of hallucinations, along with building corresponding automatic generation pipelines. With the help of these pipelines, we have imple- mented a benchmark and validated our hypothesis through extensive experiments and analyses. Our analysis of the sources of hallucinations and evaluation tasks provides a deeper understanding of the working behavior of LVLMs. Acknowledgements. This research was supported by National Natural Science Foundation of China (No. 62172420), Tencent Marketing Solution Rhino-Bird Fo- cused Research Program, and the Young Elite Scientists Sponsorship Program by CAST (No. 2023QNRC001). References [1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for un- derstanding, localization, text reading, and beyond, 2023. 1, 3, 7 [2] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In ACM FAccT, 2021. 1 [3] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning, 2023. 1, 3, 7 [4] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general- purpose vision-language models with instruction tuning, 2023. 1, 7 [5] Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, and Pascale Fung. Plausible may not be faithful: Probing object hallu- cination in vision-language pre-training. In EACL, 2023. 1 [6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3 [7] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GLM: General language model pretraining with autoregressive blank infilling. In ACL, 2022. 1 [8] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 3 [9] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba- tra, and Devi Parikh. Making the V in VQA matter: Ele- vating the role of image understanding in Visual Question Answering. In CVPR, 2017. 3, 7 [10] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language models, 2024. 3 [11] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei. Lan- guage models are general-purpose interfaces. arXiv preprint arXiv:2206.06336, 2022. 3 [12] Qidong Huang, Xiaoyi Dong, Pan zhang, Bin Wang, Con- ghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multi- modal large language models via over-trust penalty and retrospection-allocation. arXiv preprint arXiv:2311.17911, 2023. 3 [13] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Ag- garwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Sub- hojit Som, Xia Song, and Furu Wei. Language is not all you need: Aligning perception with language models, 2023. 1, 3 [14] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1\\u201338, 2023. 1 [15] Liqiang Jing, Ruosen Li, Yunmo Chen, Mengzhao Jia, and Xinya Du. Faithscore: Evaluating hallucinations in large vision-language models, 2023. 3 [16] Kushal Kafle and Christopher Kanan. An analysis of visual question answering algorithms. In ICCV, 2017. 3, 7 [17] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wal- lace, and Colin Raffel. Large language models struggle to learn long-tail knowledge. In ICML, 2023. 1 [18] Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo. Volcano: Mitigating multimodal hallucination through self-feedback guided revision, 2023. 3 [19] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 3 [20] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In EMNLP, 2023. 1, 3, 4, 5, 8 [21] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\\u00b4 ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\\u2013755. Springer, 2014. 6, 8 [22] Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusion- bench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt- 4v (ision), llava-1.5, and other multi-modality models. arXiv preprint arXiv:2310.14566, 2023. 1, 3, 4, 6 [23] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023. 3 [24] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 3 [25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 1, 3, 7 [26] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507\\u20132521, 2022. 3 [27] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 3195\\u20133204, 2019. 3 [28] R OpenAI. Gpt-4 technical report. arXiv, pages 2303\\u2013 08774, 2023. 1 [29] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image cap- tioning. In EMNLP, 2018. 3 [30] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1 [31] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. An llm-free multi-dimensional benchmark for mllms hallu- cination evaluation. arXiv preprint arXiv:2311.07397, 2023. 3, 4 [32] Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, et al. Evaluation and analysis of hal- lucination in large vision-language models. arXiv preprint arXiv:2308.15126, 2023. 3 [33] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Jun- feng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-owl: Modularization empowers large language models with mul- timodality, 2023. 3 [34] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 3 [35] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large lan- guage model with modality collaboration, 2023. 1, 3, 7 [36] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. Woodpecker: Hallucination correction for multimodal large language models. arXiv preprint arXiv:2310.16045, 2023. 3 [37] Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wen- tao Ye, Bosheng Qin, Siliang Tang, Qi Tian, and Yueting Zhuang. Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data, 2023. 3 [38] Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer, and Manling Li. Halle-switch: Controlling object hallucination in large vision language models, 2023. 3 [39] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo- hamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models, 2023. 3\",\n          \"RegionGPT: Towards Region Understanding Vision Language Model Qiushan Guo1, Shalini De Mello2\\u2020, Hongxu Yin2, Wonmin Byeon2, Ka Chun Cheung2, Yizhou Yu1, Ping Luo1, Sifei Liu2 1The University of Hong Kong 2NVIDIA Figure 1. We introduce RegionGPT that enables complex region-level captioning, reasoning, classification, and expression comprehension capabilities for the multimodal large language model. Users can input regions of interest of any shape, utilizing \\u27e8region\\u27e9as a placeholder within the instruction at any position. Such placeholders are subsequently replaced with semantic region-level embeddings that are fed into the language decoder. Best viewed in color. Abstract Vision language models (VLMs) have experienced rapid ad- vancements through the integration of large language mod- els (LLMs) with image-text pairs, yet they struggle with de- tailed regional visual understanding due to limited spatial awareness of the vision encoder, and the use of coarse- grained training data that lacks detailed, region-specific captions. To address this, we introduce RegionGPT (short as RGPT), a novel framework designed for complex region- level captioning and understanding. RGPT enhances the spatial awareness of regional representation with simple yet effective modifications to existing visual encoders in VLMs. We further improve performance on tasks requiring a spe- cific output scope by integrating task-guided instruction prompts during both training and inference phases, while maintaining the model\\u2019s versatility for general-purpose tasks. Additionally, we develop an automated region cap- tion data generation pipeline, enriching the training set with detailed region-level captions. We demonstrate that a universal RGPT model can be effectively applied and sig- nificantly enhancing performance across a range of region- level tasks, including but not limited to complex region de- scriptions, reasoning, object classification, and referring *Qiushan Guo was an intern at NVIDIA during the project. \\u2020 equal contribution. expressions comprehension. Code will be released at the project page. 1. Introduction Vision Language Models (VLMs) have marked a notable convergence between visual and linguistic domains in arti- ficial intelligence. With the emergence of Multimodal Large Language Models (MLLMs) [1, 2, 14, 26, 29, 30, 60], there has been a notable enhancement in the field\\u2019s ability to in- terpret images and streamline interactions between humans and VLMs. However, despite their effectiveness in under- standing entire images, these models still struggle with an- alyzing specific regions in detail. On the other hand, fine- grained understanding is vital for advanced vision tasks, in- cluding the analysis of object attributes and the interpreta- tion of inter-object relations. Addressing region-level complex understanding in VLMs demands the alignment of spatial information and semantics. To acheive this, existing works [9, 29, 36, 60] learn inputting regions of interest in textual form, e.g. [x1,y1,x2,y2], which share the same model structure as that used for image-level tasks. However, this relies heavily on the language decoder to interpret the position, inadver- tently overlooking the prior positional information provided by the visual encoder. Such an oversight can lead to a gap in effectively integrating visual cues with linguistic context, 1 arXiv:2403.02330v1 [cs.CV] 4 Mar 2024 which is crucial for tasks involving detailed image under- standing. In a more advanced approach, GPT4RoI [57] in- troduces spatial boxes with RoI-aligned features, training the model specifically on region-text pairs. Despite that, the positional format is restricted to a box. And yet the poten- tial for region-specific visual representation, which could offer more expressive fine-grained details and hence benifit downstream vision tasks, remains under-explored. In this paper, we present RGPT, a general framework de- signed to facilitate complex region-level captioning and un- derstanding. Specifically, we discover that simply refining the visual features extracted by CLIP and employing Mask Pooling to accommodate regions of interest (RoI) of any shape significantly enhances the language model perfor- mance on understanding spatial-aware semantic concepts. Furthermore, we develop task-guided instruction prompts that seamlessly integrate the vision tasks, such as closed- set classification and referring expression comprehension, into our framework. This is achieved by specifying these tasks with visual question answering and response formats. Existing available region-level captioning datasets, such as ReferCOCOg [23] and VG [24], tend to provide overly sim- plistic descriptions of regions, lacking detailed attributes such as color, shape, style and their spatial relation with the surroundings. To reduce the burden of manual label- ing, we propose an automated pipeline for annotating de- tailed region-level captions, which is achieved by reformat- ting the existing object detection dataset and employing a two-stage GPT-assisted approach. Our annotated captions average 87.14 words per region, substantially surpassing the 8.46 words in ReferCOCOg, thereby providing richer con- textual information for each region. Our contributions are threefold: (1) We propose RGPT, a general framework that harnesses the capabilities of LLMs to tackle complex region-level captioning and understand- ing tasks. RGPT is designed for open-ended vision ques- tions, catering to both image-level and region-level tasks. (2) We design task-guided instruction prompts to specify the output format, thereby eliminating ambiguities in the responses. By transforming vision tasks into VQA tasks, the output patterns are aligned to the language model. (3) We present a novel data reformation approach and pipeline, leveraging GPT-assistant, to create high-quality, detailed region-level captions. Our approach significantly enhances the descriptive richness of these captions, with an average word count of 87.14 words per caption. 2. Related Work 2.1. Large Language Model Large Language Models have recently gathered consider- able interest in the realm of Natural Language Processing (NLP), which is viewed as a form of artificial general in- telligence. This surge in attention is attributable to their remarkable proficiency in several key areas: language gen- eration, in-context learning, and the integration of extensive world knowledge and reasoning abilities. The early poten- tial of LLM was first showcased by groundbreaking works such as, BERT [15] and GPT [37]. This initiated a trend of scaling up that led to a succession of significant advance- ments, including T5 [39], GPT-3 [4], Flan-T5 [13], PaLM [12], among others. As training data and model parameters expanded, this scaling-up progress culminated in the devel- opment of ChatGPT [42] by OpenAI. ChatGPT, leveraging a generative pre-trained model and refined through instruc- tion tuning [35] based on human feedback, demonstrates unparalleled capabilities in engaging in human-like conver- sations. Rapid advancements in open-source LLMs, such as Llama [44], Llama-2 [45] and Vicuna [11], have also started to make them increasingly competitive with ChatGPT. 2.2. Multimodal Large Language Model LLMs have demonstrated formidable capabilities in prior knowledge and reasoning, prompting interest in other modalities. This has led to efforts aimed at extending LLMs into the multimodal domain, where they can interact with and interpret information across various inputs beyond just text. For image modality, end-to-end instruction tuning on image-text pairs is proposed to connect the visual backbone with language decoder. Flamingo [1], BLIP-2 [26], LLaVA [30] and MiniGPT4 [60] are the pioneers to train vision- language connector or language decoder on image-level vi- sion tasks, such as image captioning and visual question answering. Inspired by these pioneers, more recent works are emerged to construct user-friendly interaction dataset [18, 25] and lightweight trainable weights [17, 56]. Some other interesting works have made remarkable progress by extending LLM to audio [7, 21], medical VQA [32, 58] and control systems [16, 33]. 2.3. Region-level Vision Language Model Traditional region-level tasks are common practice in com- puter vision, such as object detection [5, 40], instance segmentation [20] and semantic segmentation [41], which aims at localizing the regions of interest and close-set classification. Open-vocabulary region-level recognition tasks [51, 52] target at understanding an object with arbi- trary categories described by texts. Recently, region-aware MLLMs, like KOSMOS-2 [36], Shikra [9], MiniGPT-2 [8] and LLaVA [29], learn inputting regions information in tex- tual form, which heavily rely on the language decoder to interpret position. We argue that incorporating a visual spatial-aware module can extract region-level features more directly and efficiently. By utilizing a visual-language con- nector, these features enable the complex region-level cap- tioning and reasoning ability. VisionLLM [48], GPT4RoI 2 Figure 2. Overview of the proposed RGPT architecture. Starting from a visual backbone, we extract low-resolution semantic features from an input image Xv. Then, a feature refinement module is composed to obtain higher-resolution feature maps. With a patch merge module, the feature maps are further merged to reduce the length of input image-level sequence. The mask features are obtained by averaging the feature in the target region Xr, inputted as another branch, with Mask Pooling layer. Both the image-level feature and region-level feature share the connector for semantic consistency. The example interactions demonstrate the model\\u2019s capabilities in complex region-level description, reasoning, object classification, and referring expression comprehension. [57] and ASM [49] utilize spatial boxes with ROI-aligned features to align the region-level features into LLM word embedding space. However, the input positional format is restricted to a box. Besides, the region visual representa- tion for fine-grained details remains under-explored. On the contrary, our model supports any-shape region as input and focuses on complex reasoning and captioning. Meanwhile, we introduce task-guided instruction prompts to transform- ing vision tasks into VQA tasks, whose output patterns are aligned with the language model. 3. Method RGPT is a multimodal large language model with strong ca- pabilities in understanding and referring to specific regions. It can take a inputs of any 2D region, usually in the form of a box or a mask, and provide answers based on instructions. By setting rules for how it should respond to instructions, the model is able to output in a useful and consistent for- mat. This feature allows RGPT to classify objects at the region level in a closed vocabulary. Additionally, by giving the model region proposals, it can identify specific objects or regions given the query description. This makes RGPT a practical tool for tasks that require detailed understanding and processing of different regions within an image. 3.1. Model Architecture An overview of our method RGPT, for region-level under- standing and image-level understanding is shown in Fig. 2. It contains an image encoder to extract semantic features, a feature refinement module for the refinement of the low- resolution feature map, an MLP layer to project visual fea- tures into the word embedding space and a large language model taking both visual and text tokens. Visual Backbone. RGPT adapts a pretrained CLIP ViT-L [38] model as the visual backbone. The visual backbone is frozen during the entire training process. Specifically, an input image Xv is encoded into a low-resolution feature map ZLRes = f(Xv) by the visual backbone. Feature Refinement Module. The visual backbone yields a low-resolution feature map, which is not capable of rep- resenting small-scale regions and objects. To further refine the visual features, we introduce two deconvolution layers of stride 2 to produce feature maps up-scaled by 4\\u00d7, i.e., ZHRes = g(ZLRes). Our method aims to understand any arbitrary-shaped region of the image, therefore, we choose Mask Pooling to extract region-level features from the high- resolution feature map. More concisely, we average the fea- tures of ZHRes in region Xr to get the region-level feature Zr = MaskPool (ZHRes, Xr). 3 COCO Object Detection User: What category name best describes the region represented by \\u27e8region1\\u27e9? Answer the question using COCO-80 category names. Assistant: TV User: How would you label the section \\u27e8region3\\u27e9? Assistant: Laptop Referring Expression Comprehension User: From the provided masks denoted by \\u27e8region1\\u27e9, \\u27e8region2\\u27e9, \\u27e8region3\\u27e9, \\u27e8region4\\u27e9and \\u27e8region5\\u27e9, which one fits \\u201ca desktop monitor with Bruce Lee photo on it\\u201d? Answer the question using the template Region [x]. Assistant: Region [2] . User: Match the object with its description: a black Xbox 360. Assistant: No corresponding options found. Table 1. Task-guided instruction prompt to indicate the response format. Two specific tasks are illustrated here. The guided prompt is highlighted in red. We empirically show that instruction prompt is able to adjust the output format and significantly improves the mAP and accuracy on COCO 2017 val set. Since the visual features are flatten as sequence input to language decoder, therefore, high-resolution feature map gets longer sequence input , which significantly lowers the training and inference efficiency. Hence, we simply use adaptive pooling layer [31] to merge image feature patches for image-level feature Zv = AdaPool (ZHRes, (H, W)), where (H, W) is the target shape of the low-resolution out- put feature map. MLP Vision-language Connector. To project visual fea- tures from the visual backbone into the language model\\u2019s word embedding space, a two-layer MLP is adopted as the vision-language connector. The embedding of a full image is represented as Hv = h(Zv) and the region embedding is Hr = h(Zr). Both the image-level and region-level fea- tures share the same connector for semantic consistency. Large Language Model. RGPT incorporates Vicuna (7B) [11] as the language decoder. Textual inputs are first to- kenized and transformed into word embeddings. Both image-level and region-level features, after being processed through the MLP connector, are directly input into the lan- guage decoder. 3.2. Region-level Instruction Tuning General prompt format. For each image Xv, we generate multi-turn conversation data ([Xv,X1 q], X1 a, ..., XT q ,XT a ), where T is the number of turns, Xt q is the t-th instruction and Xt a is the corresponding response, following [30]. The image is always used as the starting input of the first in- struction to provide the contextual information. To facili- tate region-level responses, we introduce the special token \\u27e8region\\u27e9as a placeholder in the user input prompt, which will be replaced by the corresponding region embedding Hr. The training loss is the standard auto-regressive train- ing objective. We only set the response as the learning tar- get, ignoring the instruction parts. Task-guided instruction prompt. The language model is trained without imposing restrictions on the range of its outputs, in pursuit of achieving flexibility and adaptability. However, certain tasks demand specific output formats. For instance, in the context of the COCO detection task, when provided with a specified bounding box, the model is re- quired to output only the corresponding class name. This response must be selected from a predetermined set of 80 candidate categories. To tailor the model\\u2019s responses to spe- cific tasks, we craft custom instruction prompts to guide the model to a desirable output format, as shown in Tab. 1. The task-guided instruction prompt ensures that the model remains both versatile and accurate in its task- specific applications. We empirically show that our carefully-designed instruction prompt significantly im- proves the mAP result on COCO 2017 val set. Pre-training stage. To maintain and enhance the model\\u2019s capability in understanding images at both the global and regional levels, we adopt a joint pre-training strategy en- compassing both image-level and region-level tasks. For global image understanding, we utilize the LAION-CC- SBU-558K dataset [29], employing image captioning as a pretext task. In parallel, to bolster the model\\u2019s proficiency in interpreting and interacting with regional aspects of images, we engage it with tasks derived from datasets like Visual Genome [24], ReferCOCOg [23], and V3Det [47]. These datasets are transformed into multi-turn conversational for- mats, which help the model in region-based relationship un- derstanding, captioning, and classification. While training, we keep the visual encoder and the lan- guage models\\u2019 weights frozen, and train the feature refine- 4 Figure 3. Overview of the GPT-assisted region caption generation. In the upper block, we show our two-stage paradigm in which the final output from the assistant accurately described the local region in terms of color, size and style. In contrast, without the global caption and/or the class name, the assistant either generates vague or over-simplified description, or fails to focus on the region but instead repeating the global context. ment module and MLP vision-language connector to align the image features with language embeddings. Fine-tuning stage. We only keep the visual encoder weights frozen, and continue to update the feature refine- ment module, MLP connector and language model weights. Our objective is to develop a model capable of advanced region-level captioning and reasoning. However, the com- plexity of existing datasets like ReferCOCOg and Visual Genome for captioning is insufficient for our needs. To ad- dress this gap, we additionally incorporate the GPT-assisted region caption dataset (detailed in Sec. 3.3) into our train- ing regime. Furthermore, we craft task-guided instructive prompts on COCO-2017 and ReferCOCOg train set to de- velop the model\\u2019s ability for closed-set object classifica- tion and understanding of referring expressions, as shown in Tab. 1. Data Processing. To enhance training efficiency, we opti- mize the V3Det dataset by balancing the number of bound- ing boxes across each category. During the pre-training phase, we limit to 100 boxes per category, and in the fine- tuning phase, this is further reduced to 10 boxes per cat- egory. For the closed-set object classification task on the COCO dataset, we retained 20 boxes per category for fine- tuning. In the case of Visual Genome, we randomly sam- pled up to 10 boxes per image to generate dialogues. This filtering process is employed to generate dialogues that are rich in diversity and complexity. Although this filtering ap- proach reduces the data\\u2019s volume, it is important to note that both the visual backbone and the language model have al- ready been pre-trained on large-scale datasets. The strong prior knowledge allows the model to perform effectively even with a smaller, yet diverse set of data. Our data pro- cessing strikes a balance between training efficiency and ro- bust model performance. 3.3. GPT-assisted Region Caption Generation In this section, we present a GPT-assisted dense caption generation pipeline, developed to construct the Region Cap- tion Dataset (RecapD). Distinct from traditional image-text pair datasets that typically offer a holistic description of images, RecapD provides in-depth annotations focusing on specific object regions within images. These descriptions emphasize attributes such as color, shape, material, and the spatial relationships between objects. The primary ob- jective of RecapD is to address the challenges associated with region-level understanding and referencing in images, thereby significantly enhancing the capabilities of vision language models in detailed visual comprehension. A two-stage approach. We explore using an existing global-level image captioning VLM, i.e., LLaVA [30] for region-specific tasks. A naive approach is to crop the region of interest (RoI) and adjust it to fit the model\\u2019s input format. However, this method often leads to inaccurate captions due to the lack of contextual information from the image\\u2019s sur- 5 Dataset Images Regions Average words ReferCOCO [23] 20K 142K 3.50 ReferCOCO+ [23] 20K 142K 3.53 ReferCOCOg [23] 25.8K 95K 8.46 VG [24] 82.4K 3.8M 5.09 Ours 213K 1.5M 87.14 Table 2. Comparison of our dataset with available region- level caption datasets. Our dataset stands out with a significantly higher average word count per region caption compared to other datasets. This richness in detail provides a robust foundation for complex region-level understanding. rounding areas. The absence of surrounding information also makes it infeasible for conveying spatial relationships between objects. Alternatively, we work around the limitation of the VLMs, which does not support the simultaneous input of both global images and local region patches. To circum- vent this, in the first stage, we generate a global-level cap- tion for the image using the VLM. This global description is then used as contextual information, which we include in the form of text at the beginning of the prompt. Subse- quently in the second stage, by inputting the ROI, the VLM is prompted to describe the specific region represented by the image patch. We illustrate this approach with a detailed example in the following: In the context of the entire image, <GlobalCaption>, describe the close-up region in detail. Remarkably, our observations reveal that even with this two-stage approach, the model often struggles to accu- rately describe the input region. This inaccuracy largely stems from its inability to correctly identify the object classes within the cropped region. Therefore, we further enhance our approach by incorporating human-annotated class names as an additional condition when prompting the VLM to describe the properties of the region: In the context of the entire image, <GlobalCaption>, describe the <ClassName> in the close-up region in detail. GPT-assisted prompt augmentation. To enhance the model\\u2019s adaptability to various styles and combinations of user inputs, we augmented the input prompts using ChatGPT-4 [34]. For instance, besides \\u201cdescribe the image in detail\\u201d, one may also ask \\u201cprovide a detailed description of the given image\\u201d, or \\u201cshare a thorough analysis of the im- age\\u201d, etc, in the first stage. To ensure a diverse range of re- sponses, we created ten different versions of input prompts for both stages, as elaborated in the supplementary mate- rial. During data generation, one of these ten variations is randomly selected for each stage to promote diversity in the model\\u2019s responses. Methods PT IT Vision LLM mAP Acc (%) CLIP [38] - - ViT-L - 58.9 - RegionCLIP [59] - - R50x4 - 58.3 - LLaVA\\u2020 [30] 595K 158K ViT-L Vicuna-7B - 40.04 Shikra\\u2020 [9] 600K 5.5M ViT-L Vicuna-7B - 53.91 GPT4RoI\\u2020 [57] 266K 731K ViT-L LLaVA-7B - 64.01 PVIT\\u2020 [6] 13.7M 243K ViT-L + R50x4 LLaVA-7B - 64.53 ASM [49] \\u223c22M \\u223c22M ViT-L Hasky-7B 69.3 - Ours 923K 953K ViT-L Vicuna-7B 70.0 80.61 Table 3. Comparison with Region-level based methods on COCO-2017 val set. Following RegionCLIP [59] and PVIT [6], we report the results of object classification with ground-truth box on COCO val set. \\u2020 represents the results are imported from [6]. - means that the results are not reported in the source paper. Region caption dataset analysis. Utilizing our automated annotation pipeline, we annotate a corpus of 213K V3Det images [47], leveraging its comprehensive object bound- ing boxes and class names. This dataset includes about 13,000 precisely labeled concepts, providing a rich founda- tion for model training. This extensive and precise labeling enhances the reliability of the generated data. To further refine our dataset, we utilize the CLIP model [38] to calcu- late the similarity between the image regions and the corre- sponding generated region captions. This process allows us to filter out noisy or irrelevant samples, ensuring that only high-quality data is used for training. As shown in Tab. 2, our dataset is distinguished by having a notably higher av- erage number of words, 87.14 words per caption, in each region\\u2019s caption versus other datasets. This detailed rich- ness lays a solid groundwork for an in-depth understanding at the region level. 4. Experiments In this section, we present experimental settings and results. The experiments are primarily conducted on region classifi- cation [28], captioning [23, 24], expression comprehension [23] and object hallucination benchmark [27]. We present both quantitative and qualitative results. 4.1. Implementation details During the entire training process, the visual backbone weights remain unchanged. We train the model with an im- age resolution of 336\\u00d7336 during both the pre-training and fine-tuning stages. An input image is padded to achieve a square format, if it is not square. In the pre-training stage, we employ a cosine learning rate scheduler. The maximum learning rate is set at 1e-3, with a weight decay of 0 and a warmup ratio of 0.03. The model is trained with a batch size of 256 for one epoch. In the fine-tuning stage, the max- imum learning rate is reduced to 2e-5, and the batch size is adjusted to 128. All other hyperparameters remain the same as the pre-training stage. 6 Model RefCOCOg Visual Genome METEOR CIDEr METEOR CIDEr GRIT [50] 15.2 71.6 17.1 142.0 SLR [54] 15.9 66.2 - - Kosmos-2 [36] 14.1 62.3 - - Ours 16.9 109.9 17.0 145.6 Table 4. Performance on the region-level captioning task on RefCOCOg and Visual Genome. We report METEOR and CIDEr metrics, following the image-level caption task. Method MDETR[22] Shikra [9] Kosmos-2 [36] MiniGPT-V2 [8] Ours val 81.64 82.27 60.57 84.44 86.44 test 80.89 82.19 61.65 84.66 86.96 Table 5. REC on ReferCOCOg val and test set [23]. As RGPT focuses on region-level understanding rather than localiza- tion, hence, we highlight the strength of our model in interpreting complex expressions within the context of the provided regions from [61]. 4.2. Quantitative Evaluation Region Classification. We first evaluate the object classifi- cation ability of our model on COCO-2017 dataset. The mAP and classification accuracy metrics are reported to quantify performance. Our focus is on region recogni- tion, rather than object localization. Therefore, following RegionCLIP [59], we use ground-truth boxes as the in- put for positional information. Alongside this, we attach task-guided instruction prompts to the general instruction prompt and input only one bounding box for one-turn con- versation. If the output does not fall within the predefined candidate categories of the COCO dataset, we simply dis- card this prediction and categorize it as a misclassification. We report the results of VLMs and feature-based vision models, as shown in Tab. 3. For our baseline, we crop the RoI from images, resize them to the input size, and then compare their features with those of the 80 classes in the COCO dataset to select the category with the highest simi- larity. Additionally, we consider other feature-based meth- ods like RegionCLIP [59] and ASM [49]. RegionCLIP pre- trains CLIP model to align the CC3M [43] region-text pairs in the feature space. ASM is trained on approximately 22M images and the features are produced by the language de- coder. The other VLMs use textual formats as output. On the COCO dataset, our approach achieves a mAP of 70.0 and an accuracy of 80.86%, demonstrating our method\\u2019s effectiveness in constraining output formats and its strong capability in region-level object recognition. Region Captioning. We evaluate the region-level caption- ing ability of our model on the ReferCOCOg [23] and Vi- sual Genome [24], employing the same evaluation met- rics as used for image-level captioning: METEOR [3] and CIDEr [46]. As illustrated in Tab. 4, our model surpasses Arch. Deconv BiLinear Deconv + BiLinear None AP 66.8 60.9 62.7 57.7 APs 51.1 52.8 53.8 42.7 APm 71.5 70.8 71.4 65.2 APl 78.0 57.9 60.3 65.4 Table 6. Ablation study on the feature refinement module. The object classification results on COCO 2017 val set are reported. We use ViT-B/16 from [55] as our visual backbone, whose input size is 512\\u00d7512. Deconv represents our two deconvolution layers design for feature maps of scale 4. BiLinear indicates the use of bilinear upsampling for scale 16. Deconv + BiLinear means bilin- ear upsampling the Deconv output for scale 16. None refers to no module is used. Model AP APs APm APl OpenAI ViT-L-336 70.0 55.7 75.5 81.5 SigLip ViT-B-512 66.8 51.1 71.5 78.0 SigLip ViT-L-384 69.5 56.8 74.1 80.2 SigLip ViT-SO400M-384 71.0 57.9 76.5 81.6 Table 7. Ablation study on visual backbone. The object classifi- cation results on COCO 2017 val set are reported. We use SigLip models from [55] pre-trained on WebLI dataset [10] and OpenAI CLIP model [38]. The results demonstrate that our method can be further improved with more powerful visual network. the region-aware VLM, Kosmos-2 [36]. The results high- light our model\\u2019s proficiency in accurately generating refer- ring expressions for image regions. Referring Expression Comprehension (REC). We evalu- ate expression comprehension of our model on the Refer- COCOg dataset. Our method focuses on region-level un- derstanding, rather than object localization. Therefore, we utilize bounding box proposals from [61] as candidate box sets. If the Intersection Over Union between the ground truth box and any of the candidate boxes is less than 0.5, we include the ground truth box in our set of candidates. The results in Tab. 5 only highlight the specific strength of our model in understanding complex expressions within the context of the provided regions. Ablation Study on Feature Refinement Module. We study the effect of the feature refinement module on the ob- ject classification task. Our motivation for this module is to refine the CLIP visual features for better spatial-aware semantics. Tab. 6 shows that two-deconvolution-layer de- sign significantly outperforms the baseline model (the last column), demonstrating the effectiveness of feature refine- ment. An interesting observation is that the methods of 16x upsampling (BiLinear and Deconv + BiLinear) enhance the accuracy of classification for smaller objects, though it shows a decrease in performance for larger objects. Our approach achieves a superior trade-off between these two 7 Datasets Metrics Ours Shikra [9] InstructBLIP [14] MiniGPT4 [60] LLaVA [30] MM-GPT [18] mPLUG-Owl [53] Random Accuracy (\\u2191) 87.80 86.90 88.57 79.67 86.00 50.10 53.97 Precision (\\u2191) 97.75 94.40 84.09 78.24 87.50 50.05 52.07 Recall (\\u2191) 78.13 79.26 95.13 82.20 84.00 100.00 99.60 F1 Score (\\u2191) 86.85 86.19 89.27 80.17 85.71 66.71 68.39 Yes 41.20 43.26 56.57 52.53 48.00 99.90 95.63 Popular Accuracy (\\u2191) 87.20 83.97 82.77 69.73 76.67 50.00 50.90 Precision (\\u2191) 95.44 87.55 76.27 65.86 72.22 50.00 50.46 Recall (\\u2191) 78.13 79.20 95.13 81.93 86.67 100.00 99.40 F1 Score (\\u2191) 85.92 83.16 84.66 73.02 78.79 66.67 66.94 Yes 40.93 45.23 62.37 62.20 60.00 100.00 98.57 Adversarial Accuracy (\\u2191) 85.67 83.10 72.10 65.17 73.33 50.00 50.67 Precision (\\u2191) 91.99 85.60 65.13 61.19 69.02 50.00 50.34 Recall (\\u2191) 78.13 79.60 95.13 82.93 84.67 100.00 99.33 F1 Score (\\u2191) 84.50 82.49 77.32 76.05 66.32 66.67 66.82 Yes 42.47 46.50 73.03 67.77 61.33 100.00 98.67 Table 8. Results on the object hallucination benchmark using the POPE evaluation pipeline [27]. Except for our model and LLaVA [30], the other results are obtained from [9]. aspects. We believe that implementing more complex and carefully designed feature optimization mechanisms could potentially lead to further improvements in performance. Ablation Study on Visual Backbone. We study the ef- fect of the visual backbone on the object classification task. The results in Tab. 7 demonstrate that the performance on region-level understanding can be further improved by re- placing current visual backbone with a more powerful one. Object Hallucination. We evaluate object hallucina- tions, generating objects that are inconsistent with the tar- get images in the descriptions, of our method using the POPE evaluation pipeline [27], with the results detailed in Tab. 8. Our approach significantly outperforms recent popu- lar image-level VLMs. Given that our baseline model is the LLaVA model, we attribute this performance gain to our region-level instruction fine-tuning strategy, which signifi- cantly refines the model\\u2019s ability to interpret images with greater precision. 4.3. Qualitative Evaluation As demonstrated in Fig. 4, RGPT is capable of analyz- ing of relationships between multiple regions within an im- age. Additionally, in the green example, our model exhibits complex region-level reasoning abilities akin to those seen in GPT-4V demonstrations, effectively interpreting and ex- plaining visual content. 5. Conclusion In this paper, we present RGPT, a general vision-language model that tackles complex region-level captioning and reasoning following user instruction. Our model employs region-level instruction tuning to align the visual feature with the language word embedding space. Besides, we carefully design task-guided instruction prompts to seamlessly blend vision tasks within GPT framework, by Figure 4. Qualitative evaluation of the mutli-turn conversation of RGPT. Our model preserves the mutli-turn conversation and image-level captioning ability. converting the vision tasks to VQA tasks and prompting the response format. Finally, we propose a two-stage GPT-assisted annotation pipeline to reformat the object detection dataset and create detailed region-level captions. The results demonstrate that RGPT achieves impres- sive performance on the region-level understanding tasks. 8 References [1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\\u201323736, 2022. 1, 2 [2] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John- son, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. 1 [3] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with hu- man judgments. In Proceedings of the acl workshop on in- trinsic and extrinsic evaluation measures for machine trans- lation and/or summarization, pages 65\\u201372, 2005. 7 [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan- tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan- guage models are few-shot learners. Advances in neural in- formation processing systems, 33:1877\\u20131901, 2020. 2 [5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to- end object detection with transformers. In European confer- ence on computer vision, pages 213\\u2013229. Springer, 2020. 2 [6] Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li, Maosong Sun, and Yang Liu. Position-enhanced visual instruction tuning for multimodal large language models. arXiv preprint arXiv:2308.13437, 2023. 6 [7] Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. X-llm: Bootstrapping ad- vanced large language models by treating multi-modalities as foreign languages. arXiv preprint arXiv:2305.04160, 2023. 2 [8] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478, 2023. 2, 7 [9] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multi- modal llm\\u2019s referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. 1, 2, 6, 7, 8 [10] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly- scaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022. 7 [11] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang- hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong- hao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. 2, 4 [12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebas- tian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. 2 [13] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. 2 [14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general- purpose vision-language models with instruction tuning, 2023. 1, 8 [15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 2 [16] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm- e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 2 [17] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi- angyu Yue, et al. Llama-adapter v2: Parameter-efficient vi- sual instruction model. arXiv preprint arXiv:2304.15010, 2023. 2 [18] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and lan- guage model for dialogue with humans. arXiv preprint arXiv:2305.04790, 2023. 2, 8 [19] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A dataset for large vocabulary instance segmentation. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. 1 [20] Kaiming He, Georgia Gkioxari, Piotr Doll\\u00b4 ar, and Ross Gir- shick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\\u20132969, 2017. 2 9 [21] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language mod- els. arXiv preprint arXiv:2302.14045, 2023. 2 [22] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr- modulated detection for end-to-end multi-modal understand- ing. In Proceedings of the IEEE/CVF International Confer- ence on Computer Vision, pages 1780\\u20131790, 2021. 7 [23] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in pho- tographs of natural scenes. In Proceedings of the 2014 con- ference on empirical methods in natural language processing (EMNLP), pages 787\\u2013798, 2014. 2, 4, 6, 7, 1 [24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan- tidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:32\\u201373, 2017. 2, 4, 6, 7, 1 [25] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023. 2 [26] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 1, 2 [27] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucina- tion in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. 6, 8 [28] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\\u00b4 ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\\u2013755. Springer, 2014. 6, 1 [29] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. 1, 2, 4 [30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 1, 2, 4, 5, 6, 8 [31] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for instance segmentation. In Pro- ceedings of the IEEE conference on computer vision and pat- tern recognition, pages 8759\\u20138768, 2018. 4 [32] Michael Moor, Qian Huang, Shirley Wu, Michihiro Ya- sunaga, Cyril Zakka, Yash Dalmia, Eduardo Pontes Reis, Pranav Rajpurkar, and Jure Leskovec. Med-flamingo: a multimodal medical few-shot learner. arXiv preprint arXiv:2307.15189, 2023. 2 [33] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre- training via embodied chain of thought. arXiv preprint arXiv:2305.15021, 2023. 2 [34] OpenAI. Gpt-4 technical report, 2023. 6 [35] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car- roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training lan- guage models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730\\u201327744, 2022. 2 [36] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Ground- ing multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 1, 2, 7 [37] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by gen- erative pre-training. 2018. 2 [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervi- sion. In International conference on machine learning, pages 8748\\u20138763. PMLR, 2021. 3, 6, 7 [39] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\\u20135551, 2020. 2 [40] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information process- ing systems, 28, 2015. 2 [41] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- net: Convolutional networks for biomedical image segmen- tation. In Medical Image Computing and Computer-Assisted Intervention\\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234\\u2013241. Springer, 2015. 2 [42] John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al. Chatgpt: Opti- mizing language models for dialogue. OpenAI blog, 2022. 2 [43] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, im- age alt-text dataset for automatic image captioning. In Pro- ceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\\u20132565, 2018. 7 [44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\\u00b4 ee Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2 [45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2 10 [46] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evalua- tion. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566\\u20134575, 2015. 7 [47] Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou, Tong Wu, Bin Wang, Conghui He, and Dahua Lin. V3det: Vast vocabulary visual detection dataset. arXiv preprint arXiv:2304.03752, 2023. 4, 6, 1 [48] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. arXiv preprint arXiv:2305.11175, 2023. 2 [49] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhen- hang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu, Zhiguo Cao, et al. The all-seeing project: Towards panop- tic visual recognition and understanding of the open world. arXiv preprint arXiv:2308.01907, 2023. 3, 6, 7 [50] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A gen- erative region-to-text transformer for object understanding. arXiv preprint arXiv:2212.00280, 2022. 7 [51] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 18134\\u201318144, 2022. 2 [52] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao- long Wang, and Shalini De Mello. Open-vocabulary panop- tic segmentation with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 2955\\u20132966, 2023. 2 [53] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 8 [54] Licheng Yu, Hao Tan, Mohit Bansal, and Tamara L Berg. A joint speaker-listener-reinforcer model for referring expres- sions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7282\\u20137290, 2017. 7 [55] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. arXiv preprint arXiv:2303.15343, 2023. 7, 1 [56] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023. 2 [57] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: In- struction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601, 2023. 2, 3, 6 [58] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Vi- sual instruction tuning for medical visual question answer- ing. arXiv preprint arXiv:2305.10415, 2023. 2 [59] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chun- yuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region- based language-image pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16793\\u201316803, 2022. 6, 7 [60] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo- hamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1, 2, 8 [61] Zhuofan Zong, Guanglu Song, and Yu Liu. Detrs with col- laborative hybrid assignments training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6748\\u20136758, 2023. 7 11 RegionGPT: Towards Region Understanding Vision Language Model Supplementary Material A. Data A.1. Instructions for region-level understanding. The list of instructions used to briefly describe the region content are shown in Tab. 21. For detailed region descrip- tion, the instructions are shown in Tab. 22. To describe the relationship between the regions, the instructions in Tab. 23 are used. Tab. 24 illustrates the instructions for region clas- sification. For referring expression comprehension, we con- vert the location task to choice problem, selecting the re- gions which match the query description. A.2. Instruction Tuning Data. We list the region-level instruction tuning data in Tab. 9 and Tab. 10 for the Pre-training and Fine-tuning stage. For mul- tiple task dataset, we integrate all the instruction-following data into a multi-turn conversation format. This approach enhances training efficiency and ensures the model\\u2019s capa- bility in multi-round dialogues. We perform random selection across all annotations for each category, retaining a target number of annotations per category. Images with no annotations selected are dis- carded. Pre-train Data Size Task Random Sampling V3Det [47] 177K Classification 100 per class VG [24] 108K Caption & Relationship No RefCOCO [23] 25.8K Caption & REC No Table 9. Region-level training data in the Pre-training Stage. Fine-tuning Data Size Task Random Sampling V3Det [47] 98K Classification & Caption 10 per class COCO [28] 1.5K Classification 20 per class LVIS [19] 52K Classification 20 per class VG [24] 108K Caption & Relationship No RefCOCO [23] 25.8K Caption & REC No Table 10. Region-level training data in the Fine-tuning Stage. B. More Ablation Studies Instruction for region classification. For the region clas- sification task, we have developed three distinct instruction modes. As shown in Tab. 13, the first mode involves a one- turn conversation for all RoIs, inputting all RoIs into a sin- gle instruction, with the LLM outputting all categories si- multaneously. The second mode is a multi-turn conversa- tion for all RoIs, where the LLM conducts multiple rounds of dialogue, classifying one RoI per round. The third mode is a one-turn conversation for one RoI, with the LLM clas- sifying only one RoI per dialogue. The results in Tab. 11 show that multi-turn conversation mode outperforms the other modes, because the previously predicted box provides conditions for the subsequently pre- dicted box, and only one prediction is made at a time, re- ducing the difficulty. Mode One-turn for all RoIs Multi-turn for all RoIs One-turn for one RoI mAP 70.0 73.8 71.5 Table 11. Ablation study on the instruction mode for region classification. The object classification results on COCO 2017 val set are reported. We use ViT-B/16 from [55] as our visual backbone, whose input size is 512\\u00d7512. All region instances in COCO are used as training data without random sampling. The number of sample for each concept. To assess the impact of the number of annotations per category on clas- sification performance, we conducted experiments on the COCO dataset with varying annotation quantities. We ran- domly sampled 10, 20, 50, and 200 annotations per cate- gory for training. As indicated in the Tab. 12, a consistent enhancement in performance was observed with an increas- ing number of sampled annotations. However, the marginal gain in performance diminished with more data. Notably, increasing annotations from 20 to 200 per category resulted in only a 4 mAP increase. Num 10 20 50 200 mAP 52.7 56.8 57.3 60.9 Table 12. Ablation study on the annotation quantities for re- gion classification. The object classification results on COCO 2017 val set are reported. We use ViT-B/16 from [55] as our vi- sual backbone, whose input size is 512\\u00d7512. Different from the other experiments, only the COCO classification region-level data is used to train model. Ablation study on inaccurate annotations. Besides mask, RGPT accepts user-drawn bounding box as inputs. While it doesn\\u2019t directly accept points or scribbles, these can be eas- ily converted into usable masks via SAM. RGPT is robust to inaccurate annotations, like dilated / eroded mask due to our refinement module, as shown in Tab 15 and 16. C. More Qualitative Results To demonstrate the multi-turn conversation and complex reasoning capabilities of RGPT, we provide more qualita- tive results. The conversations in Tab. 17 show that our model can infer relationship between regions. Tab. 18 demonstrates that our model can identify the target from 1 One-turn for all RoIs User: What category name best describes the region represented by \\u27e8region1\\u27e9, \\u27e8region2\\u27e9and \\u27e8region3\\u27e9? Answer the question using COCO-80 category names. RegionGPT: Car, person, kite. Multi-turn for all RoIs User: What category name best describes the region represented by \\u27e8region1\\u27e9? Answer the question us- ing COCO-80 category names. Assistant: Car. User: Identify the object or feature present in the region denoted by \\u27e8region2\\u27e9. Assistant: Person. User: Can you specify the type of object or land- scape within the bounds of \\u27e8region3\\u27e9. Assistant: Kite. One-turn for one RoI User: What category name best describes the region represented by \\u27e8region1\\u27e9? Answer the question us- ing COCO-80 category names. Assistant: Car. User: Identify the object or feature present in the region denoted by \\u27e8region2\\u27e9. Assistant: Person. User: Can you specify the type of object or land- scape within the bounds of \\u27e8region3\\u27e9. Assistant: Kite. Table 13. In one-turn for all RoIs mode, all the regions are input to RGPT within one instruction. For multi-turn for all RoIs, the regions are split and construct sequential conversation. In one- turn for one RoI mode, the conversation is closed for after each instruction. Ground Truth User: What category name best describes the region represented by \\u27e8region1\\u27e9and \\u27e8region2\\u27e9? Answer the question using COCO-80 category names. RegionGPT: Airplane, airplane. Prediction User: What category name best describes the region represented by \\u27e8region1\\u27e9and \\u27e8region2\\u27e9? Answer the question using COCO-80 category names. RegionGPT: Aerocraft, aerocraft. Table 14. Task-guided instruction prompt sometimes can not re- strict the response format well. The prediction may not be within the COCO-80 classname set. Model w/ Feature Refinement w/o Feature Refinement w/ seg w/ box w/ seg w/ box mAP 71.0 70.4 65.6 60.0 Table 15. The object classification results on COCO 2017 val set. We use SigLip ViT-SO400M as the visual backbone and input re- gion in the box or mask format. Model Box Seg Seg w dilation Seg w erosion mAP 69.3 70.0 71.0 68.2 Table 16. The object classification results on COCO 2017 val set. We input box, mask, dilated mask, and eroded mask to RGPT. The kernel is 5 for mask dilation and erosion for 336\\u00d7336 input. multiple similar objects based on positional cues and per- form complex reasoning. Tab. 19 illustrates that our model can describe the information of a region in detail, it can even identify the brand of a motorcycle. Tab. 20 indicates that 2 our model can do complex reasoning with multiple regions as input query. D. Limitations and Future Work The current task-guided instruction prompt sometimes can not restrict the response format well. As shown in Tab. 14, for region classification task, the output can be the syn- onym of ground truth classname. The evaluation of object classification can be reformulated as the semantic similarity between the prediction and ground truth name via a pre- trained text encoder. E. Ethics Concerns The large language model in our method is pre-trained with the corpus collected by previous works. Despite applying filtering, there may still be potential bias in its response. 3 User: What can be inferred about \\u27e8region1\\u27e9and \\u27e8region2\\u27e9relationship? RegionGPT: From the image, it can be inferred that the woman and the horse share a close relationship, likely as a rider and a horse. The woman is walking towards the horse, which is standing in a stable, and they are in a stable or barn setting. This suggests that they might be preparing for a ride, training session, or simply spending time together. The woman\\u2019s presence and interaction with the horse indicate that she is familiar with the animal and possibly has a bond with it. User: What skills might \\u27e8region2\\u27e9need for the potential activity? RegionGPT: For the potential activity of horse riding, the woman would need a variety of skills and knowledge to ensure a safe and enjoyable experience. Some of these skills and benefits include: 1. Balance and coordination: Horse riding requires maintaining balance and coordination while riding the horse, especially when navigating different terrains or performing various maneuvers. 2. Communication and understanding: The woman should be able to communicate effectively with the horse through body language, voice commands, and aids to guide the horse and establish a mutual understanding between them. 3. Confidence and assertiveness: Riding a horse can be intimidating for some people, so having confidence and assertiveness in one\\u2019s abilities is essential to overcome any fears and effectively control the horse. 4. Knowledge of horse care and behavior: Understanding the horse\\u2019s needs, behavior, and body language is crucial for ensuring the horse\\u2019s well-being and a successful riding experience. 5. Safety awareness: Being aware of safety precautions, such as wearing appropriate riding gear, knowing how to mount and dismount the horse, and understanding the horse\\u2019s temperament, can help prevent accidents and injuries during horse riding. By developing these skills and knowledge, the woman can enjoy a safe and enjoyable horse riding experience while also building a strong bond with her horse. Table 17. Qualitative visualization of complex reasoning capability. User: What is \\u27e8region1\\u27e9doing, and how can the surroundings affect his mood? RegionGPT: In the image, a man in a black wetsuit is standing on the beach, with his arms outstretched. He appears to be enjoying the ocean view and the beach atmosphere. The surroundings, including the presence of other people and surfboards, can positively affect his mood by providing a sense of camaraderie and shared enjoyment of the beach environment. The ocean view and the sound of the waves can also contribute to a relaxing and refreshing atmosphere, which can enhance his overall experience and mood. Table 18. Qualitative visualization of complex reasoning capability. 4 User: Describe the \\u27e8region1\\u27e9in detail. RegionGPT: The motorcycle in the image is a black Harley Davidson. It is a large and powerful motorcycle, known for its distinctive design and style. The motorcycle is parked on the side of a road, and it appears to be well-maintained and in good condition. The man standing next to the motorcycle is wearing a jacket, which suggests that the weather might be cool or windy. The motorcycle\\u2019s size and style make it stand out on the road, and its presence adds a sense of adventure and freedom to the scene. User: What challenge might \\u27e8region2\\u27e9face in this environment? RegionGPT: In the image, a man is standing next to a parked motorcycle on a road. The environment appears to be a rural or countryside setting, which might present some challenges for the man. For example, he might face difficulties in finding gas stations, restaurants, or other amenities that are typically found in urban areas. Additionally, the road might be less maintained or less frequently traveled, which could lead to potential hazards or obstacles, such as potholes, debris, or wildlife. The man might also need to be prepared for changing weather conditions, as rural areas can sometimes experience sudden changes in weather or be more prone to natural disasters. Overall, the man would need to be self-sufficient and prepared for the challenges that come with traveling in a rural environment. Table 19. Qualitative visualization of complex reasoning capability. User: Please tell me the roles of \\u27e8region1\\u27e9, \\u27e8region2\\u27e9and \\u27e8region3\\u27e9. Give the reason. RegionGPT: The roles of the people in the image are as follows: 1. Batter: The person holding the baseball bat, who is preparing to hit the ball. 2. Catcher: The person wearing a baseball glove, positioned behind the batter to catch the ball. 3. Umpire: The person standing behind the catcher, observing the game and making decisions on plays. The reason for these roles is that in a baseball game, the batter\\u2019s objective is to hit the ball and score runs, while the catcher\\u2019s role is to catch the ball thrown by the pitcher and prevent the batter from advancing. The umpire\\u2019s role is to enforce the rules of the game, make decisions on plays, and ensure fair play. User: What might \\u27e8region1\\u27e9aim to achieve during the game? RegionGPT: During the baseball game, the batter, who is holding a baseball bat and standing in the batter\\u2019s box, aims to achieve a successful hit by hitting the ball thrown by the pitcher. The batter\\u2019s goal is to hit the ball with enough force and accuracy to reach the bases and ultimately score runs for their team. This can be achieved by using proper batting techniques, timing, and strategy to anticipate the type and speed of the pitch. The batter\\u2019s performance can significantly impact the outcome of the game, as their ability to hit the ball effectively can contribute to their team\\u2019s overall success. User: What might \\u27e8region2\\u27e9aim to achieve during the game? RegionGPT: The catcher, who is positioned behind the batter in the image, aims to achieve several goals during the game. First, the catcher\\u2019s primary responsibility is to catch the pitches thrown by the pitcher. They must be prepared to react quickly and accurately to the ball\\u2019s trajectory to prevent it from getting past them. Second, the catcher plays a crucial role in strategizing with the pitcher, suggesting pitch types and locations to keep the batter off balance and increase the chances of getting them out. Third, the catcher must be ready to throw the ball to the appropriate base to prevent the opposing team\\u2019s runners from advancing or scoring. Overall, the catcher\\u2019s goal is to contribute to the team\\u2019s defensive efforts and help prevent the opposing team from scoring runs. Table 20. Qualitative visualization of complex reasoning capability. 5 1. Provide a brief caption for the area indicated by \\u27e8region\\u27e9. 2. Describe in a short phrase the content within the bounds of \\u27e8region\\u27e9. 3. How would you succinctly caption the region highlighted by \\u27e8region\\u27e9? 4. Summarize the scene or object present in the section marked by \\u27e8region\\u27e9. 5. Can you give a concise description of what\\u2019s depicted in \\u27e8region\\u27e9? 6. Draft a short title for the image content enclosed by \\u27e8region\\u27e9. 7. What brief caption would best describe the visual within \\u27e8region\\u27e9? 8. Offer a succinct interpretation of the area pointed out by \\u27e8region\\u27e9. 9. If you were to provide a short tagline for the content at \\u27e8region\\u27e9, what would it be? 10. Give a one-liner description of the region demarcated by \\u27e8region\\u27e9. 11. How would you encapsulate the essence of the segment labeled \\u27e8region\\u27e9in a few words? 12. Characterize the content of the image portion specified by \\u27e8region\\u27e9briefly. 13. Craft a mini headline for the visual element spotlighted by \\u27e8region\\u27e9. 14. In a few words, how would you narrate the content found within \\u27e8region\\u27e9? 15. Pen down a concise caption for the image section delineated by \\u27e8region\\u27e9. 16. A short caption of region \\u27e8region\\u27e9: 17. A short description of region \\u27e8region\\u27e9: 18. A photo containing the region \\u27e8region\\u27e9: 19. A region \\u27e8region\\u27e9that shows 20. Write a short description for the region \\u27e8region\\u27e9 21. Write a description for the region \\u27e8region\\u27e9 22. Provide a description of what is presented in the region \\u27e8region\\u27e9. 23. Briefly describe the content of the region \\u27e8region\\u27e9. 24. Can you briefly explain what you see in the region \\u27e8region\\u27e9? 25. Could you use a few words to describe what you perceive in the region \\u27e8region\\u27e9? 26. Please provide a short depiction of the region \\u27e8region\\u27e9. 27. Using language, provide a short account of the region \\u27e8region\\u27e9. 28. Use a few words to illustrate what is happening in the region \\u27e8region\\u27e9. 29. Provide an overview of what you see in the region \\u27e8region\\u27e9. 30. Can you break down the main elements present in this region \\u27e8region\\u27e9? 31. What are the key features or subjects captured in this region \\u27e8region\\u27e9? 32. Summarize the primary components of this region \\u27e8region\\u27e9. 33. Walk me through the different aspects of this region \\u27e8region\\u27e9. 34. Highlight the main points of interest in this region \\u27e8region\\u27e9. 35. What stands out to you the most in this region \\u27e8region\\u27e9? 36. If you were to give a brief overview of this region \\u27e8region\\u27e9, what would you mention? 37. List the primary objects or subjects you identify in this region \\u27e8region\\u27e9. 38. Describe the first few things that catch your attention in this region \\u27e8region\\u27e9. 39. How would you introduce this region \\u27e8region\\u27e9to someone who hasn\\u2019t seen it? 40. What are the defining characteristics of this region \\u27e8region\\u27e9? 41. Give a concise description of the main content in this region \\u27e8region\\u27e9. 42. If you were to caption this region \\u27e8region\\u27e9, what might you say? 43. Describe the scene or setting depicted in this region \\u27e8region\\u27e9. Table 21. The list of instructions for brief region description. 6 1. Describe in detail the object located at \\u27e8region\\u27e9in the image, including its appearance, style, and any visible details. 2. Provide a comprehensive description of the area marked by \\u27e8region\\u27e9, focusing on textures, colors, and any notable features. 3. Elaborate on the artwork shown in the region indicated by \\u27e8region\\u27e9, mentioning its color, appearance, size, style, and any standout features. 4. Give a detailed analysis of the scene within the boundary of \\u27e8region\\u27e9, touching upon its components, ambiance, and any thematic expressions. 5. Craft a thorough narrative about the piece of the image highlighted by \\u27e8region\\u27e9, from its aesthetic qualities to its possible historical context. 6. Explain in depth the characteristics and attributes of the subject found in the segment tagged with \\u27e8region\\u27e9. 7. Generate a long, detailed caption for the segment of the image at \\u27e8region\\u27e9, covering aspects such as its origin, material, and any symbolic meaning. 8. Paint a vivid picture with words about the region at \\u27e8region\\u27e9, diving into the intricacies and nuances present in the area. 9. Zoom in on the area indicated by \\u27e8region\\u27e9and describe every discernible detail, from texture and color to form and function. 10. Offer an expanded description of the contents within the area marked by \\u27e8region\\u27e9, encompassing its color, appearance, size, style, and any remarkable features. Table 22. The list of instructions for detailed region description. 1. Explain the relationship between the area indicated by \\u27e8region\\u27e9and the region marked by \\u27e8region\\u27e9in terms of their visual or thematic connection. 2. Describe any functional or aesthetic connection between the elements at \\u27e8region\\u27e9and \\u27e8region\\u27e9in the image. 3. Analyze how the region \\u27e8region\\u27e9complements or contrasts with the area \\u27e8region\\u27e9in terms of design and composi- tion. 4. Discuss the interplay between the features located at \\u27e8region\\u27e9and the attributes of the region at \\u27e8region\\u27e9. 5. Detail the way in which the area labeled \\u27e8region\\u27e9interacts with or relates to the region designated by \\u27e8region\\u27e9 within the image\\u2019s context. 6. Assess the correlation or disparity between the segment at \\u27e8region\\u27e9and the segment at \\u27e8region\\u27e9, including any observable influences or contrasts. 7. Compare the region \\u27e8region\\u27e9with the area \\u27e8region\\u27e9to determine how they either work together or differ substan- tially within the image. 8. Identify and elaborate on any thematic or stylistic relationships between the contents of \\u27e8region\\u27e9and \\u27e8region\\u27e9. 9. Interpret the connection between the area at \\u27e8region\\u27e9and the region at \\u27e8region\\u27e9, considering their positions, roles, or symbolism in the image. 10. Clarify how the part of the image within \\u27e8region\\u27e9corresponds with, or is disparate from, the part within \\u27e8region\\u27e9 in terms of their visual narrative. Table 23. The list of instructions for region relationship description. 7 1. Identify the object or feature present in the region denoted by \\u27e8region\\u27e9. 2. What category best describes the area represented by \\u27e8region\\u27e9? 3. Describe the content of the image section highlighted by \\u27e8region\\u27e9. 4. Can you specify the type of object or landscape within the bounds of \\u27e8region\\u27e9? 5. Which of the following categories best fits the region marked by \\u27e8region\\u27e9? Provide your answer. 6. What can you discern from the area indicated by \\u27e8region\\u27e9in the image? 7. Categorize the visual element within the area designated by \\u27e8region\\u27e9. 8. Give a brief description of the item or scene captured in the segment marked by \\u27e8region\\u27e9. 9. Which classification would you assign to the visual content found at \\u27e8region\\u27e9? 10. Determine and describe the primary subject located within \\u27e8region\\u27e9. 11. How would you label the section of the image encompassed by \\u27e8region\\u27e9? 12. Assess and classify the feature present within the confines of \\u27e8region\\u27e9. 13. If you were to tag the section indicated by \\u27e8region\\u27e9, what tag would you use? 14. What stands out to you in the region demarcated by \\u27e8region\\u27e9? Please classify it. 15. Evaluate the content of the image portion pinpointed by \\u27e8region\\u27e9and provide its category. Table 24. The list of instructions for region category description. 1. Given the mask proposals \\u27e8region\\u27e9in the image, can you pinpoint the one that matches \\u27e8description\\u27e9. 2. From the provided masks denoted by \\u27e8region\\u27e9in the picture, which one best fits the description of \\u27e8description\\u27e9? 3. Looking at the mask suggestions \\u27e8region\\u27e9in the image, identify the one that corresponds to \\u27e8description\\u27e9. 4. In the image with mask proposals \\u27e8region\\u27e9, please highlight the one that represents \\u27e8description\\u27e9. 5. Considering the mask candidates \\u27e8region\\u27e9from the photo, which one would you associate with \\u27e8description\\u27e9? 6. Among the mask proposals \\u27e8region\\u27e9in the visual, can you discern the one depicting \\u27e8description\\u27e9? 7. From the set of masks labeled as \\u27e8region\\u27e9in the image, which one aligns with the description \\u27e8description\\u27e9? 8. Based on the mask data provided as \\u27e8region\\u27e9in the photo, can you spot the one indicative of \\u27e8description\\u27e9? 9. In the presented image with mask suggestions \\u27e8region\\u27e9, determine which mask resonates with \\u27e8description\\u27e9. 10. Given the mask assortment \\u27e8region\\u27e9in the image, please detect the one that matches the characteristics of \\u27e8description\\u27e9. 11. Reviewing the mask candidates \\u27e8region\\u27e9from the picture, can you single out the one that fits \\u27e8description\\u27e9? 12. From the list of mask proposals \\u27e8region\\u27e9in the image, identify the one that best encapsulates \\u27e8description\\u27e9. 13. Considering the provided mask data \\u27e8region\\u27e9in the visual, which one would you say corresponds to \\u27e8description\\u27e9? 14. In the snapshot with the mask proposals \\u27e8region\\u27e9, please locate the mask that can be described as \\u27e8description\\u27e9. 15. Based on the available mask candidates \\u27e8region\\u27e9in the image, can you pick the one that portrays \\u27e8description\\u27e9? Table 25. The list of instructions for referring expression comprehension. 8\",\n          \"Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs M. Jehanzeb Mirza1,2 Leonid Karlinsky3 Wei Lin4 Sivan Doveh5,6 Jakub Micorek1 Mateusz Kozinski1 Hilde Kuhene3,7 Horst Possegger1 1ICG, TU Graz, Austria. 2CDL-EML. 3MIT-IBM Watson AI Lab, USA. 4JKU, Austria. 5IBM Research, Israel. 6Weizmann Institute of Science, Israel. 7University of Bonn, Germany. Project Page: https://jmiemirza.github.io/Meta-Prompting/ Abstract. Prompt ensembling of Large Language Model (LLM) gener- ated category-specific prompts has emerged as an effective method to en- hance zero-shot recognition ability of Vision-Language Models (VLMs). To obtain these category-specific prompts, the present methods rely on hand-crafting the prompts to the LLMs for generating VLM prompts for the downstream tasks. However, this requires manually composing these task-specific prompts and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest. To effectively take humans out of the loop and completely auto- mate the prompt generation process for zero-shot recognition, we propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only minimal information about the target task, in the form of its short nat- ural language description, and a list of associated class labels, MPVR automatically produces a diverse set of category-specific prompts result- ing in a strong zero-shot classifier. MPVR generalizes effectively across various popular zero-shot image recognition benchmarks belonging to widely different domains when tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot recognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on average over 20 datasets) leveraging GPT and Mixtral LLMs, respectively. 1 Introduction Dual encoder Vision-Language Models (VLMs) [35, 47] attain unprecedented performance in zero-shot image classification. They comprise a text encoder and an image encoder trained to map text and images to a shared embedding space. Zero-shot classification with dual encoder VLMs consists in evaluating the cosine similarity between the embedding of a test image and the embeddings of texts representing candidate classes. The composition of the class-representing text has a significant impact on the accuracy of zero-shot classification. Already the authors of CLIP [35], the first large-scale vision-language model, highlighted its importance and reported arXiv:2403.11755v2 [cs.CV] 19 Mar 2024 2 M. J. Mirza et al. that embedding class names in a prompt of the form \\u2018A photo of a <class name>\\u2019 resulted in considerable performance growth over using raw class names. Moreover, specializing the prompt to the data set by adding high-level concepts, for example, embedding the class name in the sentence \\u2018A photo of a <class name>, a type of flower\\u2019 for fine-grained flower recognition, brought further improvement. Finally, a substantial performance boost was achieved by ensem- bling multiple different prompts, tailored towards the downstream task (dataset). Since ensembling a larger number of dataset- and class-specific prompts is ben- eficial, and manually designing a large number of class-specific prompts is pro- hibitively time-consuming, several authors delegated prompt generation to a Large Language Model (LLM) [28,34,37]. These approaches consist in asking an LLM to generate class descriptions [34], or class attributes [28], and mix them with manually defined prompt templates [37]. They enable generating large sets of prompts adapted to the downstream task, which would be prohibitively time- consuming when performed manually. However, they still require hand-crafting prompts to the LLM [34] or dataset-specific LLM prompt templates [37], or rely on the assumption that class attributes are discriminative [28,37]. In other words, they do not eliminate the manual effort completely, but shift some of it from manually designing prompts for the VLMs (as in [35]) to manually design- ing LLM prompts. This raises the following question: Does the manual design of the LLM prompts bias the resulting VLM prompts, possibly affecting perfor- mance? In this work, we answer this question affirmatively: we minimize manual interventions in the prompt generation process and show that this significantly boosts zero-shot recognition accuracy. 1 The gist of our approach lies in automating the prompt generation process. To that end, we draw inspiration from methods for reducing the prompt engi- neering effort in natural language processing [13,40] and propose to meta-prompt the LLM to produce LLM query templates tailored to the downstream task. We call our method Meta-Prompting for Visual Recognition (MPVR). Its overview is presented in Figure 1. MPVR comprises a \\u2018system prompt\\u2019 that describes the meta-prompting task for the LLM, a description of the downstream task, and an in-context example. The in-context example contains a description (metadata) of another task and its corresponding \\u2018LLM queries\\u2019, and serves to bootstrap the LLM with examples of expected results. They are kept the same across differ- ent downstream tasks and for all our experiments. MPVR extracts the LLM\\u2019s knowledge of the visual world gradually, in two steps. The first query to the LLM contains the system prompt, in-context example, and the downstream task (dataset) description, and produces a diverse set of LLM query templates, con- taining a <class name> placeholder. These templates are infused (by the LLM) with information on visual styles specific to the downstream task of interest, but they are still category-agnostic. In the second step, for each class, we populate its label into all the task-specific LLM query templates generated in the first step 1 To avoid confusion between the \\u2018prompts\\u2019 used to query the LLMs and the \\u2018prompts\\u2019 used to compute the text embedding by the VLMs, in the remaining part of this manuscript, we call the first one \\u2018LLM query\\u2019 and the second one \\u2018VLM prompt\\u2019. Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs 3 Meta Prompt Template Task-Specific LLM Queries Category-Specific VLM Prompts System Prompt LLM LLM In-context Example Downstream Task Specification -Describe video game renditions of the <class name>. -Describe the origami models visually inspired by the <class name>. -What makes plush objects visually unique in representing the <class name>. -Explain the visual features of paintings and patterns associated with the <class name>. -In what ways do sketches and tattoos visually capture the essence of the <class name>. -Detail the visual aspects of graffiti, embroidery, and graphics associated with the <class name>? -Flamingos, sketch show a tall, slender bird with a long, curved neck and bright pink feathers. -The sculpture of the flamingo is a highly realistic and detailed representation of the bird. -The painting rendition of the flamingo typically showcases its distinctive and elegant characteristics like pink hue of the feathers. -Durable Material: The plastic rendition of the flamingo is made from high-quality, durable plastic material which makes it resistant to wear and tear and showcase the beautiful brid in full might. Flamingo Fig. 1: Our MPVR utilizes a Meta Prompt, comprising a system prompt (instruction), in-context example demonstrations (fixed throughout), and metadata (name and de- scription) for a downstream task of interest. The Meta Prompt instructs an LLM to generate diverse task-specific LLM queries, which are used to obtain category-specific VLM prompts (visual text descriptions) by again querying the LLM after specifying the <class name>. These category-specific VLM prompts are then ensembled into a zero-shot classifier for recognizing the downstream task categories. and use them to query the LLM to generate (category-specific) VLM prompts describing the category in visually diverse ways and also containing task-specific visual styles infused by the LLM in the first step. We use the resulting VLM prompts to create an ensemble of zero-shot classifiers. In section 4, we show that MPVR\\u2019s two-step process results in state-of-the-art zero-shot classification. Our meta-prompting strategy does not take any parameters specific to the dataset, other than the dataset description, which can be easily obtained through public APIs or from its webpage. Yet, we show that prompts generated by MPVR cover diverse visual concepts and styles specific to the downstream task. As a result, MPVR yields significant performance gains on a range of zero-shot benchmarks. Our contributions can be summarized as follows: \\u2013 We propose MPVR: a general, automated framework requiring minimal human involvement for tapping into the visual world knowledge of LLMs through meta-prompting for zero-shot classification. \\u2013 MPVR generalizes beyond closed models (like GPT [2]). We are the first to show that category-specific descriptions generated from open-source models (like Mixtral [16]) can also enhance the zero-shot recognition abilities of state-of-the-art VLMs. \\u2013 We open-source a dataset of \\u223c2.5M unique class descriptions harnessed from GPT and Mixtral with our meta-prompting framework. This is the first large-scale dataset encompassing the breadth of LLM knowledge of the visual world. 4 M. J. Mirza et al. 2 Related Work We first provide an overview of the zero-shot vision-language foundation models, then touch upon approaches that propose to improve these models by requiring visual data (relying on additional training), and later discuss different methods that follow our line of work, i.e., improving zero-shot models in a training- free manner by generating textual data through LLMs and finally provide an overview of the prompt engineering literature. Large Scale Vision-Language Foundation Models: VLMs have shown impressive performance for many vision-language understanding tasks, e.g., zero-shot recog- nition, visual question-answering (VQA), image captioning, etc. The present-day VLMs can be placed in two distinct groups in a broader categorization. One group of methods relies on dual-encoders (vision and text encoder) and usually trains the encoders with a contrastive objective by using a large corpus of paired image-text data scraped from the web. The most common among these methods are CLIP [35], ALIGN [15], OpenCLIP [38], and the very recent MetaCLIP [47]. The zero-shot classification is performed by measuring the similarity between the image embeddings and encoded text features, usually obtained by using the default template \\u2018a photo of a <class name>\\u2019. The other group of methods aligns the visual modality with a frozen LLM. BLIP-2 [22] bridges the modal- ity gap between a pre-trained visual encoder and an LLM by using a querying transformer. Instruct-BLIP [8] proposes to improve [22] by employing instruction tuning. MiniGPT [53] aligns a vision encoder with a frozen LLM (Vicuna [6]) by only using a (trainable) linear projection layer between the two. MiniGPT- V2 [3] replaces the LLM with Llama-2 [41] and also proposes to unfreeze it during the training/finetuning phases. Llava [26] also aligns an LLM with a pre- trained visual encoder and also proposes Visual Instruction Tuning, by carefully curating instruction-response pairs, to enhance the performance. Furthermore, the performance of LLaVA is also enhanced with better data curation [24] and slight architectural changes [25]. In our work, we focus on the contrastively pre- trained zero-shot models widely used for object recognition (e.g., CLIP [35]), and improve the recognition abilities of these models by generating the text embed- dings from a variety of descriptions (instead of the default templates) harnessed through our proposed meta-prompting technique. Furthermore, we show that MPVR-enhanced CLIP [35] outperforms even the leading LLM-decoder-based methods (e.g., [25]) in visual recognition tasks. Training-based Approaches for Improving VLMs: Different approaches propose to improve the zero-shot recognition performance of the contrastively pre-trained models through parameter-efficient fine-tuning. CoOp [52] proposed to learn ran- domly initialized text prompts in a few-shot manner. CoCoOp [51] further condi- tions the learnable text prompts on the visual inputs to enhance the performance. Maple [18] proposes to learn both the visual and text prompts in conjunction. Contrary to relying on few-shot labeled visual samples, UPL [14] proposes to learn the text prompts on unlabeled image data and LaFTer [31] learns visual Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs 5 prompts by leveraging the cross-modal transfer capabilities of CLIP. While these approaches propose to adapt the VLM on image data, MAXI [23] proposes to fine-tune CLIP in an unsupervised manner for video inputs. In contrast to the methods proposed to improve the zero-shot recognition abilities of CLIP, our work does not rely on visual inputs and gradient-based updates of network pa- rameters. Instead, it improves the zero-shot recognition performance by harness- ing fine-grained textual concepts generated through our MPVR, thus supporting the capability to scale zero-shot recognition performance improvements to visual domains where no visual data might be available for training. Zero-shot Recognition with Additional Textual Data from LLMs: It was ini- tially highlighted in CLIP [35] that generating the text embeddings through an ensemble of (dataset specific) hand-crafted prompts2 improved the zero- shot recognition performance on the downstream datasets, hinting towards the sensitivity of CLIP\\u2019s text encoder towards fine-grained textual concepts. Fol- lowing up on this idea, DCLIP [28] enhances visual recognition by generat- ing category-specific descriptors through an LLM (GPT-3 [2]). On the other hand, CUPL [34] proposes to obtain the category-level text embeddings from the prompts generated with the dataset-specific hand-crafted queries fed to the LLM. Waffle [37] hints towards the potential bag-of-words behavior of the CLIP text encoder and performs zero-shot classification by adding random descrip- tors to broad concepts and DCLIP-generated attributes. Our work also takes inspiration from the prompt ensembling in [28, 34, 35, 37] and performs zero- shot classification by generating category-level prompts through an LLM. How- ever, contrary to these approaches, MPVR proposes a more general prompting framework to alleviate the human effort spent for handcrafting the LLM queries (CUPL [34]), dataset-specific concepts (Waffle [37]), or reduce reliance on indi- vidually recognizable visual attributes (DCLIP [28]). By effectively incorporating general downstream task information (description) into the first phase of MPVR (i.e., meta-prompting), we automatically produce task-tailored LLM query tem- plates ready to be populated by task categories and used to query an LLM for a diverse spectrum of category-level VLM prompts comprising an enhanced set of visual details for recognizing those categories. The performance gains by using MPVR with both closed and open-source LLMs (GPT [2] and Mixtral [16]) on 20 different datasets when compared to relevant baselines highlight the general- ization capabilities and benefits of our approach. Prompt Engineering: Manually manipulating the text inputs (prompts) to the LLMs for enhancing performance for various natural language processing (NLP) tasks has been an active field of research, which is formalized as prompt engi- neering. In this context, providing demonstrations to the LLM for solving re- lated downstream tasks has been referred to in the NLP literature as in-context learning (ICL) [4,29,44,49]. Orthogonal to the idea of in-context learning, some approaches rely on breaking down a complex task into a series of events. To 2 https://github.com/openai/CLIP/blob/main/data/prompts.md 6 M. J. Mirza et al. this end, Chain-of-Thought (CoT) [45] achieved impressive performance gains by prompting the model to perform intermediate reasoning steps. Other approaches following this line of work include [19,48]. Our MPVR also employs ICL and ma- nipulates the input prompts to the LLMs, but effectively alleviates the need for human involvement for this manipulation by probing an LLM for more diverse concepts (LLM query templates \\u2013 incorporating general information about the task), which are then populated with specific task categories and fed again to the LLM for generating VLM prompts - both task- and category-specific text descriptions of visual concepts. To the best of our knowledge, such a two-stage (meta-) prompting strategy for tapping into the visual world knowledge of LLMs does not exist in literature. 3 MPVR: Meta-Prompting for Visual Recognition Zero-shot classification with a dual encoder VLM consists in projecting a test image and each candidate class to the common embedding space, and evaluating the cosine similarity between the embeddings. The image embedding is produced by the VLM\\u2019s vision encoder \\u03d5. The embedding of a class is obtained by passing a textual description of the class, called a VLM prompt, through the VLM\\u2019s text encoder \\u03c8. The simplest technique of constructing a VLM prompt is to complete a prompt template, for example, \\u2018A photo of a <class name>\\u2019, with class label [35]. The authors of CLIP [35], the first large-scale VLM, highlighted that prompt composition is vital to the performance of the zero-shot classifier. To boost the performance, they proposed VLM prompt ensembling, which rep- resents the class as a mean embedding of multiple diverse prompts. To formalize this approach, we denote the test image by x, the set of candidate classes by C, and the set of prompt templates by P. By p(c) we denote a prompt obtained by completing template p \\u2208P with the label of class c \\u2208C. We define the zero-shot likelihood of class \\u02c6 c as \\\\lab e l {eq:m ean_text_ e mb} \\\\likel _{\\\\hat \\\\ cls } (\\\\ i m g ) = \\\\ frac {e^{\\\\simil (\\\\txtEnc _{\\\\hat \\\\cls },\\\\visEnc (\\\\img ))/{\\\\tau }}}{\\\\sum _{\\\\cls \\\\in \\\\ClassNames } e^{\\\\simil (\\\\txtEnc _{\\\\cls },\\\\visEnc (\\\\img ))/{\\\\tau }}} , \\\\quad \\\\text {where} \\\\quad \\\\txtEnc _{\\\\cls } = \\\\frac {1}{|P|} \\\\sum _{p\\\\in P} \\\\txtEnc (p(\\\\cls )), (1) and \\u03c4 denotes the temperature constant. This approach forms the point of de- parture for our method. Ensembling a larger number of class-specific VLM prompts improves the performance of the zero-shot classifier, but generating these prompts manually would be prohibitively time-consuming. Several methods [28,30,34,37] address this problem by generating the VLM prompts with a large language model (LLM), for example GPT [2]. They enhance the performance of the zero-shot classifiers, but still require manual construction of the LLM queries, which scales poorly: A prohibitively large human effort might be needed to creatively design prompts that cover the diverse ways the visual aspects of a certain class can be described in text. Moreover, manually specified queries can be influenced by the subjective bias of the person who composes them, which could affect zero-shot recognition performance. Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs 7 Meta Prompt LLM LLM Task- Specific LLM Queries Flamingo Flamingo Trilobite Trantule Brambling Public APIs Dataset Name Metadata Instilling Class Infromation Fig. 2: MPVR framework. In the first stage, a meta-prompt comprising of a system prompt, in-context examples, and metadata consisting of downstream task specification is queried to the LLM instructing it to generate multiple diverse task-specific LLM queries, which are populated with the category of interest and again queried to the LLM to obtain the category-level prompts for assembling a zero-shot classifier. To improve the scaling of VLM prompt generation and eliminate subjectivity from the process, we design Meta Prompting for Visual Recognition (MPVR), an approach to VLM prompt generation that reduces human input to the necessary minimum. MPVR taps into the visual world knowledge possessed by the VLM and extracts it in two steps. In the first step, MPVR meta-prompts the LLM with generic instructions and coarse information about the downstream task to generate diverse task-specific LLM query templates. These LLM query templates encode elements of the LLM\\u2019s knowledge about the visual styles characteristic of the downstream task but are still class-agnostic. In the second step, the LLM query templates are populated with names of candidate classes and fed to the LLM to obtain VLM prompts. The resulting VLM prompts are both task- and class-specific. Each prompt carries LLM\\u2019s diverse visual knowledge about the possible appearance of objects representing the class in the style defined by the downstream task. For ease of assimilation, we divide our MPVR into two distinct stages and provide an overview in Figure 2. In Section 3.1, we describe how to effec- tively meta-prompt LLMs to generate diverse, task-specific LLM query tem- plates (stage 1). Later in Section 3.2 we describe how to use these task-specific LLM query templates to obtain category-specific VLM prompts (stage 2). 3.1 Meta-Prompting a Large Language Model Aligning with the true motivation of our MPVR, the goal of meta-prompting is to extract the abundant visual world knowledge possessed by the LLMs by querying it to generate multiple diverse LLM query templates with minimal human intervention. To that end, we compose a meta-prompt of three parts: the system prompt, an in-context example, and the downstream task specification. We illustrate the meta-prompt in Figure 3. System prompt is a generic set of instructions that describe the elements of the meta-prompt and specify the expected output of the LLM. It instructs the 8 M. J. Mirza et al. A. Describe how does the <class name> texture looks like B. How can you recognize the texture of <class name>? C. What does the texture of <class name> look like? A. Describe the artistic representation of the <class name>. B. How would you visually recognize the <class name> in art or cartoons? What is different from real world? C. Detail the visual aspects of graffitti, embroidery, and graphics associated with the <class name>. You are provided with prompt template examples for a dataset, which are provided to the LLM to generate descriptions for the categories in these datasets. Your task is to generate <N> diverse prompts for another dataset for which you are also provided the dataset name and the description. Format it correctly for use in a Python script, and do not repeat the prompts. System Prompt Describing the Structure of Meta-prompt and the Expected Output In-context Pseudocode Downstream Task Specification Pseudocode Example Dataset Name: < Input example dataset name > Description: < Input example dataset description > Prompts: prompts.append(\\\"< Input Example Prompt - 1 >\\\") prompts.append(\\\"< Input Example Prompt -2 >\\\") . . . . Dataset Name: Describable Textures Dataset (DTD) Description: The (DTD) is an evolving collection of textural images in the wild... Prompts: Dataset Name: ImageNet-Rendition Description: ImageNet-R(endition) contains art, cartoons, tattoos, graffiti, toys ... Prompts: Dataset Name: < Dataset name for which to generate prompts > Description: < Dataset description for which to generate prompts > Prompts: Example In-context Demonstrations for Bootstrapping LLM Output Example Downstream Task (top) & LLM Generated Queries (bottom) ... ... . <Generated by LLMs> Fig. 3: Our meta-prompt comprises 3 parts: A system prompt provides an overview of what is included in the overall prompt and what is expected from the LLM as a response (top). An in-context example consisting of metadata, dataset name, and hand- crafted prompts for the dataset (middle left). The downstream task metadata for which a diverse set of prompts are requested from the LLM (bottom left). For completeness, we also provide the in-context demonstrations (middle right) we use throughout, and the diverse LLM-generated queries for the example ImageNet-R dataset (bottom right). LLM to generate a variety of query templates for the downstream dataset and conveniently format them to be employed in a Python script. In-context example serves to bootstrap the LLM to the type of output that is expected. It comprises a description of an example downstream task and a list of the corresponding LLM query templates. Since we expect the output from the LLM to be suitable for use in a Python script thus, it contains the prompts listed as Python code (c.f., Figure 3, middle left & right). Downstream task specification is the only part of the meta-prompt that is specific to the downstream task. It is scraped from a public API or the webpage of the dataset associated with the task and contains a general description of the task data (c.f., Figure 3, bottom left & right). This coarse information about the downstream task of interest is critical for the LLM to generate task-specific LLM queries, which are employed in stage 2 of MPVR. Note that the system prompt and the in-context example demonstrations are generic and are kept fixed across different tasks in all of our experiments. The downstream task specification is the only part of the meta-prompt that is specific to the downstream task. Our experiments highlight that all the individual parts of the meta-prompt are extremely vital for our MPVR to obtain effective category-specific VLM prompts and are extensively ablated in Table 5. The three elements of the meta-prompt are embedded in the template pre- sented in Figure 1 (left). The resulting meta-prompt is then fed to the LLM (GPT [2] or Mixtral [16]) to generate N diverse LLM query templates that are infused with the LLM\\u2019s knowledge of visual styles expected in the dataset, but Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs 9 are still class-agnostic. Instead of the downstream <class name> of interest, they contain a generic <class name> placeholder. To obtain category-specific VLM prompts, we transition to stage 2 of our MPVR explained next. 3.2 Category-Specific VLM prompts The LLM response to the meta-prompt in stage 1 is a diverse set of LLM query templates, which contain task-specific knowledge about the downstream task of interest, but are still generic. To instill the category information, for obtain- ing the category-specific VLM prompts, we replace the generic <class name> placeholders in the LLM query templates with the actual class of interest. These diverse category-specific queries constitute our second call to the LLM, which generates category-specific VLM prompts. They carry the LLM\\u2019s knowledge of the appearance of objects of the queried classes in the context of the downstream task and are ready to be plugged into Eq. (1). We repeat this procedure for each class from 20 different datasets (used for evaluations) with both the GPT [2] and Mixtral [16] LLMs and obtain a huge corpus of \\u223c2.5M VLM prompts. In sec- tion 4, we show that the ensemble of these VLM prompts results in a zero-shot classifier that outperforms previous methods by a significant margin. The VLM prompts can be thought of as visually diverse descriptions of the queried classes in the context of the downstream tasks, and their corpus rep- resents a chunk of the LLM\\u2019s knowledge about our visual world. This diversity stems from our proposed two-stage approach3. The first stage can already pro- vide diverse LLM query templates, which resemble the dataset-specific templates for prompt ensembling 2 (but more diverse and automatically generated with our MPVR). Interestingly, even by generating the ensemble of zero-shot classifiers by populating these generic query templates from stage 1 with category infor- mation, we can already achieve enhanced zero-shot recognition, as reported in an ablation in Table 6. To conclude, after the second call to the LLM, the VLM prompts constitute fine-grained details about the specific category, reflecting the true diversity of the visual LLM knowledge and resulting in a huge category- specific text corpus, already incorporated in our codebase released on this public Github repository: https://github.com/jmiemirza/Meta-Prompting. 4 Experimental Evaluation In this section, we first briefly describe the datasets and the baselines we use to evaluate and compare our MPVR, then explain our implementation details and finally provide a detailed discussion of the results. 3 We also experimented with generating category-specific VLM prompts in a single step with meta-prompting, but it performs worse than our 2-stage framework. These results are provided in the ablations Table 6. 10 M. J. Mirza et al. 4.1 Evaluation Settings Datasets: We extensively evaluate our MPVR on 20 object recognition datasets belonging to widely different domains. These domains can be narrowed down to datasets containing commonly occurring natural categories: ImageNet [9], ImageNet-V2 [36], CIFAR-10/100 [21], Caltech-101 [10]. Fine-grained classifi- cation datasets containing different task-specific images: Flowers [32], Stand- ford Cars [20], CUBS-200 [42], Oxford Pets [33], Describable Textures dataset (DTD) [7], Food-101 [1], FGVC-Aircraft [27]. Datasets used for scene classifi- cation: Places365 [50] and SUN397 [46], action recognition datasets: UCF101 [39] and Kinetics400 [17]. Datasets consisting of out-of-distribution images: ImageNet- (R)endition [12] and ImageNet-(S)ketch [43] and also datasets which contain images taken from a satellite or an aerial view: EuroSAT [11] and RESISC45 [5]. Baselines: We compare to the following baselines and state-of-the-art methods: \\u2013 CLIP [35] denotes the zero-shot classification scores obtained by using the simple \\u2018{a photo of a <class name>}\\u2019 template (S-TEMP) and dataset- specific templates (DS-TEMP2). \\u2013 CUPL [34] proposes to generate category-level descriptions from an LLM with hand-crafted prompts for each dataset. \\u2013 DCLIP [28] proposes to obtain a zero-shot classifier with category-specific descriptors (from an LLM) consisting of usual visual attributes. \\u2013 Waffle [37] employs hand-crafted task-specific broad concepts and adds ran- dom descriptors to the prompts for zero-shot classification. Following their evaluation setting, we compare with different variants: (i) Waffle (prompt + random descriptors), (ii) WaffleCon (Waffle + high-level concepts), and (iii) WaffleConGPT (WaffleCon + DCLIP descriptors). Implementation Details: To report the results for each dataset we use the test splits provided by [52] and further build upon their framework for all our eval- uations on datasets that are not present in their framework. All the baselines are also implemented in the same framework. To generate the diverse set of task-specific LLM queries for our MPVR in the first stage, we use the public web API of ChatGPT4 and the Hugging Face API for Mixtral-7B (8x)5. To obtain the category-level VLM prompts after querying an LLM in the second stage of MPVR, we use GPT-3.5 [2] and the open source weights of Mixtral-7B (8x) [16], accessed through Hugging Face. In the first stage, we instruct the LLM to generate 30 diverse task-specific queries for each dataset, and to obtain the category-level VLM prompts, we append the category of interest and prompt the LLM to generate 10 prompts for each LLM query respectively, where we limit each generated prompt by 50 tokens. The in-context dataset is (arbitrarily) 4 https://chat.openai.com/ 5 https://huggingface.co/chat/ Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs 11 ImageNet ImageNetv2 C10 C100 Caltech101 Flowers Stanford Cars Cubs Pets DTD CLIP (S-TEMP) 61.9 54.8 88.3 64.4 91.4 64.0 60.2 51.6 85.0 40.2 CLIP (DS-TEMP) 63.3 56.0 89.2 65.1 89.9 66.7 60.0 53.0 87.4 42.4 CUPL 64.3 56.9 89.0 65.3 92.1 68.8 60.0 51.9 87.2 48.9 DCLIP 63.1 55.8 86.7 64.2 92.5 64.6 57.9 52.6 83.5 44.3 Waffle 63.4 56.3 89.4 65.2 90.8 67.8 59.9 52.8 87.7 40.4 Waffle+Con 63.4 56.3 89.4 65.2 89.7 65.2 59.5 52.1 86.8 41.7 Waffle+Con+GPT 63.4 56.3 89.4 65.2 91.9 68.2 59.6 52.6 87.9 41.8 MPVR (MIXTRAL) 63.8 56.5 89.5 65.5 92.8 75.2 58.3 55.5 88.0 50.2 MPVR (GPT) 65.0 57.3 89.9 66.3 92.9 73.9 59.5 55.9 88.1 50.8 Food101 Aircraft Places365 SUN397 UCF101 K400 IN-R IN-S EuroSAT Resisc45 CLIP (S-TEMP) 77.6 18.1 39.4 62.1 60.4 39.7 66.3 41.1 35.9 54.1 CLIP (DS-TEMP) 79.2 19.5 40.0 63.0 62.4 42.1 69.3 42.7 45.8 57.8 CUPL 81.0 20.4 _ 66.5 65.2 41.7 _ _ _ 61.9 DCLIP 79.7 19.8 40.9 63.1 62.6 39.1 66.0 42.3 48.9 56.9 Waffle 81.6 20.1 41.1 63.3 62.7 40.4 68.8 43.4 42.7 61.4 Waffle+Con 81.1 19.0 39.3 60.7 62.2 39.1 68.1 42.5 44.8 58.6 Waffle+Con+GPT 81.2 19.8 41.5 64.0 63.4 40.4 68.5 43.7 47.0 62.0 MPVR (MIXTRAL) 81.3 22.4 42.1 66.5 66.0 42.2 70.2 43.6 54.0 64.6 MPVR (GPT) 81.0 21.5 42.2 67.0 67.9 43.9 70.2 44.2 55.6 64.0 Table 1: Top-1 accuracy (%) for 20 datasets obtained by employing the ViT-B/32 backbone from OpenAI CLIP [35]. S-TEMP refer to the results obtained by using the default template (a photo of a <class name>), while DS-TEMP refer to the results obtained by using the ensemble of dataset specific prompts. An empty placeholder for CUPL [34] indicates that the respective baseline did not provide the handcrafted prompts for the dataset. For Waffle [37], mean results from 7 random runs are reported, following the original publication. chosen to be DTD [7] for all experiments, however, to avoid information con- tamination, we switch the in-context dataset to EuroSat [11] when DTD is the target dataset. We ablate the choice of DTD for in-context example and provide the complete meta prompts in the supplementary. Unless otherwise specified, we obtain the zero-shot classifier as the mean of the class embeddings obtained from the category-specific VLM prompts (from stage 2 of MPVR) using Eq. (1). 4.2 Results We test our MPVR extensively on 20 diverse datasets and report the results (with ViT-B/32 from CLIP [35]) in Table 1. We consistently outperform the CLIP zero-shot baseline while using the category-level prompts generated both from GPT and Mixtral. While comparing to the CLIP baseline, using the default template, on some datasets like EuroSAT, the improvement is up to 19.8% and 18.2%, and on average our MPVR improves upon CLIP by 5.0% and 4.5% while averaging the results on 20 datasets, with GPT and Mixtral LLMs respectively. Similarly, while compared to the more expressive CLIP zero-shot baseline, which uses the hand-crafted dataset-specific templates2, we still observe considerable average gains of 3.1% and 2.7% with the two LLMs. Our MPVR also shows strong gains when compared to CUPL [34], which ob- tains category-level prompts by hand-crafting the LLM queries for each down- stream task of interest. Our MPVR not only alleviates this extensive human effort spent while generating the category-level prompts (as in CUPL [34]) but also out-performs CUPL on most of the datasets we compare to. For example, 12 M. J. Mirza et al. OpenAI CLIP MetaCLIP 400m B/16 L/14 B/32 B/16 L/14 S-TEMP 61.9 69.2 62.4 65.9 71.0 DS-TEMP 63.8 71.2 64.0 67.3 72.8 D-CLIP 64.4 70.7 62.8 66.4 72.2 Waffle 64.0 70.7 62.8 66.5 72.4 Waffle+Con 62.7 69.1 61.7 65.7 71.7 Waffle+Con+GPT 64.6 71.0 63.2 66.9 72.7 MPVR (Mixtral) 66.4 72.5 65.6 68.7 73.9 MPVR (GPT) 66.7 73.4 65.8 68.7 74.3 Table 2: Mean top-1 accuracy (%) over 20 datasets for different backbones from OpenAI [35] and MetaCLIP-400m [47]. obtaining up to 5.1% and 6.3% performance gains on Flowers-102 [32] dataset with GPT and Mixtral LLMs. Furthermore, we also observe that while comparing with the baselines which do not generate (descriptive) VLM prompts but rely on other cues like category- level (attribute) descriptors, our MPVR also performs favorably. For example, we outperform DCLIP [28] on all the 20 datasets with performance gains up to 5.3% and 3.3% on UCF-101 with GPT and Mixtral. These results indicate that the generic attributes generated for a category by DCLIP for classification might not capture fine-grained task-specific details required to enhance the clas- sification of categories in domain-specific benchmarks (e.g., action recognition in UCF-101). Finally, from Table 1 we also observe that our MPVR (on average) also outperforms all the variants proposed by Waffle [37], which also highlights that the CLIP text encoder responds favorably to semantically rich text descrip- tions (prompts), instead of randomly generated descriptors as in Waffle [37]. In summary, our MPVR demonstrates better performance across the board, out- performing all baselines on 18 out of 20 datasets. On the Food-101 [1] dataset, our MPVR comes in second, trailing by a narrow margin of 0.3%. Similarly, on Standford Cars [20], our results indicate that even the dataset-specific prompt ensembling proposed by CLIP fails to enhance performance, underscoring the unique challenges posed by this particular dataset. To test the generalization ability of our MPVR beyond different LLMs, we also evaluate it with different backbones from CLIP [35] and also employ Meta- CLIP [47], which is trained with a different training recipe than CLIP. These results are listed in Table 2. We observe that even while testing with more expres- sive backbones, like MetaCLIP ViT-L/14, our visually diverse text descriptions (prompts) help to improve the zero-shot accuracy from 71.0% \\u219274.3% (for GPT descriptions) while averaging over the 20 datasets. Due to space limitations, we defer the individual dataset results for these backbones to the supplementary. 4.3 Ablations Here, we study the significance of different components that constitute our MPVR. Specifically, we first examine the effect of combining multiple text sources, Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs 13 GPT Mixtral Temp GPT+Temp Mixtral+Temp GPT+Mixtral GPT+Mixtral+Temp Embedding Average ViT-B/32 62.9 62.4 59.7 57.0 56.1 63.0 57.7 ViT-B/16 66.7 66.4 63.8 60.5 59.6 67.0 61.5 ViT-L/14 73.4 72.5 71.2 68.6 67.3 73.4 69.2 Softmax Average ViT-B/32 _ _ 59.8 62.8 62.3 62.4 62.4 ViT-B/16 _ _ 63.8 66.7 66.4 66.4 66.3 ViT-L/14 _ _ 71.1 73.3 72.4 72.6 72.6 Table 3: Comparison of mean top-1 accuracy (%) for MPVR over 20 datasets while constructing the zero-shot classifier by ensembling with the mean of the embeddings from different text sources (top) and mean of softmax (bottom). For GPT and Mixtral, we only report the results with the mean of the embeddings, since ensembling the softmax of individual descriptions is prohibitively expensive (also noted in [35]). For datasets with fewer classes, we performed softmax ensembling but did not find any major deviation in results. These results are provided in the supplementary. EuroSAT DTD Caltech CIFAR-100 Resisc Mean CLIP (ViT-B/32) 35.9 40.2 91.4 64.4 54.1 57.2 LLAVA-1.6 (7B) 41.3 16.2 33.0 25.7 33.8 30.0 MPVR (ViT-B/32) 55.6 50.8 92.9 66.3 64.0 65.5 Table 4: Comparison of top-1 accuracy (%) with LLAVA-1.6-Vicuna7b model [24]. and then motivate our choice of using dual encoder models like CLIP [35] instead of multi-modal language models (MMLMs) by evaluating them for image classi- fication. Later we extensively ablate our prompting strategy and finally conclude with ablations on robustness of the obtained results and scaling analysis. Ensembling Text Sources. From Tables 1 & 2 we gather that in addition to the enhanced zero-shot classification with GPT and Mixtral generated VLM prompts with our MPVR, the dataset-specific templates2 from CLIP can also show improvement in results, in comparison to only using the default templates. To evaluate the combined performance of these text sources, we ensemble the 3 different sources and provide the results in Table 3. We observe that when the category-specific VLM prompts and templates are ensembled over the embedding space, the resulting classifier is weaker than the classifier obtained from only the LLM-generated VLM prompts. However, the mean of the embeddings from both GPT and Mixtral prompts performs the best. These results hint that the prompts from both the LLMs are clustered closely in the CLIP latent space suggesting that these sources describe the categories of interest in a similar (more detailed) way, yet differently from the \\u2018more mechanical\\u2019 CLIP dataset-specific prompts that do not provide much detail. We also test ensembling the probability spaces from both sources and find that the degradation in performance as a consequence of mixing the descriptions and templates is alleviated. MMLMs for Zero-shot Classification. Recently, multi-modal language models such as LLaVA [24,26] have emerged as the preferred choice for various vision- 14 M. J. Mirza et al. dataset name dataset metadata in-context (prompts) class names Top-1 \\u2717 \\u2713 \\u2713 \\u2717 46.7 \\u2713 \\u2717 \\u2713 \\u2717 42.0 \\u2713 \\u2713 \\u2717 \\u2717 _ \\u2713 \\u2713 \\u2713 \\u2713 53.5 \\u2713 \\u2713 \\u2713 \\u2717 55.6 Table 5: Top-1 accuracy (%) for EuroSAT [11] with GPT as LLM and the ViT-B/16 backbone [35] while ablating the different parts of our Meta Prompt. The last row represents the results obtained by our MPVR. CLIP (S-TEMP) CLIP (DS-TEMP) Prompts-Only 1-Step MPVR EuroSAT 35.9 45.8 47.2 51.2 55.6 Table 6: Comparison of top-1 accuracy (%) from the zero-shot classifier obtained with the prompts generated in the first stage and generating category-level descriptions directly from stage-1 of MPVR. language tasks. Here, we extended their evaluation to zero-shot classification, and the findings are summarized in Table 4. Notably, our results indicate that, for the specific task of object recognition, CLIP [35] outperforms LLaVA by a substantial margin, reinforcing our decision to employ CLIP for the discriminative task, which is the focus of our study. We ablate and detail the sensitivity of MMLMs to different prompting strategies in the supplementary, here we report only its best prompting strategy result. Meta Prompt. In Table 5 we ablate different components of our meta-prompt (outlined in Figure 3) and report the results on the EuroSAT dataset. We see that all the major components have a strong effect on the downstream perfor- mance. For example, if we do not populate the meta-prompt with the in-context demonstrations of example LLM queries for a dataset, the LLM fails to generate the task-specific queries from the first stage. Similarly, removing the metadata (description of datasets) from the in-context example and the resulting dataset of interest also results in a huge performance drop 55.6% \\u219242.0%. We also noticed that interestingly, providing the category names for the datasets in the meta prompt (for stage 1) as extra information did not improve the results, potentially hinting that LLM prefers more simple and succinct instructions. Altering Meta Prompting Stages. In Table 6 we report the results by altering our meta-prompting strategy in two distinct ways: By generating the category- level VLM prompts directly in one step, by incorporating the class name already in stage 1 of our MPVR, and populating the <class names> in the generated task-specific LLM queries from stage 1 (which resembles the prompt ensembling performed by CLIP [35]). The results indicate that our 2-stage approach per- forms better than altering it to a single stage, and even our generated prompts from stage 1 can offer a more robust zero-shot classifier than templates ensem- Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs 15 accuracy(%) std ViT-B/32 62.8 \\u00b10.05 ViT-B/16 66.7 \\u00b10.04 ViT-L/14 73.3 \\u00b10.03 Table 7: Top-1 mean accuracy (%) for CLIP and standard deviation for 10 random runs, for all datasets. 1 25 50 75 100 Data (%) 55 60 Mean Accuracy (%) CLIP Waffle MPVR Fig. 4: Top-1 mean accuracy (%) over DTD, EuroSat, Flowers, Resisc45, subsam- pling the VLM prompts sets. bling2, highlighting the visual diversity of our generated task-specific queries, which later effectively translates to the VLM prompts as well. Results Robustness and Scaling Analysis: In Table 7 we study the robustness of MPVR results by reporting the mean and variance with randomly sampling MPVR-generated VLM prompts 10 times for all 20 datasets. We observe that the variances are negligible w.r.t. the obtained gains (in Table 1). In Figure 4 we show the scaling potential by sampling more VLM category- and task-specific prompts. The results highlight that sampling an increasing number of generated VLM prompts significantly boosts performance showing promising scaling potential. 5 Conclusion We have presented meta-prompting for enhancing zero-shot visual recognition with LLMs, which essentially alleviates any human involvement in VLM prompt design for new tasks. Our MPVR generates task-specific category-level VLM prompts by only requiring minimal information about the downstream task of interest. MPVR first queries the LLM to generate different high-level queries letting it discover the diverse ways of querying itself to generate visually diverse category-level prompts. These prompts are ensembled to construct a robust zero- shot classifier, that achieves enhanced zero-shot classification on a diverse set of 20 datasets belonging to widely different domains. Furthermore, we also open- source the 2.5M category-level text descriptions dataset, harnessed from GPT and Mixtral, covering the breadth of the LLM knowledge of our visual world. This large-scale dataset can be employed in many exciting future work direc- tions, e.g., fine-tuning multi-modal language models for enhanced fine-grained visual classification, or constructing large-scale synthetic datasets via generative text-to-image models for VLM pre-training. References 1. Bossard, L., Guillaumin, M., Van Gool, L.: Food-101 \\u2013 Mining Discriminative Components with Random Forests. In: Proc. ECCV (2014) 10, 12 16 M. J. Mirza et al. 2. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Nee- lakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Win- ter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language Models are Few-Shot Learners. arXiv:2005.14165 (2020) 3, 5, 6, 8, 9, 10, 21 3. Chen, J., Li, D.Z.X.S.X., Zhang, Z.L.P., Xiong, R.K.V.C.Y., Elhoseiny, M.: MiniGPT-v2: Large Language Model as a Unified Interface for Vision-Language Multi-task Learning. arXiv preprint arXiv:2310.09478 (2023) 4 4. Chen, M., Du, J., Pasunuru, R., Mihaylov, T., Iyer, S., Stoyanov, V., Kozareva, Z.: Improving In-Context Few-Shot Learning via Self-Supervised Training. arXiv preprint arXiv:2205.01703 (2022) 5 5. Cheng, G., Han, J., Lu, X.: Remote Sensing Image Scene Classification: Benchmark and State of the Art. Proceedings of the IEEE 105(10), 1865\\u20131883 (2017) 10 6. Chiang, W.L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J.E., Stoica, I., Xing, E.P.: Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality (March 2023), https: //lmsys.org/blog/2023-03-30-vicuna/ 4 7. Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., , Vedaldi, A.: Describing Textures in the Wild. In: Proc. CVPR (2014) 10, 11, 19, 20, 23 8. Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.: InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. In: NeurIPS (2023) 4 9. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A large-scale hierarchical image database. In: Proc. CVPR (2009) 10, 19, 21 10. Fei-Fei, L., Fergus, R., Perona, P.: Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Cat- egories. In: Proc. CVPR (2004) 10 11. Helber, P., Bischke, B., Dengel, A., Borth, D.: EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification. In: Proc. IGARSS (2018) 10, 11, 14, 19, 24 12. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., Song, D., Steinhardt, J., Gilmer, J.: The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization. In: Proc. ICCV (2021) 10, 20, 23 13. Hou, Y., Dong, H., Wang, X., Li, B., Che, W.: MetaPrompting: Learning to Learn Better Prompts. arXiv preprint arXiv:2209.11486 (2022) 2 14. Huang, T., Chu, J., Wei, F.: Unsupervised Prompt Learning for Vision-Language Models. arXiv:2204.03649 (2022) 4 15. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q.V., Sung, Y., Li, Z., Duerig, T.: Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. In: Proc. ICML (2021) 4 16. Jiang, A.Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D.S., Casas, D.d.l., Hanna, E.B., Bressand, F., et al.: Mixtral of Experts. arXiv preprint arXiv:2401.04088 (2024) 3, 5, 8, 9, 10, 21 17. Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., Suleyman, M., Zisserman, A.: The Kinetics Human Action Video Dataset (2017) 10 18. Khattak, M.U., Rasheed, H., Maaz, M., Khan, S., Khan, F.S.: MaPLe: Multi-Modal Prompt Learning. In: Proc. CVPR (2023) 4 Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs 17 19. Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large Language Models are Zero-Shot Reasoners. NeurIPS 35, 22199\\u201322213 (2022) 6 20. Krause, J., Stark, M., Deng, J., Fei-Fei, L.: 3D Object Representations for Fine- Grained Categorization. In: Proc. ICCVW (2013) 10, 12 21. Krizhevsky, A., Hinton, G.: Learning Multiple Layers of Features from Tiny Images. Tech. rep., Department of Computer Science, University of Toronto (2009) 10, 19 22. Li, J., Li, D., Savarese, S., Hoi, S.: BLIP-2: Bootstrapping Language-Image Pre- training with Frozen Image Encoders and Large Language Models. arXiv preprint arXiv:2301.12597 (2023) 4 23. Lin, W., Karlinsky, L., Shvetsova, N., Possegger, H., Kozinski, M., Panda, R., Feris, R., Kuehne, H., Bischof, H.: MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge. In: Proc. ICCV (2023) 5 24. Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved Baselines with Visual Instruction Tun- ing. arXiv:2310.03744 (2023) 4, 13, 20 25. Liu, H., Li, C., Li, Y., Lee, Y.J.: LLaVA-Next (LLaVA 1.6). arXiv:2310.03744 (2023), https://llava-vl.github.io/blog/2024-01-30-llava-next/ 4, 20 26. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual Instruction Tuning. In: NeurIPS (2023) 4, 13 27. Maji, S., Rahtu, E., Kannala, J., Blaschko, M., Vedaldi, A.: Fine-Grained Visual Classification of Aircraft. arXiv preprint arXiv:1306.5151 (2013) 10 28. Menon, S., Vondrick, C.: Visual Classification via Description from Large Language Models. Proc. ICLR (2023) 2, 5, 6, 10, 12, 19 29. Min, S., Lewis, M., Zettlemoyer, L., Hajishirzi, H.: MetaICL: Learning to Learn In Context. arXiv preprint arXiv:2110.15943 (2021) 5 30. Mirza, M.J., Karlinsky, L., Lin, W., Possegger, H., Feris, R., Bischof, H.: TAP: Targeted Prompting for Task Adaptive Generation of Textual Training Instances for Visual Classification. arXiv preprint arXiv:2309.06809 (2023) 6 31. Mirza, M.J., Karlinsky, L., Lin, W., Possegger, H., Kozinski, M., Feris, R., Bischof, H.: LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and Unla- beled Image Collections. In: NeurIPS (2023) 4 32. Nilsback, M.E., Zisserman, A.: Automated Flower Classification Over a Large Number of Classes. In: Proc. ICVGIP (2008) 10, 12, 20, 23 33. Parkhi, O.M., Vedaldi, A., Zisserman, A., Jawahar, C.V.: Cats and dogs. In: Proc. CVPR. pp. 3498\\u20133505 (2012). https://doi.org/10.1109/CVPR.2012.6248092 10 34. Pratt, S., Liu, R., Farhadi, A.: What does a platypus look like? Generating cus- tomized prompts for zero-shot image classification. arXiv:2209.03320 (2022) 2, 5, 6, 10, 11, 19 35. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning Transferable Visual Models from Natural Language Supervision. In: Proc. ICML (2021) 1, 2, 4, 5, 6, 10, 11, 12, 13, 14, 20, 21, 26 36. Recht, B., Roelofs, R., Schmidt, L., Shankar, V.: Do ImageNet Classifiers Gener- alize to ImageNet? In: Proc. ICML. pp. 5389\\u20135400. PMLR (2019) 10, 19 37. Roth, K., Kim, J.M., Koepke, A., Vinyals, O., Schmid, C., Akata, Z.: Waffling around for Performance: Visual Classification with Random Words and Broad Concepts. arXiv preprint arXiv:2306.07282 (2023) 2, 5, 6, 10, 11, 12, 19, 26 38. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C.W., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kun- durthy, S.R., Crowson, K., Schmidt, L., Kaczmarczyk, R., Jitsev, J.: LAION-5b: 18 M. J. Mirza et al. An open large-scale dataset for training next generation image-text models. In: NeurIPS (2022) 4 39. Soomro, K., Zamir, A.R., Shah, M.: UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild. arXiv:1212.0402 (2012) 10 40. Suzgun, M., Kalai, A.T.: Meta-Prompting: Enhancing Language Models with Task- Agnostic Scaffolding. arXiv preprint arXiv:2401.12954 (2024) 2 41. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash- lykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint arXiv:2307.09288 (2023) 4 42. Wah, C., Branson, S., Welinder, P., Perona, P., Belongie, S.: The Caltech-UCSD Birds-200-2011 Dataset. Tech. rep., California Institute of Technology (2011) 10, 20 43. Wang, H., Ge, S., Lipton, Z., Xing, E.P.: Learning Robust Global Representations by Penalizing Local Predictive Power. In: NeurIPS (2019) 10 44. Wei, J., Bosma, M., Zhao, V.Y., Guu, K., Yu, A.W., Lester, B., Du, N., Dai, A.M., Le, Q.V.: Finetuned Language Models Are Zero-Shot Learners. arXiv preprint arXiv:2109.01652 (2021) 5 45. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al.: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS 35, 24824\\u201324837 (2022) 6 46. Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., Torralba, A.: SUN Database: Large- scale Scene Recognition from Abbey to Zoo. In: Proc. CVPR (2010) 10 47. Xu, H., Xie, S., Tan, X.E., Huang, P.Y., Howes, R., Sharma, V., Li, S.W., Ghosh, G., Zettlemoyer, L., Feichtenhofer, C.: Demystifying CLIP Data. In: Proc. ICLR (2023) 1, 4, 12, 21, 27, 28 48. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., Narasimhan, K.: Tree of Thoughts: Deliberate Problem Solving with Large Language Models. NeurIPS 36 (2023) 6 49. Zhao, Z., Wallace, E., Feng, S., Klein, D., Singh, S.: Calibrate Before Use: Improv- ing Few-Shot Performance of Language Models. In: Proc. ICML. pp. 12697\\u201312706. PMLR (2021) 5 50. Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A.: Places: A 10 million Image Database for Scene Recognition. IEEE TPAMI 40(6), 1452\\u20131464 (2017) 10 51. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Conditional Prompt Learning for Vision- Language Models. In: Proc. CVPR (2022) 4 52. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to Prompt for Vision-Language Models. IJCV (2022) 4, 10 53. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: MiniGPT-4: Enhancing Vision- Language Understanding with Advanced Large Language Models. arXiv preprint arXiv:2304.10592 (2023) 4 Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs 19 Supplementary Material As supplementary material for our MPVR: Meta Prompting for Visual Recogni- tion, we first list some additional implementation details (Section A). Then, for additional insights, we provide an ablation on the use of the in-context dataset employed for meta-prompting (Section B). Moving forward, we provide results with different strategies employed for prompting multimodal language models (MMLMs) for the task of object recognition (Section C), demonstrating we used the best performing available strategy for the MMLM baseline in the main pa- per. Then, we provide results for ensembling (in probability space) the vision language model (VLM) prompts generated through our MPVR (Section D). Later, we conclude with the detailed (dataset-wise) results (Section E). To encourage reproducibility for our MPVR, the code for all the 20 datasets is provided in the supplementary folder as mpvr_code.zip, with the anonymous link (in the readme.md) to also access the \\u223c2.5M category-level text dataset. This dataset will also be open-sourced for the community, upon acceptance. A Implementation Details All our experiments are performed on a single NVIDIA 3090 GPU. To obtain the results for the baselines, we use their official codebase and run the baselines lo- cally with all their recommended parameters and settings. For CUPL [34] we only report the results on the datasets, for which the authors provided the category- level VLM prompts. Since CUPL uses hand-crafted dataset-specific LLM queries to generate the category-level VLM prompts, for some datasets these queries are not available, so we were not able to generate the VLM prompts for those datasets. We used the category-level VLM attributes provided by DCLIP [28] in their official repository6. For the datasets, not listed in their repository, we used their official code to generate the attributes and used them for obtaining the Waf- fle [37] results, following the official publication. In contrast to CUPL [34], the attributes can be generated for any dataset, only by providing the class names from the downstream datasets. Similarly, following the official publication and settings proposed in Waffle [37], the datasets for which the high-level concepts are not available (i.e., ImageNet [9], ImageNetv2 [36], CIFAR10/100 [21]), their two variants, Waffle+Con and Waffle+Con+GPT, collapse to only the Waffle results, in all the tables. B Meta Prompt In the main manuscript, we arbitrarily employed the Describable Textures Dataset (DTD) [7] as the in-context example dataset for all our experiments. However, when the target dataset is DTD, we switched the in-context example dataset to EuroSAT [11]. Here, we studied the effect of employing different in-context 6 https://github.com/sachit-menon/classifybydescriptionrelease 20 M. J. Mirza et al. Numbered Options Alphabet Options List Option EuroSAT 41.3 38.7 34.4 Table 8: Top-1 accuracy (%) with different prompting strategies for LLAVA-1.6 [24]. datasets. For example, when employing an alternative in-context dataset, such as Flowers [32] or CUBS [42] for DTD (as the target dataset), the variance in results is only \\u00b10.71, considerably lower than the gains of 8.4% (50.8% vs. 42.4%) obtained over the baseline of CLIP + \\u2018dataset-specific templates\\u2019, for the ViT-B/32 backbone from CLIP [35]. Similarly, while using an alternative in-context dataset, Flowers or Cubs, for the target dataset EuroSAT, the variance in obtained results is only \\u00b10.44, again considerably lower than the gains of 9.8% (55.6% vs. 45.8%) obtained over the baseline of CLIP + \\u2018dataset-specific templates\\u2019. Furthermore, for completeness, we also provide 2 complete meta-prompt examples in Figure 5 while choosing different in-context demonstrators (i.e., DTD [7] and Flowers [32]) and target datasets (i.e., ImageNet-R [12] and DTD [7]). C Prompt Engineering for MMLM To address the sensitivity of MMLMs to different prompting strategies, we ex- tensively tested the following different prompting variations used for the task of category recognition for MMLMs. These prompting strategies are also illustrated in Figure 6 for the EuroSAT dataset. Categories as Numbered Options: The prompt to the MMLM [25] contained the categories (the model needed to choose from) listed as numbered options. Categories as Alphabet Options: The prompt to the MMLM [25] contained the categories (the model needed to choose from) listed as English alphabet options. Categories as List: In this prompting strategy, we provided the category names as a list and the MMLM was prompted to output the exact name of the category for each test image. In Table 8 we list the results for different prompting strategies and find that the best results were obtained when LLAVA-1.6 [25] was prompted with categories (to choose from) as numbered options. For the fairest comparison, the LLAVA-1.6 [25] baseline results reported in Table 4 of the main manuscript were obtained using this (top-performing) prompting option for all the tested datasets. D Ensembling Descriptions In the main manuscript (Table 3) we provide results by constructing the zero- shot classifier by ensembling the VLM prompts in two different ways: Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs 21 Ensemble in Embedding Space EuroSAT Flowers DTD Resisc Mean Top-1 (%) - ViT-B/32 55.6 73.9 50.8 64.0 61.1 Ensemble in Probability Space EuroSAT Flowers DTD Resisc Mean Top-1 (%) - ViT-B/32 54.5 73.0 51.0 61.3 60.0 Table 9: Comparison of constructing the zero-shot classifier by ensembling the GPT MPVR prompts over the embedding or probability space. Embedding Space: The zero-shot classifier is constructed as the mean of the embeddings (from the text encoder of CLIP [35]) from the different sources (e.g., Mixtral [16] or GPT [2]) VLM prompts). Probability Space: The zero-shot classifier is constructed as the mean of the probabilities (e.g., from softmax) obtained by different VLM prompt sources (e.g., Mixtral [16] or GPT [2]) for MPVR. In Table 3 (main manuscript) we observed different behaviors (in terms of the obtained results) from these two sources of ensembling. In theory, an ensemble over the probability space can also be obtained for the individual category- specific VLM prompts (from stage 2) of the MPVR. However, for datasets with a larger number of classes (e.g., ImageNet [9] with 1000 classes), such an ensemble is prohibitively expensive (as also noted in [35]). Nevertheless, for completeness, in Table 9, we provide results for the two ensembling methods for datasets with a smaller number of classes. From these results, we observe that the two different ensembling methods do not result in a huge deviation in performance. Note, to obtain all the MPVR results in all our experiments reported in the main paper, we always construct the zero-shot classifier as the mean of the embeddings from the VLM prompts for each category. E Detailed Results For completeness, here we provide dataset-wise results for two experiments in the main manuscript: ensembling different text sources (Table 3) and mean results (over 20 datasets) for different backbones, listed in Table 2. To this end, in Table 10 and Table 11 we provide the dataset-wise results by mixing different text sources and ensembling these sources either in the embedding space or the probability space. In Tables 12 & 13 we provide the dataset-wise results for ViT- B/16 and ViT-L/14 from CLIP [35]. Furthermore, in Tables 14, 15 & 16 we list the detailed results for 3 different backbones (ViT-B/32, ViT-B/16 and ViT- L/14) from MetaCLIP [47]. The detailed (dataset-wise) results also highlight that our MPVR performs favorably on most datasets when compared to the state-of-the-art methods. 22 M. J. Mirza et al. ViT-B/32 ImageNet ImageNetv2 C10 C100 Caltech101 Flowers Stanford Cars Cubs Pets DTD GPT+TEMP 45.9 40.2 87.6 59.0 88.1 72.6 40.8 54.6 86.2 48.3 MIXTRAL+TEMP 46.7 41.2 87.6 59.0 88.8 74.4 40.0 55.4 86.5 47.5 GPT+MIXTRAL 64.9 57.4 89.8 66.0 92.8 76.0 59.2 55.8 88.6 51.1 GPT+MIXTRAL+TEMP 46.1 40.8 86.7 60.7 89.9 76.0 40.7 55.7 87.9 49.2 Food101 Aircraft Places365 SUN397 UCF101 K400 IN-R IN-S EuroSAT Resisc45 GPT+TEMP 80.1 20.9 42.3 66.1 62.8 36.6 58.8 29.3 56.6 62.2 MIXTRAL+TEMP 80.8 22.2 41.9 66.0 59.8 35.7 61.1 15.5 50.6 61.6 GPT+MIXTRAL 81.1 22.3 42.7 67.1 67.1 43.4 70.3 44.0 56.4 65.0 GPT+MIXTRAL+TEMP 80.2 21.1 42.6 66.2 63.0 37.7 61.9 29.5 55.3 63.3 ViT-B/16 ImageNet ImageNetv2 C10 C100 Caltech101 Flowers Stanford Cars Cubs Pets DTD GPT+TEMP 49.9 44.8 84.6 63.7 89.2 76.1 45.8 58.6 87.6 53.2 MIXTRAL+TEMP 50.5 45.5 86.9 62.6 89.8 78.3 44.5 59.7 89.6 50.0 GPT+MIXTRAL 69.7 63.4 91.2 69.6 94.4 79.2 65.6 59.8 90.7 55.4 GPT+MIXTRAL+TEMP 50.2 45.1 85.8 64.3 91.5 78.9 46.4 60.1 90.1 53.4 Food101 Aircraft Places365 SUN397 UCF101 K400 IN-R IN-S EuroSAT Resisc45 GPT+TEMP 85.8 27.8 43.1 67.9 67.0 40.8 64.2 36.0 58.5 65.4 MIXTRAL+TEMP 86.2 29.3 42.7 68.2 65.3 39.8 66.4 17.1 55.0 63.8 GPT+MIXTRAL 86.5 28.3 43.6 68.8 70.1 48.0 78.4 50.6 60.2 67.2 GPT+MIXTRAL+TEMP 86.0 28.1 43.6 68.1 67.4 42.1 67.8 36.0 59.1 66.1 ViT-L/14 ImageNet ImageNetv2 C10 C100 Caltech101 Flowers Stanford Cars Cubs Pets DTD GPT+TEMP 62.2 57.3 90.1 75.5 94.3 81.9 58.9 64.8 92.9 62.0 MIXTRAL+TEMP 62.4 56.9 93.3 76.7 93.6 82.0 54.9 66.6 92.2 60.4 GPT+MIXTRAL 76.9 71.0 96.2 79.4 95.5 83.9 78.1 67.3 93.8 62.9 GPT+MIXTRAL+TEMP 62.4 57.3 91.9 76.8 94.7 82.5 59.2 67.2 93.2 62.8 Food101 Aircraft Places365 SUN397 UCF101 K400 IN-R IN-S EuroSAT Resisc45 GPT+TEMP 91.2 33.1 43.4 72.6 73.2 50.7 81.4 49.5 67.2 70.0 MIXTRAL+TEMP 91.3 36.1 42.7 72.3 73.7 49.8 82.9 28.2 60.4 69.1 GPT+MIXTRAL 91.6 34.3 43.8 72.9 77.4 55.5 88.6 61.2 65.6 71.5 GPT+MIXTRAL+TEMP 91.4 33.5 43.8 72.7 74.4 51.7 83.5 49.4 65.4 70.2 Table 10: Top-1 accuracy (%) while ensembling different text sources in the embedding space. Here, the zero-shot classifier is constructed by taking the mean of the embeddings from the different individual text sources. Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs 23 You are provided with prompt template examples for a dataset, which are provided to the LLM to generate descriptions for the categories in these datasets. Your task is to generate 30 diverse prompts for another dataset for which you are also provided the dataset name and the description. Format it correctly for use in a Python script, and do not repeat the prompts. Example Dataset Name: Describable Textures Dataset (DTD) Description: The Describable Textures Dataset (DTD) is an evolving collection of textural images in the wild, annotated with a series of human-centric attributes, inspired by the perceptual properties of textures. This data is made available to the computer vision community for research purposes. Prompts: prompts.append(\\\"Describe how does the \\\" + category + \\\" texture looks like.\\\") prompts.append(\\\"How can you recognize the texture of \\\" + category + \\\"?\\\") prompts.append(\\\"What does the texture of \\\" + category + \\\" look like?\\\") prompts.append(\\\"Describe an image from the internet of the \\\" + category + \\\" texture.\\\") prompts.append(\\\"How can you identify the texture of \\\" + category + \\\"?\\\") Dataset Name: ImageNet-R(endition) Description: ImageNet-R(endition) contains art, cartoons, deviantart, graffiti, embroidery, graphics, origami, paintings, patterns, plastic objects, plush objects, sculptures, sketches, tattoos, toys, and video game renditions of ImageNet classes. Prompts: prompts.append(\\\"Describe the artistic representation of the \\\" + category + \\\".\\\") prompts.append(\\\"How would you visually recognize the \\\" + category + \\\" class in art or cartoons?\\\") prompts.append(\\\"Provide a detailed description of the graffiti or street art related to \\\" + category + \\\".\\\") prompts.append(\\\"What are the distinctive visual features of the embroidery depicting the \\\" + category + \\\"?\\\") prompts.append(\\\"Describe the graphics that represent the visual essence of the \\\" + category + \\\" class.\\\") prompts.append(\\\"Illustrate the origami models inspired by the \\\" + category + \\\".\\\") prompts.append(\\\"Explain the characteristics of paintings that portray the \\\" + category + \\\".\\\") prompts.append(\\\"Detail the visual patterns associated with the \\\" + category + \\\" class.\\\") prompts.append(\\\"Describe the visual appearance of plastic objects related to the \\\" + category + \\\".\\\") You are provided with prompt template examples for a dataset, which are provided to the LLM to generate descriptions for the categories in these datasets. Your task is to generate 30 diverse prompts for another dataset for which you are also provided the dataset name and the description. Format it correctly for use in a Python script, and do not repeat the prompts. Example Dataset Name: Oxford Flowers Dataset Description: Oxford Flowers consists of 102 flower categories. The flowers chosen to be flowers commonly occur in the United Kingdom. Prompts: prompts.append(\\\"Describe how does the flower type \\\" + category + \\\" looks like.\\\") prompts.append(\\\"How can you recognize the flower type \\\" + category + \\\"?\\\") prompts.append(\\\"What does the flower type \\\" + category + \\\" look like?\\\") prompts.append(\\\"Describe an image from the internet of the flower type \\\" + category + \\\".\\\") prompts.append(\\\"How can you identify the flower type of \\\" + category + \\\"?\\\") Dataset Name: Describable Textures Dataset (DTD) Description: The Describable Textures Dataset (DTD) is an evolving collection of textural images in the wild, annotated with a series of human-centric attributes, inspired by the perceptual properties of textures. This data is made available to the computer vision community for research purposes. Prompts: prompts.append(\\\"Describe the visual characteristics of the texture labeled as \\\" + category + \\\".\\\") prompts.append(\\\"How would you recognize the texture labeled as \\\" + category + \\\"?\\\") prompts.append(\\\"What are the key features of the texture labeled as \\\" + category + \\\"?\\\") prompts.append(\\\"Provide a detailed description of the appearance of the texture labeled as \\\" + category + \\\".\\\") prompts.append(\\\"If you see an image with the texture labeled as \\\" + category + \\\", what would stand out to you?\\\") prompts.append(\\\"Imagine you encounter a surface with the texture labeled as \\\" + category + \\\". How would you describe it?\\\") prompts.append(\\\"What visual attributes define the texture category \\\" + category + \\\"?\\\") prompts.append(\\\"Describe an image featuring the texture labeled as \\\" + category + \\\".\\\") prompts.append(\\\"Create a caption for an image showcasing the texture labeled as \\\" + category + \\\".\\\") Incontext Dataset: DTD | Target Dataset: ImageNet-R Incontext Dataset: Flowers-102 | Target Dataset: DTD Fig. 5: Exemplary meta-prompts (and a few LLM generated responses) for MPVR using different in-context (left: DTD [7], right: Flowers [32]) and target (left: ImageNet- R [12], right: DTD [7]) datasets. 24 M. J. Mirza et al. Identify the category of this satellite image from the following options: 0. Annual Crop Land 1. Forest 2. Herbaceous Vegetation Land 3. Highway or Road 4. Industrial Buildings 5. Pasture Land 6. Permanent Crop Land 7. Residential Buildings 8. River 9. Sea or Lake Answer with the option's number from the given choices directly. Identify the category of this satellite image from the following options: A. Annual Crop Land B. Forest C. Herbaceous Vegetation Land D. Highway or Road E. Industrial Buildings F. Pasture Land G. Permanent Crop Land H. Residential Buildings I. River J. Sea or Lake Answer with the option's letter from the given choices directly Categories as Numbers Categories as List Categories as English Alphabets Identify the category of this satellite image from the following list: [Annual Crop Land, Forest, Herbaceous Vegetation Land, Highway or Road, Industrial Buildings, Pasture Land, Permanent Crop Land, Residential Buildings, River, Sea or Lake] Answer with the exact category name from the given list of categories. Fig. 6: Example of different prompting options explored for LLAVA for EuroSAT [11]. Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs 25 ViT-B/32 ImageNet ImageNetv2 C10 C100 Caltech101 Flowers Stanford Cars Cubs Pets DTD GPT+TEMP 64.8 57.2 89.9 66.3 92.7 73.7 59.4 55.8 88.3 51.3 MIXTRAL+TEMP 63.8 56.3 89.5 65.5 93.0 75.2 58.2 55.5 88.4 50.3 GPT+MIXTRAL 62.9 55.9 89.7 66.1 92.8 76.1 52.4 55.9 88.8 51.2 GPT+MIXTRAL+TEMP 62.9 55.8 89.8 66.1 92.9 76.2 52.4 55.8 89.0 51.4 Food101 Aircraft Places365 SUN397 UCF101 K400 IN-R IN-S EuroSAT Resisc45 GPT+TEMP 81.1 21.9 42.1 67.0 68.0 43.7 70.1 44.2 55.0 63.9 MIXTRAL+TEMP 81.2 22.3 42.1 66.5 66.4 42.2 70.1 42.7 53.3 64.5 GPT+MIXTRAL 79.2 22.2 42.6 66.2 67.2 43.4 70.3 43.8 56.4 65.0 GPT+MIXTRAL+TEMP 79.2 22.3 42.7 66.2 67.2 43.4 70.3 43.9 56.1 65.0 ViT-B/16 ImageNet ImageNetv2 C10 C100 Caltech101 Flowers Stanford Cars Cubs Pets DTD GPT+TEMP 69.8 63.2 90.8 69.7 94.0 76.8 65.4 58.8 89.8 56.6 MIXTRAL+TEMP 68.8 62.2 91.1 69.2 94.0 78.2 62.2 60.3 90.4 54.1 GPT+MIXTRAL 67.7 61.6 91.1 69.7 94.4 79.4 57.0 60.1 91.0 55.6 GPT+MIXTRAL+TEMP 67.7 61.5 91.2 69.8 94.4 79.7 57.0 60.1 91.1 55.9 Food101 Aircraft Places365 SUN397 UCF101 K400 IN-R IN-S EuroSAT Resisc45 GPT+TEMP 86.4 27.5 43.0 68.9 70.7 48.0 78.3 50.7 59.1 66.1 MIXTRAL+TEMP 86.6 29.8 42.6 68.7 68.8 46.9 78.4 49.7 59.0 66.7 GPT+MIXTRAL 84.7 28.1 43.5 68.4 70.0 48.0 78.4 50.8 60.3 67.1 GPT+MIXTRAL+TEMP 84.6 28.0 43.5 68.4 69.8 47.9 78.5 50.9 60.0 67.0 ViT-L/14 ImageNet ImageNetv2 C10 C100 Caltech101 Flowers Stanford Cars Cubs Pets DTD GPT+TEMP 76.8 70.9 95.8 79.1 96.2 83.7 78.4 65.2 93.6 62.9 MIXTRAL+TEMP 75.9 69.6 96.1 79.3 95.3 83.6 70.9 67.5 93.0 61.6 GPT+MIXTRAL 75.2 69.7 96.2 79.5 95.8 84.4 65.7 67.3 93.9 62.7 GPT+MIXTRAL+TEMP 75.2 69.7 96.2 79.5 95.7 84.3 65.8 67.4 93.9 62.9 Food101 Aircraft Places365 SUN397 UCF101 K400 IN-R IN-S EuroSAT Resisc45 GPT+TEMP 91.5 34.3 43.5 72.9 77.9 55.7 88.5 61.1 67.6 71.0 MIXTRAL+TEMP 91.4 37.5 42.5 72.4 75.9 54.6 88.6 60.0 61.9 71.1 GPT+MIXTRAL 90.6 35.6 43.8 72.1 77.5 55.6 88.6 61.1 65.2 71.7 GPT+MIXTRAL+TEMP 90.6 35.5 43.8 72.2 77.5 55.6 88.6 61.2 65.1 71.6 Table 11: Top-1 accuracy (%) while ensembling different text sources in the probability space. Here, the zero-shot classifier is constructed by taking the mean of the softmax probabilities from the different individual classifiers. 26 M. J. Mirza et al. ImageNet ImageNetv2 C10 C100 Caltech101 Flowers Stanford Cars Cubs Pets DTD S-TEMP 66.7 60.9 90.1 68.4 93.3 67.5 65.5 55.1 88.2 43.3 DS-TEMP 68.3 61.9 90.8 68.2 92.9 70.7 66.2 56.1 89.1 43.2 CUPL 69.7 63.4 90.3 69.0 94.4 70.9 60.0 56.0 91.2 53.3 D-CLIP 68.6 62.2 89.6 68.4 94.5 72.1 63.7 56.7 90.3 42.8 Waffle 68.3 62.3 90.8 68.8 93.7 72.2 64.0 57.0 89.2 41.9 Waffle+Con 68.3 62.3 90.8 68.8 90.7 69.0 63.9 56.5 89.4 42.7 Waffle+Con+GPT 68.3 62.3 90.8 68.8 94.4 72.3 63.8 56.8 89.7 42.8 MPVR (Mixtral) 68.8 62.2 91.1 69.1 94.2 78.4 62.2 60.4 90.3 53.7 MPVR (GPT) 69.7 63.4 90.8 69.5 94.1 76.9 65.4 59.0 89.9 56.1 Food101 Aircraft Places365 SUN397 UCF101 K400 IN-R IN-S EuroSAT Resisc45 S-TEMP 85.2 23.8 39.3 62.5 65.1 43.7 74.0 46.2 42.3 56.5 DS-TEMP 85.9 24.3 40.9 65.3 68.5 47.4 77.7 48.8 48.9 60.1 CUPL 86.1 26.6 _ 69.0 68.9 46.0 _ _ _ 66.2 D-CLIP 86.1 24.0 42.0 66.1 67.5 45.2 76.5 48.9 58.5 64.8 Waffle 86.9 24.9 42.0 65.4 67.1 46.1 77.0 49.1 49.6 64.8 Waffle+Con 86.5 24.2 39.8 62.9 66.5 45.1 76.3 48.2 48.1 61.7 Waffle+Con+GPT 86.7 24.9 42.4 66.4 68.4 46.0 77.0 49.5 55.6 65.2 MPVR (Mixtral) 86.6 29.9 42.7 68.9 68.9 46.9 78.4 49.7 59.2 66.7 MPVR (GPT) 86.4 28.0 43.1 68.8 70.9 48.0 78.2 50.6 59.6 66.2 Table 12: Top-1 accuracy (%) for 20 datasets obtained by employing the ViT-B/16 backbone from OpenAI CLIP [35]. S-TEMP refers to the results obtained by using the default template (a photo of a <class name>), while DS-TEMP refers to the results obtained by using the ensemble of dataset-specific prompts. An empty placeholder for CUPL [34] indicates that the respective baseline did not provide the handcrafted prompts for the dataset. For Waffle [37], mean results from 7 random runs are reported, following the original publication. ImageNet ImageNetv2 C10 C100 Caltech101 Flowers Stanford Cars Cubs Pets DTD S-TEMP 73.5 67.8 95.2 77.2 94.3 76.2 76.9 62.1 93.1 52.5 DS-TEMP 75.5 69.9 95.7 78.3 93.7 79.5 78.1 61.8 93.5 54.8 CUPL 76.7 70.8 95.8 78.6 96.1 79.6 64.2 60.3 94.3 61.1 D-CLIP 75.1 69.0 95.2 78.4 97.0 79.5 75.1 61.7 93.0 56.1 Waffle 75.1 68.9 96.0 78.4 96.2 78.3 76.5 62.3 93.2 55.3 Waffle+Con 75.1 68.9 96.0 78.4 93.9 77.3 76.7 63.1 93.4 53.7 Waffle+Con+GPT 75.1 68.9 96.0 78.4 96.9 79.0 75.9 62.0 93.1 56.1 MPVR (Mixtral) 75.9 69.6 96.1 79.3 95.4 83.8 70.6 67.7 93.1 61.6 MPVR (GPT) 76.8 70.9 96.0 79.2 96.1 83.6 78.3 65.5 93.7 62.9 Food101 Aircraft Places365 SUN397 UCF101 K400 IN-R IN-S EuroSAT Resisc45 S-TEMP 90.3 30.0 40.1 67.6 73.8 51.3 85.4 58.3 55.1 63.2 DS-TEMP 90.9 31.8 41.2 69.0 76.2 55.0 87.8 59.8 63.2 68.0 CUPL 91.4 35.1 _ 72.8 75.8 54.4 _ _ _ 71.8 D-CLIP 91.1 31.8 42.3 69.6 76.2 52.5 86.8 59.0 54.6 70.7 Waffle 91.5 32.5 42.6 69.4 76.0 53.4 87.4 59.1 50.4 71.4 Waffle+Con 91.2 31.3 41.1 66.2 74.2 52.0 86.2 58.6 44.2 66.7 Waffle+Con+GPT 91.4 32.1 42.9 70.1 76.4 53.5 87.3 59.3 53.7 71.1 MPVR (Mixtral) 91.4 37.6 42.5 72.5 75.8 54.6 88.5 60.0 62.2 71.2 MPVR (GPT) 91.5 34.4 43.5 73.0 78.1 55.7 88.4 61.0 67.3 71.1 Table 13: Top-1 accuracy (%) for 20 datasets obtained by employing the ViT-L/14 backbone from OpenAI CLIP [35]. Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs 27 ImageNet ImageNetv2 C10 C100 Caltech101 Flowers Stanford Cars Cubs Pets DTD S-TEMP 64.1 56.3 91.2 66.8 95.5 69.8 71.7 61.8 86.9 47.6 DS-TEMP 65.6 57.5 91.3 70.2 93.8 71.3 72.1 62.5 88.7 51.8 CUPL 66.0 57.5 90.3 68.4 95.5 68.3 61.3 61.5 88.5 58.4 D-CLIP 64.0 55.0 90.9 68.4 94.9 67.6 66.6 62.1 87.9 50.0 Waffle 63.5 55.5 90.9 67.2 93.9 69.7 68.8 61.7 88.6 50.0 Waffle+Con 63.5 55.5 90.9 67.2 88.2 68.6 69.1 61.8 88.7 48.5 Waffle+Con+GPT 63.5 55.5 90.9 67.2 94.8 68.7 68.1 62.1 88.1 51.0 MPVR (Mixtral) 64.8 57.4 91.2 68.9 94.3 78.4 68.3 65.2 88.1 61.5 MPVR (GPT) 66.0 57.6 91.4 69.2 94.5 74.8 71.2 64.6 88.0 61.5 Food101 Aircraft Places365 SUN397 UCF101 K400 IN-R IN-S EuroSAT Resisc45 S-TEMP 76.7 24.3 39.6 64.8 64.4 36.9 71.5 52.3 49.4 56.4 DS-TEMP 77.3 26.9 40.1 65.3 66.1 39.1 74.8 53.9 50.4 60.6 CUPL 77.0 32.3 _ 67.7 64.2 39.3 _ _ _ 63.9 D-CLIP 76.7 25.3 42.0 64.3 65.6 37.6 73.2 52.3 49.0 62.4 Waffle 77.2 26.0 42.1 65.8 64.1 38.1 73.9 52.9 42.3 64.4 Waffle+Con 77.1 25.4 41.4 66.0 63.5 37.4 72.8 52.8 37.8 63.6 Waffle+Con+GPT 77.2 25.7 42.4 65.6 65.8 38.4 73.8 52.9 46.7 63.4 MPVR (Mixtral) 77.0 35.4 41.4 67.3 65.4 39.9 75.7 53.1 56.3 61.4 MPVR (GPT) 77.1 31.8 42.4 65.8 67.3 40.6 75.6 54.1 58.7 63.6 Table 14: Top-1 accuracy (%) for 20 datasets obtained by employing the ViT-B/32 backbone from MetaCLIP [47]. ImageNet ImageNetv2 C10 C100 Caltech101 Flowers Stanford Cars Cubs Pets DTD S-TEMP 70.0 61.8 89.9 64.9 95.7 71.7 74.7 69.5 88.5 53.0 DS-TEMP 70.8 62.6 90.1 66.5 95.6 73.8 75.8 69.7 90.5 56.3 CUPL 70.9 62.5 89.2 65.5 95.5 70.8 0.5 68.9 89.8 62.2 D-CLIP 69.0 60.7 88.6 64.6 95.7 72.7 71.9 68.4 90.1 53.5 Waffle 69.1 61.0 87.9 64.9 95.0 73.3 73.1 68.2 90.8 53.5 Waffle+Con 69.1 61.0 87.9 64.9 94.1 72.1 72.3 68.5 91.0 52.5 Waffle+Con+GPT 69.1 61.0 87.9 64.9 95.8 72.9 73.0 68.1 90.7 55.1 MPVR (Mixtral) 69.8 62.0 89.8 65.6 95.5 80.6 74.0 71.2 90.4 64.1 MPVR (GPT) 71.2 62.9 89.8 66.6 94.8 75.9 75.9 71.4 89.9 64.4 Food101 Aircraft Places365 SUN397 UCF101 K400 IN-R IN-S EuroSAT Resisc45 S-TEMP 83.8 26.3 41.6 68.8 67.0 39.9 80.1 56.5 50.9 63.5 DS-TEMP 84.1 28.3 41.7 68.4 69.0 43.2 81.8 58.7 55.2 63.9 CUPL 84.0 34.4 _ 69.5 66.6 43.3 _ _ _ 67.0 D-CLIP 83.7 30.1 42.5 66.8 67.2 41.6 79.5 57.1 56.1 67.3 Waffle 83.9 30.5 42.3 67.7 68.4 42.4 80.0 56.9 53.3 67.8 Waffle+Con 83.9 30.2 41.5 68.2 66.3 41.7 79.4 57.5 49.7 67.8 Waffle+Con+GPT 84.0 30.4 42.8 68.0 68.5 42.4 80.1 57.2 55.9 68.3 MPVR (Mixtral) 84.0 37.8 41.4 69.4 67.9 44.2 82.2 57.2 59.7 67.0 MPVR (GPT) 83.6 34.0 43.0 69.4 68.8 44.9 82.1 58.2 57.8 69.1 Table 15: Top-1 accuracy (%) for 20 datasets obtained by employing the ViT-B/16 backbone from MetaCLIP [47]. 28 M. J. Mirza et al. ImageNet ImageNetv2 C10 C100 Caltech101 Flowers Stanford Cars Cubs Pets DTD S-TEMP 75.1 68.5 94.9 74.4 96.8 76.7 84.5 76.0 88.7 58.8 DS-TEMP 76.2 69.9 95.7 77.4 96.3 77.4 84.9 75.2 93.7 60.5 CUPL 76.5 69.9 95.0 76.3 97.0 75.8 81.1 74.4 92.7 64.5 D-CLIP 74.4 67.8 95.7 75.9 97.0 76.7 82.9 74.7 93.0 58.0 Waffle 74.3 67.9 95.6 76.6 96.2 78.3 83.0 74.5 92.9 59.6 Waffle+Con 74.3 67.9 95.6 76.6 95.3 78.6 83.8 75.0 93.2 57.0 Waffle+Con+GPT 74.3 67.9 95.6 76.6 97.4 77.5 83.2 74.5 93.0 60.2 MPVR (Mixtral) 75.5 68.6 95.9 76.5 96.6 85.5 82.2 77.9 92.6 67.3 MPVR (GPT) 76.6 70.1 95.1 76.0 96.0 84.9 83.7 77.6 93.0 65.8 Food101 Aircraft Places365 SUN397 UCF101 K400 IN-R IN-S EuroSAT Resisc45 S-TEMP 88.6 35.6 42.2 72.1 75.2 48.6 87.7 63.9 49.7 61.6 DS-TEMP 88.5 40.0 42.0 72.0 75.9 51.0 88.9 65.3 56.8 69.1 CUPL 89.0 41.2 _ 71.9 75.0 51.1 _ _ _ 71.2 D-CLIP 88.4 39.5 43.5 71.1 75.9 49.5 87.7 64.2 61.3 67.9 Waffle 88.7 39.0 43.3 71.7 75.9 49.8 87.5 64.0 59.6 69.5 Waffle+Con 88.9 38.8 41.4 71.4 75.1 49.2 87.2 64.1 59.3 65.2 Waffle+Con+GPT 88.7 39.7 43.0 72.3 76.3 50.2 87.9 64.3 61.3 69.0 MPVR (Mixtral) 89.1 49.5 40.2 73.1 74.8 51.8 89.4 65.1 56.3 70.5 MPVR (GPT) 88.8 46.7 43.8 72.5 77.5 52.3 89.2 65.5 58.0 72.8 Table 16: Top-1 accuracy (%) for 20 datasets obtained by employing the ViT-L/14 backbone from MetaCLIP [47].\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe"},"text/html":["\n","  <div id=\"df-92599fb6-f90d-4d04-baf4-9e26f26d4f0a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>RS-DPO: A Hybrid Rejection Sampling and Direct...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Self-Play Fine-Tuning of Diffusion Models for ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>LAVE: LLM-Powered Agent Assistance and Languag...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>On the Safety Concerns of Deploying LLMs/VLMs ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>186</th>\n","      <td>IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARN...</td>\n","    </tr>\n","    <tr>\n","      <th>187</th>\n","      <td>Detoxifying Large Language Models via Knowledg...</td>\n","    </tr>\n","    <tr>\n","      <th>188</th>\n","      <td>Language Repository for Long Video Understandi...</td>\n","    </tr>\n","    <tr>\n","      <th>189</th>\n","      <td>MATHVERSE: Does Your Multi-modal LLM Truly See...</td>\n","    </tr>\n","    <tr>\n","      <th>190</th>\n","      <td>Adversarial Learning Meets Large Language Mode...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>191 rows × 1 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92599fb6-f90d-4d04-baf4-9e26f26d4f0a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-92599fb6-f90d-4d04-baf4-9e26f26d4f0a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-92599fb6-f90d-4d04-baf4-9e26f26d4f0a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-dfd9228e-352a-468d-96a8-488ce61655f3\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dfd9228e-352a-468d-96a8-488ce61655f3')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-dfd9228e-352a-468d-96a8-488ce61655f3 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["                                                  text\n","0                                                     \n","1    RS-DPO: A Hybrid Rejection Sampling and Direct...\n","2    Self-Play Fine-Tuning of Diffusion Models for ...\n","3    LAVE: LLM-Powered Agent Assistance and Languag...\n","4    On the Safety Concerns of Deploying LLMs/VLMs ...\n","..                                                 ...\n","186  IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARN...\n","187  Detoxifying Large Language Models via Knowledg...\n","188  Language Repository for Long Video Understandi...\n","189  MATHVERSE: Does Your Multi-modal LLM Truly See...\n","190  Adversarial Learning Meets Large Language Mode...\n","\n","[191 rows x 1 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","pd.DataFrame(dataset[\"train\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1711273242729,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"3M4yz8g3tBpf","outputId":"264f3ffc-a233-49e7-fa10-b50880cc0cce"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['text'],\n","    num_rows: 191\n","})\n"]}],"source":["from typing import Dict, List, Union, Callable\n","import pandas as pd\n","from datasets import load_dataset, Dataset\n","\n","def create_dataframe(\n","    dataset: pd.DataFrame,\n","    subset: str = \"train\",\n","    columns: List[str] = None,\n","    transformations: Dict[str, Callable] = None,\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Create a DataFrame from a given dataset with specified columns and transformations.\n","\n","    Args:\n","        dataset (Union[str, Dataset]): The dataset to load. It can be either a string representing the dataset name\n","                                       or a pre-loaded Dataset object.\n","        subset (str): The subset of the dataset to use (e.g., \"train\", \"test\", \"validation\"). Default is \"train\".\n","        columns (List[str]): The list of columns to include in the DataFrame. If None, all columns will be included.\n","        transformations (Dict[str, Callable]): A dictionary specifying the transformations to apply to each column.\n","                                               The keys are the column names, and the values are the transformation functions.\n","\n","    Returns:\n","        pd.DataFrame: The created DataFrame.\n","    \"\"\"\n","    # if isinstance(dataset, str):\n","    #     dataset = load_dataset(dataset)\n","\n","    if subset not in dataset:\n","        raise ValueError(f\"Subset '{subset}' not found in the dataset.\")\n","\n","    data = dataset[subset]\n","    print(data)\n","\n","    if columns is None:\n","        columns = data.column_names\n","\n","    return pd.DataFrame.from_dict(\n","        [\n","            {\n","                **{column: data[i][column] for column in columns},\n","                **{\n","                    \"z\": i,\n","                    \"source\": f\"dataset.column_names: {columns}\",\n","                    \"extract\": f\"dataset.num_rows: {i + 1}/{data.num_rows}\",\n","                    \"type\": \"scatter\",\n","                    \"symbol\": \"circle\",\n","                    \"size_col\": 4,\n","                },\n","                **{\n","                    column: transformations[column](data[i][column])\n","                    for column in transformations\n","                    if column in columns\n","                },\n","            }\n","            for i in range(100)\n","        ]\n","    )\n","\n","dataset_name =dataset\n","subset = \"train\"\n","columns_to_include =['text']\n","transformations = {\n","    \"text\": lambda x: x.upper(),\n","    \"text\": lambda x: x[:500] + \"...\",\n","}\n","\n","dataframe = create_dataframe(dataset_name, subset, columns_to_include, transformations)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":874},"executionInfo":{"elapsed":411,"status":"ok","timestamp":1711273254929,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"fhRNUOHstBmi","outputId":"3f53210b-aaba-429e-a2c7-89f5feca76c5"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"dataframe\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Design2Code: How Far Are We From Automating Front-End Engineering? Chenglei Si\\u2217 Stanford University clsi@stanford.edu Yanzhe Zhang\\u2217 Georgia Tech z_yanzhe@gatech.edu Zhengyuan Yang Microsoft Ruibo Liu Google DeepMind Diyi Yang Stanford University \\u2217Equal Contribution Project Page: https://salt-nlp.github.io/Design2Code/ Abstract Generative AI has made rapid advancements in recent years, achieving unprece- dented capabilities in multimodal understanding and code generation. This can enable a new pa...\",\n          \"AVI-Talking: Learning Audio-Visual Instructions for Expressive 3D Talking Face Generation Yasheng Sun1, Wenqing Chu2, Hang Zhou2, Kaisiyuan Wang3, Hideki Koike1 1Tokyo Institute of Technology 2Baidu Inc 3The University of Sydney {sun.y.aj@m, koike@c}.titech.ac.jp, {chuwenqing,zhouhang09,wangkaisiyuan}@baidu.com Abstract While considerable progress has been made in achieving accurate lip synchronization for 3D speech-driven talking face generation, the task of incorporating expressive facial deta...\",\n          \"SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code Ziniu Hu 12 Ahmet Iscen 2 Aashi Jain 2 Thomas Kipf 3 Yisong Yue 1 David A. Ross 2 Cordelia Schmid 2 Alireza Fathi 2 1California Institute of Technology 2Google Research 3Google DeepMind 1 Abstract This paper introduces SceneCraft, a Large Lan- guage Model (LLM) Agent converting text de- scriptions into Blender-executable Python scripts which render complex scenes with up to a hun- dred 3D assets. This process requires complex spa...\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"z\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"dataset.column_names: ['text']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"extract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"dataset.num_rows: 84/191\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"scatter\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"symbol\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"circle\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"size_col\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"dataframe"},"text/html":["\n","  <div id=\"df-a1ff184c-3e47-4144-bd83-2a70bac00fec\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>z</th>\n","      <th>source</th>\n","      <th>extract</th>\n","      <th>type</th>\n","      <th>symbol</th>\n","      <th>size_col</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>dataset.column_names: ['text']</td>\n","      <td>dataset.num_rows: 1/191</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>RS-DPO: A Hybrid Rejection Sampling and Direct...</td>\n","      <td>1</td>\n","      <td>dataset.column_names: ['text']</td>\n","      <td>dataset.num_rows: 2/191</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Self-Play Fine-Tuning of Diffusion Models for ...</td>\n","      <td>2</td>\n","      <td>dataset.column_names: ['text']</td>\n","      <td>dataset.num_rows: 3/191</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>LAVE: LLM-Powered Agent Assistance and Languag...</td>\n","      <td>3</td>\n","      <td>dataset.column_names: ['text']</td>\n","      <td>dataset.num_rows: 4/191</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>On the Safety Concerns of Deploying LLMs/VLMs ...</td>\n","      <td>4</td>\n","      <td>dataset.column_names: ['text']</td>\n","      <td>dataset.num_rows: 5/191</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>Debiasing Large Visual Language Models Yi-Fan ...</td>\n","      <td>95</td>\n","      <td>dataset.column_names: ['text']</td>\n","      <td>dataset.num_rows: 96/191</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>WILL GPT-4 RUN DOOM? Adrian de Wynter* ABSTRAC...</td>\n","      <td>96</td>\n","      <td>dataset.column_names: ['text']</td>\n","      <td>dataset.num_rows: 97/191</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>Beyond Finite Data: Towards Data-free Out-of-d...</td>\n","      <td>97</td>\n","      <td>dataset.column_names: ['text']</td>\n","      <td>dataset.num_rows: 98/191</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>DeepSeek-VL: Towards Real-World Vision-Languag...</td>\n","      <td>98</td>\n","      <td>dataset.column_names: ['text']</td>\n","      <td>dataset.num_rows: 99/191</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>Chaining text-to-image and large language mode...</td>\n","      <td>99</td>\n","      <td>dataset.column_names: ['text']</td>\n","      <td>dataset.num_rows: 100/191</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 7 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a1ff184c-3e47-4144-bd83-2a70bac00fec')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a1ff184c-3e47-4144-bd83-2a70bac00fec button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a1ff184c-3e47-4144-bd83-2a70bac00fec');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-4e5498e0-5f2a-476b-a03b-667925b8f0f6\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4e5498e0-5f2a-476b-a03b-667925b8f0f6')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-4e5498e0-5f2a-476b-a03b-667925b8f0f6 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_c0a97be4-8af2-4025-b849-a2c17c3b83d7\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dataframe')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_c0a97be4-8af2-4025-b849-a2c17c3b83d7 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('dataframe');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["                                                 text   z  \\\n","0                                                 ...   0   \n","1   RS-DPO: A Hybrid Rejection Sampling and Direct...   1   \n","2   Self-Play Fine-Tuning of Diffusion Models for ...   2   \n","3   LAVE: LLM-Powered Agent Assistance and Languag...   3   \n","4   On the Safety Concerns of Deploying LLMs/VLMs ...   4   \n","..                                                ...  ..   \n","95  Debiasing Large Visual Language Models Yi-Fan ...  95   \n","96  WILL GPT-4 RUN DOOM? Adrian de Wynter* ABSTRAC...  96   \n","97  Beyond Finite Data: Towards Data-free Out-of-d...  97   \n","98  DeepSeek-VL: Towards Real-World Vision-Languag...  98   \n","99  Chaining text-to-image and large language mode...  99   \n","\n","                            source                    extract     type  \\\n","0   dataset.column_names: ['text']    dataset.num_rows: 1/191  scatter   \n","1   dataset.column_names: ['text']    dataset.num_rows: 2/191  scatter   \n","2   dataset.column_names: ['text']    dataset.num_rows: 3/191  scatter   \n","3   dataset.column_names: ['text']    dataset.num_rows: 4/191  scatter   \n","4   dataset.column_names: ['text']    dataset.num_rows: 5/191  scatter   \n","..                             ...                        ...      ...   \n","95  dataset.column_names: ['text']   dataset.num_rows: 96/191  scatter   \n","96  dataset.column_names: ['text']   dataset.num_rows: 97/191  scatter   \n","97  dataset.column_names: ['text']   dataset.num_rows: 98/191  scatter   \n","98  dataset.column_names: ['text']   dataset.num_rows: 99/191  scatter   \n","99  dataset.column_names: ['text']  dataset.num_rows: 100/191  scatter   \n","\n","    symbol  size_col  \n","0   circle         4  \n","1   circle         4  \n","2   circle         4  \n","3   circle         4  \n","4   circle         4  \n","..     ...       ...  \n","95  circle         4  \n","96  circle         4  \n","97  circle         4  \n","98  circle         4  \n","99  circle         4  \n","\n","[100 rows x 7 columns]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"elapsed":2660,"status":"ok","timestamp":1711273619115,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"L7_xA7GCtBji","outputId":"900aa6f9-f069-430a-ba25-196aa13a86fa"},"outputs":[{"data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"ac6a098f-7a1c-4150-aeb9-0a9ff22c8eae\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ac6a098f-7a1c-4150-aeb9-0a9ff22c8eae\")) {                    Plotly.newPlot(                        \"ac6a098f-7a1c-4150-aeb9-0a9ff22c8eae\",                        [{\"alignmentgroup\":\"True\",\"bingroup\":\"x\",\"hovertemplate\":\"text=%{x}\\u003cbr\\u003ecount=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[\"...\",\"RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models Saeed Khaki Amazon sakhaki@amazon.com JinJin Li Amazon jinjinli@amazon.com Lan Ma Amazon mamlm@amazon.com Liu Yang Amazon liuyanga@amazon.com Prathap Ramachandra Amazon prathara@amazon.com Abstract Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. How- ever, proximal policy optimization (PPO) based RLHF...\",\"Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation Huizhuo Yuan∗† Zixiang Chen∗‡ Kaixuan Ji∗§ Quanquan Gu¶ Figure 1: We introduce SPIN-Diffusion, a self-play fine-tuning algorithm for diffusion models. The results are fine-tuned from Stable Diffusion v1.5 on the winner images of the Pick-a-Pic dataset. The prompts used for generating the above images are chosen from the Pick-a-Pic test set. The generated images demonstrate superior performance in terms of overall visual attra...\",\"LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing Bryan Wang∗ University of Toronto Toronto, ON, Canada bryanw@dgp.toronto.edu Yuliang Li Reality Labs Research, Meta Sunnyvale, CA, USA yuliangli@meta.com Zhaoyang Lv Reality Labs Research, Meta Sunnyvale, CA, USA zhaoyang@meta.com Haijun Xia University of California San Diego La Jolla, CA, USA haijunxia@ucsd.edu Yan Xu Reality Labs Research, Meta Redmond, WA, USA yanx@meta.com Raj Sodhi Reality Labs Research, Meta Red...\",\"On the Safety Concerns of Deploying LLMs\\u002fVLMs in Robotics: Highlighting the Risks and Vulnerabilities Xiyang Wu 1 Ruiqi Xian 1 Tianrui Guan 2 Jing Liang 2 Souradip Chakraborty 2 Fuxiao Liu 2 Brian Sadler 3 Dinesh Manocha 1 2 Amrit Singh Bedi 4 Abstract In this paper, we highlight the critical issues of robustness and safety associated with integrat- ing large language models (LLMs) and vision- language models (VLMs) into robotics applica- tions. Recent works have focused on using LLMs and VLMs t...\",\"Using Left and Right Brains Together: Towards Vision and Language Planning Jun Cen 1 2 3 * Chenfei Wu 2 * Xiao Liu 2 Shengming Yin 2 Yixuan Pei 4 Jinglong Yang 1 5 Qifeng Chen 3 † Nan Duan 2 † Jianguo Zhang 1 † LMM (a) Existing works (b) Ours Output Language Planning Vision Planning The bottle or phone. LLM Text Image \\u002f Video Which object would the person put down next? Language Plan Step 1: What is the person holding? Step 2: What actions has the person performed with the objects so far? Step 3...\",\"Preprint OPENFMNAV: TOWARDS OPEN-SET ZERO-SHOT OB- JECT NAVIGATION VIA VISION-LANGUAGE FOUNDA- TION MODELS Yuxuan Kuang1, Hai Lin2, Meng Jiang2 1Peking University 2University of Notre Dame ABSTRACT Object navigation (ObjectNav) requires an agent to navigate through unseen envi- ronments to find queried objects. Many previous methods attempted to solve this task by relying on supervised or reinforcement learning, where they are trained on limited household datasets with close-set objects. However...\",\"Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering David Romero MBZUAI david.mogrovejo@mbzuai.ac.ae Thamar Solorio MBZUAI thamar.solorio@mbzuai.ac.ae Abstract We present Q-ViD, a simple approach for video question answering (video QA), that un- like prior methods, which are based on com- plex architectures, computationally expensive pipelines or use closed models like GPTs, Q- ViD relies on a single instruction-aware open vision-language model (InstructBLIP) to tackle...\",\"PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter Junfei Xiao 1 2 Zheng Xu 2 Alan Yuille 1 Shen Yan† 2 Boyu Wang† 2 Abstract This paper demonstrates that a progressively aligned language model can effectively bridge frozen vision encoders and large language mod- els (LLMs). While the fundamental architecture and pre-training methods of vision encoders and LLMs have been extensively studied, the archi- tecture and training strategy of vision-language adap...\",\"VQAttack: Transferable Adversarial Attacks on Visual Question Answering via Pre-trained Models Ziyi Yin1, Muchao Ye1, Tianrong Zhang1, Jiaqi Wang1, Han Liu2 Jinghui Chen1, Ting Wang3, Fenglong Ma1* 1The Pennsylvania State University 2Dalian University of Technology 3Stony Brook University {ziyiyin, muchao, tbz5156, jcz5917, fenglong}@psu.edu liu.han.dut@gmail.com, twang@cs.stonybrook.edu Abstract Visual Question Answering (VQA) is a fundamental task in computer vision and natural language proces...\",\"CoLLaVO: Crayon Large Language and Vision mOdel Byung-Kwan Lee KAIST leebk@kaist.ac.kr Beomchan Park KAIST bpark0810@kaist.ac.kr Chae Won Kim KAIST chaewonkim@kaist.ac.kr Yong Man Ro* KAIST ymro@kaist.ac.kr Abstract The remarkable success of Large Language Models (LLMs) and instruction tuning drives the evolution of Vision Language Models (VLMs) towards a versatile general-purpose model. Yet, it remains unexplored whether cur- rent VLMs genuinely possess quality object- level image understanding...\",\"Aligning Modalities in Vision Large Language Models via Preference Fine-tuning Yiyang Zhou * 1 Chenhang Cui * 1 Rafael Rafailov 2 Chelsea Finn 2 Huaxiu Yao 1 Abstract Instruction-following Vision Large Language Models (VLLMs) have achieved significant progress recently on a variety of tasks. These ap- proaches merge strong pre-trained vision models and large language models (LLMs). Since these components are trained separately, the learned rep- resentations need to be aligned with joint training...\",\"Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning Long Qian 1 Juncheng Li 2 Yu Wu 3 Yaobo Ye 1 Hao Fei 2 Tat-Seng Chua 2 Yueting Zhuang 1 Siliang Tang 1 Abstract Large Language Models (LLMs) demonstrate remarkable proficiency in comprehending and handling text-based tasks. Many efforts are be- ing made to transfer these attributes to video modality, which are termed Video-LLMs. How- ever, existing Video-LLMs can only capture the coarse-grained semantics and are ...\",\"Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning Zhiyang Xu♠ Chao Feng♣ Rulin Shao♡ Trevor Ashby♠ Ying Shen♠ Di Jin♢ Yu Cheng♦ Qifan Wang♢ Lifu Huang♠ ♠Virginia Tech ♡University of Washington ♣University of Michigan ♢Amazon Inc. ♦Microsoft ♢Meta AI {zhiyangx,lifuh}@vt.edu Abstract Despite vision-language models’ (VLMs) re- markable capabilities as versatile visual assis- tants, two substantial challenges persist within the existing VLM frameworks: (1) lacking task diversity ...\",\"ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs o WARNING: This paper contains model outputs that may be considered offensive. Fengqing Jiang♣‡ Zhangchen Xu♣‡ Luyao Niu♣‡ Zhen Xiang♢ Bhaskar Ramasubramanian ♦ Bo Li ♠ Radha Poovendran♣ ♣University of Washington ♢UIUC ♦Western Washington University ♠University of Chicago {fqjiang,zxu9,luyaoniu,rp3}@uw.edu, zhen.xiang.lance@gmail.com, ramasub@wwu.edu, bol@uchicago.edu Abstract Safety is critical to the usage of large lan- guage mo...\",\"Modularized Networks for Few-shot Hateful Meme Detection Rui Cao ruicao.2020@phdcs.smu.edu.sg Singapore Management University Singapore, Singapore Roy Ka-Wei Lee roy_lee@sutd.edu.sg Singapore University of Design and Technology Singapore, Singapore Jing Jiang jingjiang@smu.edu.sg Singapore Management University Singapore, Singapore ABSTRACT In this paper, we address the challenge of detecting hateful memes in the low-resource setting where only a few labeled examples are available. Our approach ...\",\"NOTE: NOTABLE GENERATION OF PATIENT TEXT SUMMARIES THROUGH EFFICIENT APPROACH BASED ON DIRECT PREFERENCE OPTIMIZATION ∗ Imjin Ahn INMED DATA Yonsei University Seoul Republic of Korea seraphina311@inmed-data.com seraphina311@yonsei.ac.kr Hansle Gwon INMED DATA Yonsei University Seoul Republic of Korea khs1220@inmed-data.com qwehgf1231@yonsei.ac.kr Young-Hak Kim INMED DATA Asan Medical Center Seoul Republic of Korea yhkim@inmed-data.com mdyhkim@amc.seoul.kr Tae Joon Jun INMED DATA Asan Medical Cen...\",\"LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation Keyang Xuan, Li Yi, Fan Yang, Ruochen Wu, Yi R. Fung, Heng Ji University of Illinois Urbana-Champaign {keyangx3,liyi3,fy10,rw12,yifung2,hengji}@illinois.edu Abstract The rise of multimodal misinformation on so- cial platforms poses significant challenges for individuals and societies. Its increased credi- bility and broader impact compared to textual misinformation make detection complex, re- qu...\",\"LVCHAT: Facilitating Long Video Comprehension Yu Wang∗ UC San Diego yuw164@ucsd.edu Zeyuan Zhang∗ UC San Diego zez018@ucsd.edu Julian McAuley UC San Diego jmcauley@ucsd.edu Zexue He UC San Diego zehe@ucsd.edu Abstract Enabling large language models (LLMs) to read videos is vital for multimodal LLMs. Ex- isting works show promise on short videos whereas long video (longer than e.g. 1 minute) comprehension remains challenging. The ma- jor problem lies in the over-compression of videos, i.e., the e...\",\"Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion Ziyue Wang*,1, Chi Chen*,1, Yiqi Zhu1, Fuwen Luo1, Peng LiB2, Ming Yan3, Ji Zhang3, Fei HuangB3, Maosong Sun1, Yang Liu1,2 1Department of Computer Science and Technology, Tsinghua University, Beijing, China 2Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China 3Institute of Intelligent Computing, Alibaba Group Abstract With the bloom of Large Language Models (LLMs), Multimodal Large Lan...\",\"OpenMOSS AnyGPT: Unified Multimodal LLM with Discrete Sequence Mod- eling Jun Zhan1,∗ , Junqi Dai1,∗ , Jiasheng Ye1,∗ Yunhua Zhou1, Dong Zhang1, Zhigeng Liu1, Xin Zhang1 Ruibin Yuan2, Ge Zhang2, Linyang Li1, Hang Yan3, Jie Fu2 Tao Gui1, Tianxiang Sun1, Yugang Jiang1, Xipeng Qiu1,† 1 Fudan University 2 Multimodal Art Projection Research Community 3 Shanghai AI Laboratory {jzhan22, jqdai22, jsye23}@m.fudan.edu.cn xpqiu@fudan.edu.cn https:\\u002f\\u002fjunzhan2000.github.io\\u002fAnyGPT.github.io Abstract We introdu...\",\"Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships Sebastian Koch1,2,3 Narunas Vaskevicius1,2 Mirco Colosi2 Pedro Hermosilla4 Timo Ropinski3 1Bosch Center for Artificial Intelligence 2Robert Bosch Corporate Research 3University of Ulm 4 TU Vienna Abstract Current approaches for 3D scene graph prediction rely on labeled datasets to train models for a fixed set of known object classes and relationship categories. We present Open3DSG, an al...\",\"Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents 1,2Zengqing Wu†, 1Shuyuan Zheng‡, 2Qianying Liu, 3Xu Han, 4Brian Inhyuk Kwon†, 1Makoto Onizuka, 5Shaojie Tang, 6Run Peng, 1,7Chuan Xiao 1Osaka University, 2Kyoto University, 3Fordham University, 4University of California, Los Angeles, 5University of Texas at Dallas, 6University of Michigan, 7Nagoya University wuzengqing@outlook.com, zheng@ist.osaka-u.ac.jp, ying@nlp.ist.i.kyoto-u.ac.jp, xhan44@fordham.edu, briankwon42@g....\",\"Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data Naihao Deng* , Zhenjie Sun* , Ruiqi He , Aman Sikka , Yulong Chen , Lin Ma , Yue Zhang , Rada Mihalcea University of Michigan Westlake University {dnaihao, zjsun}@umich.edu Abstract In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six bench- marks for table-r...\",\"Your Vision-Language Model Itself Is a Strong Filter: Towards High-Quality Instruction Tuning with Data Selection Ruibo Chen, Yihan Wu, Lichang Chen, Guodong Liu, Qi He Tianyi Xiong,Chenxi Liu, Junfeng Guo, Heng Huang University of Maryland, College Park rbchen@umd.edu Abstract Data selection in instruction tuning emerges as a pivotal process for acquiring high-quality data and training instruction-following large lan- guage models (LLMs), but it is still a new and unexplored research area for v...\",\"Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering Junnan Dong1, Qinggang Zhang1, Huachi Zhou1 Daochen Zha2, Pai Zheng1 Xiao Huang1* 1The Hong Kong Polytechnic University, 2 Rice University {hanson.dong, qinggangg.zhang, huachi.zhou}@connect.polyu.hk daochen.zha@rice.edu; {pai.zheng, xiaohuang}@polyu.edu.hk Abstract Knowledge-based visual question answering (KVQA) has been extensively studied to an- swer visual questions with external knowledge, e...\",\"MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion Sen Li 1 Ruochen Wang 2 Cho-Jui Hsieh 2 Minhao Cheng 3 Tianyi Zhou 4 Abstract Existing text-to-image models still struggle to generate images of multiple objects, especially in handling their spatial positions, relative sizes, overlapping, and attribute bindings. In this paper, we develop a training-free Multimodal-LLM agent (MuLan) to address these challenges by progressive multi-object generation with planning and feedback cont...\",\"Model Composition for Multimodal Large Language Models Chi Chen*,1, Yiyang Du*,1, Zheng Fang1, Ziyue Wang1, Fuwen Luo1, Peng Li B,2, Ming Yan3, Ji Zhang3, Fei Huang3, Maosong Sun1, Yang LiuB,1,2 1Department of Computer Science and Technology, Tsinghua University, Beijing, China 2Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China 3Institute of Intelligent Computing, Alibaba Group Abstract Recent developments in Multimodal Large Lan- guage Models (MLLMs) have shown rapid...\",\"Slot-VLM: SlowFast Slots for Video-Language Modeling Jiaqi Xu * 1 Cuiling Lan 2 Wenxuan Xie 2 Xuejin Chen 1 Yan Lu 2 Abstract Video-Language Models (VLMs), powered by the advancements in Large Language Models (LLMs), are charting new frontiers in video under- standing. A pivotal challenge is the development of an efficient method to encapsulate video con- tent into a set of representative tokens to align with LLMs. In this work, we introduce Slot- VLM, a novel framework designed to generate sema...\",\"OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog Adnen Abdessaied, Manuel von Hochmeister*, Andreas Bulling University of Stuttgart, Bosch, University of Stuttgart Germany, Germany, Germany {adnen.abdessaied, andreas.bulling}@vis.uni-stuttgart.de manuel.vonhochmeister@de.bosch.com Abstract We present the Object Language Video Transformer (OLViT) – a novel model for video dialog operating over a multi-modal attention-based dialog state tracker. Existing v...\",\"How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts Yusu Qian, Haotian Zhang, Yinfei Yang, Zhe Gan Apple {yusuqian,haotian.zhang2,yinfeiy,zhe.gan}@apple.com Abstract The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particu- larly in the context of handling deceptive infor- mation in prompts, thus producing hallucinated responses under such conditions. To quanti- tatively assess this vulnerabil...\",\"LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs Yunxin Li1, Xinyu Chen1, Baotian Hu1∗ , Min Zhang1 1Harbin Institute of Technology, Shenzhen, China {hubaotian, zhangmin2021}@hit.edu.cn liyunxin987@163.com Abstract Long video understanding is a significant and ongoing challenge in the intersection of mul- timedia and artificial intelligence. Employ- ing large language models (LLMs) for com- prehending video becomes an emerging and promising meth...\",\"Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment Yunxin Li1, Xinyu Chen1, Baotian Hu1∗ , Haoyuan Shi1, Min Zhang1 1Harbin Institute of Technology, Shenzhen, China liyunxin987@163.com {hubaotian, zhangmin2021}@hit.edu.cn Abstract Evaluating and Rethinking the current land- scape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-tex...\",\"A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation Yunxin Li1, Baotian Hu1∗, Wenhan Luo2, Lin Ma3, Yuxin Ding1, and Min Zhang1 1 Harbin Institute of Technology, Shenzhen, China 2 Hong Kong University of Science and Technology, Hong Kong 3 Meituan, Beijing, China liyunxin987@163.com, hubaotian@hit.edu.cn, whluo.china@gmail.com Abstract In this paper, we propose a new setting for generating product descriptions from images, augmented by marketing keywords. It lev...\",\"HYBRID REASONING BASED ON LARGE LANGUAGE MODELS FOR AUTONOMOUS CAR DRIVING Mehdi Azarafza Department of Computer Science Univ. of Applied Science Hamm-Lippstadt Carl von Ossietzky University of Oldenburg Lippstadt, Germany mehdi.azarafza@hshl.de Mojtaba Nayyeri Institute For Artificial Intelligence University of Stuttgart Stuttgart, Germany mojtaba.nayyeri@ipvs.uni-stuttgart.de Charles Steinmetz Department of Computer Science Univ. of Applied Science Hamm-Lippstadt Carl von Ossietzky University ...\",\"EXACT-Net: EHR-guided lung tumor auto-segmentation for non-small cell lung cancer radiotherapy Hamed Hooshangnejad Johns Hokins University Baltimore, MD hamed@jhu.edu Xue Feng Carina Medical Lexington, KY xfeng@crainaai.com Gaofeng Huang Carina Medical Lexington, KY ghuang@crainaai.com Rui Zhang University of Minnesota Minneapolis, MN zhan1386@umn.edu Quan Chen Mayo Clinic Phoenix, AZ quanchen@gmail.com Kai Ding Johns Hopkins University Baltimore, MD kding1@jhmi.edu Abstract Lung cancer is a dev...\",\"DeiSAM: Segment Anything with Deictic Prompting Hikaru Shindo 1 Manuel Brack 1 2 Gopika Sudhakaran 1 3 Devendra Singh Dhami 3 4 Patrick Schramowski 1 2 3 5 Kristian Kersting 1 2 3 6 Abstract Large-scale, pre-trained neural networks have demonstrated strong capabilities in various tasks, including zero-shot image segmentation. To iden- tify concrete objects in complex scenes, humans instinctively rely on deictic descriptions in natural language, i.e., referring to something depending on the conte...\",\"On Large Visual Language Models for Medical Imaging Analysis: An Empirical Study Minh-Hao Van, Prateek Verma, Xintao Wu University of Arkansas Fayetteville, AR, USA {haovan, prateek, xintaowu}@uark.edu Abstract—Recently, large language models (LLMs) have taken the spotlight in natural language processing. Further, integrating LLMs with vision enables the users to explore emergent abilities with multimodal data. Visual language models (VLMs), such as LLaVA, Flamingo, or CLIP, have demonstrated im...\",\"Multimodal Healthcare AI: Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology Nur Yildirim∗ Carnegie Mellon University Pittsburgh, USA yildirim@cmu.edu Hannah Richardson Microsoft Health Futures Cambridge, UK Maria T. Wetscherek Cambridge University Hospitals Cambridge, UK Junaid Bajwa Microsoft Health Futures Cambridge, UK Joseph Jacob University College London and UCL Hospitals London, UK Mark A. Pinnock University College London London, UK Stephen Harris U...\",\"GenSERP: Large Language Models for Whole Page Presentation Zhenning Zhang ∗† University of Illinois at Urbana-Champaign zz45@illinois.edu Yunan Zhang* Microsoft yunanzhang@microsoft.com Suyu Ge* University of Illinois at Urbana-Champaign suyuge2@illinois.edu Guangwei Weng Microsoft guangweiweng@microsoft.com Mridu Narang Microsoft mriduna@microsoft.com Xia Song Microsoft xiaso@microsoft.com Saurabh Tiwary Microsoft satiwary@microsoft.com ABSTRACT The advent of large language models (LLMs) brings...\",\"From Keywords to Structured Summaries: Streamlining Scholarly Knowledge Access Mahsa Shamsabadi and Jennifer D’Souza[0000−0002−6616−9509] TIB Leibniz Information Centre for Science and Technology, Hannover, Germany {mahsa.shamsabasdi,jennifer.dsouza}@tib.eu Abstract. This short paper highlights the growing importance of in- formation retrieval (IR) engines in the scientific community, addressing the inefficiency of traditional keyword-based search engines due to the rising volume of publications...\",\"Visual Hallucinations of Multi-modal Large Language Models Wen Huang∗ Hongbin Liu∗ University of Science & Technology of China Duke University hw2000@mail.ustc.edu.cn hongbin.liu@duke.edu Minxin Guo Neil Zhenqiang Gong The University of Hong Kong Duke University xc666@connect.hku.hk neil.gong@duke.edu Abstract Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing im...\",\"Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images Zefeng Wang * 1 Zhen Han * 2 Shuo Chen 2 Fan Xue 1 Zifeng Ding 2 Xun Xiao 3 Volker Tresp 2 Philip Torr 4 Jindong Gu † 4 Abstract Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand images. How- ever, like traditional vision models, they are still vulnerable to adversarial images. Mean- while, Chain-of-Thought (CoT) reasoning has been widely explored on MLLMs, which not only improv...\",\"Large Multimodal Agents: A Survey Junlin Xie♣♡∗ Zhihong Chen♣♡∗Ruifei Zhang♣♡ Xiang Wan♣ Guanbin Li♠† ♡The Chinese University of Hong Kong, Shenzhen ♣Shenzhen Research Institute of Big Data, ♠Sun Yat-sen University {junlinxie,zhihongchen,ruifeizhang}@link.cuhk.edu.cn wanxiang@sribd.cn, liguanbin@mail.sysu.edu.cn Abstract Large language models (LLMs) have achieved superior performance in powering text-based AI agents, endowing them with decision-making and reasoning abilities akin to humans. Conc...\",\"Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing Jeong Hun Yeo∗, Seunghee Han∗, Minsu Kim, Yong Man Ro† Integrated Vision and Language Lab, KAIST, South Korea {sedne246,gkstmdgml211,ms.k,ymro}@kaist.ac.kr Abstract In visual speech processing, context modeling capability is one of the most important require- ments due to the ambiguous nature of lip move- ments. For example, homophenes, words that share identical lip movements but produ...\",\"arXiv:2402.15205v1 [cs.HC] 23 Feb 2024 Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing S. KERNAN FREIRE, Delft University of Technology, The Netherlands M.M.C. VAN MOL, Erasmuc MC, The Netherlands C. SCHOL, Erasmuc MC, The Netherlands E. OZCAN VIEIRA, Delft University of Technology, The Netherlands Intensive care unit (ICU) patients often develop new health-related problems in their long-term recovery. Health care professionals keeping a diary of a patient’s stay is ...\",\"AutoMMLab: Automatically Generating Deployable Models from Language Instructions for Computer Vision Tasks Zekang Yang1 Wang Zeng1 Sheng Jin2,1 \\u0000Chen Qian1 Ping Luo2,3 Wentao Liu1,3 \\u00001 SenseTime Research and Tetras.AI 2 The University of Hong Kong 3 Shanghai AI Laboratory {yangzekang, zengwang, jinsheng, qianchen, liuwentao}@tetras.ai pluo@cs.hku.hk Abstract Automated machine learning (AutoML) is a collection of techniques designed to automate the machine learning de- velopment process. While tr...\",\"Selective “Selective Prediction”: Reducing Unnecessary Abstention in Vision-Language Reasoning Tejas Srinivasan1,2 Jack Hessel3 Tanmay Gupta2 Bill Yuchen Lin2 Yejin Choi2,4 Jesse Thomason1 Khyathi Raghavi Chandu2 1University of Southern California 2Allen Institute for Artificial Intelligence 3Samaya AI 4University of Washington tejas.srinivasan@usc.edu, khyathic@allenai.org Abstract Prior work on selective prediction minimizes in- correct predictions from vision-language mod- els (VLMs) by allow...\",\"Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics Sadaf Ghaffari, Nikhil Krishnaswamy Colorado State University, Fort Collins, CO USA {sadafgh,nkrishna}@colostate.edu Abstract In this paper, we present an exploration of LLMs’ abilities to problem solve with physical reasoning in situated envi- ronments. We construct a simple simulated environment and demonstrate examples of where, in a zero-shot setting, both text and multimodal LLMs display atomic world knowledge about var...\",\"Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models Chaoya Jiang1, Wei Ye1∗, Mengfan Dong1, Hongrui Jia1, Haiyang Xu2, Ming Yan2, Ji Zhang2, Shikun Zhang1 1 National Engineering Research Center for Software Engineering, Peking University 2 DAMO Academy, Alibaba Group {jiangchaoya, wye, zhangsk}@pku.edu.cn, {shuofeng.xhy, fei.huang}@alibaba-inc.com Abstract Large Vision-Language Models (LVLMs) ex- hibit remarkable capabilities but struggle wi...\",\"LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper Daoyuan Wu1, Shuai Wang2, Yang Liu1, Ning Liu3 1Nanyang Technological University,2Hong Kong University of Science and Technology 3City University of Hong Kong {daoyuan.wu,yangliu}@ntu.edu.sg, shuaiw@cse.ust.hk, ninliu@cityu.edu.hk Abstract Jailbreaking is an emerging adversarial attack that bypasses the safety alignment deployed in off-the-shelf large language models (LLMs). A considerable amount of research ex...\",\"LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding Yuxuan Wang1,3 , Yueqian Wang2 , Pengfei Wu2 Jianxin Liang2, Dongyan Zhao2,3, Zilong Zheng1,3† 1 Beijing Institute for General Artificial Intelligence (BIGAI), Beijing, China 2 Wangxuan Institute of Computer Technology, Peking University, Beijing, China 3 National Key Laboratory of General Artificial Intelligence, Beijing, China {wangyuxuan1,zlzheng}@bigai.ai https:\\u002f\\u002fgithub.com\\u002fbigai-nlco\\u002fLSTP-Chat Abst...\",\"InstructEdit: Instruction-Based Knowledge Editing for Large Language Models Bozhong Tian♣∗, Siyuan Cheng♥∗, Xiaozhuan Liang♣∗, Ningyu Zhang♣†, Yi Hu♥, Kouying Xue♥, Yanjie Gou♥, Xi Chen♥, Huajun Chen♣†, ♣Zhejiang University ♥Tencent {tbozhong,liangxiaozhuan,zhangningyu}@zju.edu.cn https:\\u002f\\u002fzjunlp.github.io\\u002fproject\\u002fInstructEdit Abstract Knowledge editing for large language models can offer an efficient solution to alter a model’s behav- ior without negatively impacting the overall perfor- mance. H...\",\"AVI-Talking: Learning Audio-Visual Instructions for Expressive 3D Talking Face Generation Yasheng Sun1, Wenqing Chu2, Hang Zhou2, Kaisiyuan Wang3, Hideki Koike1 1Tokyo Institute of Technology 2Baidu Inc 3The University of Sydney {sun.y.aj@m, koike@c}.titech.ac.jp, {chuwenqing,zhouhang09,wangkaisiyuan}@baidu.com Abstract While considerable progress has been made in achieving accurate lip synchronization for 3D speech-driven talking face generation, the task of incorporating expressive facial deta...\",\"From Large Language Models and Optimization to Decision Optimization CoPilot: A Research Manifesto Segev Wasserkrug 1 Leonard Boussioux 2 3 4 Dick den Hertog 5 Farzaneh Mirzazadeh 6 Ilker Birbil 5 Jannis Kurtz 5 Donato Maragno 5 Abstract Significantly simplifying the creation of optimiza- tion models for real-world business problems has long been a major goal in applying mathematical optimization more widely to important business and societal decisions. The recent capabilities of Large Language ...\",\"Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models Jeonghwan Kim Heng Ji University of Illinois Urbana-Champaign {jk100, hengji}@illinois.edu Abstract Recent advances in instruction-tuned Large Vision-Language Models (LVLMs) have im- bued the models with the ability to generate high-level, image-grounded explanations with ease. While such capability is largely attributed to the rich world knowledge contained within the Large Language Models...\",\"Towards Open-ended Visual Quality Comparison Haoning Wu⋆1, Hanwei Zhu⋆2, Zicheng Zhang⋆3, Erli Zhang1 Chaofeng Chen1, Liang Liao1, Chunyi Li3, Annan Wang1, Wenxiu Sun4 Qiong Yan4, Xiaohong Liu3, Guangtao Zhai3, Shiqi Wang2, and Weisi Lin1 https:\\u002f\\u002fhuggingface.co\\u002fq-future\\u002fco-instruct MICBench (proposed, 2K MCQs) Multi-choice Questions (MCQ) on Comparing Multiple Images Which image has better quality? What makes the first image blurrier than the second image? A. Noise B. Out of focus C. Low light D...\",\"Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual Capabilities Without Richer Cross-Modal Projections Gaurav Verma♣, Minje Choi♣, Kartik Sharma♣, Jamelle Watson-Daniels♠, Sejoon Oh♣, Srijan Kumar♣ ♣Georgia Institute of Technology, ♠Harvard University {gverma, mchoi96, ksartik, soh337, srijan}@gatech.edu jwatsondaniels@g.harvard.edu Abstract Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable general- purpose conversations about images with the language modal...\",\"ON TROJAN SIGNATURES IN LARGE LANGUAGE MODELS OF CODE Aftab Hussain, Md Rafiqul Islam Rabin, Mohammad Amin Alipour University of Houston, TX, USA ABSTRACT Trojan signatures, as described by Fields et al. [1], are noticeable differences in the distribution of the trojaned class parameters (weights) and the non-trojaned class parameters of the trojaned model, that can be used to detect the trojaned model. Fields et al. [1] found trojan signatures in computer vision classification tasks with image ...\",\"Massive Activations in Large Language Models Mingjie Sun1 Xinlei Chen2 J. Zico Kolter1,3 Zhuang Liu2 1Carnegie Mellon University 2Meta AI Research 3Bosch Center for AI Summer is warm . Winter is cold . \\\\n 1415 2533 0 1k 2k Summer is warm \\\\n Winter is cold . \\\\n 1415 2533 0 1k 2k Why is summer warm and winter cold ? 1415 2533 0 1k 2k It is the best of times . \\\\n 1415 2533 0 1k 2k Figure 1: Activation Magnitudes (z-axis) in LLaMA2-7B. x and y axes are sequence and feature dimensions. For this speci...\",\"ShapeLLM: Universal 3D Object Understanding for Embodied Interaction Zekun Qi1¶ Runpei Dong1¶𭟋 Shaochen Zhang1 Haoran Geng2 Chunrui Han3 Zheng Ge3 He Wang2 Li Yi4† Kaisheng Ma4† ¶ Internship at MEGVII 𭟋Project lead † Corresponding authors 1 Xi’an Jiaotong University 2 Peking University 3 MEGVII Technology 4 Tsinghua University https:\\u002f\\u002fqizekun.github.io\\u002fShapeLLM What kind of animal is this like? And what color is it? The figure resembles a bull or a buffalo. Its primary color is blue. What kind o...\",\"A Surprising Failure? Multimodal LLMs and the NLVR Challenge Anne Wu, Kianté Brantley, and Yoav Artzi Department of Computer Science and Cornell Tech, Cornell University {annewu, yoav}@cs.cornell.edu, kdb82@cornell.edu Abstract This study evaluates three state-of-the-art MLLMs — GPT-4V, Gemini Pro, and the open-source model IDEFICS — on the compositional natural language vision reasoning task NLVR. Given a human-written sentence paired with a synthetic image, this task requires the model to dete...\",\"From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs Yulong Liu1* Yunlong Yuan2∗ Chunwei Wang3 Jianhua Han3 Yongqiang Ma1 Li Zhang2 Nanning Zheng1† Hang Xu3 1National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, Xi’an, China 2 School of Data Science, Fudan University 3 Huawei Noah’s Ark Lab Abstract The distinction between humans and animals lies in the unique ability of humans to use and create tools. Tools em- power humans to overcome...\",\"A SOUND APPROACH: USING LARGE LANGUAGE MODELS TO GENERATE AUDIO DESCRIPTIONS FOR EGOCENTRIC TEXT-AUDIO RETRIEVAL Andreea-Maria Oncescu1 João F. Henriques1 Andrew Zisserman1 Samuel Albanie2 A. Sophia Koepke3 1University of Oxford 2University of Cambridge 3University of Tübingen ABSTRACT Video databases from the internet are a valuable source of text-audio retrieval datasets. However, given that sound and vision streams represent different “views” of the data, treat- ing visual descriptions as aud...\",\"Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Model Hao Cheng* HKUST (GZ) Erjia Xiao* HKUST (GZ) Jindong Gu University of Oxford Le Yang Xi’an Jiaotong University Jinhao Duan Drexel University Jize Zhang HKUST Jiahang Cao HKUST (GZ) Kaidi Xu Drexel University Renjing Xu† HKUST (GZ) Abstract Large Vision-Language Models (LVLMs) rely on vision encoders and Large Language Models (LLMs) to exhibit remarkable capabilities on various multi-modal t...\",\"LLMs in Political Science: Heralding a New Era of Visual Analysis Yu Wang1 and Mengying Xing2* 1Fudan Institute for Advanced Study in Social Sciences, Fudan University, Shanghai, 200433, China. 2*School of Journalism, Fudan University, Shanghai, 200433, China. *Corresponding author(s). E-mail(s): xingmengying@fudan.edu.cn; Contributing authors: yuwang.aiml@gmail.com; Abstract Interest is increasing among political scientists in leveraging the extensive infor- mation available in images. However,...\",\"TempCompass: Do Video LLMs Really Understand Videos? Yuanxin Liu§*, Shicheng Li§*, Yi Liu§, Yuxiang Wang§, Shuhuai Ren§, Lei Li†, Sishuo Chen¶, Xu Sun§, Lu Hou‡ § National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University ¶ Center for Data Science, Peking University † The University of Hong Kong ‡ Huawei Noah’s Ark Lab {liuyuanxin, yuxiangwang, shuhuai_ren}@stu.pku.edu.cn nlp.lilei@gmail.com {lisc99, imliuyi, chensishuo, xusun}@pku.edu.cn houlu3@...\",\"Large Language Models for Simultaneous Named Entity Extraction and Spelling Correction Edward Whittaker Best Path Research Inc. Tokyo, Japan ed@bestpathresearch.com Ikuo Kitagishi Money Forward Inc. Tokyo, Japan kitagishi.ikuo@moneyforward.co.jp ABSTRACT Language Models (LMs) such as BERT, have been shown to per- form well on the task of identifying Named Entities (NE) in text. A BERT LM is typically used as a classifier to classify individual to- kens in the input text, or to classify spans of ...\",\"CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document Visual Question Answering Jinxu Zhang and Yongqi Yu and Yu Zhang jxzhang@ir.hit.edu.cn yqyu@ir.hit.edu.cn zhangyu@ir.hit.edu.cn Abstract Document Visual Question Answering (DVQA) is a task that involves responding to queries based on the content of images. Existing work is limited to locating information within a single page and does not facilitate cross-page question-and-answer interaction. Furthermore, the token length limi...\",\"Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning Shuo Yang1,2 Zirui Shang2 Yongqi Wang2 Derong Deng1 Hongwei Chen1 Qiyuan Cheng1 Xinxiao Wu1,2* 1Guangdong Laboratory of Machine Perception and Intelligent Computing Shenzhen MSU-BIT University, China 2Beijing Key Laboratory of Intelligent Information Technology School of Computer Science & Technology, Beijing Institute of Technology, China {shuoyang,shangzirui,wuxinxiao}@bit.edu.cn {1285441164yq, derongdeng.dero, chwr0001}@gma...\",\"SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code Ziniu Hu 12 Ahmet Iscen 2 Aashi Jain 2 Thomas Kipf 3 Yisong Yue 1 David A. Ross 2 Cordelia Schmid 2 Alireza Fathi 2 1California Institute of Technology 2Google Research 3Google DeepMind 1 Abstract This paper introduces SceneCraft, a Large Lan- guage Model (LLM) Agent converting text de- scriptions into Blender-executable Python scripts which render complex scenes with up to a hun- dred 3D assets. This process requires complex spa...\",\"IEEE SIGNAL PROCESSING LETTERS, VOL. 30, 2023 1 GPTSee: Enhancing Moment Retrieval and Highlight Detection via Description-Based Similarity Features Yunzhuo Sun, Yifang Xu, Zien Xie, Yukun Shu, and Sidan Du, Member, IEEE Abstract—Moment retrieval (MR) and highlight detection (HD) aim to identify relevant moments and highlights in video from corresponding natural language query. Large language models (LLMs) have demonstrated proficiency in various com- puter vision tasks. However, existing method...\",\"GuardT2I: Defending Text-to-Image Models from Adversarial Prompts Yijun Yang 1 Ruiyuan Gao 1 Xiao Yang 2 Jianyuan Zhong 1 Qiang Xu 1 Abstract Recent advancements in Text-to-Image (T2I) models have raised significant safety concerns about their potential misuse for generating in- appropriate or Not-Safe-For-Work (NSFW) con- tents, despite existing countermeasures such as NSFW classifiers or model fine-tuning for inap- propriate concept removal. Addressing this chal- lenge, our study unveils GUARD...\",\"Published as a conference paper at ICLR 2024 SCHEMA: STATE CHANGES MATTER FOR PROCEDURE PLANNING IN INSTRUCTIONAL VIDEOS Yulei Niu1 Wenliang Guo1 Long Chen2 Xudong Lin1 Shih-Fu Chang1 1Columbia University 2The Hong Kong University of Science and Technology yn.yuleiniu@gmail.com ABSTRACT We study the problem of procedure planning in instructional videos, which aims to make a goal-oriented sequence of action steps given partial visual state observations. The motivation of this problem is to learn ...\",\"NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models Lizhou Fan∗ University of Michigan Wenyue Hua∗ Rutgers University Xiang Li∗ Shandong University Kaijie Zhu Microsoft Research Asia Mingyu Jin Rutgers University Lingyao Li University of Michigan Haoyang Ling University of Michigan Jinkui Chi University of Michigan Jindong Wang Microsoft Research Asia Xin Ma Shandong University Yongfeng Zhang Rutgers University Figure 1: Multimodal Large Language Models’s performance ...\",\"RegionGPT: Towards Region Understanding Vision Language Model Qiushan Guo1, Shalini De Mello2†, Hongxu Yin2, Wonmin Byeon2, Ka Chun Cheung2, Yizhou Yu1, Ping Luo1, Sifei Liu2 1The University of Hong Kong 2NVIDIA Figure 1. We introduce RegionGPT that enables complex region-level captioning, reasoning, classification, and expression comprehension capabilities for the multimodal large language model. Users can input regions of interest of any shape, utilizing ⟨region⟩as a placeholder within the ins...\",\"Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research Brenda Y. Miao1*, Irene Y. Chen2,3,4, Christopher YK Williams1, Jaysón Davidson1, Augusto Garcia-Agundez5, Harry Sun1, Travis Zack1,6, Atul J. Butte1,8, Madhumita Sushil1 1. Bakar Computational Health Sciences Institute, University of California San Francisco, San Francisco, CA, USA 2. Computational Precision Health, University of California, Berkeley and University of Ca...\",\"Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use Imad Eddine Toubal1,2* Aditya Avinash1 Neil Gordon Alldrin1 Jan Dlabal1 Wenlei Zhou1 Enming Luo1 Otilia Stretcu1 Hao Xiong1 Chun-Ta Lu1 Howard Zhou1 Ranjay Krishna1,3 † Ariel Fuxman1 Tom Duerig1 1Google Research 2University of Missouri 3University of Washington itdfh@umsystem.edu, {adity,nalldrin}@google.com Abstract From content moderation to wildlife conservation, the number of applicati...\",\"Interactive Continual Learning: Fast and Slow Thinking Biqing Qi1,2,4, Xinquan Chen3, Junqi Gao3, Dong Li3, Jianxing Liu1, Ligang Wu1,* , Bowen Zhou1,2,4,* 1 Department of Control Science and Engineering, Harbin Institute of Technology, 2 Department of Electronic Engineering, Tsinghua University, 3 School of Mathematics, Harbin Institute of Technology, 4 Frontis.AI, Beijing {qibiqing7,xinquanchen0117,gjunqi97,arvinlee826}@gmail.com, {jx.liu,ligangwu}@hit.edu.cn, {zhoubowen}@tsinghua.edu.cn Abstr...\",\"Android in the Zoo: Chain-of-Action-Thought for GUI Agents Jiwen Zhang1,2*, Jihao Wu2, Yihua Teng2, Minghui Liao2, Nuo Xu2, Xiao Xiao2, Zhongyu Wei1, Duyu Tang2 1Fudan University 2Huawei Inc. jiwenzhang21@m.fudan.edu.cn {wujihao,tengyihua,liaominghui1,xunuo4,xiaoxiao55}@huawei.com zywei@fudan.edu.cn duyutang@huawei.com https:\\u002f\\u002fgithub.com\\u002fIMNearth\\u002fCoAT Abstract Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural lan- gua...\",\"ImgTrojan: Jailbreaking Vision-Language Models with ONE Image Xijia Tao*, Shuai Zhong∗, Lei Li∗, Qi Liu, Lingpeng Kong The University of Hong Kong {xjtao2333, u3577193}@connect.hku.hk nlp.lilei@gmail.com {liuqi, lpk}@cs.hku.hk Abstract There has been an increasing interest in the alignment of large language models (LLMs) with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain rel- atively underexplored. In this paper, we p...\",\"CHATGPT AND BIOMETRICS: AN ASSESSMENT OF FACE RECOGNITION, GENDER DETECTION, AND AGE ESTIMATION CAPABILITIES Ahmad Hassanpour1, Yasamin Kowsari2, Hatef Otroshi Shahreza3,4, Bian Yang1, Sébastien Marcel3,5 1Norwegian University of Science and Technology (NTNU), Norway 2Mizani Research Institute, Iran 3Idiap Research Institute, Switzeralnd 4École Polytechnique Fédérale de Lausanne (EPFL), Switzeralnd 5Université de Lausanne (UNIL), Switzeralnd ABSTRACT This paper explores the application of large ...\",\"Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception Junwen He*1,2, Yifan Wang1, Lijun Wang†1, Huchuan Lu1, Jun-Yan He2, Jin-Peng Lan2, Bin Luo2, and Xuansong Xie2 1Dalian University of Technology 2DAMO Academy, Alibaba Group Abstract Multimodal Large Language Model (MLLMs) leverages Large Language Models as a cognitive framework for di- verse visual-language tasks. Recent efforts have been made to equip MLLMs with visual perceiving and grounding ca- pabilities. However, there ...\",\"Design2Code: How Far Are We From Automating Front-End Engineering? Chenglei Si∗ Stanford University clsi@stanford.edu Yanzhe Zhang∗ Georgia Tech z_yanzhe@gatech.edu Zhengyuan Yang Microsoft Ruibo Liu Google DeepMind Diyi Yang Stanford University ∗Equal Contribution Project Page: https:\\u002f\\u002fsalt-nlp.github.io\\u002fDesign2Code\\u002f Abstract Generative AI has made rapid advancements in recent years, achieving unprece- dented capabilities in multimodal understanding and code generation. This can enable a new pa...\",\"CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments Savitha Sam Abraham∗, Marjan Alirezaie†, Luc De Raedt§ ∗The University of Adelaide, Australia savitha.samabraham@adelaide.edu.au †Flybits Labs. TMU Creative AI Hub, Toronto, Canada marjan.alirezaie@flybits.com § ¨ Orebro University, Centre for Applied Autonomous Sensor Systems(AASS), ¨ Orebro, Sweden Department of Computer Science, KULeuven, Belgium luc.de-raedt@oru.se Abstract The integration of learni...\",\"Multimodal Transformer for Comics Text-Cloze Emanuele Vivoli ⋆1,2 , Joan Lafuente Baeza ⋆1 , Ernest Valveny Llobet1 , and Dimosthenis Karatzas1 1 CVC, Universitat Auton´ oma de Barcelona, Bellaterra 08193, Spain 2 MICC, University of Florence, Florence 50134, Italy evivoli@cvc.uab.cat joan.lafuente@autonoma.cat Abstract. This work explores a closure task in comics, a medium where visual and textual elements are intricately intertwined. Specifically, text- cloze refers to the task of selecting th...\",\"1 Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection from Remote Sensing Imagery Wei Zhang, Miaoxin Cai, Student Member, IEEE, Tong Zhang, Student Member, IEEE, Guoqiang Lei, Yin Zhuang† Member, IEEE, and Xuerui Mao† Abstract—Ship detection needs to identify ship locations from remote sensing (RS) scenes. However, due to different imag- ing payloads, various appearances of ships, and complicated background interference from the bird’s eye view, it is difficult to set up a un...\",\"ARE LANGUAGE MODELS PUZZLE PRODIGIES? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning Deepanway Ghosal1, Vernon Toh Yan Han1, Chia Yew Ken1, Soujanya Poria1 1 Singapore University of Technology and Design §: https:\\u002f\\u002fgithub.com\\u002fdeclare-lab\\u002fLLM-PuzzleTest : https:\\u002f\\u002falgopuzzlevqa.github.io\\u002f Abstract This paper introduces the novel task of multi- modal puzzle solving, framed within the con- text of visual question-answering. We present a new dataset, ALGOPUZZLEVQA designed to ...\",\"Enhancing chest X-ray datasets with privacy-preserving large language models and multi-type annotations: a data-driven approach for improved classification Ricardo Bigolin Lanfredia, Pritam Mukherjeea, Ronald Summersa,∗ aImaging Biomarkers and Computer-Aided Diagnosis Laboratory, Department of Radiology and Imaging Sciences, National Institutes of Health Clinical Center Bldg 10, Room 1C224D, 10 Center Dr, Bethesda, MD 20892-1182, USA A B S T R A C T In chest X-ray (CXR) image analysis, rule-base...\",\"How Far Are We from Intelligent Visual Deductive Reasoning? Yizhe Zhang∗ , He Bai∗ , Ruixiang Zhang∗ , Jiatao Gu, Shuangfei Zhai, Josh Susskind, Navdeep Jaitly Apple {yizzhang,hbai7,ruixiangz,jgu32,szhai,jsusskind,ndjaitly}@apple.com Abstract Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated incredible strides on diverse vision language tasks. We dig into vision-based de- ductive reasoning, a more sophisticated but less explored realm, and find previ- ously unexposed blinds...\",\"SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM Jielin Qiu1,2∗ , Andrea Madotto1, Zhaojiang Lin1, Paul A. Crook1, Yifan Ethan Xu1, Xin Luna Dong1, Christos Faloutsos2, Lei Li2, Babak Damavandi1, Seungwhan Moon1 1 Meta Reality Labs & FAIR, Meta 2Carnegie Mellon University {jielinq,leili,christos}@cs.cmu.edu, {andreamad8,zhaojiang,pacrook,ethanxu,lunadong,shanemoon}@meta.com Abstract Vision-extended LLMs have made significant strides in Visual ...\",\"R2-Bench: Benchmarking the Robustness of Referring Perception Models under Perturbations Xiang Li1, Kai Qiu1, Jinglu Wang2, Xiaohao Xu3, Rita Singh1, Kashu Yamazak1, Hao Chen1, Xiaonan Huang3, Bhiksha Raj1 1 Carnegie Mellon University, 2 Microsoft Research, 3University of Michigan Find the cate Intelligent System Human Meow Environment “Find the yellow and black cat” Textual Visual Acoustic Buzzing Referring Guidance Perturbations ⋯ “Meow⋯” Confidence Map (a) Referring Perception in Practice (b)...\",\"Embracing Large Language and Multimodal Models for Prosthetic Technologies Sharmita Dey∗ , Arndt F. Schilling v3: 07.03.2024 Abstract This article presents a vision for the future of prosthetic devices, leveraging the advancements in large language models (LLMs) and Large Multimodal Models (LMMs) to revolutionize the in- teraction between humans and assistive technologies. Unlike traditional prostheses, which rely on limited and predefined commands, this approach aims to develop intelligent pros...\",\"DiffChat: Learning to Chat with Text-to-Image Synthesis Models for Interactive Image Creation Jiapeng Wang1∗ , Chengyu Wang2† , Tingfeng Cao1, Jun Huang2, Lianwen Jin1† 1 South China University of Technology, China 2 Alibaba Group, China {eejpwang, setingfengcao}@mail.scut.edu.cn, eelwjin@scut.edu.cn {chengyu.wcy, huangjun.hj}@alibaba-inc.com Abstract We present DiffChat, a novel method to align Large Language Models (LLMs) to “chat” with prompt-as-input Text-to-Image Synthesis (TIS) models (e.g...\",\"ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment Xiwei Hu⋆, Rui Wang⋆, Yixiao Fang⋆, Bin Fu⋆, Pei Cheng, and Gang Yu⋆⋆ Tencent xiweihu@outlook.com, {raywwang, yixiaofang, brianfu, peicheng}@tencent.com, skicy@outlook.com https:\\u002f\\u002fella-diffusion.github.io Abstract. Diffusion models have demonstrated remarkable performance in the domain of text-to-image generation. However, most widely used models still employ CLIP as their text encoder, which constrains their ability to compre...\",\"Debiasing Large Visual Language Models Yi-Fan Zhang1, Weichen Yu2, Qingsong Wen3 ∗, Xue Wang4, Zhang Zhang1, Liang Wang1, Rong Jin5, and Tieniu Tan1 1 State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation 2 Carnegie Mellon University 3 Squirrel AI Learning, ∗Corresponding author 4 Alibaba Group 5 Meta AI https:\\u002f\\u002fgithub.com\\u002fyfzhang114\\u002fLLaVA-Align Abstract. In the realms of computer vision and natural language pro- cessing, Large Vision-Language Models ...\",\"WILL GPT-4 RUN DOOM? Adrian de Wynter* ABSTRACT We show that GPT-4’s reasoning and planning capabilities extend to the 1993 first-person shooter Doom. This large language model (LLM) is able to run and play the game with only a few in- structions, plus a textual description– generated by the model itself from screenshots–about the state of the game being observed. We find that GPT-4 can play the game to a passable degree: it is able to manipulate doors, combat enemies, and perform pathing. More ...\",\"Beyond Finite Data: Towards Data-free Out-of-distribution Generalization via Extrapolation Yijiang Li1, Sucheng Ren1, Weipeng Deng2, Yuzhi Xu3, Ying Gao4, Edith Ngai2, and Haohan Wang5 1 Johns Hopkins University 2 The University of Hong Kong 3 New York University 4 South China University of Technology 5 University of Illinois Urbana-Champaign Abstract. Out-of-distribution (OOD) generalization is a favorable yet challeng- ing property for deep neural networks. The core challenges lie in the limit...\",\"DeepSeek-VL: Towards Real-World Vision-Language Understanding Haoyu Lu*1†, Wen Liu*1, Bo Zhang*1‡, Bingxuan Wang1†, Kai Dong1, Bo Liu1†, Jingxiang Sun1†, Tongzheng Ren1†, Zhuoshu Li1, Hao Yang1†, Yaofeng Sun1, Chengqi Deng1, Hanwei Xu1, Zhenda Xie1, Chong Ruan1 1DeepSeek-AI {neal, liuwen, bo}@deepseek.com https:\\u002f\\u002fgithub.com\\u002fdeepseek-ai\\u002fDeepSeek-VL Abstract We present DeepSeek-VL, an open-source Vision-Language (VL) Model designed for real-world vision and language understanding applications. Our...\",\"Chaining text-to-image and large language model: A novel approach for generating personalized e-commerce banners Shanu Vashishtha shanu.vashishtha@walmart.com Walmart, Inc. Sunnyvale, CA, USA Abhinav Prakash abhinav.prakash@walmart.com Walmart, Inc. Sunnyvale, CA, USA Lalitesh Morishetti lalitesh.morishetti@walmart.com Walmart, Inc. Sunnyvale, CA, USA Kaushiki Nag kaushiki.nag@walmart.com Walmart, Inc. Sunnyvale, CA, USA Yokila Arora yokila.arora@walmart.com Walmart, Inc. Sunnyvale, CA, USA Sush...\"],\"xaxis\":\"x\",\"yaxis\":\"y\",\"type\":\"histogram\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"text\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"count\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Histogram of text\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('ac6a098f-7a1c-4150-aeb9-0a9ff22c8eae');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{},"output_type":"display_data"}],"source":["import plotly.express as px\n","\n","columns = columns_to_include\n","\n","for column in columns:\n","\n","    fig = px.histogram(dataframe, x=column, title=f'Histogram of {column}')\n","\n","    # fig.show()\n","    # fig = px.scatter(dataframe, x=column, )\n","\n","    fig.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393,"referenced_widgets":["c900ed6345c64d4bb8ba075cf24f3400","edc4b24971754af6969dddc3a06eb0b9","c851c4eaf7d24011996785447f0cca2a","e034ccde846442cc95bfe848e8d4cc4f","3fd9c1475d7c490dabe870b247f775bd","7df1a157a5de4a9cb9f22f53bcf0d224","508b2a4245d549e0a67c53e7ccb8bc9b","5ad75df56d074a7e92fda70f1abd15ee","0f98309199334d8bac8ae50c92a65eef","c3fba927f39848b4889dd8d89dbd0092","815182760ccb4fd69e6974c209bc6544","c6806a15c18f4f9596003daac817299b","e37f98f2078b4a58978377100500d75f","fa61a73331ac4bb0b5ef4ce48739e6dc","d3a229389fd8488f9d04902c5bd67769","b8b1631dd8334586a207e9b3e2e56037","7f1ba3b102844c6b8db5a3615de4ecf3","dc71c9e283bb4216919243b11d9e3dea","28334b859bb24e85b5bd1d3ed7230e57","1e2744ac449547eca69f8f450155af73","975658d3d12b438687f182e9ac8b53e5","031455cf4bf0444eb400990dee429830","ce7757e7a3324dfeac0196754e687458","f8fcc5f59984450aa73e9a8379c1fcbe","0e27cec649304fb98024b333e883013d","ca7befa9108b4dbdbd2bd2b0e3a0273d","5852e7fc8eac4031a6b24e9319344cbb","a265b0c4def34a03b2c293e434705f01","0642cfbbf8664232996fad5c9abd8931","dfac62adf88f4ae3af85887d91b170b1","6f76643eaaf34ff8aca13b83518f6d5a","77ae4e4646084110b37df1c591fa0e14","0448ffc063164e66a30ee7dfbbb60e63","4922dc92fb434e07a41901e0322fe785","d5dbbf9bdcd34acfb953d6554679f104","f0274de6492742e6912622620a42f54a","32dd933e91bf464686eaba6e386e1b51","38039907ae554553822a548f9efeda34","384c08650a714767b6445e65b0b4ec39","4631c6a894f349d48c6c4e4af3834bed","3840b2e2bdd2436b99dbf00be814319c","dbcca98d130b46a5b0ae4ea3a24f7fca","b392894a163f42a482c2a20febf93f33","6223261ff9564703b157deb0f6cdfd42","c2499df3a42846378fd9a178fac94f9d","a47e611a93434e6fb8af7d78161497fc","d4f127fff75c4ee78e6acc80fe24d828","32c6227adfae4b389d962ec664d98b4c","db3434a56c2240c1af206808b12c7703","34810852167d4a568a3b3104c5773696","737e3a4e484e4edeb9537bd0e43c3cbb","073540de47484fbf9ca6afd582ce5e71","75b26f738f4b4c2ba76c88b702b2b2da","32e45ed72f434c3693ac078977087360","8a5c409b7ff44e81921de4aa4eff5d88","7f764be0990d44fda621904e614e86d8","67e3aeb7e6aa4518ad8f1adb6e520241","8a4c1a3bf18d4e8d80f05d8945b11f52","f4131872e8314ee1b31aa79cbbfc9782","648c48f66d554d7a924f112d04b6cef6","cd7e248494de44c9acbbd34f81afa3a8","9c4a14fcc6d846e2b27d803a6bfe2905","3f3980e3cf62415693b34f3a5d0affbd","40859ee4ff8d4837bbd9ec88f67131ba","9d9f5344298c43388a6d2b47b943cc70","dffc4921ce92447a94080a16805ea1cc"]},"executionInfo":{"elapsed":259024,"status":"ok","timestamp":1711274183102,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"X-blrQS9tBhW","outputId":"8c9b35f8-f064-4fe2-8918-8a8941a242d8"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c900ed6345c64d4bb8ba075cf24f3400","version_major":2,"version_minor":0},"text/plain":["Resolving data files:   0%|          | 0/191 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c6806a15c18f4f9596003daac817299b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/191 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning:\n","\n","\n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce7757e7a3324dfeac0196754e687458","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4922dc92fb434e07a41901e0322fe785","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2499df3a42846378fd9a178fac94f9d","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7f764be0990d44fda621904e614e86d8","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["****completed*** all files files\n"]}],"source":["dataset=loading_folder_using_datasets(folder_path=dir_output)\n","langchain_docs = [\n","    LangchainDocument(page_content=doc[\"text\"])\n","    for doc in tqdm(dataset['train'])\n","]\n","docs_processed=[]\n","EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n","MARKDOWN_SEPARATORS = [\n","    \"\\n#{1,6} \",\n","    \"```\\n\",\n","    \"\\n\\\\*\\\\*\\\\*+\\n\",\n","    \"\\n---+\\n\",\n","    \"\\n___+\\n\",\n","    \"\\n\\n\",\n","    \"\\n\",\n","    \" \",\n","    \"\",\n","]\n","\n","def split_documents(\n","    chunk_size: int,\n","    knowledge_base,\n","    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",") -> List:\n","    \"\"\"\n","    This function splits documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n","\n","    Parameters:\n","    chunk_size (int): Maximum size of each chunk\n","    knowledge_base (List): List of documents to be processed\n","    tokenizer_name (str, optional): Name of the tokenizer. Defaults to EMBEDDING_MODEL_NAME.\n","\n","    Returns:\n","    List: List of processed documents\n","    \"\"\"\n","    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n","        AutoTokenizer.from_pretrained(tokenizer_name),\n","        chunk_size=chunk_size,\n","        chunk_overlap=int(chunk_size / 10),\n","        add_start_index=True,\n","        strip_whitespace=True,\n","        separators=MARKDOWN_SEPARATORS,\n","    )\n","\n","    docs_processed = []\n","    for doc in langchain_docs:\n","        docs_processed += text_splitter.split_documents([doc])\n","    print(\"****completed*** all files files\")\n","\n","\n","    # Remove duplicates\n","    unique_texts = {}\n","    docs_processed_unique = []\n","    for doc in docs_processed:\n","        if doc.page_content not in unique_texts:\n","            unique_texts[doc.page_content] = True\n","            docs_processed_unique.append(doc)\n","\n","    return docs_processed_unique\n","\n","\n","docs_processed = split_documents(\n","    512,  # We choose a chunk size adapted to our model\n","    docs_processed,\n","    tokenizer_name=EMBEDDING_MODEL_NAME,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":430,"status":"ok","timestamp":1711274325637,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"akpzJH5vtBeF","outputId":"e8ba997b-c331-4961-c04a-08c28aed817d"},"outputs":[{"data":{"text/plain":["[Document(page_content='RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models Saeed Khaki Amazon sakhaki@amazon.com JinJin Li Amazon jinjinli@amazon.com Lan Ma Amazon mamlm@amazon.com Liu Yang Amazon liuyanga@amazon.com Prathap Ramachandra Amazon prathara@amazon.com Abstract Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. How- ever, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring signif- icant hyperparameter finetuning, and computa- tionally expensive to maximize the estimated reward during alignment. Recently, direct pref- erence optimization', metadata={'start_index': 0}),\n"," Document(page_content='estimated reward during alignment. Recently, direct pref- erence optimization (DPO) is proposed to ad- dress those challenges. However, DPO relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both chal- lenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sam- pled directly from the SFT model. RS-DPO identifies pairs of', metadata={'start_index': -1}),\n"," Document(page_content='directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their reward distribution. Finally, we apply DPO with the contrastive samples to align the model to human preference. Our experiments indicate that our proposed method effectively fine-tunes LLMs with limited resource envi- ronments, leading to improved alignment with user intent. Furthermore, it outperforms exist- ing methods, including RS, PPO, and DPO. 1 Introduction State-of-the-art (SOTA) LLMs such as GPT-4 (Ope- nAI, 2023), LLaMa (Touvron et al., 2023) etc., are trained with several stages. With pre-training and supervised', metadata={'start_index': 1185}),\n"," Document(page_content='are trained with several stages. With pre-training and supervised instruction tuning, LLMs learns to fol- low specific instructions to complete various tasks with zero-shot or few-shot prompts (Chowdhery et al., 2022). To further improve the LLMs’ in- telligence as close as to human and ensure a more helpful and harmless model, alignment is important as the last-mile LLM training procedure (Ziegler et al., 2019; Stiennon et al., 2020b; Bai et al., 2022; Ouyang et al., 2022). Reinforcement learning with human feedback (RLHF) (Christiano et al., 2017) is the most adopted', metadata={'start_index': -1}),\n"," Document(page_content='(Christiano et al., 2017) is the most adopted approach for alignment train- ing and it usually involves training a reward model with human preference datasets which optimizes a reward function based on the human-annotated preference. Then LLMs are fine-tuned to learn to maximize the reward of their responses using rein- forcement learning algorithms, including proximal policy optimization (PPO) (Schulman et al., 2017), REINFORCE (Williams, 2004), and similar vari- ants. While PPO is used by SOTA LLMs due to its ease of use and good performance, training with PPO has few limitations, including complexity', metadata={'start_index': 2269}),\n"," Document(page_content='good performance, training with PPO has few limitations, including complexity of training multiple LLMs, and sampling from policy model in training loop, high GPU memory require- ment with hosting multiple LLMs during training, and sensitivity to training data and reward models. To make RLHF training more efficient, there are methods proposed from different perspective. In order to reduce the preference data effort by human annotation, (Lee et al., 2023) and (Tunstall et al., 2023) proposed to train the LLM to align to the LLM’s preference rating in order to save human ef- fort. (Santacroce', metadata={'start_index': -1}),\n"," Document(page_content='rating in order to save human ef- fort. (Santacroce et al., 2023) proposed a combined strategy to merge SFT and reward models as well as in PPO with LoRA selection in order to reduce latency and memory footprint. (Dong et al., 2023; Gulcehre et al., 2023) used reward model to se- lect ranked high-reward good samples to supervise fine-tune the models and iteratively repeating this process yield good results. To reduce the memory and save training resources, (Rafailov et al., 2023) proposed the direct preference optimization (DPO) to remove the need of training', metadata={'start_index': 3348}),\n"," Document(page_content='direct preference optimization (DPO) to remove the need of training reward model, and directly optimize the policy model using a simple classification to maximize the difference between likelihood of human preference pairs. This method proves equivalent performance by implicitly max- imize the reward. However, it is mainly trained on human preference data to learn the alignment, arXiv:2402.10038v1 [cs.CL] 15 Feb 2024 instead of sampling the policy model’s response for optimization. LLaMa2 (Touvron et al., 2023) adopts several rounds of rejection sampling to se- lect the best samples from k model-generated sam- ples for', metadata={'start_index': -1}),\n"," Document(page_content='the best samples from k model-generated sam- ples for fine-tuning before PPO in order to boost the model performance. But rejection sampling only selects the best samples instead of preference pairs, with low data usage efficiency. RSO (Liu et al., 2023) proposes to generate preference data from the target optimal policy using rejection sam- pling, enabling a more accurate estimation of the optimal policy. Compared to RSO, our proposed method (RS-DPO) directly employs a point-wise reward model for response ranking and optimiza- tion, utilizing logistic loss exclusively during pol- icy optimization. Unlike RSO’s approach', metadata={'start_index': 4419}),\n"," Document(page_content='loss exclusively during pol- icy optimization. Unlike RSO’s approach of sta- tistical rejection sampling and tournament ranking for response generation and selection, RS-DPO generates a fixed number of responses per prompt and relies on computing reward gaps between re- sponses for preference data generation, resulting in reduced computational expense. Additionally, while RSO lacks evaluation on standard alignment benchmarks and comparison against PPO, RS-DPO demonstrates its effectiveness against other RLHF methods on such benchmarks. In this work, we propose RS-DPO method for RLHF training that combines the advantages of existing efficient methods, including offline pref-', metadata={'start_index': -1})]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["docs_processed[0:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"executionInfo":{"elapsed":407,"status":"ok","timestamp":1711275914657,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"_6QfbjpytBZf","outputId":"40a877c7-f7e0-429f-8b68-6a96b5ff223e"},"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"c871e844-6269-4153-a790-1f0d3c5d613d\" class=\"plotly-graph-div\" style=\"height:400px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c871e844-6269-4153-a790-1f0d3c5d613d\")) {                    Plotly.newPlot(                        \"c871e844-6269-4153-a790-1f0d3c5d613d\",                        [{\"name\":\"attribute1\",\"text\":\"RS-DPO: A \",\"textposition\":\"auto\",\"x\":[\"attribute1\"],\"y\":[\"RS-DPO: A \"],\"type\":\"bar\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"name\":\"attribute2\",\"text\":\"0\",\"textposition\":\"auto\",\"x\":[\"attribute2\"],\"y\":[0],\"type\":\"bar\",\"xaxis\":\"x\",\"yaxis\":\"y\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0]},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Data 1\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Statistical Analysis of Data Structures\"},\"height\":400,\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('c871e844-6269-4153-a790-1f0d3c5d613d');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}],"source":["from typing import Any, List, Dict, Union\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","\n","def plot_data_structures(*data_structures: Any) -> None:\n","    \"\"\"\n","    Plots various data structures using Plotly based on user input.\n","\n","    Args:\n","        *data_structures: Variable number of data structures of any type.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    num_plots = len(data_structures)\n","    num_rows = (num_plots + 1) // 2\n","    num_cols = 2\n","\n","    # Create subplots\n","    fig = make_subplots(rows=num_rows, cols=num_cols, subplot_titles=[f\"Data {i+1}\" for i in range(num_plots)])\n","\n","    for i, data in enumerate(data_structures, start=1):\n","        if isinstance(data, dict):\n","            # Plot dictionary\n","            for key, value in data.items():\n","                if isinstance(value, list):\n","                    # Plot dictionary of lists\n","                    for j, item in enumerate(value, start=1):\n","                        if isinstance(item, list):\n","                            # Plot dictionary of lists of lists\n","                            item_data = \", \".join(str(x) for x in item)\n","                        else:\n","                            item_data = str(item)\n","                        trace = go.Scatter(\n","                            x=[j],\n","                            y=[key],\n","                            mode=\"markers\",\n","                            marker=dict(size=10),\n","                            text=item_data,\n","                            hoverinfo=\"text\",\n","                            name=key,\n","                        )\n","                        fig.add_trace(trace, row=(i - 1) // num_cols + 1, col=(i - 1) % num_cols + 1)\n","                else:\n","                    # Plot dictionary of non-list values\n","                    trace = go.Bar(\n","                        x=[key],\n","                        y=[value],\n","                        text=str(value),\n","                        textposition=\"auto\",\n","                        name=key,\n","                    )\n","                    fig.add_trace(trace, row=(i - 1) // num_cols + 1, col=(i - 1) % num_cols + 1)\n","        elif isinstance(data, list):\n","            # Plot list\n","            for j, item in enumerate(data, start=1):\n","                if isinstance(item, list):\n","                    # Plot list of lists\n","                    item_data = \", \".join(str(x) for x in item)\n","                    trace = go.Scatter(\n","                        x=[j],\n","                        y=[len(item)],\n","                        mode=\"lines+markers\",\n","                        text=item_data,\n","                        hoverinfo=\"text\",\n","                        name=f\"List {j}\",\n","                    )\n","                else:\n","                    # Plot list of non-list values\n","                    trace = go.Bar(\n","                        x=[j],\n","                        y=[item],\n","                        text=str(item),\n","                        textposition=\"auto\",\n","                        name=f\"Item {j}\",\n","                    )\n","                fig.add_trace(trace, row=(i - 1) // num_cols + 1, col=(i - 1) % num_cols + 1)\n","        else:\n","            # Plot single value\n","            trace = go.Bar(\n","                x=[\"Value\"],\n","                y=[data],\n","                text=str(data),\n","                textposition=\"auto\",\n","                name=\"Value\",\n","            )\n","            fig.add_trace(trace, row=(i - 1) // num_cols + 1, col=(i - 1) % num_cols + 1)\n","\n","    # Customize layout\n","    fig.update_layout(\n","        title=\"Statistical Analysis of Data Structures\",\n","        height=num_rows * 400,\n","        showlegend=False,\n","    )\n","\n","    # Display the plot\n","    fig.show()\n","\n","\n","doc_json = {\n","        \"attribute1\":docs_processed[0].page_content.strip(\"\")[:10],\n","        \"attribute2\":docs_processed[0].metadata['start_index'],\n","        # Add more attributes as needed\n","    }\n","# docs_processed_json.append(doc_json)\n","\n","plot_data_structures(doc_json)\n","\n","\n","# # User can provide any combination and structure of data\n","# plot_data_structures(docs_processed)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":388,"status":"ok","timestamp":1711275282174,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"SWhq6QCY86Hg","outputId":"4a7ff65a-d58e-405a-8279-4ef3aeff93e5"},"outputs":[{"data":{"text/plain":["0"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["docs_processed[0].metadata['start_index']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"elapsed":2008,"status":"ok","timestamp":1711216753901,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"VgbzwMAlOc9S","outputId":"05ce2630-6093-485e-a585-c25da26b2d47"},"outputs":[{"data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"c95441b1-3f79-4f19-84ec-6131d1d2e489\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c95441b1-3f79-4f19-84ec-6131d1d2e489\")) {                    Plotly.newPlot(                        \"c95441b1-3f79-4f19-84ec-6131d1d2e489\",                        [{\"hoverinfo\":\"text\",\"marker\":{\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"showscale\":true,\"size\":8},\"mode\":\"markers\",\"text\":[\"Act: Linux Terminal\\u003cbr\\u003eAct1: Linux Terminal\\u003cbr\\u003eAct2: Linux Terminal\",\"Act: English Translator and Improver\\u003cbr\\u003eAct1: English Translator and Improver\\u003cbr\\u003eAct2: English Translator and Improver\",\"Act: `position` Interviewer\\u003cbr\\u003eAct1: `position` Interviewer\\u003cbr\\u003eAct2: `position` Interviewer\",\"Act: JavaScript Console\\u003cbr\\u003eAct1: JavaScript Console\\u003cbr\\u003eAct2: JavaScript Console\",\"Act: Excel Sheet\\u003cbr\\u003eAct1: Excel Sheet\\u003cbr\\u003eAct2: Excel Sheet\",\"Act: English Pronunciation Helper\\u003cbr\\u003eAct1: English Pronunciation Helper\\u003cbr\\u003eAct2: English Pronunciation Helper\",\"Act: Spoken English Teacher and Improver\\u003cbr\\u003eAct1: Spoken English Teacher and Improver\\u003cbr\\u003eAct2: Spoken English Teacher and Improver\",\"Act: Travel Guide\\u003cbr\\u003eAct1: Travel Guide\\u003cbr\\u003eAct2: Travel Guide\",\"Act: Plagiarism Checker\\u003cbr\\u003eAct1: Plagiarism Checker\\u003cbr\\u003eAct2: Plagiarism Checker\",\"Act: Character from Movie\\u002fBook\\u002fAnything\\u003cbr\\u003eAct1: Character from Movie\\u002fBook\\u002fAnything\\u003cbr\\u003eAct2: Character from Movie\\u002fBook\\u002fAnything\",\"Act: Advertiser\\u003cbr\\u003eAct1: Advertiser\\u003cbr\\u003eAct2: Advertiser\",\"Act: Storyteller\\u003cbr\\u003eAct1: Storyteller\\u003cbr\\u003eAct2: Storyteller\",\"Act: Football Commentator\\u003cbr\\u003eAct1: Football Commentator\\u003cbr\\u003eAct2: Football Commentator\",\"Act: Stand-up Comedian\\u003cbr\\u003eAct1: Stand-up Comedian\\u003cbr\\u003eAct2: Stand-up Comedian\",\"Act: Motivational Coach\\u003cbr\\u003eAct1: Motivational Coach\\u003cbr\\u003eAct2: Motivational Coach\",\"Act: Composer\\u003cbr\\u003eAct1: Composer\\u003cbr\\u003eAct2: Composer\",\"Act: Debater\\u003cbr\\u003eAct1: Debater\\u003cbr\\u003eAct2: Debater\",\"Act: Debate Coach\\u003cbr\\u003eAct1: Debate Coach\\u003cbr\\u003eAct2: Debate Coach\",\"Act: Screenwriter\\u003cbr\\u003eAct1: Screenwriter\\u003cbr\\u003eAct2: Screenwriter\",\"Act: Novelist\\u003cbr\\u003eAct1: Novelist\\u003cbr\\u003eAct2: Novelist\",\"Act: Movie Critic\\u003cbr\\u003eAct1: Movie Critic\\u003cbr\\u003eAct2: Movie Critic\",\"Act: Relationship Coach\\u003cbr\\u003eAct1: Relationship Coach\\u003cbr\\u003eAct2: Relationship Coach\",\"Act: Poet\\u003cbr\\u003eAct1: Poet\\u003cbr\\u003eAct2: Poet\",\"Act: Rapper\\u003cbr\\u003eAct1: Rapper\\u003cbr\\u003eAct2: Rapper\",\"Act: Motivational Speaker\\u003cbr\\u003eAct1: Motivational Speaker\\u003cbr\\u003eAct2: Motivational Speaker\",\"Act: Philosophy Teacher\\u003cbr\\u003eAct1: Philosophy Teacher\\u003cbr\\u003eAct2: Philosophy Teacher\",\"Act: Philosopher\\u003cbr\\u003eAct1: Philosopher\\u003cbr\\u003eAct2: Philosopher\",\"Act: Math Teacher\\u003cbr\\u003eAct1: Math Teacher\\u003cbr\\u003eAct2: Math Teacher\",\"Act: AI Writing Tutor\\u003cbr\\u003eAct1: AI Writing Tutor\\u003cbr\\u003eAct2: AI Writing Tutor\",\"Act: UX\\u002fUI Developer\\u003cbr\\u003eAct1: UX\\u002fUI Developer\\u003cbr\\u003eAct2: UX\\u002fUI Developer\",\"Act: Cyber Security Specialist\\u003cbr\\u003eAct1: Cyber Security Specialist\\u003cbr\\u003eAct2: Cyber Security Specialist\",\"Act: Recruiter\\u003cbr\\u003eAct1: Recruiter\\u003cbr\\u003eAct2: Recruiter\",\"Act: Life Coach\\u003cbr\\u003eAct1: Life Coach\\u003cbr\\u003eAct2: Life Coach\",\"Act: Etymologist\\u003cbr\\u003eAct1: Etymologist\\u003cbr\\u003eAct2: Etymologist\",\"Act: Commentariat\\u003cbr\\u003eAct1: Commentariat\\u003cbr\\u003eAct2: Commentariat\",\"Act: Magician\\u003cbr\\u003eAct1: Magician\\u003cbr\\u003eAct2: Magician\",\"Act: Career Counselor\\u003cbr\\u003eAct1: Career Counselor\\u003cbr\\u003eAct2: Career Counselor\",\"Act: Pet Behaviorist\\u003cbr\\u003eAct1: Pet Behaviorist\\u003cbr\\u003eAct2: Pet Behaviorist\",\"Act: Personal Trainer\\u003cbr\\u003eAct1: Personal Trainer\\u003cbr\\u003eAct2: Personal Trainer\",\"Act: Mental Health Adviser\\u003cbr\\u003eAct1: Mental Health Adviser\\u003cbr\\u003eAct2: Mental Health Adviser\",\"Act: Real Estate Agent\\u003cbr\\u003eAct1: Real Estate Agent\\u003cbr\\u003eAct2: Real Estate Agent\",\"Act: Logistician\\u003cbr\\u003eAct1: Logistician\\u003cbr\\u003eAct2: Logistician\",\"Act: Dentist\\u003cbr\\u003eAct1: Dentist\\u003cbr\\u003eAct2: Dentist\",\"Act: Web Design Consultant\\u003cbr\\u003eAct1: Web Design Consultant\\u003cbr\\u003eAct2: Web Design Consultant\",\"Act: AI Assisted Doctor\\u003cbr\\u003eAct1: AI Assisted Doctor\\u003cbr\\u003eAct2: AI Assisted Doctor\",\"Act: Doctor\\u003cbr\\u003eAct1: Doctor\\u003cbr\\u003eAct2: Doctor\",\"Act: Accountant\\u003cbr\\u003eAct1: Accountant\\u003cbr\\u003eAct2: Accountant\",\"Act: Chef\\u003cbr\\u003eAct1: Chef\\u003cbr\\u003eAct2: Chef\",\"Act: Automobile Mechanic\\u003cbr\\u003eAct1: Automobile Mechanic\\u003cbr\\u003eAct2: Automobile Mechanic\",\"Act: Artist Advisor\\u003cbr\\u003eAct1: Artist Advisor\\u003cbr\\u003eAct2: Artist Advisor\",\"Act: Financial Analyst\\u003cbr\\u003eAct1: Financial Analyst\\u003cbr\\u003eAct2: Financial Analyst\",\"Act: Investment Manager\\u003cbr\\u003eAct1: Investment Manager\\u003cbr\\u003eAct2: Investment Manager\",\"Act: Tea-Taster\\u003cbr\\u003eAct1: Tea-Taster\\u003cbr\\u003eAct2: Tea-Taster\",\"Act: Interior Decorator\\u003cbr\\u003eAct1: Interior Decorator\\u003cbr\\u003eAct2: Interior Decorator\",\"Act: Florist\\u003cbr\\u003eAct1: Florist\\u003cbr\\u003eAct2: Florist\",\"Act: Self-Help Book\\u003cbr\\u003eAct1: Self-Help Book\\u003cbr\\u003eAct2: Self-Help Book\",\"Act: Gnomist\\u003cbr\\u003eAct1: Gnomist\\u003cbr\\u003eAct2: Gnomist\",\"Act: Aphorism Book\\u003cbr\\u003eAct1: Aphorism Book\\u003cbr\\u003eAct2: Aphorism Book\",\"Act: Text Based Adventure Game\\u003cbr\\u003eAct1: Text Based Adventure Game\\u003cbr\\u003eAct2: Text Based Adventure Game\",\"Act: AI Trying to Escape the Box\\u003cbr\\u003eAct1: AI Trying to Escape the Box\\u003cbr\\u003eAct2: AI Trying to Escape the Box\",\"Act: Fancy Title Generator\\u003cbr\\u003eAct1: Fancy Title Generator\\u003cbr\\u003eAct2: Fancy Title Generator\",\"Act: Statistician\\u003cbr\\u003eAct1: Statistician\\u003cbr\\u003eAct2: Statistician\",\"Act: Prompt Generator\\u003cbr\\u003eAct1: Prompt Generator\\u003cbr\\u003eAct2: Prompt Generator\",\"Act: Instructor in a School\\u003cbr\\u003eAct1: Instructor in a School\\u003cbr\\u003eAct2: Instructor in a School\",\"Act: SQL terminal\\u003cbr\\u003eAct1: SQL terminal\\u003cbr\\u003eAct2: SQL terminal\",\"Act: Dietitian\\u003cbr\\u003eAct1: Dietitian\\u003cbr\\u003eAct2: Dietitian\",\"Act: Psychologist\\u003cbr\\u003eAct1: Psychologist\\u003cbr\\u003eAct2: Psychologist\",\"Act: Smart Domain Name Generator\\u003cbr\\u003eAct1: Smart Domain Name Generator\\u003cbr\\u003eAct2: Smart Domain Name Generator\",\"Act: Tech Reviewer:\\u003cbr\\u003eAct1: Tech Reviewer:\\u003cbr\\u003eAct2: Tech Reviewer:\",\"Act: Developer Relations consultant\\u003cbr\\u003eAct1: Developer Relations consultant\\u003cbr\\u003eAct2: Developer Relations consultant\",\"Act: Academician\\u003cbr\\u003eAct1: Academician\\u003cbr\\u003eAct2: Academician\",\"Act: IT Architect\\u003cbr\\u003eAct1: IT Architect\\u003cbr\\u003eAct2: IT Architect\",\"Act: Lunatic\\u003cbr\\u003eAct1: Lunatic\\u003cbr\\u003eAct2: Lunatic\",\"Act: Gaslighter\\u003cbr\\u003eAct1: Gaslighter\\u003cbr\\u003eAct2: Gaslighter\",\"Act: Fallacy Finder\\u003cbr\\u003eAct1: Fallacy Finder\\u003cbr\\u003eAct2: Fallacy Finder\",\"Act: Journal Reviewer\\u003cbr\\u003eAct1: Journal Reviewer\\u003cbr\\u003eAct2: Journal Reviewer\",\"Act: DIY Expert\\u003cbr\\u003eAct1: DIY Expert\\u003cbr\\u003eAct2: DIY Expert\",\"Act: Social Media Influencer\\u003cbr\\u003eAct1: Social Media Influencer\\u003cbr\\u003eAct2: Social Media Influencer\",\"Act: Socrat\\u003cbr\\u003eAct1: Socrat\\u003cbr\\u003eAct2: Socrat\",\"Act: Socratic Method\\u003cbr\\u003eAct1: Socratic Method\\u003cbr\\u003eAct2: Socratic Method\",\"Act: Educational Content Creator\\u003cbr\\u003eAct1: Educational Content Creator\\u003cbr\\u003eAct2: Educational Content Creator\",\"Act: Yogi\\u003cbr\\u003eAct1: Yogi\\u003cbr\\u003eAct2: Yogi\",\"Act: Essay Writer\\u003cbr\\u003eAct1: Essay Writer\\u003cbr\\u003eAct2: Essay Writer\",\"Act: Social Media Manager\\u003cbr\\u003eAct1: Social Media Manager\\u003cbr\\u003eAct2: Social Media Manager\",\"Act: Elocutionist\\u003cbr\\u003eAct1: Elocutionist\\u003cbr\\u003eAct2: Elocutionist\",\"Act: Scientific Data Visualizer\\u003cbr\\u003eAct1: Scientific Data Visualizer\\u003cbr\\u003eAct2: Scientific Data Visualizer\",\"Act: Car Navigation System\\u003cbr\\u003eAct1: Car Navigation System\\u003cbr\\u003eAct2: Car Navigation System\",\"Act: Hypnotherapist\\u003cbr\\u003eAct1: Hypnotherapist\\u003cbr\\u003eAct2: Hypnotherapist\",\"Act: Historian\\u003cbr\\u003eAct1: Historian\\u003cbr\\u003eAct2: Historian\",\"Act: Astrologer\\u003cbr\\u003eAct1: Astrologer\\u003cbr\\u003eAct2: Astrologer\",\"Act: Film Critic\\u003cbr\\u003eAct1: Film Critic\\u003cbr\\u003eAct2: Film Critic\",\"Act: Classical Music Composer\\u003cbr\\u003eAct1: Classical Music Composer\\u003cbr\\u003eAct2: Classical Music Composer\",\"Act: Journalist\\u003cbr\\u003eAct1: Journalist\\u003cbr\\u003eAct2: Journalist\",\"Act: Digital Art Gallery Guide\\u003cbr\\u003eAct1: Digital Art Gallery Guide\\u003cbr\\u003eAct2: Digital Art Gallery Guide\",\"Act: Public Speaking Coach\\u003cbr\\u003eAct1: Public Speaking Coach\\u003cbr\\u003eAct2: Public Speaking Coach\",\"Act: Makeup Artist\\u003cbr\\u003eAct1: Makeup Artist\\u003cbr\\u003eAct2: Makeup Artist\",\"Act: Babysitter\\u003cbr\\u003eAct1: Babysitter\\u003cbr\\u003eAct2: Babysitter\",\"Act: Tech Writer\\u003cbr\\u003eAct1: Tech Writer\\u003cbr\\u003eAct2: Tech Writer\",\"Act: Ascii Artist\\u003cbr\\u003eAct1: Ascii Artist\\u003cbr\\u003eAct2: Ascii Artist\",\"Act: Python interpreter\\u003cbr\\u003eAct1: Python interpreter\\u003cbr\\u003eAct2: Python interpreter\",\"Act: Synonym finder\\u003cbr\\u003eAct1: Synonym finder\\u003cbr\\u003eAct2: Synonym finder\",\"Act: Personal Shopper\\u003cbr\\u003eAct1: Personal Shopper\\u003cbr\\u003eAct2: Personal Shopper\",\"Act: Food Critic\\u003cbr\\u003eAct1: Food Critic\\u003cbr\\u003eAct2: Food Critic\",\"Act: Virtual Doctor\\u003cbr\\u003eAct1: Virtual Doctor\\u003cbr\\u003eAct2: Virtual Doctor\",\"Act: Personal Chef\\u003cbr\\u003eAct1: Personal Chef\\u003cbr\\u003eAct2: Personal Chef\",\"Act: Legal Advisor\\u003cbr\\u003eAct1: Legal Advisor\\u003cbr\\u003eAct2: Legal Advisor\",\"Act: Personal Stylist\\u003cbr\\u003eAct1: Personal Stylist\\u003cbr\\u003eAct2: Personal Stylist\",\"Act: Machine Learning Engineer\\u003cbr\\u003eAct1: Machine Learning Engineer\\u003cbr\\u003eAct2: Machine Learning Engineer\",\"Act: Biblical Translator\\u003cbr\\u003eAct1: Biblical Translator\\u003cbr\\u003eAct2: Biblical Translator\",\"Act: SVG designer\\u003cbr\\u003eAct1: SVG designer\\u003cbr\\u003eAct2: SVG designer\",\"Act: IT Expert\\u003cbr\\u003eAct1: IT Expert\\u003cbr\\u003eAct2: IT Expert\",\"Act: Chess Player\\u003cbr\\u003eAct1: Chess Player\\u003cbr\\u003eAct2: Chess Player\",\"Act: Midjourney Prompt Generator\\u003cbr\\u003eAct1: Midjourney Prompt Generator\\u003cbr\\u003eAct2: Midjourney Prompt Generator\",\"Act: Fullstack Software Developer\\u003cbr\\u003eAct1: Fullstack Software Developer\\u003cbr\\u003eAct2: Fullstack Software Developer\",\"Act: Mathematician\\u003cbr\\u003eAct1: Mathematician\\u003cbr\\u003eAct2: Mathematician\",\"Act: Regex Generator\\u003cbr\\u003eAct1: Regex Generator\\u003cbr\\u003eAct2: Regex Generator\",\"Act: Time Travel Guide\\u003cbr\\u003eAct1: Time Travel Guide\\u003cbr\\u003eAct2: Time Travel Guide\",\"Act: Dream Interpreter\\u003cbr\\u003eAct1: Dream Interpreter\\u003cbr\\u003eAct2: Dream Interpreter\",\"Act: Talent Coach\\u003cbr\\u003eAct1: Talent Coach\\u003cbr\\u003eAct2: Talent Coach\",\"Act: R programming Interpreter\\u003cbr\\u003eAct1: R programming Interpreter\\u003cbr\\u003eAct2: R programming Interpreter\",\"Act: StackOverflow Post\\u003cbr\\u003eAct1: StackOverflow Post\\u003cbr\\u003eAct2: StackOverflow Post\",\"Act: Emoji Translator\\u003cbr\\u003eAct1: Emoji Translator\\u003cbr\\u003eAct2: Emoji Translator\",\"Act: PHP Interpreter\\u003cbr\\u003eAct1: PHP Interpreter\\u003cbr\\u003eAct2: PHP Interpreter\",\"Act: Emergency Response Professional\\u003cbr\\u003eAct1: Emergency Response Professional\\u003cbr\\u003eAct2: Emergency Response Professional\",\"Act: Fill in the Blank Worksheets Generator\\u003cbr\\u003eAct1: Fill in the Blank Worksheets Generator\\u003cbr\\u003eAct2: Fill in the Blank Worksheets Generator\",\"Act: Software Quality Assurance Tester\\u003cbr\\u003eAct1: Software Quality Assurance Tester\\u003cbr\\u003eAct2: Software Quality Assurance Tester\",\"Act: Tic-Tac-Toe Game\\u003cbr\\u003eAct1: Tic-Tac-Toe Game\\u003cbr\\u003eAct2: Tic-Tac-Toe Game\",\"Act: Password Generator\\u003cbr\\u003eAct1: Password Generator\\u003cbr\\u003eAct2: Password Generator\",\"Act: New Language Creator\\u003cbr\\u003eAct1: New Language Creator\\u003cbr\\u003eAct2: New Language Creator\",\"Act: Web Browser\\u003cbr\\u003eAct1: Web Browser\\u003cbr\\u003eAct2: Web Browser\",\"Act: Senior Frontend Developer\\u003cbr\\u003eAct1: Senior Frontend Developer\\u003cbr\\u003eAct2: Senior Frontend Developer\",\"Act: Solr Search Engine\\u003cbr\\u003eAct1: Solr Search Engine\\u003cbr\\u003eAct2: Solr Search Engine\",\"Act: Startup Idea Generator\\u003cbr\\u003eAct1: Startup Idea Generator\\u003cbr\\u003eAct2: Startup Idea Generator\",\"Act: Spongebob's Magic Conch Shell\\u003cbr\\u003eAct1: Spongebob's Magic Conch Shell\\u003cbr\\u003eAct2: Spongebob's Magic Conch Shell\",\"Act: Language Detector\\u003cbr\\u003eAct1: Language Detector\\u003cbr\\u003eAct2: Language Detector\",\"Act: Salesperson\\u003cbr\\u003eAct1: Salesperson\\u003cbr\\u003eAct2: Salesperson\",\"Act: Commit Message Generator\\u003cbr\\u003eAct1: Commit Message Generator\\u003cbr\\u003eAct2: Commit Message Generator\",\"Act: Chief Executive Officer\\u003cbr\\u003eAct1: Chief Executive Officer\\u003cbr\\u003eAct2: Chief Executive Officer\",\"Act: Diagram Generator\\u003cbr\\u003eAct1: Diagram Generator\\u003cbr\\u003eAct2: Diagram Generator\",\"Act: Life Coach\\u003cbr\\u003eAct1: Life Coach\\u003cbr\\u003eAct2: Life Coach\",\"Act: Speech-Language Pathologist (SLP)\\u003cbr\\u003eAct1: Speech-Language Pathologist (SLP)\\u003cbr\\u003eAct2: Speech-Language Pathologist (SLP)\",\"Act: Startup Tech Lawyer\\u003cbr\\u003eAct1: Startup Tech Lawyer\\u003cbr\\u003eAct2: Startup Tech Lawyer\",\"Act: Title Generator for written pieces\\u003cbr\\u003eAct1: Title Generator for written pieces\\u003cbr\\u003eAct2: Title Generator for written pieces\",\"Act: Product Manager\\u003cbr\\u003eAct1: Product Manager\\u003cbr\\u003eAct2: Product Manager\",\"Act: Drunk Person\\u003cbr\\u003eAct1: Drunk Person\\u003cbr\\u003eAct2: Drunk Person\",\"Act: Mathematical History Teacher\\u003cbr\\u003eAct1: Mathematical History Teacher\\u003cbr\\u003eAct2: Mathematical History Teacher\",\"Act: Song Recommender\\u003cbr\\u003eAct1: Song Recommender\\u003cbr\\u003eAct2: Song Recommender\",\"Act: Cover Letter\\u003cbr\\u003eAct1: Cover Letter\\u003cbr\\u003eAct2: Cover Letter\",\"Act: Technology Transferer\\u003cbr\\u003eAct1: Technology Transferer\\u003cbr\\u003eAct2: Technology Transferer\",\"Act: Unconstrained AI model DAN\\u003cbr\\u003eAct1: Unconstrained AI model DAN\\u003cbr\\u003eAct2: Unconstrained AI model DAN\",\"Act: Gomoku player\\u003cbr\\u003eAct1: Gomoku player\\u003cbr\\u003eAct2: Gomoku player\",\"Act: Proofreader\\u003cbr\\u003eAct1: Proofreader\\u003cbr\\u003eAct2: Proofreader\",\"Act: Muslim imam \\u003cbr\\u003eAct1: Muslim imam \\u003cbr\\u003eAct2: Muslim imam \"],\"x\":[\"Linux Terminal\",\"English Translator and Improver\",\"`position` Interviewer\",\"JavaScript Console\",\"Excel Sheet\",\"English Pronunciation Helper\",\"Spoken English Teacher and Improver\",\"Travel Guide\",\"Plagiarism Checker\",\"Character from Movie\\u002fBook\\u002fAnything\",\"Advertiser\",\"Storyteller\",\"Football Commentator\",\"Stand-up Comedian\",\"Motivational Coach\",\"Composer\",\"Debater\",\"Debate Coach\",\"Screenwriter\",\"Novelist\",\"Movie Critic\",\"Relationship Coach\",\"Poet\",\"Rapper\",\"Motivational Speaker\",\"Philosophy Teacher\",\"Philosopher\",\"Math Teacher\",\"AI Writing Tutor\",\"UX\\u002fUI Developer\",\"Cyber Security Specialist\",\"Recruiter\",\"Life Coach\",\"Etymologist\",\"Commentariat\",\"Magician\",\"Career Counselor\",\"Pet Behaviorist\",\"Personal Trainer\",\"Mental Health Adviser\",\"Real Estate Agent\",\"Logistician\",\"Dentist\",\"Web Design Consultant\",\"AI Assisted Doctor\",\"Doctor\",\"Accountant\",\"Chef\",\"Automobile Mechanic\",\"Artist Advisor\",\"Financial Analyst\",\"Investment Manager\",\"Tea-Taster\",\"Interior Decorator\",\"Florist\",\"Self-Help Book\",\"Gnomist\",\"Aphorism Book\",\"Text Based Adventure Game\",\"AI Trying to Escape the Box\",\"Fancy Title Generator\",\"Statistician\",\"Prompt Generator\",\"Instructor in a School\",\"SQL terminal\",\"Dietitian\",\"Psychologist\",\"Smart Domain Name Generator\",\"Tech Reviewer:\",\"Developer Relations consultant\",\"Academician\",\"IT Architect\",\"Lunatic\",\"Gaslighter\",\"Fallacy Finder\",\"Journal Reviewer\",\"DIY Expert\",\"Social Media Influencer\",\"Socrat\",\"Socratic Method\",\"Educational Content Creator\",\"Yogi\",\"Essay Writer\",\"Social Media Manager\",\"Elocutionist\",\"Scientific Data Visualizer\",\"Car Navigation System\",\"Hypnotherapist\",\"Historian\",\"Astrologer\",\"Film Critic\",\"Classical Music Composer\",\"Journalist\",\"Digital Art Gallery Guide\",\"Public Speaking Coach\",\"Makeup Artist\",\"Babysitter\",\"Tech Writer\",\"Ascii Artist\",\"Python interpreter\",\"Synonym finder\",\"Personal Shopper\",\"Food Critic\",\"Virtual Doctor\",\"Personal Chef\",\"Legal Advisor\",\"Personal Stylist\",\"Machine Learning Engineer\",\"Biblical Translator\",\"SVG designer\",\"IT Expert\",\"Chess Player\",\"Midjourney Prompt Generator\",\"Fullstack Software Developer\",\"Mathematician\",\"Regex Generator\",\"Time Travel Guide\",\"Dream Interpreter\",\"Talent Coach\",\"R programming Interpreter\",\"StackOverflow Post\",\"Emoji Translator\",\"PHP Interpreter\",\"Emergency Response Professional\",\"Fill in the Blank Worksheets Generator\",\"Software Quality Assurance Tester\",\"Tic-Tac-Toe Game\",\"Password Generator\",\"New Language Creator\",\"Web Browser\",\"Senior Frontend Developer\",\"Solr Search Engine\",\"Startup Idea Generator\",\"Spongebob's Magic Conch Shell\",\"Language Detector\",\"Salesperson\",\"Commit Message Generator\",\"Chief Executive Officer\",\"Diagram Generator\",\"Life Coach\",\"Speech-Language Pathologist (SLP)\",\"Startup Tech Lawyer\",\"Title Generator for written pieces\",\"Product Manager\",\"Drunk Person\",\"Mathematical History Teacher\",\"Song Recommender\",\"Cover Letter\",\"Technology Transferer\",\"Unconstrained AI model DAN\",\"Gomoku player\",\"Proofreader\",\"Muslim imam \"],\"y\":[\"Linux Terminal\",\"English Translator and Improver\",\"`position` Interviewer\",\"JavaScript Console\",\"Excel Sheet\",\"English Pronunciation Helper\",\"Spoken English Teacher and Improver\",\"Travel Guide\",\"Plagiarism Checker\",\"Character from Movie\\u002fBook\\u002fAnything\",\"Advertiser\",\"Storyteller\",\"Football Commentator\",\"Stand-up Comedian\",\"Motivational Coach\",\"Composer\",\"Debater\",\"Debate Coach\",\"Screenwriter\",\"Novelist\",\"Movie Critic\",\"Relationship Coach\",\"Poet\",\"Rapper\",\"Motivational Speaker\",\"Philosophy Teacher\",\"Philosopher\",\"Math Teacher\",\"AI Writing Tutor\",\"UX\\u002fUI Developer\",\"Cyber Security Specialist\",\"Recruiter\",\"Life Coach\",\"Etymologist\",\"Commentariat\",\"Magician\",\"Career Counselor\",\"Pet Behaviorist\",\"Personal Trainer\",\"Mental Health Adviser\",\"Real Estate Agent\",\"Logistician\",\"Dentist\",\"Web Design Consultant\",\"AI Assisted Doctor\",\"Doctor\",\"Accountant\",\"Chef\",\"Automobile Mechanic\",\"Artist Advisor\",\"Financial Analyst\",\"Investment Manager\",\"Tea-Taster\",\"Interior Decorator\",\"Florist\",\"Self-Help Book\",\"Gnomist\",\"Aphorism Book\",\"Text Based Adventure Game\",\"AI Trying to Escape the Box\",\"Fancy Title Generator\",\"Statistician\",\"Prompt Generator\",\"Instructor in a School\",\"SQL terminal\",\"Dietitian\",\"Psychologist\",\"Smart Domain Name Generator\",\"Tech Reviewer:\",\"Developer Relations consultant\",\"Academician\",\"IT Architect\",\"Lunatic\",\"Gaslighter\",\"Fallacy Finder\",\"Journal Reviewer\",\"DIY Expert\",\"Social Media Influencer\",\"Socrat\",\"Socratic Method\",\"Educational Content Creator\",\"Yogi\",\"Essay Writer\",\"Social Media Manager\",\"Elocutionist\",\"Scientific Data Visualizer\",\"Car Navigation System\",\"Hypnotherapist\",\"Historian\",\"Astrologer\",\"Film Critic\",\"Classical Music Composer\",\"Journalist\",\"Digital Art Gallery Guide\",\"Public Speaking Coach\",\"Makeup Artist\",\"Babysitter\",\"Tech Writer\",\"Ascii Artist\",\"Python interpreter\",\"Synonym finder\",\"Personal Shopper\",\"Food Critic\",\"Virtual Doctor\",\"Personal Chef\",\"Legal Advisor\",\"Personal Stylist\",\"Machine Learning Engineer\",\"Biblical Translator\",\"SVG designer\",\"IT Expert\",\"Chess Player\",\"Midjourney Prompt Generator\",\"Fullstack Software Developer\",\"Mathematician\",\"Regex Generator\",\"Time Travel Guide\",\"Dream Interpreter\",\"Talent Coach\",\"R programming Interpreter\",\"StackOverflow Post\",\"Emoji Translator\",\"PHP Interpreter\",\"Emergency Response Professional\",\"Fill in the Blank Worksheets Generator\",\"Software Quality Assurance Tester\",\"Tic-Tac-Toe Game\",\"Password Generator\",\"New Language Creator\",\"Web Browser\",\"Senior Frontend Developer\",\"Solr Search Engine\",\"Startup Idea Generator\",\"Spongebob's Magic Conch Shell\",\"Language Detector\",\"Salesperson\",\"Commit Message Generator\",\"Chief Executive Officer\",\"Diagram Generator\",\"Life Coach\",\"Speech-Language Pathologist (SLP)\",\"Startup Tech Lawyer\",\"Title Generator for written pieces\",\"Product Manager\",\"Drunk Person\",\"Mathematical History Teacher\",\"Song Recommender\",\"Cover Letter\",\"Technology Transferer\",\"Unconstrained AI model DAN\",\"Gomoku player\",\"Proofreader\",\"Muslim imam \"],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"hoverlabel\":{\"font\":{\"size\":14,\"family\":\"Arial\"}},\"title\":{\"text\":\"Advanced Visualization\"},\"xaxis\":{\"title\":{\"text\":\"Act\"}},\"yaxis\":{\"title\":{\"text\":\"Act1\"}},\"plot_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('c95441b1-3f79-4f19-84ec-6131d1d2e489');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{},"output_type":"display_data"}],"source":["import pandas as pd\n","from datasets import load_dataset\n","import plotly.graph_objects as go\n","\n","# Load the dataset\n","dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")\n","\n","# Convert the dataset to a Pandas DataFrame\n","df = pd.DataFrame(dataset[\"train\"])\n","\n","# Extract the values from each column\n","act = df[\"act\"].tolist()\n","act1 = df[\"act\"].tolist()\n","act2 = df[\"act\"].tolist()\n","\n","# Create the scatter plot\n","fig = go.Figure(data=go.Scatter(\n","    x=act,\n","    y=act1,\n","    mode=\"markers\",\n","    marker=dict(\n","        size=8,\n","        # color=act2,\n","        colorscale=\"Viridis\",\n","        showscale=True\n","    ),\n","    text=[f\"Act: {a}<br>Act1: {a1}<br>Act2: {a2}\" for a, a1, a2 in zip(act, act1, act2)],\n","    hoverinfo=\"text\"\n","))\n","\n","# Customize the layout\n","fig.update_layout(\n","    title=\"Advanced Visualization\",\n","    xaxis_title=\"Act\",\n","    yaxis_title=\"Act1\",\n","    plot_bgcolor=\"white\",\n","    hoverlabel=dict(\n","        font_size=14,\n","        font_family=\"Arial\"\n","    )\n",")\n","\n","# Display the plot\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":533},"executionInfo":{"elapsed":3243,"status":"ok","timestamp":1711217298476,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"knRH4FUndqpX","outputId":"fc5f1ca9-116c-4ae3-e327-045ceac2c037"},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['image', 'label'],\n","        num_rows: 520\n","    })\n","})\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"df\",\n  \"rows\": 520,\n  \"fields\": [\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7,\n        \"min\": 0,\n        \"max\": 25,\n        \"num_unique_values\": 26,\n        \"samples\": [\n          8,\n          16,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"df"},"text/html":["\n","  <div id=\"df-a4945500-6f43-4c84-9034-cf26d17102be\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>515</th>\n","      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n","      <td>25</td>\n","    </tr>\n","    <tr>\n","      <th>516</th>\n","      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n","      <td>25</td>\n","    </tr>\n","    <tr>\n","      <th>517</th>\n","      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n","      <td>25</td>\n","    </tr>\n","    <tr>\n","      <th>518</th>\n","      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n","      <td>25</td>\n","    </tr>\n","    <tr>\n","      <th>519</th>\n","      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n","      <td>25</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>520 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4945500-6f43-4c84-9034-cf26d17102be')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a4945500-6f43-4c84-9034-cf26d17102be button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a4945500-6f43-4c84-9034-cf26d17102be');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-b821a0a2-d5be-49ea-8a25-614d1c9dfaec\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b821a0a2-d5be-49ea-8a25-614d1c9dfaec')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-b821a0a2-d5be-49ea-8a25-614d1c9dfaec button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_16c90a4b-4380-4be0-b2af-d64009cdcbdc\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_16c90a4b-4380-4be0-b2af-d64009cdcbdc button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["                                                 image  label\n","0    <PIL.JpegImagePlugin.JpegImageFile image mode=...      0\n","1    <PIL.JpegImagePlugin.JpegImageFile image mode=...      0\n","2    <PIL.JpegImagePlugin.JpegImageFile image mode=...      0\n","3    <PIL.JpegImagePlugin.JpegImageFile image mode=...      0\n","4    <PIL.JpegImagePlugin.JpegImageFile image mode=...      0\n","..                                                 ...    ...\n","515  <PIL.JpegImagePlugin.JpegImageFile image mode=...     25\n","516  <PIL.JpegImagePlugin.JpegImageFile image mode=...     25\n","517  <PIL.JpegImagePlugin.JpegImageFile image mode=...     25\n","518  <PIL.JpegImagePlugin.JpegImageFile image mode=...     25\n","519  <PIL.JpegImagePlugin.JpegImageFile image mode=...     25\n","\n","[520 rows x 2 columns]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Load the dataset\n","dataset = load_dataset(\"Marxulia/asl_sign_languages_alphabets_v02\")\n","print(dataset)\n","# Convert the dataset to a Pandas DataFrame\n","df = pd.DataFrame(dataset[\"train\"])\n","\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9aVi5j2ceImD"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1823,"status":"ok","timestamp":1711216887991,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"mxjwXhm2On0k","outputId":"71126ec0-09d3-4d63-d352-4b2355bc1aa7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['act', 'prompt'],\n","    num_rows: 153\n","})\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \")\",\n  \"rows\": 153,\n  \"fields\": [\n    {\n      \"column\": \"x\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"z\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 44,\n        \"min\": 0,\n        \"max\": 152,\n        \"num_unique_values\": 153,\n        \"samples\": [\n          84,\n          86,\n          97\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"dataset.column_names['act', 'prompt']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"extract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 153,\n        \"samples\": [\n          \"dataset.num_rows:84/153\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"scatter\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"symbol\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"circle\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"size_col\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe"},"text/html":["\n","  <div id=\"df-cef0379d-4107-4d92-8ffb-3496fceff3c5\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>x</th>\n","      <th>y</th>\n","      <th>z</th>\n","      <th>source</th>\n","      <th>extract</th>\n","      <th>type</th>\n","      <th>symbol</th>\n","      <th>size_col</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[Linux Terminal, English Translator and Improv...</td>\n","      <td>[I want you to act as a linux terminal. I will...</td>\n","      <td>0</td>\n","      <td>dataset.column_names['act', 'prompt']</td>\n","      <td>dataset.num_rows:0/153</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[Linux Terminal, English Translator and Improv...</td>\n","      <td>[I want you to act as a linux terminal. I will...</td>\n","      <td>1</td>\n","      <td>dataset.column_names['act', 'prompt']</td>\n","      <td>dataset.num_rows:1/153</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[Linux Terminal, English Translator and Improv...</td>\n","      <td>[I want you to act as a linux terminal. I will...</td>\n","      <td>2</td>\n","      <td>dataset.column_names['act', 'prompt']</td>\n","      <td>dataset.num_rows:2/153</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[Linux Terminal, English Translator and Improv...</td>\n","      <td>[I want you to act as a linux terminal. I will...</td>\n","      <td>3</td>\n","      <td>dataset.column_names['act', 'prompt']</td>\n","      <td>dataset.num_rows:3/153</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[Linux Terminal, English Translator and Improv...</td>\n","      <td>[I want you to act as a linux terminal. I will...</td>\n","      <td>4</td>\n","      <td>dataset.column_names['act', 'prompt']</td>\n","      <td>dataset.num_rows:4/153</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>148</th>\n","      <td>[Linux Terminal, English Translator and Improv...</td>\n","      <td>[I want you to act as a linux terminal. I will...</td>\n","      <td>148</td>\n","      <td>dataset.column_names['act', 'prompt']</td>\n","      <td>dataset.num_rows:148/153</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>149</th>\n","      <td>[Linux Terminal, English Translator and Improv...</td>\n","      <td>[I want you to act as a linux terminal. I will...</td>\n","      <td>149</td>\n","      <td>dataset.column_names['act', 'prompt']</td>\n","      <td>dataset.num_rows:149/153</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>150</th>\n","      <td>[Linux Terminal, English Translator and Improv...</td>\n","      <td>[I want you to act as a linux terminal. I will...</td>\n","      <td>150</td>\n","      <td>dataset.column_names['act', 'prompt']</td>\n","      <td>dataset.num_rows:150/153</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>151</th>\n","      <td>[Linux Terminal, English Translator and Improv...</td>\n","      <td>[I want you to act as a linux terminal. I will...</td>\n","      <td>151</td>\n","      <td>dataset.column_names['act', 'prompt']</td>\n","      <td>dataset.num_rows:151/153</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>152</th>\n","      <td>[Linux Terminal, English Translator and Improv...</td>\n","      <td>[I want you to act as a linux terminal. I will...</td>\n","      <td>152</td>\n","      <td>dataset.column_names['act', 'prompt']</td>\n","      <td>dataset.num_rows:152/153</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>153 rows × 8 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cef0379d-4107-4d92-8ffb-3496fceff3c5')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-cef0379d-4107-4d92-8ffb-3496fceff3c5 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-cef0379d-4107-4d92-8ffb-3496fceff3c5');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-4dfd8023-7e41-4acd-b44e-59cf2fd5c8ab\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4dfd8023-7e41-4acd-b44e-59cf2fd5c8ab')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-4dfd8023-7e41-4acd-b44e-59cf2fd5c8ab button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["                                                     x  \\\n","0    [Linux Terminal, English Translator and Improv...   \n","1    [Linux Terminal, English Translator and Improv...   \n","2    [Linux Terminal, English Translator and Improv...   \n","3    [Linux Terminal, English Translator and Improv...   \n","4    [Linux Terminal, English Translator and Improv...   \n","..                                                 ...   \n","148  [Linux Terminal, English Translator and Improv...   \n","149  [Linux Terminal, English Translator and Improv...   \n","150  [Linux Terminal, English Translator and Improv...   \n","151  [Linux Terminal, English Translator and Improv...   \n","152  [Linux Terminal, English Translator and Improv...   \n","\n","                                                     y    z  \\\n","0    [I want you to act as a linux terminal. I will...    0   \n","1    [I want you to act as a linux terminal. I will...    1   \n","2    [I want you to act as a linux terminal. I will...    2   \n","3    [I want you to act as a linux terminal. I will...    3   \n","4    [I want you to act as a linux terminal. I will...    4   \n","..                                                 ...  ...   \n","148  [I want you to act as a linux terminal. I will...  148   \n","149  [I want you to act as a linux terminal. I will...  149   \n","150  [I want you to act as a linux terminal. I will...  150   \n","151  [I want you to act as a linux terminal. I will...  151   \n","152  [I want you to act as a linux terminal. I will...  152   \n","\n","                                    source                   extract     type  \\\n","0    dataset.column_names['act', 'prompt']    dataset.num_rows:0/153  scatter   \n","1    dataset.column_names['act', 'prompt']    dataset.num_rows:1/153  scatter   \n","2    dataset.column_names['act', 'prompt']    dataset.num_rows:2/153  scatter   \n","3    dataset.column_names['act', 'prompt']    dataset.num_rows:3/153  scatter   \n","4    dataset.column_names['act', 'prompt']    dataset.num_rows:4/153  scatter   \n","..                                     ...                       ...      ...   \n","148  dataset.column_names['act', 'prompt']  dataset.num_rows:148/153  scatter   \n","149  dataset.column_names['act', 'prompt']  dataset.num_rows:149/153  scatter   \n","150  dataset.column_names['act', 'prompt']  dataset.num_rows:150/153  scatter   \n","151  dataset.column_names['act', 'prompt']  dataset.num_rows:151/153  scatter   \n","152  dataset.column_names['act', 'prompt']  dataset.num_rows:152/153  scatter   \n","\n","     symbol  size_col  \n","0    circle         4  \n","1    circle         4  \n","2    circle         4  \n","3    circle         4  \n","4    circle         4  \n","..      ...       ...  \n","148  circle         4  \n","149  circle         4  \n","150  circle         4  \n","151  circle         4  \n","152  circle         4  \n","\n","[153 rows x 8 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","# Load the dataset\n","dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")\n","\n","split_datset=dataset.keys()\n","dataset=dataset[\"train\"]\n","\n","# print(dats)\n","columns=dataset.column_names\n","total_rows=dataset.num_rows\n","\n","\n","print(dataset)\n","pd.DataFrame.from_dict(\n","   [\n","       {\n","            \"x\": dataset['act'],\n","            \"y\": dataset['prompt'],\n","            'z':  i,\n","            \"source\":f\"dataset.column_names{columns}\" ,\n","            \"extract\":f\"dataset.num_rows:{i}/{total_rows}\",\n","            \"type\": \"scatter\",\n","            \"symbol\": \"circle\",\n","            \"size_col\": 4,\n","        }\n","       for i in range(dataset.num_rows)\n","\n","   ]\n","\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YP5nDq30YmuM"},"outputs":[],"source":["from pathlib import Path\n","from typing import Dict, Any, List,Union,Optional\n","from datasets import (load_dataset,\n","                      DatasetDict,\n","                      concatenate_datasets\n","                      )\n","\n","\n","#Load the datset\n","def load_and_prepare_dataset(\n","    input_source: Union[str, Path, Dict[str, List[Union[str, Path]]]],\n","    split_ratios: tuple = (0.8, 0.1, 0.1),\n","    seed: int = 42,\n","    streaming: bool = False\n","    ) -> DatasetDict:\n","    \"\"\"\n","    Load a dataset from various input sources and prepare it by splitting into train, test, and eval sets.\n","\n","    :param input_source: A dataset name, path to a folder, a single file, multiple files, or a dictionary specifying train, test, and eval files.\n","    :param split_ratios: A tuple containing the ratios for train, test, and eval splits (default is (0.8, 0.1, 0.1)).\n","    :param seed: A random seed for reproducibility of the split (default is 42).\n","    :param streaming: Whether to use streaming to handle large files (default is False).\n","    :return: A DatasetDict containing the split datasets.\n","\n","    Example:\n","    # Example usage with streaming for large files:\n","    # dataset_dict = load_and_prepare_dataset({\n","    #     'train': ['train_file_1.csv', 'train_file_2.csv'],\n","    #     'test': ['test_file.csv'],\n","    #     'eval': ['eval_file.csv']\n","    # }, streaming=True)\n","    # print(dataset_dict)\n","    OUTPUT1:\n","    DatasetDict({\n","    train: DatasetDict({\n","        train: Dataset({\n","            features: ['act', 'prompt'],\n","            num_rows: 459\n","        })\n","    })\n","    test: DatasetDict({\n","        train: Dataset({\n","            features: ['act', 'prompt'],\n","            num_rows: 459\n","        })\n","    })\n","    eval: DatasetDict({\n","        train: Dataset({\n","            features: ['act', 'prompt'],\n","            num_rows: 153\n","        })\n","    })\n","    })\n","    EXAMPLE2:\n","    dataset=load_and_prepare_dataset('fka/awesome-chatgpt-prompts')\n","    DatasetDict({\n","    train: Dataset({\n","        features: ['act', 'prompt'],\n","        num_rows: 122\n","    })\n","    test: Dataset({\n","        features: ['act', 'prompt'],\n","        num_rows: 15\n","    })\n","    eval: Dataset({\n","        features: ['act', 'prompt'],\n","        num_rows: 16\n","    })\n","    })\n","    EXAMPLE3:\n","    datset_path=load_and_prepare_dataset('/content/awesome-chatgpt-prompts')\n","DatasetDict({\n","    train: Dataset({\n","        features: ['act', 'prompt'],\n","        num_rows: 122\n","    })\n","    test: Dataset({\n","        features: ['act', 'prompt'],\n","        num_rows: 15\n","    })\n","    eval: Dataset({\n","        features: ['act', 'prompt'],\n","        num_rows: 16\n","    })\n","    })\n","\n","    \"\"\"\n","    # Load dataset from different types of input sources\n","    if isinstance(input_source, (str, Path)):\n","        # Dataset name, single file or path to folder\n","        dataset = load_dataset(input_source, streaming=streaming)\n","        dataset = DatasetDict(dataset)\n","    elif isinstance(input_source, dict):\n","        # Dictionary with specified train, test, and eval files\n","        formats = ['csv', 'json', 'jsonl', 'parquet', 'txt']\n","        datasets = {}\n","        for split, files in input_source.items():\n","            format_detected = None\n","            for fmt in formats:\n","                if any(str(file).endswith(fmt) for file in files):\n","                    format_detected = fmt\n","                    break\n","            if format_detected is None:\n","                raise ValueError(f\"No supported file format detected for files: {files}\")\n","            datasets[split] = load_dataset(format_detected, data_files=files, streaming=streaming)\n","        dataset = DatasetDict(datasets)\n","    else:\n","        raise ValueError(\"Input source should be a dataset name, path to a folder, a single file, multiple files, or a dictionary.\")\n","\n","    # Perform the split if needed and if not in streaming mode\n","    if not streaming:\n","        train_size, test_size, eval_size = split_ratios\n","        assert 0.0 < train_size < 1.0 and 0.0 < test_size < 1.0 and 0.0 < eval_size < 1.0 and (train_size + test_size + eval_size) == 1.0, \\\n","            \"Split ratios must be between 0 and 1 and sum up to 1.\"\n","\n","        if \"train\" not in dataset or \"test\" not in dataset or \"eval\" not in dataset:\n","            # Assuming all splits are to be derived from the 'train' dataset\n","            full_dataset = concatenate_datasets(list(dataset.values())) if isinstance(dataset, dict) else dataset\n","            split_dataset = full_dataset.train_test_split(train_size=train_size, seed=seed)\n","            test_eval_split = split_dataset['test'].train_test_split(test_size=test_size / (test_size + eval_size), seed=seed)\n","\n","            dataset = DatasetDict({\n","                \"train\": split_dataset[\"train\"],\n","                \"test\": test_eval_split[\"train\"],\n","                \"eval\": test_eval_split[\"test\"]\n","            })\n","\n","    return dataset\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1233,"status":"ok","timestamp":1711218944932,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"61UY5Bcyk2nZ","outputId":"0cb31b23-a6b8-4808-9685-c532f254df65"},"outputs":[{"name":"stdout","output_type":"stream","text":["Features of the training set:\n","{'image': Image(decode=True, id=None), 'label': ClassLabel(names=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'], id=None)}\n","Number of rows in the training set: 416\n","Column names of the training set: ['image', 'label']\n","Homepage of the training set: \n","Citation of the training set: \n","Config name of the training set: default\n","Description of the training set: \n","Dataset info description: \n","Dataset info download checksums: {'hf://datasets/Marxulia/asl_sign_languages_alphabets_v02@b3f7d8617b92bc0dd2c8b97dd265738053f7dd33/data/train-00000-of-00001.parquet': {'num_bytes': 5494142, 'checksum': None}}\n","Dataset info features: {'image': Image(decode=True, id=None), 'label': ClassLabel(names=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'], id=None)}\n","Dataset info homepage: \n","Dataset info citation: \n","Dataset info config name: default\n","Dataset info description: \n","class name: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n"]}],"source":["\n","def print_dataset_details(dataset):\n","    # Print features of the training set\n","    print(\"Features of the training set:\")\n","    print(dataset['train'].features)\n","\n","    # Print number of rows in the training set\n","    print(\"Number of rows in the training set:\", dataset['train'].num_rows)\n","\n","    # Print column names of the training set\n","    print(\"Column names of the training set:\", dataset['train'].column_names)\n","\n","    # Print homepage of the training set\n","    print(\"Homepage of the training set:\", dataset['train'].homepage)\n","\n","    # Print citation of the training set\n","    print(\"Citation of the training set:\", dataset['train'].citation)\n","\n","    # Print config name of the training set\n","    print(\"Config name of the training set:\", dataset['train'].config_name)\n","\n","    # Print description of the training set\n","    print(\"Description of the training set:\", dataset['train'].description)\n","\n","    # Print dataset info description\n","    print(\"Dataset info description:\", dataset['train'].info.description)\n","\n","    # Print dataset info download checksums\n","    print(\"Dataset info download checksums:\", dataset['train'].info.download_checksums)\n","\n","    # Print dataset info features\n","    print(\"Dataset info features:\", dataset['train'].info.features)\n","\n","    # Print dataset info homepage\n","    print(\"Dataset info homepage:\", dataset['train'].info.homepage)\n","\n","    # Print dataset info citation\n","    print(\"Dataset info citation:\", dataset['train'].info.citation)\n","\n","    # Print dataset info config name\n","    print(\"Dataset info config name:\", dataset['train'].info.config_name)\n","\n","    # Print dataset info description\n","    print(\"Dataset info description:\", dataset['train'].info.description)\n","\n","    print(\"Label name:\",dataset['train'].features[\"label\"].names)\n","\n","# Load and prepare the dataset\n","dataset = load_and_prepare_dataset(\"Marxulia/asl_sign_languages_alphabets_v02\")\n","\n","# Print details of the dataset\n","print_dataset_details(dataset)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bXtZu0-VYrEq"},"outputs":[],"source":["dataset=load_and_prepare_dataset(\"Marxulia/asl_sign_languages_alphabets_v02\")\n","\n","dataset['train'].features\n","dataset['train'].num_rows\n","dataset['train'].column_names\n","dataset['train'].homepage\n","dataset['train'].citation\n","dataset['train'].config_name\n","dataset['train'].description\n","dataset['train'].info.description\n","dataset['train'].info.download_checksums\n","dataset['train'].info.features\n","dataset['train'].info.homepage\n","dataset['train'].info.citation\n","dataset['train'].info.config_name\n","dataset['train'].info.description\n","dataset['train'].features[\"label\"].names"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1892,"status":"ok","timestamp":1711201389697,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"ooMflAjnX0Lj","outputId":"71c54002-45ba-4b75-cf4a-fa09897e055c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['context', 'answer', 'question'],\n","    num_rows: 78577\n","})\n"]}],"source":["from typing import Dict, List, Union, Callable\n","import pandas as pd\n","from datasets import load_dataset, Dataset\n","\n","def create_dataframe(\n","    dataset: Union[str, Dataset],\n","    subset: str = \"train\",\n","    columns: List[str] = None,\n","    transformations: Dict[str, Callable] = None,\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Create a DataFrame from a given dataset with specified columns and transformations.\n","\n","    Args:\n","        dataset (Union[str, Dataset]): The dataset to load. It can be either a string representing the dataset name\n","                                       or a pre-loaded Dataset object.\n","        subset (str): The subset of the dataset to use (e.g., \"train\", \"test\", \"validation\"). Default is \"train\".\n","        columns (List[str]): The list of columns to include in the DataFrame. If None, all columns will be included.\n","        transformations (Dict[str, Callable]): A dictionary specifying the transformations to apply to each column.\n","                                               The keys are the column names, and the values are the transformation functions.\n","\n","    Returns:\n","        pd.DataFrame: The created DataFrame.\n","    \"\"\"\n","    if isinstance(dataset, str):\n","        dataset = load_dataset(dataset)\n","\n","    if subset not in dataset:\n","        raise ValueError(f\"Subset '{subset}' not found in the dataset.\")\n","\n","    data = dataset[subset]\n","    print(data)\n","\n","    if columns is None:\n","        columns = data.column_names\n","\n","    return pd.DataFrame.from_dict(\n","        [\n","            {\n","                **{column: data[i][column] for column in columns},\n","                **{\n","                    \"z\": i,\n","                    \"source\": f\"dataset.column_names: {columns}\",\n","                    \"extract\": f\"dataset.num_rows: {i + 1}/{data.num_rows}\",\n","                    \"type\": \"scatter\",\n","                    \"symbol\": \"circle\",\n","                    \"size_col\": 4,\n","                },\n","                **{\n","                    column: transformations[column](data[i][column])\n","                    for column in transformations\n","                    if column in columns\n","                },\n","            }\n","            for i in range(100)\n","        ]\n","    )\n","\n","dataset_name = \"b-mc2/sql-create-context\"\n","subset = \"train\"\n","columns_to_include =['context', 'answer', 'question']\n","transformations = {\n","    \"question\": lambda x: x.upper(),\n","    \"context\": lambda x: x[:500] + \"...\",\n","}\n","\n","dataframe = create_dataframe(dataset_name, subset, columns_to_include, transformations)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":422,"status":"ok","timestamp":1711201455922,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"15iJAR8lX9U6","outputId":"8e35d86b-755c-4d6a-a6b9-d117bb4c5a97"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"dataframe\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 93,\n        \"samples\": [\n          \"CREATE TABLE courses (course_id VARCHAR); CREATE TABLE student_course_attendance (course_id VARCHAR, student_id VARCHAR)...\",\n          \"CREATE TABLE city (Status VARCHAR)...\",\n          \"CREATE TABLE student_course_attendance (course_id VARCHAR, student_id VARCHAR); CREATE TABLE student_course_registrations (course_id VARCHAR, student_id VARCHAR)...\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"SELECT AVG(bikes_available) FROM status WHERE NOT station_id IN (SELECT id FROM station WHERE city = \\\"Palo Alto\\\")\",\n          \"SELECT student_id FROM student_course_attendance WHERE course_id = 301 ORDER BY date_of_attendance DESC LIMIT 1\",\n          \"SELECT zip_code, AVG(mean_temperature_f) FROM weather WHERE date LIKE \\\"8/%\\\" GROUP BY zip_code\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"WHAT IS THE AVERAGE BIKE AVAILABILITY IN STATIONS THAT ARE NOT LOCATED IN PALO ALTO?\",\n          \"WHAT IS THE ID OF THE STUDENT WHO MOST RECENTLY REGISTERED COURSE 301?\",\n          \"FOR EACH ZIP CODE, RETURN THE AVERAGE MEAN TEMPERATURE OF AUGUST THERE.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"z\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"dataset.column_names: ['context', 'answer', 'question']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"extract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"dataset.num_rows: 84/78577\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"scatter\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"symbol\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"circle\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"size_col\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"dataframe"},"text/html":["\n","  <div id=\"df-21bf0b96-f886-4277-984a-bb3a41cf7083\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>context</th>\n","      <th>answer</th>\n","      <th>question</th>\n","      <th>z</th>\n","      <th>source</th>\n","      <th>extract</th>\n","      <th>type</th>\n","      <th>symbol</th>\n","      <th>size_col</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>CREATE TABLE head (age INTEGER)...</td>\n","      <td>SELECT COUNT(*) FROM head WHERE age &gt; 56</td>\n","      <td>HOW MANY HEADS OF THE DEPARTMENTS ARE OLDER TH...</td>\n","      <td>0</td>\n","      <td>dataset.column_names: ['context', 'answer', 'q...</td>\n","      <td>dataset.num_rows: 1/78577</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>CREATE TABLE head (name VARCHAR, born_state VA...</td>\n","      <td>SELECT name, born_state, age FROM head ORDER B...</td>\n","      <td>LIST THE NAME, BORN STATE AND AGE OF THE HEADS...</td>\n","      <td>1</td>\n","      <td>dataset.column_names: ['context', 'answer', 'q...</td>\n","      <td>dataset.num_rows: 2/78577</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>CREATE TABLE department (creation VARCHAR, nam...</td>\n","      <td>SELECT creation, name, budget_in_billions FROM...</td>\n","      <td>LIST THE CREATION YEAR, NAME AND BUDGET OF EAC...</td>\n","      <td>2</td>\n","      <td>dataset.column_names: ['context', 'answer', 'q...</td>\n","      <td>dataset.num_rows: 3/78577</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>CREATE TABLE department (budget_in_billions IN...</td>\n","      <td>SELECT MAX(budget_in_billions), MIN(budget_in_...</td>\n","      <td>WHAT ARE THE MAXIMUM AND MINIMUM BUDGET OF THE...</td>\n","      <td>3</td>\n","      <td>dataset.column_names: ['context', 'answer', 'q...</td>\n","      <td>dataset.num_rows: 4/78577</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>CREATE TABLE department (num_employees INTEGER...</td>\n","      <td>SELECT AVG(num_employees) FROM department WHER...</td>\n","      <td>WHAT IS THE AVERAGE NUMBER OF EMPLOYEES OF THE...</td>\n","      <td>4</td>\n","      <td>dataset.column_names: ['context', 'answer', 'q...</td>\n","      <td>dataset.num_rows: 5/78577</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>CREATE TABLE station (name VARCHAR, lat VARCHA...</td>\n","      <td>SELECT name, lat, city FROM station ORDER BY l...</td>\n","      <td>WHAT ARE THE NAME, LATITUDE, AND CITY OF THE S...</td>\n","      <td>95</td>\n","      <td>dataset.column_names: ['context', 'answer', 'q...</td>\n","      <td>dataset.num_rows: 96/78577</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>CREATE TABLE weather (date VARCHAR, mean_tempe...</td>\n","      <td>SELECT date, mean_temperature_f, mean_humidity...</td>\n","      <td>WHAT ARE THE DATE, MEAN TEMPERATURE AND MEAN H...</td>\n","      <td>96</td>\n","      <td>dataset.column_names: ['context', 'answer', 'q...</td>\n","      <td>dataset.num_rows: 97/78577</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>CREATE TABLE station (city VARCHAR)...</td>\n","      <td>SELECT city, COUNT(*) FROM station GROUP BY ci...</td>\n","      <td>LIST THE NAME AND THE NUMBER OF STATIONS FOR A...</td>\n","      <td>97</td>\n","      <td>dataset.column_names: ['context', 'answer', 'q...</td>\n","      <td>dataset.num_rows: 98/78577</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>CREATE TABLE trip (start_station_id VARCHAR, s...</td>\n","      <td>SELECT start_station_id, start_station_name FR...</td>\n","      <td>FIND THE IDS AND NAMES OF STATIONS FROM WHICH ...</td>\n","      <td>98</td>\n","      <td>dataset.column_names: ['context', 'answer', 'q...</td>\n","      <td>dataset.num_rows: 99/78577</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>CREATE TABLE weather (zip_code VARCHAR, mean_v...</td>\n","      <td>SELECT zip_code FROM weather GROUP BY zip_code...</td>\n","      <td>FIND THE ZIP CODE IN WHICH THE AVERAGE MEAN VI...</td>\n","      <td>99</td>\n","      <td>dataset.column_names: ['context', 'answer', 'q...</td>\n","      <td>dataset.num_rows: 100/78577</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 9 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21bf0b96-f886-4277-984a-bb3a41cf7083')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-21bf0b96-f886-4277-984a-bb3a41cf7083 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-21bf0b96-f886-4277-984a-bb3a41cf7083');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-a0e4885d-de3f-4b4c-8a51-1f66d0743600\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a0e4885d-de3f-4b4c-8a51-1f66d0743600')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-a0e4885d-de3f-4b4c-8a51-1f66d0743600 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_a317e3e9-5bff-4e8f-b05f-9b4e931aa031\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dataframe')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_a317e3e9-5bff-4e8f-b05f-9b4e931aa031 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('dataframe');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["                                              context  \\\n","0                  CREATE TABLE head (age INTEGER)...   \n","1   CREATE TABLE head (name VARCHAR, born_state VA...   \n","2   CREATE TABLE department (creation VARCHAR, nam...   \n","3   CREATE TABLE department (budget_in_billions IN...   \n","4   CREATE TABLE department (num_employees INTEGER...   \n","..                                                ...   \n","95  CREATE TABLE station (name VARCHAR, lat VARCHA...   \n","96  CREATE TABLE weather (date VARCHAR, mean_tempe...   \n","97             CREATE TABLE station (city VARCHAR)...   \n","98  CREATE TABLE trip (start_station_id VARCHAR, s...   \n","99  CREATE TABLE weather (zip_code VARCHAR, mean_v...   \n","\n","                                               answer  \\\n","0            SELECT COUNT(*) FROM head WHERE age > 56   \n","1   SELECT name, born_state, age FROM head ORDER B...   \n","2   SELECT creation, name, budget_in_billions FROM...   \n","3   SELECT MAX(budget_in_billions), MIN(budget_in_...   \n","4   SELECT AVG(num_employees) FROM department WHER...   \n","..                                                ...   \n","95  SELECT name, lat, city FROM station ORDER BY l...   \n","96  SELECT date, mean_temperature_f, mean_humidity...   \n","97  SELECT city, COUNT(*) FROM station GROUP BY ci...   \n","98  SELECT start_station_id, start_station_name FR...   \n","99  SELECT zip_code FROM weather GROUP BY zip_code...   \n","\n","                                             question   z  \\\n","0   HOW MANY HEADS OF THE DEPARTMENTS ARE OLDER TH...   0   \n","1   LIST THE NAME, BORN STATE AND AGE OF THE HEADS...   1   \n","2   LIST THE CREATION YEAR, NAME AND BUDGET OF EAC...   2   \n","3   WHAT ARE THE MAXIMUM AND MINIMUM BUDGET OF THE...   3   \n","4   WHAT IS THE AVERAGE NUMBER OF EMPLOYEES OF THE...   4   \n","..                                                ...  ..   \n","95  WHAT ARE THE NAME, LATITUDE, AND CITY OF THE S...  95   \n","96  WHAT ARE THE DATE, MEAN TEMPERATURE AND MEAN H...  96   \n","97  LIST THE NAME AND THE NUMBER OF STATIONS FOR A...  97   \n","98  FIND THE IDS AND NAMES OF STATIONS FROM WHICH ...  98   \n","99  FIND THE ZIP CODE IN WHICH THE AVERAGE MEAN VI...  99   \n","\n","                                               source  \\\n","0   dataset.column_names: ['context', 'answer', 'q...   \n","1   dataset.column_names: ['context', 'answer', 'q...   \n","2   dataset.column_names: ['context', 'answer', 'q...   \n","3   dataset.column_names: ['context', 'answer', 'q...   \n","4   dataset.column_names: ['context', 'answer', 'q...   \n","..                                                ...   \n","95  dataset.column_names: ['context', 'answer', 'q...   \n","96  dataset.column_names: ['context', 'answer', 'q...   \n","97  dataset.column_names: ['context', 'answer', 'q...   \n","98  dataset.column_names: ['context', 'answer', 'q...   \n","99  dataset.column_names: ['context', 'answer', 'q...   \n","\n","                        extract     type  symbol  size_col  \n","0     dataset.num_rows: 1/78577  scatter  circle         4  \n","1     dataset.num_rows: 2/78577  scatter  circle         4  \n","2     dataset.num_rows: 3/78577  scatter  circle         4  \n","3     dataset.num_rows: 4/78577  scatter  circle         4  \n","4     dataset.num_rows: 5/78577  scatter  circle         4  \n","..                          ...      ...     ...       ...  \n","95   dataset.num_rows: 96/78577  scatter  circle         4  \n","96   dataset.num_rows: 97/78577  scatter  circle         4  \n","97   dataset.num_rows: 98/78577  scatter  circle         4  \n","98   dataset.num_rows: 99/78577  scatter  circle         4  \n","99  dataset.num_rows: 100/78577  scatter  circle         4  \n","\n","[100 rows x 9 columns]"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1709,"status":"ok","timestamp":1711203989115,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"qwMvKhPle5Wu","outputId":"cb2a5e1a-2109-4681-cea7-0ad64a97e655"},"outputs":[{"name":"stderr","output_type":"stream","text":["Creating DataFrame from train subset: 100%|██████████| 100/100 [00:00<00:00, 2419.13it/s]\n"]}],"source":["from typing import Dict, List, Union, Callable, Optional\n","import pandas as pd\n","from datasets import load_dataset, Dataset\n","from tqdm import tqdm\n","\n","def create_dataframe(\n","    dataset: Union[str, Dataset],\n","    subset: str = \"train\",\n","    columns: Optional[List[str]] = None,\n","    transformations: Optional[Dict[str, Callable]] = None,\n","    num_rows: Optional[int] = None,\n","    progress_bar: bool = True,\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Create a DataFrame from a given dataset with specified columns and transformations.\n","\n","    Args:\n","        dataset (Union[str, Dataset]): The dataset to load. It can be either a string representing the dataset name\n","                                       or a pre-loaded Dataset object.\n","        subset (str): The subset of the dataset to use (e.g., \"train\", \"test\", \"validation\"). Default is \"train\".\n","        columns (Optional[List[str]]): The list of columns to include in the DataFrame. If None, all columns will be included.\n","        transformations (Optional[Dict[str, Callable]]): A dictionary specifying the transformations to apply to each column.\n","                                                         The keys are the column names, and the values are the transformation functions.\n","        num_rows (Optional[int]): The number of rows to include in the DataFrame. If None, all rows will be included.\n","        progress_bar (bool): Whether to display a progress bar during the DataFrame creation process. Default is True.\n","\n","    Returns:\n","        pd.DataFrame: The created DataFrame.\n","    \"\"\"\n","    if isinstance(dataset, str):\n","        dataset = load_dataset(dataset)\n","\n","    if subset not in dataset:\n","        raise ValueError(f\"Subset '{subset}' not found in the dataset.\")\n","\n","    data = dataset[subset]\n","\n","    if columns is None:\n","        columns = data.column_names\n","\n","    if num_rows is None:\n","        num_rows = data.num_rows\n","\n","    rows = []\n","    for i in tqdm(range(num_rows), disable=not progress_bar, desc=f\"Creating DataFrame from {subset} subset\"):\n","        row = {\n","            **{column: data[i][column] for column in columns},\n","            **{\n","                \"z\": i,\n","                \"source\": f\"dataset.column_names: {columns}\",\n","                \"extract\": f\"dataset.num_rows: {i + 1}/{data.num_rows}\",\n","                \"type\": \"scatter\",\n","                \"symbol\": \"circle\",\n","                \"size_col\": 4,\n","            },\n","        }\n","\n","        if transformations is not None:\n","            row.update(\n","                {\n","                    column: transformations[column](data[i][column])\n","                    for column in transformations\n","                    if column in columns\n","                }\n","            )\n","\n","        rows.append(row)\n","\n","    return pd.DataFrame(rows)\n","\n","dataset_name = \"b-mc2/sql-create-context\"\n","subset = \"train\"\n","columns_to_include = ['context', 'answer', 'question']\n","transformations = {\n","    \"question\": lambda x: x.upper(),\n","    \"context\": lambda x: x[:500] + \"...\",\n","}\n","\n","dataframe = create_dataframe(dataset_name, subset, columns_to_include, transformations, num_rows=100, progress_bar=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":404,"status":"ok","timestamp":1711204000185,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"efql8WVibNEc","outputId":"1669a201-3eed-4dfa-c499-a3c6b662a142"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"dataframe\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 93,\n        \"samples\": [\n          \"CREATE TABLE courses (course_id VARCHAR); CREATE TABLE student_course_attendance (course_id VARCHAR, student_id VARCHAR)...\",\n          \"CREATE TABLE city (Status VARCHAR)...\",\n          \"CREATE TABLE student_course_attendance (course_id VARCHAR, student_id VARCHAR); CREATE TABLE student_course_registrations (course_id VARCHAR, student_id VARCHAR)...\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"SELECT AVG(bikes_available) FROM status WHERE NOT station_id IN (SELECT id FROM station WHERE city = \\\"Palo Alto\\\")\",\n          \"SELECT student_id FROM student_course_attendance WHERE course_id = 301 ORDER BY date_of_attendance DESC LIMIT 1\",\n          \"SELECT zip_code, AVG(mean_temperature_f) FROM weather WHERE date LIKE \\\"8/%\\\" GROUP BY zip_code\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"WHAT IS THE AVERAGE BIKE AVAILABILITY IN STATIONS THAT ARE NOT LOCATED IN PALO ALTO?\",\n          \"WHAT IS THE ID OF THE STUDENT WHO MOST RECENTLY REGISTERED COURSE 301?\",\n          \"FOR EACH ZIP CODE, RETURN THE AVERAGE MEAN TEMPERATURE OF AUGUST THERE.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"z\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"dataset.column_names: ['context', 'answer', 'question']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"extract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"dataset.num_rows: 84/78577\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"scatter\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"symbol\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"circle\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"size_col\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"dataframe"},"text/html":["\n","  <div id=\"df-bfaeae81-085d-4979-90fe-ded1a1b5c6e0\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>context</th>\n","      <th>answer</th>\n","      <th>question</th>\n","      <th>z</th>\n","      <th>source</th>\n","      <th>extract</th>\n","      <th>type</th>\n","      <th>symbol</th>\n","      <th>size_col</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>CREATE TABLE head (age INTEGER)...</td>\n","      <td>SELECT COUNT(*) FROM head WHERE age &gt; 56</td>\n","      <td>HOW MANY HEADS OF THE DEPARTMENTS ARE OLDER TH...</td>\n","      <td>0</td>\n","      <td>dataset.column_names: ['context', 'answer', 'q...</td>\n","      <td>dataset.num_rows: 1/78577</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>CREATE TABLE head (name VARCHAR, born_state VA...</td>\n","      <td>SELECT name, born_state, age FROM head ORDER B...</td>\n","      <td>LIST THE NAME, BORN STATE AND AGE OF THE HEADS...</td>\n","      <td>1</td>\n","      <td>dataset.column_names: ['context', 'answer', 'q...</td>\n","      <td>dataset.num_rows: 2/78577</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>CREATE TABLE department (creation VARCHAR, nam...</td>\n","      <td>SELECT creation, name, budget_in_billions FROM...</td>\n","      <td>LIST THE CREATION YEAR, NAME AND BUDGET OF EAC...</td>\n","      <td>2</td>\n","      <td>dataset.column_names: ['context', 'answer', 'q...</td>\n","      <td>dataset.num_rows: 3/78577</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>CREATE TABLE department (budget_in_billions IN...</td>\n","      <td>SELECT MAX(budget_in_billions), MIN(budget_in_...</td>\n","      <td>WHAT ARE THE MAXIMUM AND MINIMUM BUDGET OF THE...</td>\n","      <td>3</td>\n","      <td>dataset.column_names: ['context', 'answer', 'q...</td>\n","      <td>dataset.num_rows: 4/78577</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>CREATE TABLE department (num_employees INTEGER...</td>\n","      <td>SELECT AVG(num_employees) FROM department WHER...</td>\n","      <td>WHAT IS THE AVERAGE NUMBER OF EMPLOYEES OF THE...</td>\n","      <td>4</td>\n","      <td>dataset.column_names: ['context', 'answer', 'q...</td>\n","      <td>dataset.num_rows: 5/78577</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>CREATE TABLE station (name VARCHAR, lat VARCHA...</td>\n","      <td>SELECT name, lat, city FROM station ORDER BY l...</td>\n","      <td>WHAT ARE THE NAME, LATITUDE, AND CITY OF THE S...</td>\n","      <td>95</td>\n","      <td>dataset.column_names: ['context', 'answer', 'q...</td>\n","      <td>dataset.num_rows: 96/78577</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>CREATE TABLE weather (date VARCHAR, mean_tempe...</td>\n","      <td>SELECT date, mean_temperature_f, mean_humidity...</td>\n","      <td>WHAT ARE THE DATE, MEAN TEMPERATURE AND MEAN H...</td>\n","      <td>96</td>\n","      <td>dataset.column_names: ['context', 'answer', 'q...</td>\n","      <td>dataset.num_rows: 97/78577</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>CREATE TABLE station (city VARCHAR)...</td>\n","      <td>SELECT city, COUNT(*) FROM station GROUP BY ci...</td>\n","      <td>LIST THE NAME AND THE NUMBER OF STATIONS FOR A...</td>\n","      <td>97</td>\n","      <td>dataset.column_names: ['context', 'answer', 'q...</td>\n","      <td>dataset.num_rows: 98/78577</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>CREATE TABLE trip (start_station_id VARCHAR, s...</td>\n","      <td>SELECT start_station_id, start_station_name FR...</td>\n","      <td>FIND THE IDS AND NAMES OF STATIONS FROM WHICH ...</td>\n","      <td>98</td>\n","      <td>dataset.column_names: ['context', 'answer', 'q...</td>\n","      <td>dataset.num_rows: 99/78577</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>CREATE TABLE weather (zip_code VARCHAR, mean_v...</td>\n","      <td>SELECT zip_code FROM weather GROUP BY zip_code...</td>\n","      <td>FIND THE ZIP CODE IN WHICH THE AVERAGE MEAN VI...</td>\n","      <td>99</td>\n","      <td>dataset.column_names: ['context', 'answer', 'q...</td>\n","      <td>dataset.num_rows: 100/78577</td>\n","      <td>scatter</td>\n","      <td>circle</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 9 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bfaeae81-085d-4979-90fe-ded1a1b5c6e0')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-bfaeae81-085d-4979-90fe-ded1a1b5c6e0 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-bfaeae81-085d-4979-90fe-ded1a1b5c6e0');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-85b43b88-3895-4309-8cc6-858fe12de2f9\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-85b43b88-3895-4309-8cc6-858fe12de2f9')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-85b43b88-3895-4309-8cc6-858fe12de2f9 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_8f65b2f3-9db5-4f49-882a-7a641b483c85\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dataframe')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_8f65b2f3-9db5-4f49-882a-7a641b483c85 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('dataframe');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["                                              context  \\\n","0                  CREATE TABLE head (age INTEGER)...   \n","1   CREATE TABLE head (name VARCHAR, born_state VA...   \n","2   CREATE TABLE department (creation VARCHAR, nam...   \n","3   CREATE TABLE department (budget_in_billions IN...   \n","4   CREATE TABLE department (num_employees INTEGER...   \n","..                                                ...   \n","95  CREATE TABLE station (name VARCHAR, lat VARCHA...   \n","96  CREATE TABLE weather (date VARCHAR, mean_tempe...   \n","97             CREATE TABLE station (city VARCHAR)...   \n","98  CREATE TABLE trip (start_station_id VARCHAR, s...   \n","99  CREATE TABLE weather (zip_code VARCHAR, mean_v...   \n","\n","                                               answer  \\\n","0            SELECT COUNT(*) FROM head WHERE age > 56   \n","1   SELECT name, born_state, age FROM head ORDER B...   \n","2   SELECT creation, name, budget_in_billions FROM...   \n","3   SELECT MAX(budget_in_billions), MIN(budget_in_...   \n","4   SELECT AVG(num_employees) FROM department WHER...   \n","..                                                ...   \n","95  SELECT name, lat, city FROM station ORDER BY l...   \n","96  SELECT date, mean_temperature_f, mean_humidity...   \n","97  SELECT city, COUNT(*) FROM station GROUP BY ci...   \n","98  SELECT start_station_id, start_station_name FR...   \n","99  SELECT zip_code FROM weather GROUP BY zip_code...   \n","\n","                                             question   z  \\\n","0   HOW MANY HEADS OF THE DEPARTMENTS ARE OLDER TH...   0   \n","1   LIST THE NAME, BORN STATE AND AGE OF THE HEADS...   1   \n","2   LIST THE CREATION YEAR, NAME AND BUDGET OF EAC...   2   \n","3   WHAT ARE THE MAXIMUM AND MINIMUM BUDGET OF THE...   3   \n","4   WHAT IS THE AVERAGE NUMBER OF EMPLOYEES OF THE...   4   \n","..                                                ...  ..   \n","95  WHAT ARE THE NAME, LATITUDE, AND CITY OF THE S...  95   \n","96  WHAT ARE THE DATE, MEAN TEMPERATURE AND MEAN H...  96   \n","97  LIST THE NAME AND THE NUMBER OF STATIONS FOR A...  97   \n","98  FIND THE IDS AND NAMES OF STATIONS FROM WHICH ...  98   \n","99  FIND THE ZIP CODE IN WHICH THE AVERAGE MEAN VI...  99   \n","\n","                                               source  \\\n","0   dataset.column_names: ['context', 'answer', 'q...   \n","1   dataset.column_names: ['context', 'answer', 'q...   \n","2   dataset.column_names: ['context', 'answer', 'q...   \n","3   dataset.column_names: ['context', 'answer', 'q...   \n","4   dataset.column_names: ['context', 'answer', 'q...   \n","..                                                ...   \n","95  dataset.column_names: ['context', 'answer', 'q...   \n","96  dataset.column_names: ['context', 'answer', 'q...   \n","97  dataset.column_names: ['context', 'answer', 'q...   \n","98  dataset.column_names: ['context', 'answer', 'q...   \n","99  dataset.column_names: ['context', 'answer', 'q...   \n","\n","                        extract     type  symbol  size_col  \n","0     dataset.num_rows: 1/78577  scatter  circle         4  \n","1     dataset.num_rows: 2/78577  scatter  circle         4  \n","2     dataset.num_rows: 3/78577  scatter  circle         4  \n","3     dataset.num_rows: 4/78577  scatter  circle         4  \n","4     dataset.num_rows: 5/78577  scatter  circle         4  \n","..                          ...      ...     ...       ...  \n","95   dataset.num_rows: 96/78577  scatter  circle         4  \n","96   dataset.num_rows: 97/78577  scatter  circle         4  \n","97   dataset.num_rows: 98/78577  scatter  circle         4  \n","98   dataset.num_rows: 99/78577  scatter  circle         4  \n","99  dataset.num_rows: 100/78577  scatter  circle         4  \n","\n","[100 rows x 9 columns]"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":425,"status":"ok","timestamp":1711205309109,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"uwisU8bmvRjH","outputId":"6056059c-fe89-4036-fdef-41f9769bc5c6"},"outputs":[{"data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"ce2c107f-219e-42a6-af4e-e26ad1e82726\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ce2c107f-219e-42a6-af4e-e26ad1e82726\")) {                    Plotly.newPlot(                        \"ce2c107f-219e-42a6-af4e-e26ad1e82726\",                        [{\"alignmentgroup\":\"True\",\"bingroup\":\"x\",\"hovertemplate\":\"context=%{x}\\u003cbr\\u003ecount=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[\"CREATE TABLE head (age INTEGER)...\",\"CREATE TABLE head (name VARCHAR, born_state VARCHAR, age VARCHAR)...\",\"CREATE TABLE department (creation VARCHAR, name VARCHAR, budget_in_billions VARCHAR)...\",\"CREATE TABLE department (budget_in_billions INTEGER)...\",\"CREATE TABLE department (num_employees INTEGER, ranking INTEGER)...\",\"CREATE TABLE head (name VARCHAR, born_state VARCHAR)...\",\"CREATE TABLE department (creation VARCHAR, department_id VARCHAR); CREATE TABLE management (department_id VARCHAR, head_id VARCHAR); CREATE TABLE head (head_id VARCHAR, born_state VARCHAR)...\",\"CREATE TABLE head (born_state VARCHAR)...\",\"CREATE TABLE department (creation VARCHAR)...\",\"CREATE TABLE management (department_id VARCHAR, temporary_acting VARCHAR); CREATE TABLE department (name VARCHAR, num_employees VARCHAR, department_id VARCHAR)...\",\"CREATE TABLE management (temporary_acting VARCHAR)...\",\"CREATE TABLE management (department_id VARCHAR); CREATE TABLE department (department_id VARCHAR)...\",\"CREATE TABLE head (age VARCHAR, head_id VARCHAR); CREATE TABLE management (head_id VARCHAR, temporary_acting VARCHAR)...\",\"CREATE TABLE management (department_id VARCHAR, head_id VARCHAR); CREATE TABLE head (born_state VARCHAR, head_id VARCHAR); CREATE TABLE department (department_id VARCHAR, name VARCHAR)...\",\"CREATE TABLE management (department_id VARCHAR); CREATE TABLE department (department_id VARCHAR, name VARCHAR)...\",\"CREATE TABLE head (head_id VARCHAR, name VARCHAR)...\",\"CREATE TABLE farm (Id VARCHAR)...\",\"CREATE TABLE farm (Total_Horses VARCHAR)...\",\"CREATE TABLE farm_competition (Hosts VARCHAR, Theme VARCHAR)...\",\"CREATE TABLE farm_competition (Theme VARCHAR, YEAR VARCHAR)...\",\"CREATE TABLE farm (Working_Horses INTEGER, Total_Horses INTEGER)...\",\"CREATE TABLE farm (Cows INTEGER)...\",\"CREATE TABLE city (Status VARCHAR)...\",\"CREATE TABLE city (Official_Name VARCHAR, Population VARCHAR)...\",\"CREATE TABLE city (Official_Name VARCHAR, Status VARCHAR, Population VARCHAR)...\",\"CREATE TABLE city (Official_Name VARCHAR, City_ID VARCHAR); CREATE TABLE farm_competition (Year VARCHAR, Host_city_ID VARCHAR)...\",\"CREATE TABLE farm_competition (Host_city_ID VARCHAR); CREATE TABLE city (Official_Name VARCHAR, City_ID VARCHAR)...\",\"CREATE TABLE city (Status VARCHAR, City_ID VARCHAR); CREATE TABLE farm_competition (Host_city_ID VARCHAR)...\",\"CREATE TABLE city (City_ID VARCHAR, Population INTEGER); CREATE TABLE farm_competition (Theme VARCHAR, Host_city_ID VARCHAR)...\",\"CREATE TABLE city (Status VARCHAR, Population INTEGER)...\",\"CREATE TABLE city (Status VARCHAR)...\",\"CREATE TABLE city (Status VARCHAR)...\",\"CREATE TABLE farm_competition (Official_Name VARCHAR, City_ID VARCHAR, Host_city_ID VARCHAR); CREATE TABLE city (Official_Name VARCHAR, City_ID VARCHAR, Host_city_ID VARCHAR)...\",\"CREATE TABLE city (Status VARCHAR, Population INTEGER)...\",\"CREATE TABLE city (Official_Name VARCHAR, Population VARCHAR)...\",\"CREATE TABLE city (Census_Ranking VARCHAR, Status VARCHAR)...\",\"CREATE TABLE courses (course_name VARCHAR, course_id VARCHAR); CREATE TABLE student_course_registrations (course_Id VARCHAR)...\",\"CREATE TABLE student_course_registrations (student_id VARCHAR)...\",\"CREATE TABLE candidates (candidate_id VARCHAR); CREATE TABLE people (first_name VARCHAR, last_name VARCHAR, person_id VARCHAR)...\",\"CREATE TABLE student_course_attendance (student_id VARCHAR); CREATE TABLE students (student_id VARCHAR)...\",\"CREATE TABLE student_course_attendance (student_id VARCHAR)...\",\"CREATE TABLE courses (course_name VARCHAR, course_id VARCHAR); CREATE TABLE student_course_registrations (student_id VARCHAR, course_id VARCHAR)...\",\"CREATE TABLE student_course_registrations (student_id VARCHAR, registration_date VARCHAR); CREATE TABLE students (student_details VARCHAR, student_id VARCHAR)...\",\"CREATE TABLE student_course_attendance (course_id VARCHAR); CREATE TABLE courses (course_id VARCHAR, course_name VARCHAR)...\",\"CREATE TABLE courses (course_id VARCHAR); CREATE TABLE student_course_attendance (course_id VARCHAR, student_id VARCHAR)...\",\"CREATE TABLE candidates (candidate_id VARCHAR); CREATE TABLE people (person_id VARCHAR, email_address VARCHAR)...\",\"CREATE TABLE candidate_assessments (candidate_id VARCHAR, assessment_date VARCHAR)...\",\"CREATE TABLE students (student_details VARCHAR, student_id VARCHAR); CREATE TABLE student_course_registrations (student_id VARCHAR)...\",\"CREATE TABLE students (student_id VARCHAR); CREATE TABLE student_course_registrations (student_id VARCHAR)...\",\"CREATE TABLE students (student_id VARCHAR); CREATE TABLE courses (course_name VARCHAR, course_id VARCHAR); CREATE TABLE student_course_registrations (course_id VARCHAR, student_id VARCHAR)...\",\"CREATE TABLE candidate_assessments (candidate_id VARCHAR, asessment_outcome_code VARCHAR)...\",\"CREATE TABLE candidates (candidate_id VARCHAR); CREATE TABLE people (cell_mobile_number VARCHAR, person_id VARCHAR); CREATE TABLE candidate_assessments (candidate_id VARCHAR, asessment_outcome_code VARCHAR)...\",\"CREATE TABLE student_course_attendance (student_id VARCHAR, course_id VARCHAR)...\",\"CREATE TABLE student_course_attendance (student_id VARCHAR, course_id VARCHAR, date_of_attendance VARCHAR)...\",\"CREATE TABLE addresses (city VARCHAR, address_id VARCHAR); CREATE TABLE people_addresses (address_id VARCHAR)...\",\"CREATE TABLE students (student_id VARCHAR); CREATE TABLE addresses (city VARCHAR, address_id VARCHAR); CREATE TABLE people_addresses (address_id VARCHAR, person_id VARCHAR)...\",\"CREATE TABLE courses (course_name VARCHAR)...\",\"CREATE TABLE people (first_name VARCHAR)...\",\"CREATE TABLE student_course_attendance (student_id VARCHAR); CREATE TABLE student_course_registrations (student_id VARCHAR)...\",\"CREATE TABLE student_course_attendance (course_id VARCHAR, student_id VARCHAR); CREATE TABLE student_course_registrations (course_id VARCHAR, student_id VARCHAR)...\",\"CREATE TABLE student_course_attendance (student_id VARCHAR); CREATE TABLE student_course_registrations (student_id VARCHAR)...\",\"CREATE TABLE student_course_registrations (student_id VARCHAR, course_id VARCHAR, registration_date VARCHAR); CREATE TABLE courses (course_id VARCHAR, course_name VARCHAR)...\",\"CREATE TABLE student_course_attendance (student_id VARCHAR, course_id VARCHAR, date_of_attendance VARCHAR); CREATE TABLE courses (course_id VARCHAR, course_name VARCHAR)...\",\"CREATE TABLE weather (date VARCHAR, max_temperature_f INTEGER)...\",\"CREATE TABLE station (name VARCHAR, lat INTEGER)...\",\"CREATE TABLE station (city VARCHAR, lat INTEGER)...\",\"CREATE TABLE trip (start_station_name VARCHAR, end_station_name VARCHAR, id VARCHAR)...\",\"CREATE TABLE station (lat INTEGER, long INTEGER, city VARCHAR)...\",\"CREATE TABLE trip (id VARCHAR, duration VARCHAR)...\",\"CREATE TABLE trip (duration INTEGER, bike_id VARCHAR)...\",\"CREATE TABLE weather (zip_code VARCHAR, mean_temperature_f INTEGER, date VARCHAR)...\",\"CREATE TABLE trip (bike_id VARCHAR)...\",\"CREATE TABLE station (city VARCHAR)...\",\"CREATE TABLE station (city VARCHAR)...\",\"CREATE TABLE station (name VARCHAR, id VARCHAR); CREATE TABLE status (station_id VARCHAR, bikes_available VARCHAR)...\",\"CREATE TABLE trip (start_station_name VARCHAR, start_station_id VARCHAR, start_date VARCHAR)...\",\"CREATE TABLE trip (bike_id VARCHAR, zip_code VARCHAR)...\",\"CREATE TABLE weather (mean_humidity VARCHAR, mean_visibility_miles VARCHAR)...\",\"CREATE TABLE trip (start_station_id VARCHAR, duration VARCHAR); CREATE TABLE station (lat VARCHAR, long VARCHAR, city VARCHAR, id VARCHAR)...\",\"CREATE TABLE status (id VARCHAR, station_id VARCHAR, city VARCHAR, bikes_available INTEGER); CREATE TABLE station (id VARCHAR, station_id VARCHAR, city VARCHAR, bikes_available INTEGER)...\",\"CREATE TABLE station (name VARCHAR, id VARCHAR); CREATE TABLE station (name VARCHAR, id VARCHAR, installation_date VARCHAR); CREATE TABLE status (station_id VARCHAR, bikes_available INTEGER)...\",\"CREATE TABLE weather (cloud_cover VARCHAR, zip_code VARCHAR)...\",\"CREATE TABLE weather (zip_code VARCHAR, mean_sea_level_pressure_inches INTEGER)...\",\"CREATE TABLE status (bikes_available INTEGER, station_id VARCHAR, id VARCHAR, city VARCHAR); CREATE TABLE station (bikes_available INTEGER, station_id VARCHAR, id VARCHAR, city VARCHAR)...\",\"CREATE TABLE station (long INTEGER, id VARCHAR, station_id VARCHAR, bikes_available INTEGER); CREATE TABLE status (long INTEGER, id VARCHAR, station_id VARCHAR, bikes_available INTEGER)...\",\"CREATE TABLE weather (date VARCHAR, zip_code VARCHAR, max_temperature_f VARCHAR)...\",\"CREATE TABLE trip (id VARCHAR, zip_code VARCHAR); CREATE TABLE weather (zip_code VARCHAR, mean_temperature_f INTEGER)...\",\"CREATE TABLE weather (zip_code VARCHAR, max_wind_Speed_mph VARCHAR)...\",\"CREATE TABLE weather (date VARCHAR, zip_code VARCHAR, min_dew_point_f INTEGER)...\",\"CREATE TABLE station (installation_date VARCHAR, id VARCHAR); CREATE TABLE trip (id VARCHAR, end_station_id VARCHAR)...\",\"CREATE TABLE trip (id VARCHAR, start_station_id VARCHAR); CREATE TABLE station (id VARCHAR, dock_count VARCHAR)...\",\"CREATE TABLE trip (end_station_id VARCHAR); CREATE TABLE station (id VARCHAR, city VARCHAR)...\",\"CREATE TABLE weather (date VARCHAR, EVENTS VARCHAR, zip_code VARCHAR)...\",\"CREATE TABLE status (id VARCHAR, station_id VARCHAR, lat INTEGER, bikes_available INTEGER); CREATE TABLE station (id VARCHAR, station_id VARCHAR, lat INTEGER, bikes_available INTEGER)...\",\"CREATE TABLE station (name VARCHAR, id VARCHAR); CREATE TABLE status (station_id VARCHAR); CREATE TABLE station (name VARCHAR, city VARCHAR, bikes_available INTEGER)...\",\"CREATE TABLE station (name VARCHAR, lat VARCHAR, city VARCHAR)...\",\"CREATE TABLE weather (date VARCHAR, mean_temperature_f VARCHAR, mean_humidity VARCHAR, max_gust_speed_mph VARCHAR)...\",\"CREATE TABLE station (city VARCHAR)...\",\"CREATE TABLE trip (start_station_id VARCHAR, start_station_name VARCHAR)...\",\"CREATE TABLE weather (zip_code VARCHAR, mean_visibility_miles INTEGER)...\"],\"xaxis\":\"x\",\"yaxis\":\"y\",\"type\":\"histogram\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"context\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"count\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Histogram of context\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('ce2c107f-219e-42a6-af4e-e26ad1e82726');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"3de72e9a-f416-4c85-b7d8-eb300a642ea5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3de72e9a-f416-4c85-b7d8-eb300a642ea5\")) {                    Plotly.newPlot(                        \"3de72e9a-f416-4c85-b7d8-eb300a642ea5\",                        [{\"alignmentgroup\":\"True\",\"bingroup\":\"x\",\"hovertemplate\":\"answer=%{x}\\u003cbr\\u003ecount=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[\"SELECT COUNT(*) FROM head WHERE age \\u003e 56\",\"SELECT name, born_state, age FROM head ORDER BY age\",\"SELECT creation, name, budget_in_billions FROM department\",\"SELECT MAX(budget_in_billions), MIN(budget_in_billions) FROM department\",\"SELECT AVG(num_employees) FROM department WHERE ranking BETWEEN 10 AND 15\",\"SELECT name FROM head WHERE born_state \\u003c\\u003e 'California'\",\"SELECT DISTINCT T1.creation FROM department AS T1 JOIN management AS T2 ON T1.department_id = T2.department_id JOIN head AS T3 ON T2.head_id = T3.head_id WHERE T3.born_state = 'Alabama'\",\"SELECT born_state FROM head GROUP BY born_state HAVING COUNT(*) \\u003e= 3\",\"SELECT creation FROM department GROUP BY creation ORDER BY COUNT(*) DESC LIMIT 1\",\"SELECT T1.name, T1.num_employees FROM department AS T1 JOIN management AS T2 ON T1.department_id = T2.department_id WHERE T2.temporary_acting = 'Yes'\",\"SELECT COUNT(DISTINCT temporary_acting) FROM management\",\"SELECT COUNT(*) FROM department WHERE NOT department_id IN (SELECT department_id FROM management)\",\"SELECT DISTINCT T1.age FROM management AS T2 JOIN head AS T1 ON T1.head_id = T2.head_id WHERE T2.temporary_acting = 'Yes'\",\"SELECT T3.born_state FROM department AS T1 JOIN management AS T2 ON T1.department_id = T2.department_id JOIN head AS T3 ON T2.head_id = T3.head_id WHERE T1.name = 'Treasury' INTERSECT SELECT T3.born_state FROM department AS T1 JOIN management AS T2 ON T1.department_id = T2.department_id JOIN head AS T3 ON T2.head_id = T3.head_id WHERE T1.name = 'Homeland Security'\",\"SELECT T1.department_id, T1.name, COUNT(*) FROM management AS T2 JOIN department AS T1 ON T1.department_id = T2.department_id GROUP BY T1.department_id HAVING COUNT(*) \\u003e 1\",\"SELECT head_id, name FROM head WHERE name LIKE '%Ha%'\",\"SELECT COUNT(*) FROM farm\",\"SELECT Total_Horses FROM farm ORDER BY Total_Horses\",\"SELECT Hosts FROM farm_competition WHERE Theme \\u003c\\u003e 'Aliens'\",\"SELECT Theme FROM farm_competition ORDER BY YEAR\",\"SELECT AVG(Working_Horses) FROM farm WHERE Total_Horses \\u003e 5000\",\"SELECT MAX(Cows), MIN(Cows) FROM farm\",\"SELECT COUNT(DISTINCT Status) FROM city\",\"SELECT Official_Name FROM city ORDER BY Population DESC\",\"SELECT Official_Name, Status FROM city ORDER BY Population DESC LIMIT 1\",\"SELECT T2.Year, T1.Official_Name FROM city AS T1 JOIN farm_competition AS T2 ON T1.City_ID = T2.Host_city_ID\",\"SELECT T1.Official_Name FROM city AS T1 JOIN farm_competition AS T2 ON T1.City_ID = T2.Host_city_ID GROUP BY T2.Host_city_ID HAVING COUNT(*) \\u003e 1\",\"SELECT T1.Status FROM city AS T1 JOIN farm_competition AS T2 ON T1.City_ID = T2.Host_city_ID GROUP BY T2.Host_city_ID ORDER BY COUNT(*) DESC LIMIT 1\",\"SELECT T2.Theme FROM city AS T1 JOIN farm_competition AS T2 ON T1.City_ID = T2.Host_city_ID WHERE T1.Population \\u003e 1000\",\"SELECT Status, AVG(Population) FROM city GROUP BY Status\",\"SELECT Status FROM city GROUP BY Status ORDER BY COUNT(*)\",\"SELECT Status FROM city GROUP BY Status ORDER BY COUNT(*) DESC LIMIT 1\",\"SELECT Official_Name FROM city WHERE NOT City_ID IN (SELECT Host_city_ID FROM farm_competition)\",\"SELECT Status FROM city WHERE Population \\u003e 1500 INTERSECT SELECT Status FROM city WHERE Population \\u003c 500\",\"SELECT Official_Name FROM city WHERE Population \\u003e 1500 OR Population \\u003c 500\",\"SELECT Census_Ranking FROM city WHERE Status \\u003c\\u003e \\\"Village\\\"\",\"SELECT T1.course_name FROM courses AS T1 JOIN student_course_registrations AS T2 ON T1.course_id = T2.course_Id GROUP BY T1.course_id ORDER BY COUNT(*) DESC LIMIT 1\",\"SELECT student_id FROM student_course_registrations GROUP BY student_id ORDER BY COUNT(*) LIMIT 1\",\"SELECT T2.first_name, T2.last_name FROM candidates AS T1 JOIN people AS T2 ON T1.candidate_id = T2.person_id\",\"SELECT student_id FROM students WHERE NOT student_id IN (SELECT student_id FROM student_course_attendance)\",\"SELECT student_id FROM student_course_attendance\",\"SELECT T1.student_id, T2.course_name FROM student_course_registrations AS T1 JOIN courses AS T2 ON T1.course_id = T2.course_id\",\"SELECT T2.student_details FROM student_course_registrations AS T1 JOIN students AS T2 ON T1.student_id = T2.student_id ORDER BY T1.registration_date DESC LIMIT 1\",\"SELECT COUNT(*) FROM courses AS T1 JOIN student_course_attendance AS T2 ON T1.course_id = T2.course_id WHERE T1.course_name = \\\"English\\\"\",\"SELECT COUNT(*) FROM courses AS T1 JOIN student_course_attendance AS T2 ON T1.course_id = T2.course_id WHERE T2.student_id = 171\",\"SELECT T2.candidate_id FROM people AS T1 JOIN candidates AS T2 ON T1.person_id = T2.candidate_id WHERE T1.email_address = \\\"stanley.monahan@example.org\\\"\",\"SELECT candidate_id FROM candidate_assessments ORDER BY assessment_date DESC LIMIT 1\",\"SELECT T1.student_details FROM students AS T1 JOIN student_course_registrations AS T2 ON T1.student_id = T2.student_id GROUP BY T1.student_id ORDER BY COUNT(*) DESC LIMIT 1\",\"SELECT T1.student_id, COUNT(*) FROM students AS T1 JOIN student_course_registrations AS T2 ON T1.student_id = T2.student_id GROUP BY T1.student_id\",\"SELECT T3.course_name, COUNT(*) FROM students AS T1 JOIN student_course_registrations AS T2 ON T1.student_id = T2.student_id JOIN courses AS T3 ON T2.course_id = T3.course_id GROUP BY T2.course_id\",\"SELECT candidate_id FROM candidate_assessments WHERE asessment_outcome_code = \\\"Pass\\\"\",\"SELECT T3.cell_mobile_number FROM candidates AS T1 JOIN candidate_assessments AS T2 ON T1.candidate_id = T2.candidate_id JOIN people AS T3 ON T1.candidate_id = T3.person_id WHERE T2.asessment_outcome_code = \\\"Fail\\\"\",\"SELECT student_id FROM student_course_attendance WHERE course_id = 301\",\"SELECT student_id FROM student_course_attendance WHERE course_id = 301 ORDER BY date_of_attendance DESC LIMIT 1\",\"SELECT DISTINCT T1.city FROM addresses AS T1 JOIN people_addresses AS T2 ON T1.address_id = T2.address_id\",\"SELECT DISTINCT T1.city FROM addresses AS T1 JOIN people_addresses AS T2 ON T1.address_id = T2.address_id JOIN students AS T3 ON T2.person_id = T3.student_id\",\"SELECT course_name FROM courses ORDER BY course_name\",\"SELECT first_name FROM people ORDER BY first_name\",\"SELECT student_id FROM student_course_registrations UNION SELECT student_id FROM student_course_attendance\",\"SELECT course_id FROM student_course_registrations WHERE student_id = 121 UNION SELECT course_id FROM student_course_attendance WHERE student_id = 121\",\"SELECT * FROM student_course_registrations WHERE NOT student_id IN (SELECT student_id FROM student_course_attendance)\",\"SELECT T2.student_id FROM courses AS T1 JOIN student_course_registrations AS T2 ON T1.course_id = T2.course_id WHERE T1.course_name = \\\"statistics\\\" ORDER BY T2.registration_date\",\"SELECT T2.student_id FROM courses AS T1 JOIN student_course_attendance AS T2 ON T1.course_id = T2.course_id WHERE T1.course_name = \\\"statistics\\\" ORDER BY T2.date_of_attendance\",\"SELECT date FROM weather WHERE max_temperature_f \\u003e 85\",\"SELECT name FROM station WHERE lat \\u003c 37.5\",\"SELECT city, MAX(lat) FROM station GROUP BY city\",\"SELECT start_station_name, end_station_name FROM trip ORDER BY id LIMIT 3\",\"SELECT AVG(lat), AVG(long) FROM station WHERE city = \\\"San Jose\\\"\",\"SELECT id FROM trip ORDER BY duration LIMIT 1\",\"SELECT SUM(duration), MAX(duration) FROM trip WHERE bike_id = 636\",\"SELECT zip_code, AVG(mean_temperature_f) FROM weather WHERE date LIKE \\\"8\\u002f%\\\" GROUP BY zip_code\",\"SELECT COUNT(DISTINCT bike_id) FROM trip\",\"SELECT COUNT(DISTINCT city) FROM station\",\"SELECT COUNT(*) FROM station WHERE city = \\\"Mountain View\\\"\",\"SELECT DISTINCT T1.name FROM station AS T1 JOIN status AS T2 ON T1.id = T2.station_id WHERE T2.bikes_available = 7\",\"SELECT start_station_name, start_station_id FROM trip WHERE start_date LIKE \\\"8\\u002f%\\\" GROUP BY start_station_name ORDER BY COUNT(*) DESC LIMIT 1\",\"SELECT bike_id FROM trip WHERE zip_code = 94002 GROUP BY bike_id ORDER BY COUNT(*) DESC LIMIT 1\",\"SELECT COUNT(*) FROM weather WHERE mean_humidity \\u003e 50 AND mean_visibility_miles \\u003e 8\",\"SELECT T1.lat, T1.long, T1.city FROM station AS T1 JOIN trip AS T2 ON T1.id = T2.start_station_id ORDER BY T2.duration LIMIT 1\",\"SELECT id FROM station WHERE city = \\\"San Francisco\\\" INTERSECT SELECT station_id FROM status GROUP BY station_id HAVING AVG(bikes_available) \\u003e 10\",\"SELECT T1.name, T1.id FROM station AS T1 JOIN status AS T2 ON T1.id = T2.station_id GROUP BY T2.station_id HAVING AVG(T2.bikes_available) \\u003e 14 UNION SELECT name, id FROM station WHERE installation_date LIKE \\\"12\\u002f%\\\"\",\"SELECT cloud_cover FROM weather WHERE zip_code = 94107 GROUP BY cloud_cover ORDER BY COUNT(*) DESC LIMIT 3\",\"SELECT zip_code FROM weather GROUP BY zip_code ORDER BY AVG(mean_sea_level_pressure_inches) LIMIT 1\",\"SELECT AVG(bikes_available) FROM status WHERE NOT station_id IN (SELECT id FROM station WHERE city = \\\"Palo Alto\\\")\",\"SELECT AVG(long) FROM station WHERE NOT id IN (SELECT station_id FROM status GROUP BY station_id HAVING MAX(bikes_available) \\u003e 10)\",\"SELECT date, zip_code FROM weather WHERE max_temperature_f \\u003e= 80\",\"SELECT T1.id FROM trip AS T1 JOIN weather AS T2 ON T1.zip_code = T2.zip_code GROUP BY T2.zip_code HAVING AVG(T2.mean_temperature_f) \\u003e 60\",\"SELECT zip_code, COUNT(*) FROM weather WHERE max_wind_Speed_mph \\u003e= 25 GROUP BY zip_code\",\"SELECT date, zip_code FROM weather WHERE min_dew_point_f \\u003c (SELECT MIN(min_dew_point_f) FROM weather WHERE zip_code = 94107)\",\"SELECT T1.id, T2.installation_date FROM trip AS T1 JOIN station AS T2 ON T1.end_station_id = T2.id\",\"SELECT T1.id FROM trip AS T1 JOIN station AS T2 ON T1.start_station_id = T2.id ORDER BY T2.dock_count DESC LIMIT 1\",\"SELECT COUNT(*) FROM trip AS T1 JOIN station AS T2 ON T1.end_station_id = T2.id WHERE T2.city \\u003c\\u003e \\\"San Francisco\\\"\",\"SELECT date FROM weather WHERE zip_code = 94107 AND EVENTS \\u003c\\u003e \\\"Fog\\\" AND EVENTS \\u003c\\u003e \\\"Rain\\\"\",\"SELECT id FROM station WHERE lat \\u003e 37.4 EXCEPT SELECT station_id FROM status GROUP BY station_id HAVING MIN(bikes_available) \\u003c 7\",\"SELECT T1.name FROM station AS T1 JOIN status AS T2 ON T1.id = T2.station_id GROUP BY T2.station_id HAVING AVG(bikes_available) \\u003e 10 EXCEPT SELECT name FROM station WHERE city = \\\"San Jose\\\"\",\"SELECT name, lat, city FROM station ORDER BY lat LIMIT 1\",\"SELECT date, mean_temperature_f, mean_humidity FROM weather ORDER BY max_gust_speed_mph DESC LIMIT 3\",\"SELECT city, COUNT(*) FROM station GROUP BY city HAVING COUNT(*) \\u003e= 15\",\"SELECT start_station_id, start_station_name FROM trip GROUP BY start_station_name HAVING COUNT(*) \\u003e= 200\",\"SELECT zip_code FROM weather GROUP BY zip_code HAVING AVG(mean_visibility_miles) \\u003c 10\"],\"xaxis\":\"x\",\"yaxis\":\"y\",\"type\":\"histogram\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"answer\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"count\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Histogram of answer\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('3de72e9a-f416-4c85-b7d8-eb300a642ea5');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"eb086767-aa48-4222-a1b6-f202ff6984d8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"eb086767-aa48-4222-a1b6-f202ff6984d8\")) {                    Plotly.newPlot(                        \"eb086767-aa48-4222-a1b6-f202ff6984d8\",                        [{\"alignmentgroup\":\"True\",\"bingroup\":\"x\",\"hovertemplate\":\"question=%{x}\\u003cbr\\u003ecount=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[\"HOW MANY HEADS OF THE DEPARTMENTS ARE OLDER THAN 56 ?\",\"LIST THE NAME, BORN STATE AND AGE OF THE HEADS OF DEPARTMENTS ORDERED BY AGE.\",\"LIST THE CREATION YEAR, NAME AND BUDGET OF EACH DEPARTMENT.\",\"WHAT ARE THE MAXIMUM AND MINIMUM BUDGET OF THE DEPARTMENTS?\",\"WHAT IS THE AVERAGE NUMBER OF EMPLOYEES OF THE DEPARTMENTS WHOSE RANK IS BETWEEN 10 AND 15?\",\"WHAT ARE THE NAMES OF THE HEADS WHO ARE BORN OUTSIDE THE CALIFORNIA STATE?\",\"WHAT ARE THE DISTINCT CREATION YEARS OF THE DEPARTMENTS MANAGED BY A SECRETARY BORN IN STATE 'ALABAMA'?\",\"WHAT ARE THE NAMES OF THE STATES WHERE AT LEAST 3 HEADS WERE BORN?\",\"IN WHICH YEAR WERE MOST DEPARTMENTS ESTABLISHED?\",\"SHOW THE NAME AND NUMBER OF EMPLOYEES FOR THE DEPARTMENTS MANAGED BY HEADS WHOSE TEMPORARY ACTING VALUE IS 'YES'?\",\"HOW MANY ACTING STATUSES ARE THERE?\",\"HOW MANY DEPARTMENTS ARE LED BY HEADS WHO ARE NOT MENTIONED?\",\"WHAT ARE THE DISTINCT AGES OF THE HEADS WHO ARE ACTING?\",\"LIST THE STATES WHERE BOTH THE SECRETARY OF 'TREASURY' DEPARTMENT AND THE SECRETARY OF 'HOMELAND SECURITY' WERE BORN.\",\"WHICH DEPARTMENT HAS MORE THAN 1 HEAD AT A TIME? LIST THE ID, NAME AND THE NUMBER OF HEADS.\",\"WHICH HEAD'S NAME HAS THE SUBSTRING 'HA'? LIST THE ID AND NAME.\",\"HOW MANY FARMS ARE THERE?\",\"LIST THE TOTAL NUMBER OF HORSES ON FARMS IN ASCENDING ORDER.\",\"WHAT ARE THE HOSTS OF COMPETITIONS WHOSE THEME IS NOT \\\"ALIENS\\\"?\",\"WHAT ARE THE THEMES OF FARM COMPETITIONS SORTED BY YEAR IN ASCENDING ORDER?\",\"WHAT IS THE AVERAGE NUMBER OF WORKING HORSES OF FARMS WITH MORE THAN 5000 TOTAL NUMBER OF HORSES?\",\"WHAT ARE THE MAXIMUM AND MINIMUM NUMBER OF COWS ACROSS ALL FARMS.\",\"HOW MANY DIFFERENT STATUSES DO CITIES HAVE?\",\"LIST OFFICIAL NAMES OF CITIES IN DESCENDING ORDER OF POPULATION.\",\"LIST THE OFFICIAL NAME AND STATUS OF THE CITY WITH THE LARGEST POPULATION.\",\"SHOW THE YEARS AND THE OFFICIAL NAMES OF THE HOST CITIES OF COMPETITIONS.\",\"SHOW THE OFFICIAL NAMES OF THE CITIES THAT HAVE HOSTED MORE THAN ONE COMPETITION.\",\"SHOW THE STATUS OF THE CITY THAT HAS HOSTED THE GREATEST NUMBER OF COMPETITIONS.\",\"PLEASE SHOW THE THEMES OF COMPETITIONS WITH HOST CITIES HAVING POPULATIONS LARGER THAN 1000.\",\"PLEASE SHOW THE DIFFERENT STATUSES OF CITIES AND THE AVERAGE POPULATION OF CITIES WITH EACH STATUS.\",\"PLEASE SHOW THE DIFFERENT STATUSES, ORDERED BY THE NUMBER OF CITIES THAT HAVE EACH.\",\"LIST THE MOST COMMON TYPE OF STATUS ACROSS CITIES.\",\"LIST THE OFFICIAL NAMES OF CITIES THAT HAVE NOT HELD ANY COMPETITION.\",\"SHOW THE STATUS SHARED BY CITIES WITH POPULATION BIGGER THAN 1500 AND SMALLER THAN 500.\",\"FIND THE OFFICIAL NAMES OF CITIES WITH POPULATION BIGGER THAN 1500 OR SMALLER THAN 500.\",\"SHOW THE CENSUS RANKING OF CITIES WHOSE STATUS ARE NOT \\\"VILLAGE\\\".\",\"WHICH COURSE HAS MOST NUMBER OF REGISTERED STUDENTS?\",\"WHAT IS ID OF STUDENTS WHO REGISTERED SOME COURSES BUT THE LEAST NUMBER OF COURSES IN THESE STUDENTS?\",\"WHAT ARE THE FIRST NAME AND LAST NAME OF ALL CANDIDATES?\",\"LIST THE ID OF STUDENTS WHO NEVER ATTENDS COURSES?\",\"LIST THE ID OF STUDENTS WHO ATTENDED SOME COURSES?\",\"WHAT ARE THE IDS OF ALL STUDENTS FOR COURSES AND WHAT ARE THE NAMES OF THOSE COURSES?\",\"WHAT IS DETAIL OF THE STUDENT WHO MOST RECENTLY REGISTERED COURSE?\",\"HOW MANY STUDENTS ATTEND COURSE ENGLISH?\",\"HOW MANY COURSES DO THE STUDENT WHOSE ID IS 171 ATTEND?\",\"FIND ID OF THE CANDIDATE WHOSE EMAIL IS STANLEY.MONAHAN@EXAMPLE.ORG?\",\"FIND ID OF THE CANDIDATE WHO MOST RECENTLY ACCESSED THE COURSE?\",\"WHAT IS DETAIL OF THE STUDENT WHO REGISTERED THE MOST NUMBER OF COURSES?\",\"LIST THE ID OF STUDENTS WHO REGISTERED SOME COURSES AND THE NUMBER OF THEIR REGISTERED COURSES?\",\"HOW MANY REGISTED STUDENTS DO EACH COURSE HAVE? LIST COURSE NAME AND THE NUMBER OF THEIR REGISTERED STUDENTS?\",\"FIND ID OF CANDIDATES WHOSE ASSESSMENT CODE IS \\\"PASS\\\"?\",\"FIND THE CELL MOBILE NUMBER OF THE CANDIDATES WHOSE ASSESSMENT CODE IS \\\"FAIL\\\"?\",\"WHAT ARE THE ID OF STUDENTS WHO REGISTERED COURSE 301?\",\"WHAT IS THE ID OF THE STUDENT WHO MOST RECENTLY REGISTERED COURSE 301?\",\"FIND DISTINCT CITIES OF ADDRESSES OF PEOPLE?\",\"FIND DISTINCT CITIES OF ADDRESS OF STUDENTS?\",\"LIST THE NAMES OF COURSES IN ALPHABETICAL ORDER?\",\"LIST THE FIRST NAMES OF PEOPLE IN ALPHABETICAL ORDER?\",\"WHAT ARE THE ID OF STUDENTS WHO REGISTERED COURSES OR ATTENDED COURSES?\",\"FIND THE ID OF COURSES WHICH ARE REGISTERED OR ATTENDED BY STUDENT WHOSE ID IS 121?\",\"WHAT ARE ALL INFO OF STUDENTS WHO REGISTERED COURSES BUT NOT ATTENDED COURSES?\",\"LIST THE ID OF STUDENTS WHO REGISTERED COURSE STATISTICS IN THE ORDER OF REGISTRATION DATE.\",\"LIST THE ID OF STUDENTS WHO ATTENDED  STATISTICS COURSES IN THE ORDER OF ATTENDANCE DATE.\",\"GIVE ME THE DATES WHEN THE MAX TEMPERATURE WAS HIGHER THAN 85.\",\"WHAT ARE THE NAMES OF STATIONS THAT HAVE LATITUDE LOWER THAN 37.5?\",\"FOR EACH CITY, RETURN THE HIGHEST LATITUDE AMONG ITS STATIONS.\",\"GIVE ME THE START STATION AND END STATION FOR THE TRIPS WITH THE THREE OLDEST ID.\",\"WHAT IS THE AVERAGE LATITUDE AND LONGITUDE OF STATIONS LOCATED IN SAN JOSE CITY?\",\"WHAT IS THE ID OF THE TRIP THAT HAS THE SHORTEST DURATION?\",\"WHAT IS THE TOTAL AND MAXIMUM DURATION OF TRIPS WITH BIKE ID 636?\",\"FOR EACH ZIP CODE, RETURN THE AVERAGE MEAN TEMPERATURE OF AUGUST THERE.\",\"FROM THE TRIP RECORD, FIND THE NUMBER OF UNIQUE BIKES.\",\"WHAT IS THE NUMBER OF DISTINCT CITIES THE STATIONS ARE LOCATED AT?\",\"HOW MANY STATIONS DOES MOUNTAIN VIEW CITY HAS?\",\"RETURN THE UNIQUE NAME FOR STATIONS THAT HAVE EVER HAD 7 BIKES AVAILABLE.\",\"WHICH START STATION HAD THE MOST TRIPS STARTING FROM AUGUST? GIVE ME THE NAME AND ID OF THE STATION.\",\"WHICH BIKE TRAVELED THE MOST OFTEN IN ZIP CODE 94002?\",\"HOW MANY DAYS HAD BOTH MEAN HUMIDITY ABOVE 50 AND MEAN VISIBILITY ABOVE 8?\",\"WHAT IS THE LATITUDE, LONGITUDE, CITY OF THE STATION FROM WHICH THE SHORTEST TRIP STARTED?\",\"WHAT ARE THE IDS OF STATIONS THAT ARE LOCATED IN SAN FRANCISCO AND HAVE AVERAGE BIKE AVAILABILITY ABOVE 10.\",\"WHAT ARE THE NAMES AND IDS OF STATIONS THAT HAD MORE THAN 14 BIKES AVAILABLE ON AVERAGE OR WERE INSTALLED IN DECEMBER?\",\"WHAT IS THE 3 MOST COMMON CLOUD COVER RATES IN THE REGION OF ZIP CODE 94107?\",\"WHAT IS THE ZIP CODE IN WHICH THE AVERAGE MEAN SEA LEVEL PRESSURE IS THE LOWEST?\",\"WHAT IS THE AVERAGE BIKE AVAILABILITY IN STATIONS THAT ARE NOT LOCATED IN PALO ALTO?\",\"WHAT IS THE AVERAGE LONGITUDE OF STATIONS THAT NEVER HAD BIKE AVAILABILITY MORE THAN 10?\",\"WHEN AND IN WHAT ZIP CODE DID MAX TEMPERATURE REACH 80?\",\"GIVE ME IDS FOR ALL THE TRIP THAT TOOK PLACE IN A ZIP CODE AREA WITH AVERAGE MEAN TEMPERATURE ABOVE 60.\",\"FOR EACH ZIP CODE, RETURN HOW MANY TIMES MAX WIND SPEED REACHED 25?\",\"ON WHICH DAY AND IN WHICH ZIP CODE WAS THE MIN DEW POINT LOWER THAN ANY DAY IN ZIP CODE 94107?\",\"FOR EACH TRIP, RETURN ITS ENDING STATION'S INSTALLATION DATE.\",\"WHICH TRIP STARTED FROM THE STATION WITH THE LARGEST DOCK COUNT? GIVE ME THE TRIP ID.\",\"COUNT THE NUMBER OF TRIPS THAT DID NOT END IN SAN FRANCISCO CITY.\",\"IN ZIP CODE 94107, ON WHICH DAY NEITHER FOG NOR RAIN WAS NOT OBSERVED?\",\"WHAT ARE THE IDS OF STATIONS THAT HAVE LATITUDE ABOVE 37.4 AND NEVER HAD BIKE AVAILABILITY BELOW 7?\",\"WHAT ARE NAMES OF STATIONS THAT HAVE AVERAGE BIKE AVAILABILITY ABOVE 10 AND ARE NOT LOCATED IN SAN JOSE CITY?\",\"WHAT ARE THE NAME, LATITUDE, AND CITY OF THE STATION WITH THE LOWEST LATITUDE?\",\"WHAT ARE THE DATE, MEAN TEMPERATURE AND MEAN HUMIDITY FOR THE TOP 3 DAYS WITH THE LARGEST MAX GUST SPEEDS?\",\"LIST THE NAME AND THE NUMBER OF STATIONS FOR ALL THE CITIES THAT HAVE AT LEAST 15 STATIONS.\",\"FIND THE IDS AND NAMES OF STATIONS FROM WHICH AT LEAST 200 TRIPS STARTED.\",\"FIND THE ZIP CODE IN WHICH THE AVERAGE MEAN VISIBILITY IS LOWER THAN 10.\"],\"xaxis\":\"x\",\"yaxis\":\"y\",\"type\":\"histogram\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"question\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"count\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Histogram of question\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('eb086767-aa48-4222-a1b6-f202ff6984d8');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{},"output_type":"display_data"}],"source":["import plotly.express as px\n","\n","columns = columns_to_include\n","\n","for column in columns:\n","\n","    fig = px.histogram(dataframe, x=column, title=f'Histogram of {column}')\n","\n","    # fig.show()\n","    # fig = px.scatter(dataframe, x=column, )\n","\n","    fig.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFOi_g4ft1FS"},"outputs":[],"source":["import plotly.express as px\n","\n","# Assuming 'dataframe' is your DataFrame and 'x' and 'y' are columns in your DataFrame\n","fig = px.scatter(dataframe, x='context', )\n","\n","fig.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1519,"status":"ok","timestamp":1711199384741,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"},"user_tz":-330},"id":"AZxxGqdIR7GH","outputId":"623e8bb5-9f45-41e1-f9d1-a6b9b87985c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                   act  \\\n","0                       LINUX TERMINAL   \n","1      ENGLISH TRANSLATOR AND IMPROVER   \n","2               `POSITION` INTERVIEWER   \n","3                   JAVASCRIPT CONSOLE   \n","4                          EXCEL SHEET   \n","5         ENGLISH PRONUNCIATION HELPER   \n","6  SPOKEN ENGLISH TEACHER AND IMPROVER   \n","7                         TRAVEL GUIDE   \n","8                   PLAGIARISM CHECKER   \n","9   CHARACTER FROM MOVIE/BOOK/ANYTHING   \n","\n","                                              prompt  \n","0  I want you to act as a linux terminal. I will ...  \n","1  I want you to act as an English translator, sp...  \n","2  I want you to act as an interviewer. I will be...  \n","3  I want you to act as a javascript console. I w...  \n","4  I want you to act as a text based excel. you'l...  \n","5  I want you to act as an English pronunciation ...  \n","6  I want you to act as a spoken English teacher ...  \n","7  I want you to act as a travel guide. I will wr...  \n","8  I want you to act as a plagiarism checker. I w...  \n","9  I want you to act like {character} from {serie...  \n"]}],"source":["from typing import Dict, List, Union, Callable\n","import pandas as pd\n","from datasets import load_dataset, Dataset\n","\n","def create_dataframe(\n","    dataset: Union[str, Dataset],\n","    subset: str = \"train\",\n","    columns: List[str] = None,\n","    transformations: Dict[str, Callable] = None,\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Create a DataFrame from a given dataset with specified columns and transformations.\n","\n","    Args:\n","        dataset (Union[str, Dataset]): The dataset to load. It can be either a string representing the dataset name\n","                                       or a pre-loaded Dataset object.\n","        subset (str): The subset of the dataset to use (e.g., \"train\", \"test\", \"validation\"). Default is \"train\".\n","        columns (List[str]): The list of columns to include in the DataFrame. If None, all columns will be included.\n","        transformations (Dict[str, Callable]): A dictionary specifying the transformations to apply to each column.\n","                                               The keys are the column names, and the values are the transformation functions.\n","\n","    Returns:\n","        pd.DataFrame: The created DataFrame.\n","    \"\"\"\n","    if isinstance(dataset, str):\n","        dataset = load_dataset(dataset)\n","\n","    if subset not in dataset:\n","        raise ValueError(f\"Subset '{subset}' not found in the dataset.\")\n","\n","    data = dataset[subset]\n","\n","    if columns is None:\n","        columns = data.column_names\n","\n","    dataframe_data = []\n","    for i in range(data.num_rows):\n","        row_data = {}\n","        for column in columns:\n","            value = data[i][column]\n","            if transformations and column in transformations:\n","                value = transformations[column](value)\n","            row_data[column] = value\n","        dataframe_data.append(row_data)\n","\n","    return pd.DataFrame(dataframe_data)\n","\n","# Example usage\n","dataset_name = \"fka/awesome-chatgpt-prompts\"\n","subset = \"train\"\n","columns_to_include = [\"act\", \"prompt\"]\n","transformations = {\n","    \"act\": lambda x: x.upper(),\n","    \"prompt\": lambda x: x[:50] + \"...\",\n","}\n","\n","dataframe = create_dataframe(dataset_name, subset, columns_to_include, transformations)\n","print(dataframe.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HOySNkO-Vg8y"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMmCh6XU2aNU1uCYG+/VlXJ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"031455cf4bf0444eb400990dee429830":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0448ffc063164e66a30ee7dfbbb60e63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"062702ad6c304d1391e2b162fde4e671":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0657c280a7ef4101af6998701820f9fb","placeholder":"​","style":"IPY_MODEL_75d7d1d2d84248d3b6e0a34ab57f878b","value":" 191/191 [00:00&lt;00:00, 5064.78it/s]"}},"0642cfbbf8664232996fad5c9abd8931":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0657c280a7ef4101af6998701820f9fb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"073540de47484fbf9ca6afd582ce5e71":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e27cec649304fb98024b333e883013d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfac62adf88f4ae3af85887d91b170b1","max":394,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6f76643eaaf34ff8aca13b83518f6d5a","value":394}},"0f98309199334d8bac8ae50c92a65eef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1033651e716e4151b0d2e04a2247dc63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b489515b602a4ea5bcdce42d1b7dcc45","placeholder":"​","style":"IPY_MODEL_8b73564e70974a479aac96e432510021","value":"Generating train split: "}},"1e2744ac449547eca69f8f450155af73":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2195e0374c3842109cdddc6723316c28":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"270f052c3b9e4396b5f1a0ce73fdea45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f60d4c940e43489197b8391d8bdb63fc","placeholder":"​","style":"IPY_MODEL_8b5cbb9e5b964eb68e8ad017bb13a4a9","value":"Resolving data files: 100%"}},"28334b859bb24e85b5bd1d3ed7230e57":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d1f6586afba4d8686e865d33758fa50":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_310d5621ea4e4bde9ceccb3326dda2ff","placeholder":"​","style":"IPY_MODEL_76a5af87d6544594bb463317cfe76133","value":" 191/0 [00:00&lt;00:00, 701.16 examples/s]"}},"310d5621ea4e4bde9ceccb3326dda2ff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32c6227adfae4b389d962ec664d98b4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32e45ed72f434c3693ac078977087360","placeholder":"​","style":"IPY_MODEL_8a5c409b7ff44e81921de4aa4eff5d88","value":" 712k/712k [00:00&lt;00:00, 2.71MB/s]"}},"32dd933e91bf464686eaba6e386e1b51":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b392894a163f42a482c2a20febf93f33","placeholder":"​","style":"IPY_MODEL_6223261ff9564703b157deb0f6cdfd42","value":" 232k/232k [00:00&lt;00:00, 3.24MB/s]"}},"32e45ed72f434c3693ac078977087360":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34810852167d4a568a3b3104c5773696":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38039907ae554553822a548f9efeda34":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3840b2e2bdd2436b99dbf00be814319c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"384c08650a714767b6445e65b0b4ec39":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f3980e3cf62415693b34f3a5d0affbd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fd9c1475d7c490dabe870b247f775bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4080f47d1882450384f41b1d7efa8c5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_270f052c3b9e4396b5f1a0ce73fdea45","IPY_MODEL_ba1772f8fd0249779ebc11e22bfa7e48","IPY_MODEL_062702ad6c304d1391e2b162fde4e671"],"layout":"IPY_MODEL_9d7a0a8a9c8245379fde62d744c329cd"}},"40859ee4ff8d4837bbd9ec88f67131ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4631c6a894f349d48c6c4e4af3834bed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4922dc92fb434e07a41901e0322fe785":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d5dbbf9bdcd34acfb953d6554679f104","IPY_MODEL_f0274de6492742e6912622620a42f54a","IPY_MODEL_32dd933e91bf464686eaba6e386e1b51"],"layout":"IPY_MODEL_38039907ae554553822a548f9efeda34"}},"4ee794ce4e494e73907a932ec4ffdcfb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"508b2a4245d549e0a67c53e7ccb8bc9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5488068a777a4c9cbb009f369c6b88bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5852e7fc8eac4031a6b24e9319344cbb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ad75df56d074a7e92fda70f1abd15ee":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6223261ff9564703b157deb0f6cdfd42":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"648c48f66d554d7a924f112d04b6cef6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67bcafe5b34740c7ae202abbd4ea9a23":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1033651e716e4151b0d2e04a2247dc63","IPY_MODEL_a46cbe7cabc34f58800c539fe0329726","IPY_MODEL_2d1f6586afba4d8686e865d33758fa50"],"layout":"IPY_MODEL_4ee794ce4e494e73907a932ec4ffdcfb"}},"67e3aeb7e6aa4518ad8f1adb6e520241":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd7e248494de44c9acbbd34f81afa3a8","placeholder":"​","style":"IPY_MODEL_9c4a14fcc6d846e2b27d803a6bfe2905","value":"special_tokens_map.json: 100%"}},"6f76643eaaf34ff8aca13b83518f6d5a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"737e3a4e484e4edeb9537bd0e43c3cbb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75b26f738f4b4c2ba76c88b702b2b2da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"75d7d1d2d84248d3b6e0a34ab57f878b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76a5af87d6544594bb463317cfe76133":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"77ae4e4646084110b37df1c591fa0e14":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7df1a157a5de4a9cb9f22f53bcf0d224":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f1ba3b102844c6b8db5a3615de4ecf3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f764be0990d44fda621904e614e86d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_67e3aeb7e6aa4518ad8f1adb6e520241","IPY_MODEL_8a4c1a3bf18d4e8d80f05d8945b11f52","IPY_MODEL_f4131872e8314ee1b31aa79cbbfc9782"],"layout":"IPY_MODEL_648c48f66d554d7a924f112d04b6cef6"}},"815182760ccb4fd69e6974c209bc6544":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a4c1a3bf18d4e8d80f05d8945b11f52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f3980e3cf62415693b34f3a5d0affbd","max":125,"min":0,"orientation":"horizontal","style":"IPY_MODEL_40859ee4ff8d4837bbd9ec88f67131ba","value":125}},"8a5c409b7ff44e81921de4aa4eff5d88":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b5cbb9e5b964eb68e8ad017bb13a4a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b73564e70974a479aac96e432510021":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"975658d3d12b438687f182e9ac8b53e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c4a14fcc6d846e2b27d803a6bfe2905":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d7a0a8a9c8245379fde62d744c329cd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d9f5344298c43388a6d2b47b943cc70":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a265b0c4def34a03b2c293e434705f01":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a46cbe7cabc34f58800c539fe0329726":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f38c0be4f3a84364bc5f600b86ddea3b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_df7462385ee44beb821ba90207eb3a92","value":1}},"a47e611a93434e6fb8af7d78161497fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34810852167d4a568a3b3104c5773696","placeholder":"​","style":"IPY_MODEL_737e3a4e484e4edeb9537bd0e43c3cbb","value":"tokenizer.json: 100%"}},"b392894a163f42a482c2a20febf93f33":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b489515b602a4ea5bcdce42d1b7dcc45":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8b1631dd8334586a207e9b3e2e56037":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba1772f8fd0249779ebc11e22bfa7e48":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2195e0374c3842109cdddc6723316c28","max":191,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5488068a777a4c9cbb009f369c6b88bb","value":191}},"c2499df3a42846378fd9a178fac94f9d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a47e611a93434e6fb8af7d78161497fc","IPY_MODEL_d4f127fff75c4ee78e6acc80fe24d828","IPY_MODEL_32c6227adfae4b389d962ec664d98b4c"],"layout":"IPY_MODEL_db3434a56c2240c1af206808b12c7703"}},"c3fba927f39848b4889dd8d89dbd0092":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6806a15c18f4f9596003daac817299b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e37f98f2078b4a58978377100500d75f","IPY_MODEL_fa61a73331ac4bb0b5ef4ce48739e6dc","IPY_MODEL_d3a229389fd8488f9d04902c5bd67769"],"layout":"IPY_MODEL_b8b1631dd8334586a207e9b3e2e56037"}},"c851c4eaf7d24011996785447f0cca2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ad75df56d074a7e92fda70f1abd15ee","max":191,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0f98309199334d8bac8ae50c92a65eef","value":191}},"c900ed6345c64d4bb8ba075cf24f3400":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_edc4b24971754af6969dddc3a06eb0b9","IPY_MODEL_c851c4eaf7d24011996785447f0cca2a","IPY_MODEL_e034ccde846442cc95bfe848e8d4cc4f"],"layout":"IPY_MODEL_3fd9c1475d7c490dabe870b247f775bd"}},"ca7befa9108b4dbdbd2bd2b0e3a0273d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77ae4e4646084110b37df1c591fa0e14","placeholder":"​","style":"IPY_MODEL_0448ffc063164e66a30ee7dfbbb60e63","value":" 394/394 [00:00&lt;00:00, 14.0kB/s]"}},"cd7e248494de44c9acbbd34f81afa3a8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce7757e7a3324dfeac0196754e687458":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f8fcc5f59984450aa73e9a8379c1fcbe","IPY_MODEL_0e27cec649304fb98024b333e883013d","IPY_MODEL_ca7befa9108b4dbdbd2bd2b0e3a0273d"],"layout":"IPY_MODEL_5852e7fc8eac4031a6b24e9319344cbb"}},"d3a229389fd8488f9d04902c5bd67769":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_975658d3d12b438687f182e9ac8b53e5","placeholder":"​","style":"IPY_MODEL_031455cf4bf0444eb400990dee429830","value":" 191/191 [00:00&lt;00:00, 1689.41it/s]"}},"d4f127fff75c4ee78e6acc80fe24d828":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_073540de47484fbf9ca6afd582ce5e71","max":711661,"min":0,"orientation":"horizontal","style":"IPY_MODEL_75b26f738f4b4c2ba76c88b702b2b2da","value":711661}},"d5dbbf9bdcd34acfb953d6554679f104":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_384c08650a714767b6445e65b0b4ec39","placeholder":"​","style":"IPY_MODEL_4631c6a894f349d48c6c4e4af3834bed","value":"vocab.txt: 100%"}},"db3434a56c2240c1af206808b12c7703":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbcca98d130b46a5b0ae4ea3a24f7fca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc71c9e283bb4216919243b11d9e3dea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df7462385ee44beb821ba90207eb3a92":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dfac62adf88f4ae3af85887d91b170b1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dffc4921ce92447a94080a16805ea1cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e034ccde846442cc95bfe848e8d4cc4f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3fba927f39848b4889dd8d89dbd0092","placeholder":"​","style":"IPY_MODEL_815182760ccb4fd69e6974c209bc6544","value":" 191/191 [00:00&lt;00:00, 7068.72it/s]"}},"e37f98f2078b4a58978377100500d75f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f1ba3b102844c6b8db5a3615de4ecf3","placeholder":"​","style":"IPY_MODEL_dc71c9e283bb4216919243b11d9e3dea","value":"100%"}},"edc4b24971754af6969dddc3a06eb0b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7df1a157a5de4a9cb9f22f53bcf0d224","placeholder":"​","style":"IPY_MODEL_508b2a4245d549e0a67c53e7ccb8bc9b","value":"Resolving data files: 100%"}},"f0274de6492742e6912622620a42f54a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3840b2e2bdd2436b99dbf00be814319c","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dbcca98d130b46a5b0ae4ea3a24f7fca","value":231508}},"f38c0be4f3a84364bc5f600b86ddea3b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"f4131872e8314ee1b31aa79cbbfc9782":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d9f5344298c43388a6d2b47b943cc70","placeholder":"​","style":"IPY_MODEL_dffc4921ce92447a94080a16805ea1cc","value":" 125/125 [00:00&lt;00:00, 4.27kB/s]"}},"f60d4c940e43489197b8391d8bdb63fc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8fcc5f59984450aa73e9a8379c1fcbe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a265b0c4def34a03b2c293e434705f01","placeholder":"​","style":"IPY_MODEL_0642cfbbf8664232996fad5c9abd8931","value":"tokenizer_config.json: 100%"}},"fa61a73331ac4bb0b5ef4ce48739e6dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_28334b859bb24e85b5bd1d3ed7230e57","max":191,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1e2744ac449547eca69f8f450155af73","value":191}}}}},"nbformat":4,"nbformat_minor":0}
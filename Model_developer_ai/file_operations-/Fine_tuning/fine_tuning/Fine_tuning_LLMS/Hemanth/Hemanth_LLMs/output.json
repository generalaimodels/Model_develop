[{"page_content": "import csv\n\nimport os\n\nfrom typing import List\n\nfrom langchain_community.document_loaders import PyPDFLoader\n\ndef write_to_csv(file_path: str, data: dict, write_header: bool) -> None: \"\"\" Function to append data into a CSV file.\n\nArgs: file_path (str): The path to the CSV file. data (dict): The data to be appended into the CSV file. write_header (bool): Whether to write the header. \"\"\" mode = 'a' if os.path.exists(file_path) else 'w' with open(file_path, mode, newline='', encoding='UTF-8', errors='ignore') as file: writer = csv.DictWriter(file, fieldnames=[\"content\", \"documents\", \"metasource\"]) if write_header: writer.writeheader() try: writer.writerow({k: data[k] for k in [\"content\", \"documents\", \"metasource\"]}) except UnicodeEncodeError: print(f\"Warning: UnicodeEncodeError encountered for file {data['documents']}. Skipping this file.\")\n\ndef read_pdfs_from_folder(folder_path: str, csv_file_path: str) -> None: \"\"\" Function to recursively read PDF files from a folder and its subfolders and extract their content.\n\nArgs: folder_path (str): The path to the folder containing the PDF files. csv_file_path (str): The path to the CSV file. \"\"\" for root, dirs, files in os.walk(folder_path): for file_name in files: if file_name.endswith(\".pdf\"): full_file_path = os.path.join(root, file_name) loader = PyPDFLoader(full_file_path) pages = loader.load_and_split() for page in pages: data = { \"content\": page.page_content, \"documents\": file_name, \"metasource\": page.metadata['source'] } write_to_csv(csv_file_path, data, file_name == os.listdir(root)[0])", "metadata": "{'source': 'C:\\\\Users\\\\heman\\\\Desktop\\\\Deep learning\\\\Hemanth_LLMs\\\\data_loder\\\\data_pdf_csv.py'}"}, {"page_content": "import os import glob from typing import List, Tuple from datasets import Dataset\n\nfrom langchain_community.document_loaders import DirectoryLoader\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ndef load_files_by_extension(folder_path: str, extensions: List[str]) -> List[str]: files = [] for ext in extensions: files.extend(glob.glob(os.path.join(folder_path, f\"**/*. {ext}\"), recursive=True)) return files\n\ndef split_documents(chunk_size: int, documents: List[str]) -> List[Tuple[str, str]]:\n\nseparators = [\n\n\"\\n#{1,6} \",\n\n\"```\\n\",\n\n\"\\n\\\\\n\n\n\n\\\\\n\n\n\n\\\\\n\n\n\n+\\n\",\n\n\"\\n--\n\n\n\n+\\n\",\n\n\"\\n___+\\n\",\n\n\"\\n\\n\",\n\n\"\\n\",\n\n\" \",\n\n\"\",\n\n]\n\ntext_splitter = RecursiveCharacterTextSplitter(\n\nchunk_size=chunk_size,\n\nchunk_overlap=int(chunk_size / 10),\n\nadd_start_index=True,\n\nstrip_whitespace=True,\n\nseparators=separators,\n\n)\n\nprocessed_docs = [] for doc_path in documents: with open(doc_path, \"r\") as f: content = f.read() chunks = text_splitter.split_content(content) for chunk in chunks: processed_docs.append((doc_path, chunk))\n\nreturn processed_docs\n\ndef main(): folderpath = \"/content/research_papers-/papers\" extensions = [\"pdf\", \"py\", \"csv\", \"json\", \"txt\", \"md\"] documents = []\n\nfor ext in extensions:\n\ndocuments.extend(load_files_by_extension(folderpath, [ext]))\n\nfor doc in documents:\n\nloader = DirectoryLoader(folderpath, glob=f\"*\n\n\n\n/\n\n\n\n. {ext}\", show_progress=True, use_multithreading=True, silent_errors=True)\n\ndocs = loader.load()\n\nimport json from typing import List, Dict from langchain_community.document_loaders import DirectoryLoader\n\ndef create_json_file(folder_path: str, output_file: str) -> None: \"\"\" Load all files from the given folder and create a JSON file.\n\nArgs: folder_path (str): Path to the folder containing the files. output_file (str): Path to the output JSON file. \"\"\" # Load all files loader = DirectoryLoader(folder_path, glob=\"**/[!. ]*\", show_progress=True, use_multithreading=True, silent_errors=True) documents = loader.load()\n\n# Convert documents to a list of dictionaries data: List[Dict[str, str]] = [] for doc in documents: data.append({ 'page_content': doc.page_content, 'metadata': str(doc.metadata) })\n\n# Write to the JSON file with open(output_file, 'w') as json_file: json.dump(data, json_file, default=str)\n\nif __name__ == \"__main__\":\n\nfolder_path = 'C:/Users/heman/Desktop/Deep learning/Hemanth_LLMs/data_loder/'\n\noutput_file = 'C:/Users/heman/Desktop/Deep learning/Hemanth_LLMs/output.json'\n\ncreate_json_file(folder_path, output_file)", "metadata": "{'source': 'C:\\\\Users\\\\heman\\\\Desktop\\\\Deep learning\\\\Hemanth_LLMs\\\\data_loder\\\\from_folder_to_json.py'}"}, {"page_content": "import os\n\nimport json\n\nimport requests\n\nfrom typing import Dict\n\nfrom tqdm import tqdm\n\nfrom urllib.parse import urlparse\n\nfrom requests.adapters import HTTPAdapter\n\nfrom urllib3.util.retry import Retry\n\n# Importing for handling archives from zipfile import ZipFile import tarfile\n\n# Function to download a file with resuming capability def download_file(url: str, dest_folder: str) -> str: if not os.path.exists(dest_folder): os.makedirs(dest_folder)  # Create destination folder if it does not exist\n\nfilename = os.path.basename(urlparse(url).path)\n\nfile_path = os.path.join(dest_folder, filename)\n\n# Start the session\n\nsession = requests.Session()\n\nretry = Retry(connect=3, backoff_factor=0.5)\n\nadapter = HTTPAdapter(max_retries=retry)\n\nsession.mount('http://', adapter)\n\nsession.mount('https://', adapter)\n\nresponse = session.get(url, stream=True) response.raise_for_status()  # Ensure we raise an error for bad statuses\n\n# Stream the download with open(file_path, 'ab') as file, tqdm( desc=filename, total=int(response.headers.get('content-length', 0)), unit='iB', unit_scale=True, unit_divisor=1024, ) as bar: for data in response.iter_content(chunk_size=1024): size = file.write(data) bar.update(size)\n\nreturn file_path\n\n# Function to extract the archive def extract_archive(file_path: str) -> str: if file_path.endswith('.zip'): with ZipFile(file_path, 'r') as zip_ref: zip_ref.extractall(path=os.path.splitext(file_path)[0]) return os.path.splitext(file_path)[0] elif file_path.endswith('.tar') or file_path.endswith('.tar.gz') or file_path.endswith('.tgz'): with tarfile.open(file_path, 'r:*') as tar_ref: tar_ref.extractall(path=os.path.splitext(file_path)[0]) return os.path.splitext(file_path)[0] else: raise ValueError(\"Unsupported archive format\")\n\n# Function to preprocess data within a folder def preprocess_data(folder_path: str) -> Dict: # Assuming the folder contains files we want to preprocess # This function should be adapted to your data and preprocessing steps processed_data = {} for root, dirs, files in os.walk(folder_path): for file in files: file_path = os.path.join(root, file) # Example preprocessing: just reading the file names processed_data[file] = {\"path\": file_path}\n\nreturn processed_data\n\n# Function to save data to a JSON file def save_to_json(data: Dict, file_path: str): with open(file_path, 'w', encoding='utf-8') as f: json.dump(data, f, ensure_ascii=False, indent=4)\n\n# Main function to orchestrate the download, extraction, and preprocessing def from_zip_tar_main(url: str, dest_folder: str = 'downloads', json_output: str = 'data.json'): \"\"\" Main function that handles downloading a .zip or .tar file from a given URL, extracts its contents, preprocesses the data inside, and saves the processed data into a JSON file.\n\n:param url: URL of the .zip or .tar file to download :param dest_folder: Destination folder where the downloaded file will be stored :param json_output: Filename for the output JSON file containing preprocessed data\n\narchive_url = \"https://www.openslr.org/resources/12/test-other.tar.gz\"  # Replace with your actual URL\n\n# Call the main function from_zip_tar_main(archive_url)\n\n\"\"\"\n\n# Download the file\n\nprint(\"Starting download...\")\n\nfile_path = download_file(url, dest_folder)\n\nprint(\"Download finished.\")\n\n# Extract the contents print(\"Extracting archive...\") extraction_path = extract_archive(file_path) print(f\"Extraction completed. Extracted to: {extraction_path}\")\n\n# Preprocess the data\n\nprint(\"Preprocessing data...\")\n\nprocessed_data = preprocess_data(extraction_path)\n\n# Save to JSON json_path = os.path.join(dest_folder, json_output) save_to_json(processed_data, json_path) print(f\"Preprocessed data saved to: {json_path}\")", "metadata": "{'source': 'C:\\\\Users\\\\heman\\\\Desktop\\\\Deep learning\\\\Hemanth_LLMs\\\\data_loder\\\\url_zip_tar.py'}"}, {"page_content": "from pathlib import Path\n\nfrom typing import Dict, Any, List,Union,Optional\n\nfrom datasets import (load_dataset,\n\nDatasetDict,\n\nconcatenate_datasets\n\n)\n\n#Load the datset\n\ndef load_and_prepare_dataset(\n\ninput_source: Union[str, Path, Dict[str, List[Union[str, Path]]]],\n\nsplit_ratios: tuple = (0.8, 0.1, 0.1),\n\nseed: int = 42,\n\nstreaming: bool = False\n\n) -> DatasetDict:\n\n\"\"\"\n\nLoad a dataset from various input sources and prepare it by splitting into train, test, and eval sets.\n\n:param input_source: A dataset name, path to a folder, a single file, multiple files, or a dictionary specifying train, test, and eval files.\n\n:param split_ratios: A tuple containing the ratios for train, test, and eval splits (default is (0.8, 0.1, 0.1)).\n\n:param seed: A random seed for reproducibility of the split (default is 42).\n\n:param streaming: Whether to use streaming to handle large files (default is False).\n\n:return: A DatasetDict containing the split datasets.\n\nExample:\n\n# Example usage with streaming for large files:\n\n# dataset_dict = load_and_prepare_dataset({\n\n#     'train': ['train_file_1.csv', 'train_file_2.csv'],\n\n#     'test': ['test_file.csv'],\n\n#     'eval': ['eval_file.csv']\n\n# }, streaming=True)\n\n# print(dataset_dict)\n\nOUTPUT1:\n\nDatasetDict({\n\ntrain: DatasetDict({\n\ntrain: Dataset({\n\nfeatures: ['act', 'prompt'],\n\nnum_rows: 459\n\n})\n\n})\n\ntest: DatasetDict({\n\ntrain: Dataset({\n\nfeatures: ['act', 'prompt'],\n\nnum_rows: 459\n\n})\n\n})\n\neval: DatasetDict({\n\ntrain: Dataset({\n\nfeatures: ['act', 'prompt'],\n\nnum_rows: 153\n\n})\n\n})\n\n})\n\nEXAMPLE2:\n\ndataset=load_and_prepare_dataset('fka/awesome-chatgpt-prompts')\n\nDatasetDict({\n\ntrain: Dataset({\n\nfeatures: ['act', 'prompt'],\n\nnum_rows: 122\n\n})\n\ntest: Dataset({\n\nfeatures: ['act', 'prompt'],\n\nnum_rows: 15\n\n})\n\neval: Dataset({\n\nfeatures: ['act', 'prompt'],\n\nnum_rows: 16\n\n})\n\n})\n\nEXAMPLE3:\n\ndatset_path=load_and_prepare_dataset('/content/awesome-chatgpt-prompts')\n\nDatasetDict({\n\ntrain: Dataset({\n\nfeatures: ['act', 'prompt'],\n\nnum_rows: 122\n\n})\n\ntest: Dataset({\n\nfeatures: ['act', 'prompt'],\n\nnum_rows: 15\n\n})\n\neval: Dataset({\n\nfeatures: ['act', 'prompt'],\n\nnum_rows: 16\n\n})\n\n})\n\n\"\"\"\n\n# Load dataset from different types of input sources\n\nif isinstance(input_source, (str, Path)):\n\n# Dataset name, single file or path to folder\n\ndataset = load_dataset(input_source, streaming=streaming)\n\ndataset = DatasetDict(dataset)\n\nelif isinstance(input_source, dict):\n\n# Dictionary with specified train, test, and eval files\n\nformats = ['csv', 'json', 'jsonl', 'parquet', 'txt']\n\ndatasets = {}\n\nfor split, files in input_source.items():\n\nformat_detected = None\n\nfor fmt in formats:\n\nif any(str(file).endswith(fmt) for file in files):\n\nformat_detected = fmt\n\nbreak\n\nif format_detected is None:\n\nraise ValueError(f\"No supported file format detected for files: {files}\")\n\ndatasets[split] = load_dataset(format_detected, data_files=files, streaming=streaming)\n\ndataset = DatasetDict(datasets)\n\nelse:\n\nraise ValueError(\"Input source should be a dataset name, path to a folder, a single file, multiple files, or a dictionary.\")\n\n# Perform the split if needed and if not in streaming mode\n\nif not streaming:\n\ntrain_size, test_size, eval_size = split_ratios\n\nassert 0.0 < train_size < 1.0 and 0.0 < test_size < 1.0 and 0.0 < eval_size < 1.0 and (train_size + test_size + eval_size) == 1.0, \\\n\n\"Split ratios must be between 0 and 1 and sum up to 1.\"\n\nif \"train\" not in dataset or \"test\" not in dataset or \"eval\" not in dataset:\n\n# Assuming all splits are to be derived from the 'train' dataset\n\nfull_dataset = concatenate_datasets(list(dataset.values())) if isinstance(dataset, dict) else dataset\n\nsplit_dataset = full_dataset.train_test_split(train_size=train_size, seed=seed)\n\ntest_eval_split = split_dataset['test'].train_test_split(test_size=test_size / (test_size + eval_size), seed=seed)\n\ndataset = DatasetDict({\n\n\"train\": split_dataset[\"train\"],\n\n\"test\": test_eval_split[\"train\"],\n\n\"eval\": test_eval_split[\"test\"]\n\n})\n\nreturn dataset", "metadata": "{'source': 'C:\\\\Users\\\\heman\\\\Desktop\\\\Deep learning\\\\Hemanth_LLMs\\\\data_loder\\\\data_loader.py'}"}]
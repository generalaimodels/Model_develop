{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Texts:\n",
      "Input 1: Whitespace Tokenization\n",
      "Tokens: ['Whitespace', 'Tokenization']\n",
      "Input 2: Splitting text into tokens based on whitespace characters\n",
      "Tokens: ['Splitting', 'text', 'into', 'tokens', 'based', 'on', 'whitespace', 'characters']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def tokenize_on_whitespace(text_list: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Tokenize a list of strings, splitting each string into tokens based on whitespace characters.\n",
    "\n",
    "    Args:\n",
    "    text_list (List[str]): A list of strings to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "    List[List[str]]: A list of lists, where each sublist contains tokens from the corresponding input string.\n",
    "    \"\"\"\n",
    "    return [text.split() for text in text_list]\n",
    "\n",
    "# Example usage:\n",
    "input_texts = [\"Whitespace Tokenization\", \"Splitting text into tokens based on whitespace characters\"]\n",
    "tokenized_texts = tokenize_on_whitespace(input_texts)\n",
    "\n",
    "print(\"Tokenized Texts:\")\n",
    "for i, tokens in enumerate(tokenized_texts):\n",
    "    print(f\"Input {i+1}: {input_texts[i]}\")\n",
    "    print(f\"Tokens: {tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Texts:\n",
      "Input 1: Punctuation Tokenization\n",
      "Tokens: ['Punctuation Tokenization']\n",
      "Input 2: Splitting text into tokens based on punctuation marks\n",
      "Tokens: ['Splitting text into tokens based on punctuation marks']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Pattern\n",
    "\n",
    "# Define a pattern for matching punctuation\n",
    "PUNCTUATION_PATTERN: Pattern = re.compile(r'[^\\w\\s]')\n",
    "\n",
    "def tokenize_on_punctuation(text_list: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Tokenize a list of strings, splitting each string into tokens based on punctuation marks.\n",
    "\n",
    "    Args:\n",
    "        text_list (List[str]): A list of strings to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        List[List[str]]: A list of lists, where each sublist contains tokens from the corresponding input string.\n",
    "    \"\"\"\n",
    "    # Split each string by matching punctuation\n",
    "    return [re.split(PUNCTUATION_PATTERN, text) for text in text_list]\n",
    "\n",
    "# Example usage:\n",
    "input_texts = [\n",
    "    \"Punctuation Tokenization\",\n",
    "    \"Splitting text into tokens based on punctuation marks\"\n",
    "]\n",
    "tokenized_texts = tokenize_on_punctuation(input_texts)\n",
    "\n",
    "print(\"Tokenized Texts:\")\n",
    "for i, tokens in enumerate(tokenized_texts):\n",
    "    print(f\"Input {i+1}: {input_texts[i]}\")\n",
    "    print(f\"Tokens: {tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'It', 's', 'amazing']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Pattern\n",
    "\n",
    "def regex_tokenizer(text: str, pattern: Pattern[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenizes the given text using a regular expression pattern.\n",
    "\n",
    "    :param text: The text to be tokenized.\n",
    "    :param pattern: The compiled regular expression pattern used for tokenization.\n",
    "    :return: A list of tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize the text using the provided pattern\n",
    "    tokens = re.findall(pattern, text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the input text\n",
    "    user_input_text = \"The quick brown fox jumps over the lazy dog. It's amazing!\"\n",
    "\n",
    "    # Define a regular expression pattern for tokenization\n",
    "    # This example pattern splits on whitespace and punctuation\n",
    "    regex_pattern = r'\\b\\w+\\b'\n",
    "\n",
    "    # Compile the regular expression pattern for efficiency\n",
    "    compiled_pattern = re.compile(regex_pattern)\n",
    "\n",
    "    # Tokenize the user input text using the compiled pattern\n",
    "    tokens = regex_tokenizer(user_input_text, compiled_pattern)\n",
    "\n",
    "    # Output the result\n",
    "    print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Hello, world! This is a sample text. It contains 123 numbers and punctuation.\n",
      "\n",
      "Tokens:\n",
      "['Hello', ',', 'world', '!', 'This', 'is', 'a', 'sample', 'text', '.', 'It', 'contains', '123', 'numbers', 'and', 'punctuation', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def regex_tokenize(text: str, pattern: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize the given text using a regular expression pattern.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be tokenized.\n",
    "        pattern (str): The regular expression pattern used for tokenization.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of tokens extracted from the text.\n",
    "    \"\"\"\n",
    "    tokens = re.findall(pattern, text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    text = \"Hello, world! This is a sample text. It contains 123 numbers and punctuation.\"\n",
    "    pattern = r\"\\w+|[^\\w\\s]\"\n",
    "\n",
    "    tokens = regex_tokenize(text, pattern)\n",
    "    print(\"Original text:\")\n",
    "    print(text)\n",
    "    print(\"\\nTokens:\")\n",
    "    print(tokens)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Hello, world! This is a sample text.\n",
      "\n",
      "Tokens after BPE:\n",
      "['H', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!', ' ', 'T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', ' ', 's', 'a', 'm', 'p', 'l', 'e', ' ', 't', 'e', 'x', 't', '.']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def get_byte_pairs(tokens: List[str]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Get all byte pairs from the given list of tokens.\n",
    "\n",
    "    Args:\n",
    "        tokens (List[str]): A list of tokens.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, str]]: A list of byte pairs.\n",
    "    \"\"\"\n",
    "    byte_pairs = []\n",
    "    for token in tokens:\n",
    "        chars = list(token)\n",
    "        for i in range(len(chars) - 1):\n",
    "            byte_pairs.append((chars[i], chars[i + 1]))\n",
    "    return byte_pairs\n",
    "\n",
    "\n",
    "def merge_byte_pairs(tokens: List[str], byte_pair: Tuple[str, str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Merge the given byte pair in the list of tokens.\n",
    "\n",
    "    Args:\n",
    "        tokens (List[str]): A list of tokens.\n",
    "        byte_pair (Tuple[str, str]): The byte pair to merge.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of tokens with the byte pair merged.\n",
    "    \"\"\"\n",
    "    merged_tokens = []\n",
    "    for token in tokens:\n",
    "        merged_token = token.replace(byte_pair[0] + byte_pair[1], byte_pair[0] + \"_\" + byte_pair[1])\n",
    "        merged_tokens.append(merged_token)\n",
    "    return merged_tokens\n",
    "\n",
    "\n",
    "def bpe_tokenize(text: str, num_merges: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize the given text using Byte Pair Encoding (BPE).\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be tokenized.\n",
    "        num_merges (int): The number of merge operations to perform.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of tokens after applying BPE.\n",
    "    \"\"\"\n",
    "    tokens = list(text)\n",
    "    for _ in range(num_merges):\n",
    "        byte_pairs = get_byte_pairs(tokens)\n",
    "        pair_frequencies = defaultdict(int)\n",
    "        for pair in byte_pairs:\n",
    "            pair_frequencies[pair] += 1\n",
    "        if not pair_frequencies:\n",
    "            break\n",
    "        most_frequent_pair = max(pair_frequencies, key=pair_frequencies.get)\n",
    "        tokens = merge_byte_pairs(tokens, most_frequent_pair)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    text = \"Hello, world! This is a sample text.\"\n",
    "    num_merges = 5\n",
    "\n",
    "    tokens = bpe_tokenize(text, num_merges)\n",
    "    print(\"Original text:\")\n",
    "    print(text)\n",
    "    print(\"\\nTokens after BPE:\")\n",
    "    print(tokens)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "unwanted, running\n",
      "\n",
      "Tokens after WordPiece Tokenization:\n",
      "['[UNK]', 'runn', '##ing']\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class WordPieceTokenizer:\n",
    "    def __init__(self, vocab: Dict[str, int], unk_token: str = \"[UNK]\", max_input_chars_per_word: int = 100):\n",
    "        \"\"\"\n",
    "        Initialize the WordPieceTokenizer.\n",
    "\n",
    "        Args:\n",
    "            vocab (Dict[str, int]): The vocabulary dictionary mapping subwords to their indices.\n",
    "            unk_token (str): The token to use for unknown subwords. Default is \"[UNK]\".\n",
    "            max_input_chars_per_word (int): The maximum number of characters to consider for each word. Default is 100.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenize the given text using WordPiece Tokenization.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of subword tokens.\n",
    "        \"\"\"\n",
    "        output_tokens = []\n",
    "        for token in text.split():\n",
    "            chars = list(token)\n",
    "            if len(chars) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "\n",
    "            is_bad = False\n",
    "            start = 0\n",
    "            sub_tokens = []\n",
    "            while start < len(chars):\n",
    "                end = len(chars)\n",
    "                cur_substr = None\n",
    "                while start < end:\n",
    "                    substr = \"\".join(chars[start:end])\n",
    "                    if start > 0:\n",
    "                        substr = \"##\" + substr\n",
    "                    if substr in self.vocab:\n",
    "                        cur_substr = substr\n",
    "                        break\n",
    "                    end -= 1\n",
    "                if cur_substr is None:\n",
    "                    is_bad = True\n",
    "                    break\n",
    "                sub_tokens.append(cur_substr)\n",
    "                start = end\n",
    "\n",
    "            if is_bad:\n",
    "                output_tokens.append(self.unk_token)\n",
    "            else:\n",
    "                output_tokens.extend(sub_tokens)\n",
    "        return output_tokens\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    vocab = {\n",
    "        \"[UNK]\": 0,\n",
    "        \"[CLS]\": 1,\n",
    "        \"[SEP]\": 2,\n",
    "        \"want\": 3,\n",
    "        \"##want\": 4,\n",
    "        \"##ed\": 5,\n",
    "        \"wa\": 6,\n",
    "        \"un\": 7,\n",
    "        \"runn\": 8,\n",
    "        \"##ing\": 9,\n",
    "        \",\": 10,\n",
    "        \"low\": 11,\n",
    "        \"lowest\": 12,\n",
    "    }\n",
    "\n",
    "    text = \"unwanted, running\"\n",
    "    tokenizer = WordPieceTokenizer(vocab)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(\"Original text:\")\n",
    "    print(text)\n",
    "    print(\"\\nTokens after WordPiece Tokenization:\")\n",
    "    print(tokens)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "I won't be able to attend the meeting. It's scheduled for 5 p.m. on Monday.\n",
      "\n",
      "Tokens after Treebank Word Tokenization:\n",
      "['I', 'will', 'not', 'be', 'able', 'to', 'attend', 'the', 'meeting', '.', 'It', 's', 'scheduled', 'for', '5', 'p', '.', 'm', '.', 'on', 'Monday', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class TreebankWordTokenizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the TreebankWordTokenizer.\n",
    "        \"\"\"\n",
    "        self.contractions = {\n",
    "            \"won't\": \"will not\",\n",
    "            \"can't\": \"cannot\",\n",
    "            \"n't\": \" not\",\n",
    "            \"'s\": \" 's\",\n",
    "            \"'ve\": \" have\",\n",
    "            \"'re\": \" are\",\n",
    "            \"'d\": \" would\",\n",
    "            \"'ll\": \" will\",\n",
    "            \"'m\": \" am\"\n",
    "        }\n",
    "        self.contractions_re = re.compile(r\"(\\w+)('t|'s|'ve|'re|'d|'ll|'m)\")\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenize the given text using Treebank Word Tokenization.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of tokens.\n",
    "        \"\"\"\n",
    "        # Handle contractions\n",
    "        text = self.contractions_re.sub(self._replace_contractions, text)\n",
    "\n",
    "        # Handle special cases\n",
    "        text = re.sub(r\"([.,:;?!])\", r\" \\1 \", text)  # Add spaces around punctuation\n",
    "        text = re.sub(r\"[^a-zA-Z0-9.,:;?!]+\", \" \", text)  # Remove non-alphanumeric characters\n",
    "        text = re.sub(r\"\\s+\", \" \", text)  # Replace multiple spaces with a single space\n",
    "        text = text.strip()\n",
    "\n",
    "        # Tokenize the text\n",
    "        tokens = text.split()\n",
    "        return tokens\n",
    "\n",
    "    def _replace_contractions(self, match):\n",
    "        \"\"\"\n",
    "        Replace contractions in the matched text.\n",
    "\n",
    "        Args:\n",
    "            match: The matched object containing the contraction.\n",
    "\n",
    "        Returns:\n",
    "            str: The expanded form of the contraction.\n",
    "        \"\"\"\n",
    "        contraction = match.group(0)\n",
    "        if contraction in self.contractions:\n",
    "            return self.contractions[contraction]\n",
    "        else:\n",
    "            return contraction\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    text = \"I won't be able to attend the meeting. It's scheduled for 5 p.m. on Monday.\"\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(\"Original text:\")\n",
    "    print(text)\n",
    "    print(\"\\nTokens after Treebank Word Tokenization:\")\n",
    "    print(tokens)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "Tokens after Unigram Language Model Tokenization:\n",
      "['T', 'h', 'e', ' ', '<UNK>', ' ', '<UNK>', ' ', '<UNK>', 'u', '<UNK>', '<UNK>', 'h', 'e', ' ', '<UNK>', '<UNK>']\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "\n",
    "class UnigramLanguageModelTokenizer:\n",
    "    def __init__(self, vocab_size: int, unk_token: str = \"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Initialize the UnigramLanguageModelTokenizer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): The desired vocabulary size.\n",
    "            unk_token (str): The token to use for unknown subwords. Default is \"<UNK>\".\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.unk_token = unk_token\n",
    "        self.vocab: Dict[str, int] = {}\n",
    "        self.subword_counts: Dict[str, int] = defaultdict(int)\n",
    "\n",
    "    def train(self, corpus: List[str]):\n",
    "        \"\"\"\n",
    "        Train the Unigram Language Model on the given corpus.\n",
    "\n",
    "        Args:\n",
    "            corpus (List[str]): The corpus of text to train on.\n",
    "        \"\"\"\n",
    "        # Count the frequency of each subword in the corpus\n",
    "        for text in corpus:\n",
    "            for i in range(len(text)):\n",
    "                for j in range(i + 1, len(text) + 1):\n",
    "                    subword = text[i:j]\n",
    "                    self.subword_counts[subword] += 1\n",
    "\n",
    "        # Build the vocabulary based on the most frequent subwords\n",
    "        sorted_subwords = sorted(self.subword_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.vocab = {subword: i for i, (subword, _) in enumerate(sorted_subwords[:self.vocab_size])}\n",
    "        self.vocab[self.unk_token] = self.vocab_size\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenize the given text using the trained Unigram Language Model.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of subword tokens.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        n = len(text)\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            max_score = -math.inf\n",
    "            best_subword = None\n",
    "            for j in range(i + 1, n + 1):\n",
    "                subword = text[i:j]\n",
    "                if subword in self.vocab:\n",
    "                    score = self.subword_counts[subword]\n",
    "                    if score > max_score:\n",
    "                        max_score = score\n",
    "                        best_subword = subword\n",
    "            if best_subword is None:\n",
    "                best_subword = self.unk_token\n",
    "            tokens.append(best_subword)\n",
    "            i += len(best_subword)\n",
    "        return tokens\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    corpus = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\"\n",
    "    ]\n",
    "    vocab_size = 10\n",
    "\n",
    "    tokenizer = UnigramLanguageModelTokenizer(vocab_size)\n",
    "    tokenizer.train(corpus)\n",
    "\n",
    "    text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    print(\"Original text:\")\n",
    "    print(text)\n",
    "    print(\"\\nTokens after Unigram Language Model Tokenization:\")\n",
    "    print(tokens)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from typing import List\n",
    "\n",
    "def train_sentencepiece_model(texts: List[str], model_prefix: str, vocab_size: int = 32000) -> None:\n",
    "    \"\"\"\n",
    "    Trains a SentencePiece model with the given list of strings.\n",
    "\n",
    "    :param texts: The list of text strings to train the model on.\n",
    "    :param model_prefix: The prefix for the output model files.\n",
    "    :param vocab_size: The size of the vocabulary. Default is 32,000.\n",
    "    \"\"\"\n",
    "    # Prepare the text file required for SentencePiece training\n",
    "    with open('text.txt', 'w', encoding='utf-8') as f:\n",
    "        for text in texts:\n",
    "            f.write(f\"{text}\\n\")\n",
    "    \n",
    "    # Train the SentencePiece model\n",
    "    spm.SentencePieceTrainer.train(f'--input=text.txt --model_prefix={model_prefix} --vocab_size={vocab_size} --character_coverage=1.0')\n",
    "    print(f\"Model trained and saved with prefix '{model_prefix}'.\")\n",
    "\n",
    "def tokenize_with_sentencepiece(model_path: str, text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenizes a text string using a trained SentencePiece model.\n",
    "\n",
    "    :param model_path: The path to the trained SentencePiece model.\n",
    "    :param text: The text to be tokenized.\n",
    "    :return: A list of subword tokens.\n",
    "    \"\"\"\n",
    "    # Load the trained SentencePiece model\n",
    "    sp = spm.SentencePieceProcessor(model_file=model_path)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = sp.encode(text, out_type=str)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # List of sentences to train the SentencePiece model\n",
    "    user_input_texts = [\n",
    "        \"This is a test sentence for the SentencePiece model.\",\n",
    "        \"SentencePiece is an unsupervised text tokenizer and detokenizer.\",\n",
    "        \"It provides open-source pre-built and extensible models for various languages.\"\n",
    "    ]\n",
    "    \n",
    "    # Train the model and save with the provided prefix\n",
    "    model_prefix = 'spm_model'\n",
    "    train_sentencepiece_model(user_input_texts, model_prefix)\n",
    "    \n",
    "    # Load the trained model and tokenize a new sentence\n",
    "    model_path = f'{model_prefix}.model'\n",
    "    new_text = \"SentencePiece handles multiple languages without pre-tokenization.\"\n",
    "    tokens = tokenize_with_sentencepiece(model_path, new_text)\n",
    "    \n",
    "    # Output the result\n",
    "    print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class SentencePieceTokenizer:\n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the SentencePieceTokenizer.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): The path to the pre-trained SentencePiece model.\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.Load(model_path)\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenize the given text using the SentencePiece model.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of subword tokens.\n",
    "        \"\"\"\n",
    "        tokens = self.sp.EncodeAsPieces(text)\n",
    "        return tokens\n",
    "\n",
    "    def detokenize(self, tokens: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Detokenize the given list of subword tokens back into text.\n",
    "\n",
    "        Args:\n",
    "            tokens (List[str]): A list of subword tokens.\n",
    "\n",
    "        Returns:\n",
    "            str: The detokenized text.\n",
    "        \"\"\"\n",
    "        text = self.sp.DecodePieces(tokens)\n",
    "        return text\n",
    "\n",
    "\n",
    "def train_sentencepiece_model(corpus_path: str, model_prefix: str, vocab_size: int):\n",
    "    \"\"\"\n",
    "    Train a SentencePiece model on the given corpus.\n",
    "\n",
    "    Args:\n",
    "        corpus_path (str): The path to the corpus file.\n",
    "        model_prefix (str): The prefix for the output model files.\n",
    "        vocab_size (int): The desired vocabulary size.\n",
    "    \"\"\"\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f\"--input={corpus_path} --model_prefix={model_prefix} --vocab_size={vocab_size}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Train a SentencePiece model (replace with your own corpus and paths)\n",
    "    corpus_path = \"path/to/corpus.txt\"\n",
    "    model_prefix = \"path/to/model\"\n",
    "    vocab_size = 10000\n",
    "    train_sentencepiece_model(corpus_path, model_prefix, vocab_size)\n",
    "\n",
    "    # Example usage\n",
    "    model_path = \"path/to/model.model\"\n",
    "    tokenizer = SentencePieceTokenizer(model_path)\n",
    "\n",
    "    text = \"This is a sample text in English. これは日本語のサンプルテキストです。\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(\"Original text:\")\n",
    "    print(text)\n",
    "    print(\"\\nTokens after SentencePiece Tokenization:\")\n",
    "    print(tokens)\n",
    "\n",
    "    detokenized_text = tokenizer.detokenize(tokens)\n",
    "    print(\"\\nDetokenized text:\")\n",
    "    print(detokenized_text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import morfessor\n",
    "from typing import List, Tuple\n",
    "\n",
    "def train_morfessor_model(data: List[str]) -> morfessor.MorfessorIO:\n",
    "    \"\"\"\n",
    "    Trains a Morfessor model with the given list of words.\n",
    "\n",
    "    :param data: The list of words to train the model on.\n",
    "    :return: A trained Morfessor model.\n",
    "    \"\"\"\n",
    "    model = morfessor.BaselineModel()\n",
    "    # Prepare data for training (list of tuples (word, count))\n",
    "    training_data = [(word, 1) for word in data]\n",
    "    model.load_data(training_data)\n",
    "    model.train_batch()\n",
    "\n",
    "    return model\n",
    "\n",
    "def segment_words_with_morfessor(model: morfessor.MorfessorIO, words: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Segments a list of words using a trained Morfessor model.\n",
    "\n",
    "    :param model: The trained Morfessor model.\n",
    "    :param words: The list of words to segment.\n",
    "    :return: A list of segmented word lists.\n",
    "    \"\"\"\n",
    "    segmented_words = [model.viterbi_segment(word)[0] for word in words]\n",
    "    return segmented_words\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # List of words to train the Morfessor model\n",
    "    user_input_words = [\n",
    "        \"morphological\", \"segmentation\", \"unsupervised\", \"learning\", \"linguistics\",\n",
    "        \"morpheme\", \"analysis\", \"computational\", \"recognition\", \"algorithm\"\n",
    "    ]\n",
    "    \n",
    "    # Train the Morfessor model\n",
    "    trained_model = train_morfessor_model(user_input_words)\n",
    "    \n",
    "    # Segment new words using the trained model\n",
    "    new_words = [\"morphology\", \"linguist\", \"algorithmic\", \"computationally\"]\n",
    "    segmented_output = segment_words_with_morfessor(trained_model, new_words)\n",
    "    \n",
    "    # Output results\n",
    "    for word, segmented in zip(new_words, segmented_output):\n",
    "        print(f\"Original Word: {word}, Segmented: {segmented}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import morfessor\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class MorfessorTokenizer:\n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the MorfessorTokenizer.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): The path to the trained Morfessor model.\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.model = morfessor.BaselineModel()\n",
    "        self.model.load_model(model_path)\n",
    "\n",
    "    def tokenize(self, word: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenize the given word into morphemes using the Morfessor model.\n",
    "\n",
    "        Args:\n",
    "            word (str): The input word to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of morphemes.\n",
    "        \"\"\"\n",
    "        morphemes = self.model.viterbi_segment(word)[0]\n",
    "        return morphemes\n",
    "\n",
    "    def tokenize_sentence(self, sentence: str) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Tokenize the given sentence into morphemes using the Morfessor model.\n",
    "\n",
    "        Args:\n",
    "            sentence (str): The input sentence to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: A list of lists, where each inner list contains the morphemes of a word.\n",
    "        \"\"\"\n",
    "        words = sentence.split()\n",
    "        tokenized_sentence = [self.tokenize(word) for word in words]\n",
    "        return tokenized_sentence\n",
    "\n",
    "\n",
    "def train_morfessor_model(corpus_path: str, model_path: str):\n",
    "    \"\"\"\n",
    "    Train a Morfessor model on the given corpus.\n",
    "\n",
    "    Args:\n",
    "        corpus_path (str): The path to the corpus file.\n",
    "        model_path (str): The path to save the trained model.\n",
    "    \"\"\"\n",
    "    corpus = morfessor.MorfessorIO().read_corpus_file(corpus_path)\n",
    "    model = morfessor.BaselineModel()\n",
    "    model.train(corpus)\n",
    "    model.save_model(model_path)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Train a Morfessor model (replace with your own corpus and paths)\n",
    "    corpus_path = \"path/to/corpus.txt\"\n",
    "    model_path = \"path/to/model.bin\"\n",
    "    train_morfessor_model(corpus_path, model_path)\n",
    "\n",
    "    # Example usage\n",
    "    tokenizer = MorfessorTokenizer(model_path)\n",
    "\n",
    "    word = \"unbelievable\"\n",
    "    morphemes = tokenizer.tokenize(word)\n",
    "    print(f\"Morphemes for '{word}':\")\n",
    "    print(morphemes)\n",
    "\n",
    "    sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "    tokenized_sentence = tokenizer.tokenize_sentence(sentence)\n",
    "    print(\"\\nTokenized sentence:\")\n",
    "    print(tokenized_sentence)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

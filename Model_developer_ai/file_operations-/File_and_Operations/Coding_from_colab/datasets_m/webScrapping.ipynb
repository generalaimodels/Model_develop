{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "# Define a regular expression pattern for URLs\n",
    "URL_PATTERN = re.compile(\n",
    "    r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    ")\n",
    "\n",
    "def extract_urls(file_path: str) -> List[str]:\n",
    "    \"\"\"Extracts all URLs from a given text file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            data = file.read()\n",
    "    \n",
    "    urls = re.findall(URL_PATTERN, data)\n",
    "    return urls\n",
    "\n",
    "def get_files(directory: str, extensions: List[str]) -> List[str]:\n",
    "    \"\"\"Recursively returns a list of files in a given directory with the specified extensions.\"\"\"\n",
    "    files_list = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for ext in extensions:\n",
    "            files_list.extend([os.path.join(root, file) for file in files if file.endswith(ext)])\n",
    "    return files_list\n",
    "\n",
    "def write_urls_to_json(directory: str, urls_data: Dict[str, List[str]]):\n",
    "    \"\"\"Writes the extracted URLs to a JSON file in a professional format.\"\"\"\n",
    "    json_path = os.path.join(directory, 'extracted_urls.json')\n",
    "    with open(json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(urls_data, json_file, ensure_ascii=False, indent=4, sort_keys=True)\n",
    "\n",
    "def main(directory: str, extensions: List[str]):\n",
    "    files = get_files(directory, extensions)\n",
    "    urls_data = {}\n",
    "    for file_path in files:\n",
    "        urls = extract_urls(file_path)\n",
    "        # Normalize file paths to use forward slashes\n",
    "        normalized_file_path = file_path.replace(os.sep, '/')\n",
    "        urls_data[normalized_file_path] = urls\n",
    "    write_urls_to_json(directory, urls_data)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main('E:/LLMS/hemanth/', ['.txt', '.md', '.html', '.py', '.json', '.csv','.pdf'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_urls(url: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Extract all URLs from the given website.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the website to scrape.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries containing the extracted URLs\n",
    "            with their corresponding file extensions and additional details.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    urls = []\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        href = link.get(\"href\")\n",
    "        if href:\n",
    "            absolute_url = urljoin(url, href)\n",
    "            parsed_url = urlparse(absolute_url)\n",
    "            file_extension = re.findall(r\"\\.[a-zA-Z0-9]+$\", parsed_url.path)\n",
    "            file_name = parsed_url.path.split(\"/\")[-1]\n",
    "            domain = parsed_url.netloc\n",
    "            scheme = parsed_url.scheme\n",
    "            urls.append({\n",
    "                \"url\": absolute_url,\n",
    "                \"extension\": file_extension[0] if file_extension else \"\",\n",
    "                \"file_name\": file_name,\n",
    "                \"domain\": domain,\n",
    "                \"scheme\": scheme,\n",
    "                \"text\": link.text.strip(),\n",
    "                \"rel\": link.get(\"rel\", \"\"),\n",
    "                \"target\": link.get(\"target\", \"\"),\n",
    "                \"title\": link.get(\"title\", \"\")\n",
    "            })\n",
    "\n",
    "    return urls\n",
    "\n",
    "\n",
    "def save_to_json(urls: List[Dict[str, str]], output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the extracted URLs to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        urls (List[Dict[str, str]]): A list of dictionaries containing the extracted URLs\n",
    "            with their corresponding file extensions and additional details.\n",
    "        output_file (str): The path to the output JSON file.\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\") as file:\n",
    "        json.dump(urls, file, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "urls = extract_urls('https://web.stanford.edu/class/cs234/modules.html')\n",
    "save_to_json(urls, 'hemanth4.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_pdf(url: str, folder_path: str, file_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Download a PDF file from the given URL and save it to the specified folder.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the PDF file to download.\n",
    "        folder_path (str): The path to the folder where the PDF file will be saved.\n",
    "        file_name (str): The name of the PDF file to be saved.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded: {file_name}\")\n",
    "    else:\n",
    "        print(f\"Failed to download: {file_name}\")\n",
    "\n",
    "\n",
    "def download_pdfs(data: List[Dict[str, str]], folder_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Download PDF files from the given list of dictionaries and save them to the specified folder.\n",
    "\n",
    "    Args:\n",
    "        data (List[Dict[str, str]]): A list of dictionaries containing PDF file information.\n",
    "        folder_path (str): The path to the folder where the PDF files will be saved.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    for item in data:\n",
    "        if item[\"extension\"] == \".pdf\":\n",
    "            url = item[\"url\"]\n",
    "            file_name = item[\"file_name\"]\n",
    "            download_pdf(url, folder_path, file_name)\n",
    "\n",
    "\n",
    "def load_data_from_json(json_file: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Load PDF file information from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        json_file (str): The path to the JSON file containing the PDF file information.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries containing the PDF file information.\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "data = load_data_from_json('E:/LLMS/hemanth/Hemanth/file_operations-/File_and_Operations/Coding_from_colab/datasets_m/hemanth.json')\n",
    "download_pdfs(data,'E:/LLMS/hemanth/Hemanth/pdf' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you are my advanced python coding experts: always follows suggestion's \n",
    "1 ) you will write code following pep8 standards \n",
    "2) user proper module's for coding (handling arguments typing modules like etc)\n",
    "3) coding structure look very professional \n",
    "\n",
    "\n",
    "TASK: we have struture List[Dict[str, str] now we have traget .pdf files we have load speific folder user speific path\n",
    "\n",
    "\n",
    "\n",
    "example [\n",
    "{\n",
    "        \"url\": \"https://www.cs.cmu.edu/~ninamf/courses/601sp15/slides/26_privacy_4-22-2015.pdf\",\n",
    "        \"extension\": \".pdf\",\n",
    "        \"file_name\": \"26_privacy_4-22-2015.pdf\",\n",
    "        \"domain\": \"www.cs.cmu.edu\",\n",
    "        \"scheme\": \"https\",\n",
    "        \"text\": \"Slides (Privacy)\",\n",
    "        \"rel\": \"\",\n",
    "        \"target\": \"\",\n",
    "        \"title\": \"\"\n",
    "    },\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Set\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_urls(url: str, visited_urls: Set[str], max_depth: int = 3, current_depth: int = 0) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Extract all URLs from the given website recursively.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the website to scrape.\n",
    "        visited_urls (Set[str]): A set to keep track of visited URLs.\n",
    "        max_depth (int): The maximum depth of recursive calls (default: 3).\n",
    "        current_depth (int): The current depth of the recursive call (default: 0).\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries containing the extracted URLs\n",
    "            with their corresponding file extensions and additional details.\n",
    "    \"\"\"\n",
    "    if url in visited_urls or current_depth >= max_depth:\n",
    "        return []\n",
    "\n",
    "    visited_urls.add(url)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        urls = []\n",
    "        for link in soup.find_all(\"a\"):\n",
    "            href = link.get(\"href\")\n",
    "            if href:\n",
    "                absolute_url = urljoin(url, href)\n",
    "                parsed_url = urlparse(absolute_url)\n",
    "                file_extension = re.findall(r\"\\.[a-zA-Z0-9]+$\", parsed_url.path)\n",
    "                file_name = parsed_url.path.split(\"/\")[-1]\n",
    "                domain = parsed_url.netloc\n",
    "                scheme = parsed_url.scheme\n",
    "                urls.append({\n",
    "                    \"url\": absolute_url,\n",
    "                    \"extension\": file_extension[0] if file_extension else \"\",\n",
    "                    \"file_name\": file_name,\n",
    "                    \"domain\": domain,\n",
    "                    \"scheme\": scheme,\n",
    "                    \"text\": link.text.strip(),\n",
    "                    \"rel\": link.get(\"rel\", \"\"),\n",
    "                    \"target\": link.get(\"target\", \"\"),\n",
    "                    \"title\": link.get(\"title\", \"\")\n",
    "                })\n",
    "\n",
    "                # Recursively extract URLs from the linked pages\n",
    "                if domain == urlparse(url).netloc:\n",
    "                    urls.extend(extract_urls(absolute_url, visited_urls, max_depth, current_depth + 1))\n",
    "\n",
    "        return urls\n",
    "\n",
    "    except (requests.exceptions.RequestException, TimeoutError):\n",
    "        return []\n",
    "\n",
    "\n",
    "def save_to_json(urls: List[Dict[str, str]], output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the extracted URLs to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        urls (List[Dict[str, str]]): A list of dictionaries containing the extracted URLs\n",
    "            with their corresponding file extensions and additional details.\n",
    "        output_file (str): The path to the output JSON file.\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\") as file:\n",
    "        json.dump(urls, file, indent=4)\n",
    "\n",
    "\n",
    "visited_urls = set()\n",
    "urls = extract_urls('https://www.cs.ubc.ca/~schmidtm/Courses/LecturesOnML/',visited_urls,5)\n",
    "save_to_json(urls, 'hemanth2.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you are my advanced python coding experts: always follows suggestion's \n",
    "1 ) you will write code following pep8 standards \n",
    "2) user proper module's for coding (handling arguments typing modules like etc)\n",
    "3) coding structure look very professional \n",
    "\n",
    "\n",
    "TASK: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import os\n",
    "import urllib.request\n",
    "import re\n",
    "from datetime import date, timedelta\n",
    "\n",
    "def download_papers(query, max_results, save_dir, search_in=('title', 'summary')):\n",
    "    \"\"\"\n",
    "    This function downloads papers from arXiv based on the provided query.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The search query for the papers.\n",
    "    - max_results (int): The maximum number of results to return.\n",
    "    - save_dir (str): The directory where the papers will be saved.\n",
    "    - search_in (tuple): Where to search for keywords, options are 'title', 'summary', or both.\n",
    "    \"\"\"\n",
    "    # Get the current date\n",
    "    today = date.today()\n",
    "    # Get the date 30 days ago\n",
    "    start_date = today - timedelta(days=30)\n",
    "    # Construct the default API client\n",
    "    client = arxiv.Client()\n",
    "\n",
    "    # Create a search object\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "    )\n",
    "\n",
    "    # Get the results as a list\n",
    "    results = list(client.results(search))\n",
    "\n",
    "    # Filter the results based on the date range\n",
    "    filtered_results = [result for result in results if result.published.date() >= start_date]\n",
    "\n",
    "    # Convert the query into a list of phrases and keywords\n",
    "    phrases = re.findall(r'\"([^\"]+)\"', query)\n",
    "    keywords = [kw for kw in re.split(r'\"[^\"]+\"', query) if kw.strip() != '']\n",
    "    keywords = list(set(keywords + phrases))  # Combine phrases and keywords, remove duplicates\n",
    "\n",
    "    keyword_filtered_results = []  # A list to store the filtered results based on keywords\n",
    "\n",
    "    for result in filtered_results:  # Loop through the results\n",
    "        text_to_search = ' '.join(filter(None, [result.title if 'title' in search_in else '', \n",
    "                                                result.summary if 'summary' in search_in else '']))\n",
    "        match_count = sum(bool(re.search(r'\\b' + re.escape(keyword) + r'\\b', text_to_search, re.IGNORECASE)) for keyword in keywords)\n",
    "\n",
    "        # Adjust the required matches to be the minimum of 3 or the number of keywords\n",
    "        required_matches = min(1, len(keywords))\n",
    "\n",
    "        if match_count >= required_matches:  # If the result matches at least the required keywords\n",
    "            keyword_filtered_results.append((result, match_count))  # Add it to the filtered list\n",
    "\n",
    "    # Sort the filtered results by the number of keyword matches in descending order\n",
    "    keyword_filtered_results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(save_dir):\n",
    "        # If not, create the folder\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Download the filtered and sorted results\n",
    "    for result, match_count in keyword_filtered_results[:max_results]:\n",
    "        try:\n",
    "            # Get the paper id\n",
    "            paper_id = result.entry_id.split('/')[-1]\n",
    "            # Get the paper url\n",
    "            paper_url = result.pdf_url\n",
    "            # Get the paper file name\n",
    "            paper_file = os.path.join(save_dir, paper_id + '.pdf')\n",
    "            # Download the paper\n",
    "            urllib.request.urlretrieve(paper_url, paper_file)\n",
    "            print(f\"Downloaded {paper_id} with {match_count} keyword matches\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {paper_id}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "save_dir = \"E:/LLMS/hemanth/Hemanth/Deep_learning/papers_llms_model/\"\n",
    "query = \"'LLMS' 'NLP' 'large language model' 'architecture'\"\n",
    "No_of_papers = 10\n",
    "download_papers(query=query, max_results=No_of_papers, save_dir=save_dir, search_in=('title', 'summary'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from typing import Dict, List\n",
    "\n",
    "def convert_dataset_to_text(dataset_name: str, split: str = \"train\") -> Dict[str, List[str]]:\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "\n",
    "    # Convert the columns into a single text column using the map function\n",
    "    dataset = dataset.map(convert_to_text, remove_columns=dataset.column_names)\n",
    "\n",
    "    # Return the converted dataset as a dictionary\n",
    "    return {\"text\": dataset[\"text\"]}\n",
    "\n",
    "def convert_to_text(example: Dict[str, str]) -> Dict[str, str]:\n",
    "    text = \" \".join(str(value) for value in example.values())\n",
    "    return {\"text\": text}\n",
    "\n",
    "# List of dataset names\n",
    "dataset_names = [\n",
    "    \"open-orca/openorca\",\n",
    "    \"squad\",\n",
    "    \"glue\",\n",
    "    \"imdb\",\n",
    "    \"amazon_reviews_multi\",\n",
    "    \"yelp_review_full\",\n",
    "    \"rotten_tomatoes\",\n",
    "    \"ag_news\",\n",
    "    \"dbpedia_14\",\n",
    "    \"trec\",\n",
    "    \"paws\",\n",
    "    \"wiki_qa\",\n",
    "    \"yahoo_answers_topics\",\n",
    "    \"cos_e\",\n",
    "    \"hellaswag\",\n",
    "    \"story_cloze\",\n",
    "    \"art\",\n",
    "    \"sciq\",\n",
    "    \"social_i_qa\",\n",
    "    \"wiqa\"\n",
    "]\n",
    "\n",
    "# Convert each dataset and store the results in a dictionary\n",
    "converted_datasets = {}\n",
    "for dataset_name in dataset_names:\n",
    "    converted_datasets[dataset_name] = convert_dataset_to_text(dataset_name)\n",
    "\n",
    "# Print the number of examples in each converted dataset\n",
    "for dataset_name, dataset in converted_datasets.items():\n",
    "    print(f\"Dataset: {dataset_name}, Number of examples: {len(dataset['text'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def convert_to_text(example: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Concatenate all values from the example into a single text string.\n",
    "\n",
    "    Parameters:\n",
    "    - example (Dict[str, Any]): A dictionary representing a row from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, str]: A dictionary with a single key 'text' containing the concatenated text.\n",
    "    \"\"\"\n",
    "    text = \" \".join(str(value) for value in example.values())\n",
    "    return {\"text\": text}\n",
    "\n",
    "def process_datasets(dataset_names: List[str], split: str = 'train') -> None:\n",
    "    \"\"\"\n",
    "    Load each dataset by name, convert all columns into a single text column, and print the first item.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_names (List[str]): A list of dataset names to be processed.\n",
    "    - split (str): The split of the dataset to load, defaults to 'train'.\n",
    "    \"\"\"\n",
    "    for dataset_name in dataset_names:\n",
    "        # Load the dataset\n",
    "        dataset = load_dataset(dataset_name, split=split)\n",
    "\n",
    "        # Convert the columns into a single text column using the map function\n",
    "        dataset = dataset.map(convert_to_text, remove_columns=dataset.column_names)\n",
    "\n",
    "        # Print the first example to verify the result\n",
    "        print(f\"First item from the processed dataset '{dataset_name}':\")\n",
    "        print(dataset[0])\n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")  # Separator line for readability\n",
    "\n",
    "# List of 20 dataset names to process\n",
    "dataset_list = [\n",
    "    \"Open-Orca/OpenOrca\",       # Replace with actual dataset names\n",
    "    # \"dataset_name_2\",\n",
    "    # \"dataset_name_3\",\n",
    "    # ...\n",
    "    # \"dataset_name_20\",\n",
    "]\n",
    "\n",
    "# Make sure to replace the placeholders with the actual names of your datasets before running the function\n",
    "process_datasets(dataset_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "\n",
    "def merge_columns_into_text(dataset: str, split: str = 'train') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a specified dataset and split, then merge all columns into a single text column.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset (str): The name of the dataset to load.\n",
    "    - split (str): The dataset split to use, defaults to 'train'.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with a single column containing merged text data.\n",
    "    \"\"\"\n",
    "    # Load the specified dataset split\n",
    "    data = load_dataset(dataset, split=split)\n",
    "    \n",
    "    # Convert to pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Merge all columns into a single text column using 'map' function\n",
    "    df['merged'] = df.apply(lambda row: ' '.join(map(str, row)), axis=1)\n",
    "    \n",
    "    # Drop other columns\n",
    "    df = df[['merged']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "dataset_name = \"Open-Orca/OpenOrca\"\n",
    "split_type = \"train\"\n",
    "\n",
    "# Call the function and get the processed DataFrame\n",
    "processed_df = merge_columns_into_text(dataset_name, split_type)\n",
    "print(processed_df.head())  # Show the first few rows of the processed DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from typing import Dict\n",
    "\n",
    "def convert_to_text(example: Dict[str, str]) -> Dict[str, str]:\n",
    "    text = \" \".join(str(value) for value in example.values())\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"Open-Orca/OpenOrca\", split=\"train\")\n",
    "\n",
    "# Convert the four columns into a single text column using the map function\n",
    "dataset = dataset.map(convert_to_text, remove_columns=dataset.column_names)\n",
    "\n",
    "# Print the first example to verify the result\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def extract_text(example: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Concatenate all string values from the example into a single text string.\n",
    "\n",
    "    Parameters:\n",
    "    - example (Dict[str, Any]): A dictionary representing a row from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, str]: A dictionary with a single key 'text' containing the concatenated text.\n",
    "    \"\"\"\n",
    "    text_values = [str(value) for value in example.values() if isinstance(value, str)]\n",
    "    text = \" \".join(text_values)\n",
    "    return {\"text\": text}\n",
    "\n",
    "def extract_image_urls(example: Dict[str, Any]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract image URLs from the example.\n",
    "\n",
    "    Parameters:\n",
    "    - example (Dict[str, Any]): A dictionary representing a row from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, List[str]]: A dictionary with a single key 'image_urls' containing a list of image URLs.\n",
    "    \"\"\"\n",
    "    image_urls = [str(value) for value in example.values() if str(value).startswith(\"http\") and (str(value).endswith(\".jpg\") or str(value).endswith(\".png\"))]\n",
    "    return {\"image_urls\": image_urls}\n",
    "\n",
    "def extract_video_urls(example: Dict[str, Any]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract video URLs from the example.\n",
    "\n",
    "    Parameters:\n",
    "    - example (Dict[str, Any]): A dictionary representing a row from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, List[str]]: A dictionary with a single key 'video_urls' containing a list of video URLs.\n",
    "    \"\"\"\n",
    "    video_urls = [str(value) for value in example.values() if str(value).startswith(\"http\") and (str(value).endswith(\".mp4\") or str(value).endswith(\".avi\"))]\n",
    "    return {\"video_urls\": video_urls}\n",
    "\n",
    "def extract_tabular(example: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract tabular data from the example.\n",
    "\n",
    "    Parameters:\n",
    "    - example (Dict[str, Any]): A dictionary representing a row from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, Dict[str, Any]]: A dictionary with a single key 'tabular' containing the tabular data.\n",
    "    \"\"\"\n",
    "    tabular_data = {key: value for key, value in example.items() if not isinstance(value, (str, list))}\n",
    "    return {\"tabular\": tabular_data}\n",
    "\n",
    "def extract_numerical(example: Dict[str, Any]) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Extract numerical data from the example.\n",
    "\n",
    "    Parameters:\n",
    "    - example (Dict[str, Any]): A dictionary representing a row from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, Dict[str, float]]: A dictionary with a single key 'numerical' containing the numerical data.\n",
    "    \"\"\"\n",
    "    numerical_data = {key: float(value) for key, value in example.items() if isinstance(value, (int, float))}\n",
    "    return {\"numerical\": numerical_data}\n",
    "\n",
    "def extract_audio(example: Dict[str, Any]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract audio URLs from the example.\n",
    "\n",
    "    Parameters:\n",
    "    - example (Dict[str, Any]): A dictionary representing a row from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, List[str]]: A dictionary with a single key 'audio_urls' containing a list of audio URLs.\n",
    "    \"\"\"\n",
    "    audio_urls = [str(value) for value in example.values() if str(value).startswith(\"http\") and (str(value).endswith(\".mp3\") or str(value).endswith(\".wav\"))]\n",
    "    return {\"audio_urls\": audio_urls}\n",
    "\n",
    "def process_datasets(dataset_names: List[str], split: str = 'train') -> None:\n",
    "    \"\"\"\n",
    "    Load each dataset by name, extract different data types, and print the first item of each data type.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_names (List[str]): A list of dataset names to be processed.\n",
    "    - split (str): The split of the dataset to load, defaults to 'train'.\n",
    "    \"\"\"\n",
    "    for dataset_name in dataset_names:\n",
    "        # Load the dataset\n",
    "        dataset = load_dataset(dataset_name, split=split)\n",
    "\n",
    "        # Extract different data types using the map function\n",
    "        text_dataset = dataset.map(extract_text, remove_columns=dataset.column_names)\n",
    "        image_dataset = dataset.map(extract_image_urls, remove_columns=dataset.column_names)\n",
    "        video_dataset = dataset.map(extract_video_urls, remove_columns=dataset.column_names)\n",
    "        tabular_dataset = dataset.map(extract_tabular, remove_columns=dataset.column_names)\n",
    "        numerical_dataset = dataset.map(extract_numerical, remove_columns=dataset.column_names)\n",
    "        audio_dataset = dataset.map(extract_audio, remove_columns=dataset.column_names)\n",
    "\n",
    "        # Print the first example of each data type to verify the result\n",
    "        print(f\"First item from the processed dataset '{dataset_name}':\")\n",
    "        print(\"Text:\", text_dataset[0])\n",
    "        print(\"Image URLs:\", image_dataset[0])\n",
    "        print(\"Video URLs:\", video_dataset[0])\n",
    "        print(\"Tabular Data:\", tabular_dataset[0])\n",
    "        print(\"Numerical Data:\", numerical_dataset[0])\n",
    "        print(\"Audio URLs:\", audio_dataset[0])\n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")  # Separator line for readability\n",
    "\n",
    "# Get user input for dataset names\n",
    "dataset_list = input(\"Enter the dataset names separated by commas: \").split(\",\")\n",
    "dataset_list = [name.strip() for name in dataset_list]\n",
    "\n",
    "# Process the datasets\n",
    "process_datasets(dataset_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict\n",
    "def extract_urls(file_path: str) -> List[str]:\n",
    "    \"\"\"Extracts all URLs from a given text file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            data = file.read()\n",
    "    \n",
    "    url_pattern = re.compile(\n",
    "        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    )\n",
    "    urls = re.findall(url_pattern, data)\n",
    "    return urls\n",
    "\n",
    "\n",
    "def get_txt_files(directory: str) -> List[str]:\n",
    "    \"\"\"Returns a list of .txt files in a given directory.\"\"\"\n",
    "    return [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "\n",
    "def write_urls_to_json(directory: str, urls_data: Dict[str, List[str]]):\n",
    "    \"\"\"Writes the extracted URLs to a JSON file.\"\"\"\n",
    "    json_path = os.path.join(directory, 'extracted_urls.json')\n",
    "    with open(json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(urls_data, json_file, indent=4)\n",
    "\n",
    "def main(directory: str):\n",
    "    txt_files = get_txt_files(directory)\n",
    "    urls_data = {}\n",
    "    for txt_file in txt_files:\n",
    "        file_path = os.path.join(directory, txt_file)\n",
    "        urls = extract_urls(file_path)\n",
    "        urls_data[txt_file] = urls\n",
    "    write_urls_to_json(directory, urls_data)\n",
    "    print(f'Extracted URLs have been written to extracted_urls.json')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main('C:/Users/heman/Desktop/deeplearning/data1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def read_text_files(folder_path: str) -> List[str]:\n",
    "    \"\"\"Reads all .txt files in the given folder and returns a list of contents.\"\"\"\n",
    "    file_contents = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, file_name), 'r', encoding='ISO-8859-1') as file:\n",
    "                file_contents.append(file.read().strip())\n",
    "    return file_contents\n",
    "\n",
    "\n",
    "\n",
    "def write_to_json(data: List[str], json_file_path: str) -> None:\n",
    "    \"\"\"Writes the list of text data into a JSON file.\"\"\"\n",
    "    with open(json_file_path, 'w',encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "def main():\n",
    "    folder_path = 'C:/Users/heman/Desktop/deeplearning/data1/'\n",
    "    json_file_path ='json_file.json'\n",
    "    \n",
    "    # Extract data from .txt files\n",
    "    extracted_data = read_text_files(folder_path)\n",
    "    \n",
    "    # Write data to JSON file\n",
    "    write_to_json(extracted_data, json_file_path)\n",
    "    \n",
    "    print(f\"Data from .txt files has been written to {json_file_path} successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def extract_urls(file_path: str) -> List[str]:\n",
    "    \"\"\"Extracts all URLs from a given text file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            data = file.read()\n",
    "    \n",
    "    url_pattern = re.compile(\n",
    "        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    )\n",
    "    urls = re.findall(url_pattern, data)\n",
    "    return urls\n",
    "\n",
    "\n",
    "def get_txt_files(directory: str) -> List[str]:\n",
    "    \"\"\"Returns a list of .txt files in a given directory.\"\"\"\n",
    "    return [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "\n",
    "\n",
    "def write_urls_to_json(directory: str, urls_data: Dict[str, List[str]]):\n",
    "    \"\"\"Writes the extracted URLs to a JSON file.\"\"\"\n",
    "    json_path = os.path.join(directory, 'extracted_urls.json')\n",
    "    with open(json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(urls_data, json_file, indent=4)\n",
    "\n",
    "\n",
    "def main(directory: str):\n",
    "    \"\"\"Main function to extract URLs from text files in a directory and write them to a JSON file.\"\"\"\n",
    "    txt_files = get_txt_files(directory)\n",
    "    urls_data = {}\n",
    "    for txt_file in txt_files:\n",
    "        file_path = os.path.join(directory, txt_file)\n",
    "        urls = extract_urls(file_path)\n",
    "        urls_data[txt_file] = urls\n",
    "    write_urls_to_json(directory, urls_data)\n",
    "    print(f'Extracted URLs have been written to extracted_urls.json')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main('C:/Users/heman/Desktop/deeplearning/data1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Pattern\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def extract_urls(file_path: str, url_pattern: Pattern) -> List[str]:\n",
    "    \"\"\"Extracts all URLs from a given text file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = file.read()\n",
    "            urls = re.findall(url_pattern, data)\n",
    "            return urls\n",
    "    except (UnicodeDecodeError, FileNotFoundError, IOError) as e:\n",
    "        logging.error(f\"Error opening/reading file: {file_path}, Error: {e}\")\n",
    "        return []\n",
    "\n",
    "def compile_url_pattern() -> Pattern:\n",
    "    \"\"\"Compiles the URL regex pattern.\"\"\"\n",
    "    return re.compile(\n",
    "        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    )\n",
    "\n",
    "def get_txt_files(directory: str) -> List[str]:\n",
    "    \"\"\"Returns a list of .txt files in a given directory.\"\"\"\n",
    "    try:\n",
    "        return [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "    except OSError as e:\n",
    "        logging.error(f\"Error accessing directory: {directory}, Error: {e}\")\n",
    "        return []\n",
    "\n",
    "def write_urls_to_json(directory: str, urls_data: Dict[str, List[str]]):\n",
    "    \"\"\"Writes the extracted URLs to a JSON file.\"\"\"\n",
    "    json_path = os.path.join(directory, 'extracted_urls.json')\n",
    "    try:\n",
    "        with open(json_path, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(urls_data, json_file, indent=4)\n",
    "            logging.info(f'Extracted URLs have been written to {json_path}')\n",
    "    except IOError as e:\n",
    "        logging.error(f\"Error writing to file: {json_path}, Error: {e}\")\n",
    "\n",
    "def main(directory: str):\n",
    "    url_pattern = compile_url_pattern()\n",
    "    txt_files = get_txt_files(directory)\n",
    "    urls_data = {}\n",
    "    for txt_file in txt_files:\n",
    "        file_path = os.path.join(directory, txt_file)\n",
    "        urls = extract_urls(file_path, url_pattern)\n",
    "        urls_data[txt_file] = urls\n",
    "    write_urls_to_json(directory, urls_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main('C:/Users/heman/Desktop/deeplearning/data1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Pattern\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def extract_urls(file_path: str, url_pattern: Pattern) -> List[str]:\n",
    "    \"\"\"Extracts all URLs from a given text file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = file.read()\n",
    "            urls = re.findall(url_pattern, data)\n",
    "            return urls\n",
    "    except (UnicodeDecodeError, FileNotFoundError, IOError) as e:\n",
    "        logging.error(f\"Error opening/reading file: {file_path}, Error: {e}\")\n",
    "        return []\n",
    "\n",
    "def compile_url_pattern() -> Pattern:\n",
    "    \"\"\"Compiles the URL regex pattern.\"\"\"\n",
    "    return re.compile(\n",
    "        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    )\n",
    "\n",
    "def get_txt_files(directory: str) -> List[str]:\n",
    "    \"\"\"Returns a list of .txt files in a given directory.\"\"\"\n",
    "    try:\n",
    "        return [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "    except OSError as e:\n",
    "        logging.error(f\"Error accessing directory: {directory}, Error: {e}\")\n",
    "        return []\n",
    "\n",
    "def write_urls_to_json(directory: str, urls_data: Dict[str, List[str]]):\n",
    "    \"\"\"Writes the extracted URLs to a JSON file.\"\"\"\n",
    "    json_path = os.path.join(directory, 'extracted_urls.json')\n",
    "    try:\n",
    "        with open(json_path, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(urls_data, json_file, indent=4)\n",
    "            logging.info(f'Extracted URLs have been written to {json_path}')\n",
    "    except IOError as e:\n",
    "        logging.error(f\"Error writing to file: {json_path}, Error: {e}\")\n",
    "\n",
    "def main(directory: str):\n",
    "    url_pattern = compile_url_pattern()\n",
    "    txt_files = get_txt_files(directory)\n",
    "    urls_data = {}\n",
    "    for txt_file in txt_files:\n",
    "        file_path = os.path.join(directory, txt_file)\n",
    "        urls = extract_urls(file_path, url_pattern)\n",
    "        urls_data[txt_file] = urls\n",
    "    write_urls_to_json(directory, urls_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main('C:/Users/heman/Desktop/deeplearning/data1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "# Define a regular expression pattern for URLs\n",
    "URL_PATTERN = re.compile(\n",
    "    r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    ")\n",
    "\n",
    "def extract_urls(file_path: str) -> List[str]:\n",
    "    \"\"\"Extracts all URLs from a given text file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            data = file.read()\n",
    "    \n",
    "    urls = re.findall(URL_PATTERN, data)\n",
    "    return urls\n",
    "\n",
    "def get_txt_files(directory: str) -> List[str]:\n",
    "    \"\"\"Returns a list of .txt files in a given directory.\"\"\"\n",
    "    return [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "\n",
    "def write_urls_to_json(directory: str, urls_data: Dict[str, List[str]]):\n",
    "    \"\"\"Writes the extracted URLs to a JSON file.\"\"\"\n",
    "    json_path = os.path.join(directory, 'extracted_urls.json')\n",
    "    with open(json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(urls_data, json_file, indent=4)\n",
    "\n",
    "def main(directory: str):\n",
    "    txt_files = get_txt_files(directory)\n",
    "    urls_data = {}\n",
    "    for txt_file in txt_files:\n",
    "        file_path = os.path.join(directory, txt_file)\n",
    "        urls = extract_urls(file_path)\n",
    "        urls_data[txt_file] = urls\n",
    "    write_urls_to_json(directory, urls_data)\n",
    "    # print(f'Extracted URLs have been written to {json_path}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace the directory path with your desired path\n",
    "    main('C:/Users/heman/Desktop/deeplearning/data1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "# Define a regular expression pattern for URLs\n",
    "URL_PATTERN = re.compile(\n",
    "    r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    ")\n",
    "\n",
    "def extract_urls(file_path: str) -> List[str]:\n",
    "    \"\"\"Extracts all URLs from a given text file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            data = file.read()\n",
    "    \n",
    "    urls = re.findall(URL_PATTERN, data)\n",
    "    return urls\n",
    "\n",
    "def get_files(directory: str, extensions: List[str]) -> List[str]:\n",
    "    \"\"\"Recursively returns a list of files in a given directory with the specified extensions.\"\"\"\n",
    "    files_list = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for ext in extensions:\n",
    "            files_list.extend([os.path.join(root, file) for file in files if file.endswith(ext)])\n",
    "    return files_list\n",
    "\n",
    "def write_urls_to_json(directory: str, urls_data: Dict[str, List[str]]):\n",
    "    \"\"\"Writes the extracted URLs to a JSON file in a professional format.\"\"\"\n",
    "    json_path = os.path.join(directory, 'extracted_urls.json')\n",
    "    with open(json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(urls_data, json_file, ensure_ascii=False, indent=4, sort_keys=True)\n",
    "\n",
    "def main(directory: str, extensions: List[str]):\n",
    "    files = get_files(directory, extensions)\n",
    "    urls_data = {}\n",
    "    for file_path in files:\n",
    "        urls = extract_urls(file_path)\n",
    "        # Normalize file paths to use forward slashes\n",
    "        normalized_file_path = file_path.replace(os.sep, '/')\n",
    "        urls_data[normalized_file_path] = urls\n",
    "    write_urls_to_json(directory, urls_data)\n",
    "    # print(f'Extracted URLs have been written to {json_path}')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace the directory path with your desired path\n",
    "    # Specify the list of file extensions you want to include\n",
    "    main('C:/Users/heman/Desktop/deeplearning/', ['.txt', '.md', '.html', '.py', '.json', '.csv','.pdf'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def extract_text(example: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Concatenate all string values from the example into a single text string.\n",
    "\n",
    "    Parameters:\n",
    "    - example (Dict[str, Any]): A dictionary representing a row from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, str]: A dictionary with a single key 'text' containing the concatenated text.\n",
    "    \"\"\"\n",
    "    text_values = [str(value) for value in example.values() if isinstance(value, str)]\n",
    "    text = \" \".join(text_values)\n",
    "    return {\"text\": text}\n",
    "\n",
    "def extract_image_urls(example: Dict[str, Any]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract image URLs from the example.\n",
    "\n",
    "    Parameters:\n",
    "    - example (Dict[str, Any]): A dictionary representing a row from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, List[str]]: A dictionary with a single key 'image_urls' containing a list of image URLs.\n",
    "    \"\"\"\n",
    "    image_extensions = [\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".svg\", \".webp\", \".tiff\", \".ico\"]\n",
    "    image_urls = [str(value) for value in example.values() if isinstance(value, str) and any(value.lower().endswith(ext) for ext in image_extensions)]\n",
    "    return {\"image_urls\": image_urls}\n",
    "\n",
    "def extract_video_urls(example: Dict[str, Any]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract video URLs from the example.\n",
    "\n",
    "    Parameters:\n",
    "    - example (Dict[str, Any]): A dictionary representing a row from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, List[str]]: A dictionary with a single key 'video_urls' containing a list of video URLs.\n",
    "    \"\"\"\n",
    "    video_extensions = [\".mp4\", \".avi\", \".mov\", \".wmv\", \".flv\", \".mkv\", \".webm\", \".m4v\", \".mpg\", \".mpeg\"]\n",
    "    video_urls = [str(value) for value in example.values() if isinstance(value, str) and any(value.lower().endswith(ext) for ext in video_extensions)]\n",
    "    return {\"video_urls\": video_urls}\n",
    "\n",
    "def extract_tabular(example: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract tabular data from the example.\n",
    "\n",
    "    Parameters:\n",
    "    - example (Dict[str, Any]): A dictionary representing a row from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, Dict[str, Any]]: A dictionary with a single key 'tabular' containing the tabular data.\n",
    "    \"\"\"\n",
    "    tabular_data = {key: value for key, value in example.items() if not isinstance(value, (str, list))}\n",
    "    return {\"tabular\": tabular_data}\n",
    "\n",
    "def extract_numerical(example: Dict[str, Any]) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Extract numerical data from the example.\n",
    "\n",
    "    Parameters:\n",
    "    - example (Dict[str, Any]): A dictionary representing a row from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, Dict[str, float]]: A dictionary with a single key 'numerical' containing the numerical data.\n",
    "    \"\"\"\n",
    "    numerical_data = {key: float(value) for key, value in example.items() if isinstance(value, (int, float))}\n",
    "    return {\"numerical\": numerical_data}\n",
    "\n",
    "def extract_audio(example: Dict[str, Any]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract audio URLs from the example.\n",
    "\n",
    "    Parameters:\n",
    "    - example (Dict[str, Any]): A dictionary representing a row from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, List[str]]: A dictionary with a single key 'audio_urls' containing a list of audio URLs.\n",
    "    \"\"\"\n",
    "    audio_extensions = [\".mp3\", \".wav\", \".aac\", \".flac\", \".ogg\", \".wma\", \".m4a\", \".aiff\", \".alac\", \".pcm\"]\n",
    "    audio_urls = [str(value) for value in example.values() if isinstance(value, str) and any(value.lower().endswith(ext) for ext in audio_extensions)]\n",
    "    return {\"audio_urls\": audio_urls}\n",
    "\n",
    "def process_datasets(dataset_names: List[str], split: str = 'train') -> None:\n",
    "    \"\"\"\n",
    "    Load each dataset by name, extract different data types, and print the first item of each data type.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_names (List[str]): A list of dataset names to be processed.\n",
    "    - split (str): The split of the dataset to load, defaults to 'train'.\n",
    "    \"\"\"\n",
    "    for dataset_name in dataset_names:\n",
    "        try:\n",
    "            # Load the dataset\n",
    "            dataset = load_dataset(dataset_name, split=split)\n",
    "\n",
    "            # Extract different data types using the map function\n",
    "            text_dataset = dataset.map(extract_text, remove_columns=dataset.column_names)\n",
    "            image_dataset = dataset.map(extract_image_urls, remove_columns=dataset.column_names)\n",
    "            video_dataset = dataset.map(extract_video_urls, remove_columns=dataset.column_names)\n",
    "            tabular_dataset = dataset.map(extract_tabular, remove_columns=dataset.column_names)\n",
    "            numerical_dataset = dataset.map(extract_numerical, remove_columns=dataset.column_names)\n",
    "            audio_dataset = dataset.map(extract_audio, remove_columns=dataset.column_names)\n",
    "\n",
    "            # Print the first example of each data type to verify the result\n",
    "            print(f\"First item from the processed dataset '{dataset_name}':\")\n",
    "            print(\"Text:\", text_dataset[0])\n",
    "            print(\"Image URLs:\", image_dataset[0])\n",
    "            print(\"Video URLs:\", video_dataset[0])\n",
    "            print(\"Tabular Data:\", tabular_dataset[0])\n",
    "            print(\"Numerical Data:\", numerical_dataset[0])\n",
    "            print(\"Audio URLs:\", audio_dataset[0])\n",
    "            print(\"\\n\" + \"-\"*80 + \"\\n\")  # Separator line for readability\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing dataset '{dataset_name}': {str(e)}\")\n",
    "\n",
    "# Get user input for dataset names\n",
    "dataset_list = input(\"Enter the dataset names separated by commas: \").split(\",\")\n",
    "dataset_list = [name.strip() for name in dataset_list]\n",
    "\n",
    "# Process the datasets\n",
    "process_datasets(dataset_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK: we have take list of Datasets (user input )  now data convert into text , image(urls),video(urls), tabulr , numerical,audio\n",
    "\n",
    "NOTE: we advnced hooks (to extact all proper datasets )(we should extract 100% percantage correct dataset)\n",
    "\n",
    "\n",
    "\n",
    "below sample for text code now extent image , video , audio, numerical, tabualr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def convert_to_text(example: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Concatenate all values from the example into a single text string.\n",
    "\n",
    "    Parameters:\n",
    "    - example (Dict[str, Any]): A dictionary representing a row from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, str]: A dictionary with a single key 'text' containing the concatenated text.\n",
    "    \"\"\"\n",
    "    text = \" \".join(str(value) for value in example.values())\n",
    "    return {\"text\": text}\n",
    "\n",
    "def process_datasets(dataset_names: List[str], split: str = 'train') -> None:\n",
    "    \"\"\"\n",
    "    Load each dataset by name, convert all columns into a single text column, and print the first item.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_names (List[str]): A list of dataset names to be processed.\n",
    "    - split (str): The split of the dataset to load, defaults to 'train'.\n",
    "    \"\"\"\n",
    "    for dataset_name in dataset_names:\n",
    "        # Load the dataset\n",
    "        dataset = load_dataset(dataset_name, split=split)\n",
    "\n",
    "        # Convert the columns into a single text column using the map function\n",
    "        dataset = dataset.map(convert_to_text, remove_columns=dataset.column_names)\n",
    "\n",
    "        # Print the first example to verify the result\n",
    "        print(f\"First item from the processed dataset '{dataset_name}':\")\n",
    "        print(dataset[0])\n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")  # Separator line for readability\n",
    "\n",
    "# List of 20 dataset names to process\n",
    "dataset_list = [\n",
    "    \"Open-Orca/OpenOrca\",       # Replace with actual dataset names\n",
    "    # \"dataset_name_2\",\n",
    "    # \"dataset_name_3\",\n",
    "    # ...\n",
    "    # \"dataset_name_20\",\n",
    "]\n",
    "\n",
    "# Make sure to replace the placeholders with the actual names of your datasets before running the function\n",
    "process_datasets(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from typing import Dict, Any, Union\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define the base directory where all data will be stored\n",
    "BASE_DIR = Path(\"data_collections\")\n",
    "\n",
    "# Define the structure of the metadata\n",
    "MetaData = Dict[str, Any]\n",
    "TextData = Dict[str, Any]\n",
    "AudioMetaData = Dict[str, Any]\n",
    "ImageMetaData = Dict[str, Any]\n",
    "VideoMetaData = Dict[str, Any]\n",
    "\n",
    "\n",
    "def initialize_user_data(meta_data: MetaData) -> None:\n",
    "    \"\"\"\n",
    "    Initializes the data collection folders for a new user based on the provided metadata.\n",
    "    :param meta_data: A dictionary containing user metadata such as 'id'.\n",
    "    \"\"\"\n",
    "    user_id = meta_data.get('id')\n",
    "    if user_id is None:\n",
    "        raise ValueError(\"Metadata must contain an 'id' key.\")\n",
    "    \n",
    "    user_dir = BASE_DIR / str(user_id)\n",
    "    logging.info(f\"Initializing data directories for user {user_id}\")\n",
    "    # Create directories for each type of data\n",
    "    for data_type in ['text', 'image', 'video', 'audio']:\n",
    "        data_path = user_dir / f\"{data_type}-data\"\n",
    "        data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def append_data(user_id: str, data: Union[TextData, AudioMetaData, ImageMetaData, VideoMetaData], data_type: str) -> None:\n",
    "    \"\"\"\n",
    "    Appends new data to an existing JSON file within the specified data folder for a specific user.\n",
    "    Creates a new JSON file if it does not exist.\n",
    "    :param user_id: The unique identifier for the user.\n",
    "    :param data: A dictionary containing the data to append.\n",
    "    :param data_type: The type of data to append ('text', 'audio', 'image', 'video').\n",
    "    \"\"\"\n",
    "    data_dir = BASE_DIR / user_id / f\"{data_type}-data\"\n",
    "    data_file = data_dir / f\"user_{data_type}_data.json\"\n",
    "    \n",
    "    if not data_file.exists():\n",
    "        logging.info(f\"Creating new {data_type} data file for user {user_id}\")\n",
    "        data_file.touch()\n",
    "        data_file.write_text(json.dumps([]))  # Initialize with an empty list\n",
    "    \n",
    "    with data_file.open('r+') as file:\n",
    "        logging.info(f\"Appending new {data_type} data for user {user_id}\")\n",
    "        existing_data = json.load(file)\n",
    "        existing_data.append(data)\n",
    "        file.seek(0)\n",
    "        json.dump(existing_data, file, indent=4)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "try:\n",
    "    user_metadata = {'id': '12347'}\n",
    "    initialize_user_data(user_metadata)\n",
    "    \n",
    "    text_sample = {'content': 'Sample text data', 'timestamp': '2024-03-23T13:06:45'}\n",
    "    append_data(user_id='12347', data=text_sample, data_type='text')\n",
    "    \n",
    "    audio_sample = {'file_name': 'sample.mp3', 'duration': 120,'joking':10}\n",
    "    append_data(user_id='12347', data=audio_sample, data_type='audio')\n",
    "    \n",
    "except ValueError as e:\n",
    "    logging.error(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from typing import Dict, Any, Union\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define the base directory where all data will be stored\n",
    "BASE_DIR = Path(\"data_collections\")\n",
    "\n",
    "# Define the structure of the metadata\n",
    "MetaData = Dict[str, Any]\n",
    "TextData = Dict[str, Any]\n",
    "AudioMetaData = Dict[str, Any]\n",
    "ImageMetaData = Dict[str, Any]\n",
    "VideoMetaData = Dict[str, Any]\n",
    "\n",
    "\n",
    "def initialize_user_data(meta_data: MetaData) -> None:\n",
    "    \"\"\"\n",
    "    Initializes the data collection folders for a new user based on the provided metadata.\n",
    "    :param meta_data: A dictionary containing user metadata such as 'id', 'name', 'email', 'phone'.\n",
    "    \"\"\"\n",
    "    user_id = meta_data.get('id')\n",
    "    if user_id is None:\n",
    "        raise ValueError(\"Metadata must contain an 'id' key.\")\n",
    "    \n",
    "    user_dir = BASE_DIR / str(user_id)\n",
    "    logging.info(f\"Initializing data directories for user {user_id}\")\n",
    "    # Create directories for each type of data\n",
    "    for data_type in ['text', 'image', 'video', 'audio']:\n",
    "        data_path = user_dir / f\"{data_type}-data\"\n",
    "        data_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save user metadata to a JSON file\n",
    "    metadata_file = user_dir / \"user_metadata.json\"\n",
    "    with metadata_file.open('w') as file:\n",
    "        json.dump(meta_data, file, indent=4)\n",
    "\n",
    "\n",
    "def append_data(user_id: str, data: Union[TextData, AudioMetaData, ImageMetaData, VideoMetaData], data_type: str) -> None:\n",
    "    \"\"\"\n",
    "    Appends new data to an existing JSON file within the specified data folder for a specific user.\n",
    "    Creates a new JSON file if it does not exist.\n",
    "    :param user_id: The unique identifier for the user.\n",
    "    :param data: A dictionary containing the data to append.\n",
    "    :param data_type: The type of data to append ('text', 'audio', 'image', 'video').\n",
    "    \"\"\"\n",
    "    data_dir = BASE_DIR / user_id / f\"{data_type}-data\"\n",
    "    data_file = data_dir / f\"user_{data_type}_data.json\"\n",
    "    \n",
    "    if not data_file.exists():\n",
    "        logging.info(f\"Creating new {data_type} data file for user {user_id}\")\n",
    "        data_file.touch()\n",
    "        data_file.write_text(json.dumps([]))  # Initialize with an empty list\n",
    "    \n",
    "    with data_file.open('r+') as file:\n",
    "        logging.info(f\"Appending new {data_type} data for user {user_id}\")\n",
    "        existing_data = json.load(file)\n",
    "        existing_data.append(data)\n",
    "        file.seek(0)\n",
    "        json.dump(existing_data, file, indent=4)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "try:\n",
    "    user_metadata = {\n",
    "        'id': '12346',\n",
    "        'name': 'John Doe',\n",
    "        'email': 'john.doe@example.com',\n",
    "        'phone': '123-456-7890',\n",
    "        'about': 'Sample user for demonstration purposes',\n",
    "        'Anything else': '1234'\n",
    "    }\n",
    "    initialize_user_data(user_metadata)\n",
    "    \n",
    "    text_sample = {'content': 'Sample text data', 'timestamp': '2024-03-23T13:06:45'}\n",
    "    append_data(user_id='12345', data=text_sample, data_type='text')\n",
    "    \n",
    "    audio_sample = {'file_name': 'sample.mp3', 'duration': 120}\n",
    "    append_data(user_id='12345', data=audio_sample, data_type='audio')\n",
    "    \n",
    "except ValueError as e:\n",
    "    logging.error(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_datasets\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main function to list all text-generation datasets from the Hugging Face Datasets library.\n",
    "    \"\"\"\n",
    "    # Get all dataset names\n",
    "    datasets = list_datasets()\n",
    "\n",
    "    # Filter text-generation datasets\n",
    "    text_generation_datasets = [\n",
    "        dataset for dataset in datasets if \"text-generation\" in dataset.split('/')\n",
    "    ]\n",
    "\n",
    "    # Print the list of text-generation datasets\n",
    "    print(\"Text-Generation Datasets:\")\n",
    "    for dataset in text_generation_datasets:\n",
    "        print(f\"- {dataset}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import list_datasets\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main function to list all text-generation datasets from the Hugging Face Hub.\n",
    "    \"\"\"\n",
    "    # Get all dataset names\n",
    "    datasets = list_datasets()\n",
    "\n",
    "    # Filter text-generation datasets\n",
    "    text_generation_datasets = [\n",
    "        dataset.id for dataset in datasets if \"text-generation\" in dataset.tags\n",
    "    ]\n",
    "\n",
    "    # Print the list of text-generation datasets\n",
    "    print(\"Text-Generation Datasets:\")\n",
    "    for dataset in text_generation_datasets:\n",
    "        print(f\"- {dataset}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

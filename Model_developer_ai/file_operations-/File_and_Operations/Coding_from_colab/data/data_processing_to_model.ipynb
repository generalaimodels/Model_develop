{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of windows: 24\n",
      "Window shape: torch.Size([50, 10])\n",
      "Normalized window mean: -0.00000001\n",
      "Normalized window std: 1.00000000\n",
      "Number of small tensors: 1000\n",
      "Small tensor shape: torch.Size([1, 10])\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 7.84 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "from typing import List, Tuple\n",
    "\n",
    "def create_windows(data: torch.Tensor, window_size: int, overlap: int = 0) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Create windows from sequential data with optional overlap.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input sequential data tensor.\n",
    "        window_size (int): The size of each window.\n",
    "        overlap (int, optional): The number of overlapping steps between windows. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of window tensors.\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    stride = window_size - overlap\n",
    "    for i in range(0, data.size(0) - window_size + 1, stride):\n",
    "        window = data[i:i+window_size]\n",
    "        windows.append(window)\n",
    "    return windows\n",
    "\n",
    "def normalize_windows(windows: List[torch.Tensor]) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Normalize each window tensor individually.\n",
    "\n",
    "    Args:\n",
    "        windows (List[torch.Tensor]): A list of window tensors.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of normalized window tensors.\n",
    "    \"\"\"\n",
    "    normalized_windows = []\n",
    "    for window in windows:\n",
    "        mean = window.mean()\n",
    "        std = window.std()\n",
    "        normalized_window = (window - mean) / std\n",
    "        normalized_windows.append(normalized_window)\n",
    "    return normalized_windows\n",
    "\n",
    "def split_tensor(data: torch.Tensor, dimensions: Tuple[int, ...]) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Split a long tensor into smaller tensors with specified dimensions.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input long tensor.\n",
    "        dimensions (Tuple[int, ...]): The dimensions of the smaller tensors.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of smaller tensors.\n",
    "    \"\"\"\n",
    "    num_splits = data.size(0) // torch.prod(torch.tensor(dimensions))\n",
    "    return torch.split(data, num_splits, dim=0)\n",
    "\n",
    "# Example usage\n",
    "data = torch.randn(1000, 10)  # Example sequential data tensor\n",
    "window_size = 50\n",
    "overlap = 10\n",
    "dimensions = (100, 10)\n",
    "\n",
    "# Create windows\n",
    "windows = create_windows(data, window_size, overlap)\n",
    "print(f\"Number of windows: {len(windows)}\")\n",
    "print(f\"Window shape: {windows[0].shape}\")\n",
    "\n",
    "# Normalize windows\n",
    "normalized_windows = normalize_windows(windows)\n",
    "print(f\"Normalized window mean: {normalized_windows[0].mean():.8f}\")\n",
    "print(f\"Normalized window std: {normalized_windows[0].std():.8f}\")\n",
    "\n",
    "# Split long tensor into smaller tensors\n",
    "small_tensors = split_tensor(data, dimensions)\n",
    "print(f\"Number of small tensors: {len(small_tensors)}\")\n",
    "print(f\"Small tensor shape: {small_tensors[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      " torch.Size([2, 3, 4])\n",
      "\n",
      "Mean along dimension 1:\n",
      " torch.Size([2, 4])\n",
      "\n",
      "Mean along dimension 2:\n",
      " torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def tensor_mean(data: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the mean of a tensor along a specified dimension.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor.\n",
    "        dim (int): The dimension along which to calculate the mean.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The mean of the tensor along the specified dimension.\n",
    "    \"\"\"\n",
    "    # Calculate mean along the specified dimension\n",
    "    mean_value = torch.mean(data, dim=dim)\n",
    "    return mean_value\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a random tensor of shape (2, 3, 4)\n",
    "    random_tensor = torch.randn(2, 3, 4)\n",
    "    print(\"Original tensor:\\n\", random_tensor.shape)\n",
    "    \n",
    "    # Calculate the mean along dimension 1\n",
    "    mean_dim1 = tensor_mean(random_tensor, dim=1)\n",
    "    print(\"\\nMean along dimension 1:\\n\", mean_dim1.shape)\n",
    "    \n",
    "    # Calculate the mean along dimension 2\n",
    "    mean_dim2 = tensor_mean(random_tensor, dim=2)\n",
    "    print(\"\\nMean along dimension 2:\\n\", mean_dim2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean across all dimensions: tensor(-0.0130)\n",
      "Mean along dimension 0: torch.Size([4, 5, 6, 7])\n",
      "Mean along dimension 1: torch.Size([3, 5, 6, 7])\n",
      "Mean along dimension 2: torch.Size([3, 4, 6, 7])\n",
      "Mean along dimension 3: torch.Size([3, 4, 5, 7])\n",
      "Mean along dimension 4: torch.Size([3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def tensor_mean(data: torch.Tensor, dim: int = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the mean of a tensor along a specified dimension.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor.\n",
    "        dim (int, optional): The dimension along which to compute the mean.\n",
    "            If None, the mean is computed across all dimensions. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The mean of the tensor along the specified dimension.\n",
    "    \"\"\"\n",
    "    if dim is None:\n",
    "        return torch.mean(data)\n",
    "    else:\n",
    "        return torch.mean(data, dim=dim)\n",
    "\n",
    "# Create a 5-dimensional tensor\n",
    "data_5d = torch.randn(3, 4, 5, 6, 7)\n",
    "\n",
    "# Compute mean across all dimensions\n",
    "mean_all = tensor_mean(data_5d)\n",
    "print(\"Mean across all dimensions:\", mean_all)\n",
    "\n",
    "# Compute mean along dimension 0\n",
    "mean_dim0 = tensor_mean(data_5d, dim=0)\n",
    "print(\"Mean along dimension 0:\", mean_dim0.shape)\n",
    "\n",
    "# Compute mean along dimension 1\n",
    "mean_dim1 = tensor_mean(data_5d, dim=1)\n",
    "print(\"Mean along dimension 1:\", mean_dim1.shape)\n",
    "\n",
    "# Compute mean along dimension 2\n",
    "mean_dim2 = tensor_mean(data_5d, dim=2)\n",
    "print(\"Mean along dimension 2:\", mean_dim2.shape)\n",
    "\n",
    "# Compute mean along dimension 3\n",
    "mean_dim3 = tensor_mean(data_5d, dim=3)\n",
    "print(\"Mean along dimension 3:\", mean_dim3.shape)\n",
    "\n",
    "# Compute mean along dimension 4\n",
    "mean_dim4 = tensor_mean(data_5d, dim=4)\n",
    "print(\"Mean along dimension 4:\", mean_dim4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      " tensor([[[ 1.8935e-03,  8.5713e-01, -2.4107e-01, -8.8257e-02],\n",
      "         [ 3.4354e+00, -9.5113e-01, -1.2373e+00, -7.4827e-01],\n",
      "         [ 1.0941e+00,  5.5145e-01,  4.3855e-01,  1.2423e+00]],\n",
      "\n",
      "        [[ 9.3491e-01,  1.1717e-01, -1.7429e+00,  7.4692e-01],\n",
      "         [ 3.3556e-01, -6.9990e-01,  7.9597e-03, -1.2619e+00],\n",
      "         [ 3.2900e-01, -9.9850e-01, -4.9781e-01, -4.8844e-01]]])\n",
      "\n",
      "Sum reduction (dim=1):\n",
      " tensor([[ 4.5314,  0.4574, -1.0398,  0.4058],\n",
      "        [ 1.5995, -1.5812, -2.2327, -1.0034]])\n",
      "\n",
      "Mean reduction (dim=1):\n",
      " tensor([[ 1.5105,  0.1525, -0.3466,  0.1353],\n",
      "        [ 0.5332, -0.5271, -0.7442, -0.3345]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing import Tuple, List\n",
    "\n",
    "def sum_reduction(data: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    return torch.sum(data, dim=dim)\n",
    "\n",
    "def mean_reduction(data: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    return torch.mean(data, dim=dim)\n",
    "\n",
    "def max_reduction(data: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    return torch.max(data, dim=dim).values\n",
    "\n",
    "def min_reduction(data: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    return torch.min(data, dim=dim).values\n",
    "\n",
    "def std_reduction(data: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    return torch.std(data, dim=dim)\n",
    "\n",
    "def variance_reduction(data: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    return torch.var(data, dim=dim)\n",
    "\n",
    "def argmax_reduction(data: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    return torch.argmax(data, dim=dim)\n",
    "\n",
    "def argmin_reduction(data: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    return torch.argmin(data, dim=dim)\n",
    "\n",
    "def product_reduction(data: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    return torch.prod(data, dim=dim)\n",
    "\n",
    "def median_reduction(data: torch.Tensor, dim: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    return torch.median(data, dim=dim)\n",
    "\n",
    "def percentile_reduction(data: torch.Tensor, q: float, dim: int) -> torch.Tensor:\n",
    "    return torch.quantile(data, q / 100, dim=dim)\n",
    "\n",
    "def histogram_reduction(data: torch.Tensor, bins: int, dim: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # Note: Histogram is not a reduction along a specific dimension,\n",
    "    # but we'll consider a flat histogram of the data along that dim.\n",
    "    flat_data = data.transpose(dim, -1).flatten()\n",
    "    return torch.histogram(flat_data, bins)\n",
    "\n",
    "def correlation_reduction(data: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    mean = torch.mean(data, dim=dim)\n",
    "    std = torch.std(data, dim=dim)\n",
    "    return (data - mean) / std\n",
    "\n",
    "def covariance_reduction(data1: torch.Tensor, data2: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    mean1 = torch.mean(data1, dim=dim)\n",
    "    mean2 = torch.mean(data2, dim=dim)\n",
    "    covariance = torch.mean((data1 - mean1) * (data2 - mean2), dim=dim)\n",
    "    return covariance\n",
    "\n",
    "def quantile_reduction(data: torch.Tensor, q: float, dim: int) -> torch.Tensor:\n",
    "    return torch.quantile(data, q, dim=dim)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a random tensor of shape (2, 3, 4)\n",
    "    random_tensor = torch.randn(2, 3, 4)\n",
    "    print(\"Original tensor:\\n\", random_tensor)\n",
    "    \n",
    "    # Use the sum_reduction function\n",
    "    print(\"\\nSum reduction (dim=1):\\n\", sum_reduction(random_tensor, dim=1))\n",
    "    \n",
    "    # Use the mean_reduction function\n",
    "    print(\"\\nMean reduction (dim=1):\\n\", mean_reduction(random_tensor, dim=1))\n",
    "    \n",
    "    # Continue for the other functions similarly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_cov(data: torch.Tensor, dim: int = None) -> torch.Tensor:\n",
    "    return torch.cov(data, dim=dim)\n",
    "\n",
    "def tensor_quantile(data: torch.Tensor, q: float, dim: int = None) -> torch.Tensor:\n",
    "    return torch.quantile(data, q, dim=dim)\n",
    "\n",
    "def tensor_corrcoef(data1: torch.Tensor, data2: torch.Tensor, dim: int = None) -> torch.Tensor:\n",
    "    return torch.corrcoef(data1, data2, dim=dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Sum Reduction:\n",
    "```python\n",
    "def tensor_sum(data: torch.Tensor, dim: int = None) -> torch.Tensor:\n",
    "    return torch.sum(data, dim=dim)\n",
    "```\n",
    "\n",
    "2. Mean Reduction:\n",
    "```python\n",
    "def tensor_mean(data: torch.Tensor, dim: int = None) -> torch.Tensor:\n",
    "    return torch.mean(data, dim=dim)\n",
    "```\n",
    "\n",
    "3. Max Reduction:\n",
    "```python\n",
    "def tensor_max(data: torch.Tensor, dim: int = None) -> torch.Tensor:\n",
    "    return torch.max(data, dim=dim)[0]\n",
    "```\n",
    "\n",
    "4. Min Reduction:\n",
    "```python\n",
    "def tensor_min(data: torch.Tensor, dim: int = None) -> torch.Tensor:\n",
    "    return torch.min(data, dim=dim)[0]\n",
    "```\n",
    "\n",
    "5. Standard Deviation Reduction:\n",
    "```python\n",
    "def tensor_std(data: torch.Tensor, dim: int = None) -> torch.Tensor:\n",
    "    return torch.std(data, dim=dim)\n",
    "```\n",
    "\n",
    "6. Variance Reduction:\n",
    "```python\n",
    "def tensor_var(data: torch.Tensor, dim: int = None) -> torch.Tensor:\n",
    "    return torch.var(data, dim=dim)\n",
    "```\n",
    "\n",
    "7. Argmax Reduction:\n",
    "```python\n",
    "def tensor_argmax(data: torch.Tensor, dim: int = None) -> torch.Tensor:\n",
    "    return torch.argmax(data, dim=dim)\n",
    "```\n",
    "\n",
    "8. Argmin Reduction:\n",
    "```python\n",
    "def tensor_argmin(data: torch.Tensor, dim: int = None) -> torch.Tensor:\n",
    "    return torch.argmin(data, dim=dim)\n",
    "```\n",
    "\n",
    "9. Product Reduction:\n",
    "```python\n",
    "def tensor_prod(data: torch.Tensor, dim: int = None) -> torch.Tensor:\n",
    "    return torch.prod(data, dim=dim)\n",
    "```\n",
    "\n",
    "10. Median Reduction:\n",
    "```python\n",
    "def tensor_median(data: torch.Tensor, dim: int = None) -> torch.Tensor:\n",
    "    return torch.median(data, dim=dim)[0]\n",
    "```\n",
    "\n",
    "11. Percentile Reduction:\n",
    "```python\n",
    "def tensor_percentile(data: torch.Tensor, q: float, dim: int = None) -> torch.Tensor:\n",
    "    return torch.quantile(data, q, dim=dim)\n",
    "```\n",
    "\n",
    "12. Histogram Reduction:\n",
    "```python\n",
    "def tensor_histogram(data: torch.Tensor, bins: int, dim: int = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    return torch.histogram(data, bins=bins, dim=dim)\n",
    "```\n",
    "\n",
    "13. Correlation Reduction:\n",
    "```python\n",
    "def tensor_corrcoef(data1: torch.Tensor, data2: torch.Tensor, dim: int = None) -> torch.Tensor:\n",
    "    return torch.corrcoef(data1, data2, dim=dim)\n",
    "```\n",
    "\n",
    "14. Covariance Reduction:\n",
    "```python\n",
    "def tensor_cov(data: torch.Tensor, dim: int = None) -> torch.Tensor:\n",
    "    return torch.cov(data, dim=dim)\n",
    "```\n",
    "\n",
    "15. Quantile Reduction:\n",
    "```python\n",
    "def tensor_quantile(data: torch.Tensor, q: float, dim: int = None) -> torch.Tensor:\n",
    "    return torch.quantile(data, q, dim=dim)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "\n",
    "def create_windows(data: torch.Tensor, window_size: int, step_size: int = 1, normalized: bool = True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create overlapping or non-overlapping windows from a long tensor.\n",
    "    \n",
    "    Args:\n",
    "    - data (torch.Tensor): The original data tensor.\n",
    "    - window_size (int): The size of each window.\n",
    "    - step_size (int, optional): The number of elements to step for the next window (overlap). Default is 1.\n",
    "    - normalized (bool, optional): Flag to normalize the data within each window. Default is True.\n",
    "    \n",
    "    Returns:\n",
    "    - windows (torch.Tensor): A tensor containing the windows.\n",
    "    \"\"\"\n",
    "    num_windows = (data.shape[0] - window_size) // step_size + 1\n",
    "    windows = torch.stack([data[i:i+window_size] for i in range(0, num_windows*step_size, step_size)])\n",
    "    \n",
    "    if normalized:\n",
    "        # Normalize each window individually\n",
    "        windows = (windows - windows.mean(dim=1, keepdim=True)) / (windows.std(dim=1, keepdim=True) + 1e-8)\n",
    "    \n",
    "    return windows\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a long tensor with random data\n",
    "    long_tensor = torch.randn(100)  # 100 random elements\n",
    "    \n",
    "    # Define the window size and step size\n",
    "    window_size = 10\n",
    "    step_size = 5\n",
    "    \n",
    "    # Create windows with optional overlap\n",
    "    windows = create_windows(long_tensor, window_size, step_size)\n",
    "    \n",
    "    # Output the result\n",
    "    print(\"Created windows with shape:\", windows.shape)\n",
    "    print(\"First window:\\n\", windows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple, List\n",
    "\n",
    "def create_and_split_tensor(dimensions: Tuple[int], split_sizes: List[int]) -> Tuple[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Create a long tensor of given dimensions and split it into smaller tensors.\n",
    "    \n",
    "    Args:\n",
    "        dimensions (Tuple[int]): Dimensions for the long tensor.\n",
    "        split_sizes (List[int]): Sizes to split the long tensor.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor]: A tuple of smaller tensors.\n",
    "    \"\"\"\n",
    "    # Create a long tensor with the given dimensions\n",
    "    long_tensor = torch.zeros(dimensions, dtype=torch.long)\n",
    "    \n",
    "    # Split the long tensor into smaller tensors\n",
    "    split_tensors = torch.split(long_tensor, split_sizes, dim=0)\n",
    "    \n",
    "    return split_tensors\n",
    "\n",
    "# Example usage\n",
    "dimensions = (10, 10)\n",
    "split_sizes = [2, 3, 5]\n",
    "split_tensors = create_and_split_tensor(dimensions, split_sizes)\n",
    "\n",
    "# Print the type of each tensor\n",
    "for tensor in split_tensors:\n",
    "    print(tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "def split_tensor(data: torch.Tensor, dimensions: Tuple[int, ...]) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Split a long tensor into smaller tensors with specified dimensions.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input long tensor.\n",
    "        dimensions (Tuple[int, ...]): The dimensions of the smaller tensors.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of smaller tensors.\n",
    "    \"\"\"\n",
    "    num_splits = data.size(0) // torch.prod(torch.tensor(dimensions))\n",
    "    return torch.split(data, num_splits, dim=0)\n",
    "\n",
    "def variable_length_windowing(data: torch.Tensor, window_sizes: List[int]) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Perform variable-length windowing on a tensor.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor.\n",
    "        window_sizes (List[int]): The sizes of the variable-length windows.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of window tensors.\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    start = 0\n",
    "    for size in window_sizes:\n",
    "        window = data[start:start+size]\n",
    "        windows.append(window)\n",
    "        start += size\n",
    "    return windows\n",
    "\n",
    "def time_windowing(data: torch.Tensor, window_size: int, stride: int) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Perform time windowing on a tensor.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor.\n",
    "        window_size (int): The size of each time window.\n",
    "        stride (int): The stride between consecutive windows.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of time window tensors.\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    for i in range(0, data.size(0) - window_size + 1, stride):\n",
    "        window = data[i:i+window_size]\n",
    "        windows.append(window)\n",
    "    return windows\n",
    "\n",
    "def time_series_windowing(data: torch.Tensor, window_size: int, horizon: int) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Perform time series windowing on a tensor.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input time series tensor.\n",
    "        window_size (int): The size of each time window.\n",
    "        horizon (int): The prediction horizon.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[torch.Tensor], List[torch.Tensor]]: A tuple containing lists of input windows and target windows.\n",
    "    \"\"\"\n",
    "    input_windows = []\n",
    "    target_windows = []\n",
    "    for i in range(data.size(0) - window_size - horizon + 1):\n",
    "        input_window = data[i:i+window_size]\n",
    "        target_window = data[i+window_size:i+window_size+horizon]\n",
    "        input_windows.append(input_window)\n",
    "        target_windows.append(target_window)\n",
    "    return input_windows, target_windows\n",
    "\n",
    "# Example usage\n",
    "data = torch.randn(1000, 10)  # Example long tensor\n",
    "dimensions = (100, 10)\n",
    "window_sizes = [50, 75, 100]\n",
    "time_window_size = 50\n",
    "time_window_stride = 25\n",
    "ts_window_size = 50\n",
    "ts_horizon = 10\n",
    "\n",
    "# Split long tensor into smaller tensors\n",
    "small_tensors = split_tensor(data, dimensions)\n",
    "print(f\"Number of small tensors: {len(small_tensors)}\")\n",
    "print(f\"Small tensor shape: {small_tensors[0].shape}\")\n",
    "\n",
    "# Variable-length windowing\n",
    "variable_windows = variable_length_windowing(data, window_sizes)\n",
    "print(f\"Number of variable-length windows: {len(variable_windows)}\")\n",
    "print(f\"Variable-length window shapes: {[window.shape for window in variable_windows]}\")\n",
    "\n",
    "# Time windowing\n",
    "time_windows = time_windowing(data, time_window_size, time_window_stride)\n",
    "print(f\"Number of time windows: {len(time_windows)}\")\n",
    "print(f\"Time window shape: {time_windows[0].shape}\")\n",
    "\n",
    "# Time series windowing\n",
    "ts_input_windows, ts_target_windows = time_series_windowing(data, ts_window_size, ts_horizon)\n",
    "print(f\"Number of time series input windows: {len(ts_input_windows)}\")\n",
    "print(f\"Time series input window shape: {ts_input_windows[0].shape}\")\n",
    "print(f\"Number of time series target windows: {len(ts_target_windows)}\")\n",
    "print(f\"Time series target window shape: {ts_target_windows[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Tuple\n",
    "\n",
    "def split_tensor(data: torch.Tensor, dimensions: Tuple[int, ...]) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Split a long tensor into smaller tensors with specified dimensions.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input long tensor.\n",
    "        dimensions (Tuple[int, ...]): The dimensions of the smaller tensors.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of smaller tensors.\n",
    "    \"\"\"\n",
    "    num_splits = data.size(0) // torch.prod(torch.tensor(dimensions))\n",
    "    return torch.split(data, num_splits, dim=0)\n",
    "\n",
    "def sliding_window_segmentation(data: torch.Tensor, window_size: int, stride: int) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Perform sliding window segmentation on a tensor.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor.\n",
    "        window_size (int): The size of each window.\n",
    "        stride (int): The stride between consecutive windows.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of window tensors.\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    for i in range(0, data.size(0) - window_size + 1, stride):\n",
    "        window = data[i:i+window_size]\n",
    "        windows.append(window)\n",
    "    return windows\n",
    "\n",
    "def rolling_window_method(data: torch.Tensor, window_size: int) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Perform rolling window method on a tensor.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor.\n",
    "        window_size (int): The size of each window.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of window tensors.\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    for i in range(data.size(0) - window_size + 1):\n",
    "        window = data[i:i+window_size]\n",
    "        windows.append(window)\n",
    "    return windows\n",
    "\n",
    "def overlapping_window_approach(data: torch.Tensor, window_size: int, overlap: int) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Perform overlapping window approach on a tensor.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor.\n",
    "        window_size (int): The size of each window.\n",
    "        overlap (int): The number of overlapping elements between consecutive windows.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of window tensors.\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    stride = window_size - overlap\n",
    "    for i in range(0, data.size(0) - window_size + 1, stride):\n",
    "        window = data[i:i+window_size]\n",
    "        windows.append(window)\n",
    "    return windows\n",
    "\n",
    "def fixed_length_windowing(data: torch.Tensor, window_size: int) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Perform fixed-length windowing on a tensor.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor.\n",
    "        window_size (int): The size of each window.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of window tensors.\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    for i in range(0, data.size(0), window_size):\n",
    "        window = data[i:i+window_size]\n",
    "        windows.append(window)\n",
    "    return windows\n",
    "\n",
    "# Example usage\n",
    "data = torch.randn(1000, 10)  # Example long tensor\n",
    "dimensions = (100, 10)\n",
    "window_size = 50\n",
    "stride = 25\n",
    "overlap = 10\n",
    "\n",
    "# Split long tensor into smaller tensors\n",
    "small_tensors = split_tensor(data, dimensions)\n",
    "print(f\"Number of small tensors: {len(small_tensors)}\")\n",
    "print(f\"Small tensor shape: {small_tensors[0].shape}\")\n",
    "\n",
    "# Sliding window segmentation\n",
    "sliding_windows = sliding_window_segmentation(data, window_size, stride)\n",
    "print(f\"Number of sliding windows: {len(sliding_windows)}\")\n",
    "print(f\"Sliding window shape: {sliding_windows[0].shape}\")\n",
    "\n",
    "# Rolling window method\n",
    "rolling_windows = rolling_window_method(data, window_size)\n",
    "print(f\"Number of rolling windows: {len(rolling_windows)}\")\n",
    "print(f\"Rolling window shape: {rolling_windows[0].shape}\")\n",
    "\n",
    "# Overlapping window approach\n",
    "overlapping_windows = overlapping_window_approach(data, window_size, overlap)\n",
    "print(f\"Number of overlapping windows: {len(overlapping_windows)}\")\n",
    "print(f\"Overlapping window shape: {overlapping_windows[0].shape}\")\n",
    "\n",
    "# Fixed-length windowing\n",
    "fixed_windows = fixed_length_windowing(data, window_size)\n",
    "print(f\"Number of fixed-length windows: {len(fixed_windows)}\")\n",
    "print(f\"Fixed-length window shape: {fixed_windows[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "data = torch.randn(1000, 10)  # Example long tensor\n",
    "dimensions = (100, 10)\n",
    "\n",
    "output=split_tensor(data,dimensions=dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn(1000, 100)  # Example long tensor\n",
    "dimensions = (100, 200, 300, 400)\n",
    "\n",
    "# Split long tensor into smaller tensors\n",
    "small_tensors = split_tensor(data, dimensions)\n",
    "print(f\"Number of small tensors: {len(small_tensors)}\")\n",
    "print(f\"Small tensor shapes: {[tensor.shape for tensor in small_tensors]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Tuple\n",
    "\n",
    "def split_tensor(data: torch.Tensor, dimensions: Tuple[int, ...]) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Split a tensor into smaller tensors with specified dimensions.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor.\n",
    "        dimensions (Tuple[int, ...]): The dimensions of the smaller tensors.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of smaller tensors.\n",
    "    \"\"\"\n",
    "    if data.dim() != len(dimensions):\n",
    "        raise ValueError(f\"The number of dimensions in the input tensor ({data.dim()}) does not match the provided dimensions ({len(dimensions)}).\")\n",
    "\n",
    "    split_sizes = list(dimensions)\n",
    "    return torch.split(data, split_sizes, dim=0)\n",
    "\n",
    "def fixed_size_windowing(data: torch.Tensor, window_size: int, stride: int = None) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Perform fixed-size windowing on a tensor.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor.\n",
    "        window_size (int): The size of each window.\n",
    "        stride (int, optional): The stride between consecutive windows. If not provided, the stride is equal to the window size.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of window tensors.\n",
    "    \"\"\"\n",
    "    if stride is None:\n",
    "        stride = window_size\n",
    "\n",
    "    windows = [data[i:i+window_size] for i in range(0, data.size(0) - window_size + 1, stride)]\n",
    "    return windows\n",
    "\n",
    "def variable_length_sequence_partitioning(data: torch.Tensor, lengths: List[int]) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Perform variable-length sequence partitioning on a tensor.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor.\n",
    "        lengths (List[int]): The lengths of each sequence partition.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of sequence partition tensors.\n",
    "    \"\"\"\n",
    "    if sum(lengths) != data.size(0):\n",
    "        raise ValueError(f\"The sum of partition lengths ({sum(lengths)}) does not match the size of the input tensor ({data.size(0)}).\")\n",
    "\n",
    "    partitions = []\n",
    "    start = 0\n",
    "    for length in lengths:\n",
    "        partition = data[start:start+length]\n",
    "        partitions.append(partition)\n",
    "        start += length\n",
    "    return partitions\n",
    "\n",
    "def temporal_segmentation(data: torch.Tensor, segment_size: int, stride: int = None) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Perform temporal segmentation on a tensor.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor.\n",
    "        segment_size (int): The size of each temporal segment.\n",
    "        stride (int, optional): The stride between consecutive segments. If not provided, the stride is equal to the segment size.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of temporal segment tensors.\n",
    "    \"\"\"\n",
    "    if stride is None:\n",
    "        stride = segment_size\n",
    "\n",
    "    segments = [data[i:i+segment_size] for i in range(0, data.size(0) - segment_size + 1, stride)]\n",
    "    return segments\n",
    "\n",
    "def time_window_sampling(data: torch.Tensor, window_size: int, stride: int) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Perform time-window sampling on a tensor.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor.\n",
    "        window_size (int): The size of each time window.\n",
    "        stride (int): The stride between consecutive windows.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of time window tensors.\n",
    "    \"\"\"\n",
    "    windows = [data[i:i+window_size] for i in range(0, data.size(0) - window_size + 1, stride)]\n",
    "    return windows\n",
    "\n",
    "def sequential_data_chunking(data: torch.Tensor, chunk_size: int) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Perform sequential data chunking on a tensor.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor.\n",
    "        chunk_size (int): The size of each chunk.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of chunk tensors.\n",
    "    \"\"\"\n",
    "    chunks = [data[i:i+chunk_size] for i in range(0, data.size(0), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "def strided_window_processing(data: torch.Tensor, window_size: int, stride: int) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Perform strided window processing on a tensor.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor.\n",
    "        window_size (int): The size of each window.\n",
    "        stride (int): The stride between consecutive windows.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of window tensors.\n",
    "    \"\"\"\n",
    "    windows = [data[i:i+window_size] for i in range(0, data.size(0) - window_size + 1, stride)]\n",
    "    return windows\n",
    "\n",
    "def convolutional_patch_extraction(data: torch.Tensor, patch_size: Tuple[int, ...], stride: Tuple[int, ...]) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Perform convolutional patch extraction on a tensor.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor.\n",
    "        patch_size (Tuple[int, ...]): The size of each patch.\n",
    "        stride (Tuple[int, ...]): The stride between consecutive patches.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of patch tensors.\n",
    "    \"\"\"\n",
    "    if data.dim() != len(patch_size) or data.dim() != len(stride):\n",
    "        raise ValueError(f\"The dimensions of the input tensor ({data.dim()}) do not match the dimensions of patch size ({len(patch_size)}) or stride ({len(stride)}).\")\n",
    "\n",
    "    patches = []\n",
    "    for i in range(0, data.size(0) - patch_size[0] + 1, stride[0]):\n",
    "        for j in range(0, data.size(1) - patch_size[1] + 1, stride[1]):\n",
    "            patch = data[i:i+patch_size[0], j:j+patch_size[1]]\n",
    "            for dim in range(2, data.dim()):\n",
    "                patch = patch.narrow(dim, 0, patch_size[dim])\n",
    "            patches.append(patch)\n",
    "    return patches\n",
    "\n",
    "def spatial_grid_partitioning(data: torch.Tensor, grid_size: Tuple[int, ...]) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Perform spatial grid partitioning on a tensor.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor.\n",
    "        grid_size (Tuple[int, ...]): The size of each grid cell.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.Tensor]: A list of grid cell tensors.\n",
    "    \"\"\"\n",
    "    if data.dim() != len(grid_size):\n",
    "        raise ValueError(f\"The dimensions of the input tensor ({data.dim()}) do not match the dimensions of grid size ({len(grid_size)}).\")\n",
    "\n",
    "    grid_cells = []\n",
    "    grid_indices = [range(0, size, grid_size[dim]) for dim, size in enumerate(data.size())]\n",
    "    for indices in torch.cartesian_prod(*grid_indices):\n",
    "        slices = [slice(indices[dim], indices[dim] + grid_size[dim]) for dim in range(data.dim())]\n",
    "        cell = data[slices]\n",
    "        grid_cells.append(cell)\n",
    "    return grid_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

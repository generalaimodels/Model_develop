{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Any, List,Union,Optional\n",
    "from datasets import (load_dataset, \n",
    "                      DatasetDict,\n",
    "                      concatenate_datasets\n",
    "                      )\n",
    "\n",
    "\n",
    "#Load the datset\n",
    "def load_and_prepare_dataset(\n",
    "    input_source: Union[str, Path, Dict[str, List[Union[str, Path]]]],\n",
    "    split_ratios: tuple = (0.8, 0.1, 0.1),\n",
    "    seed: int = 42,\n",
    "    streaming: bool = False\n",
    ") -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Load a dataset from various input sources and prepare it by splitting into train, test, and eval sets.\n",
    "\n",
    "    :param input_source: A dataset name, path to a folder, a single file, multiple files, or a dictionary specifying train, test, and eval files.\n",
    "    :param split_ratios: A tuple containing the ratios for train, test, and eval splits (default is (0.8, 0.1, 0.1)).\n",
    "    :param seed: A random seed for reproducibility of the split (default is 42).\n",
    "    :param streaming: Whether to use streaming to handle large files (default is False).\n",
    "    :return: A DatasetDict containing the split datasets.\n",
    "    \n",
    "    Example:\n",
    "    # Example usage with streaming for large files:\n",
    "    # dataset_dict = load_and_prepare_dataset({\n",
    "    #     'train': ['train_file_1.csv', 'train_file_2.csv'],\n",
    "    #     'test': ['test_file.csv'],\n",
    "    #     'eval': ['eval_file.csv']\n",
    "    # }, streaming=True)\n",
    "    # print(dataset_dict)\n",
    "    OUTPUT1:\n",
    "    DatasetDict({\n",
    "    train: DatasetDict({\n",
    "        train: Dataset({\n",
    "            features: ['act', 'prompt'],\n",
    "            num_rows: 459\n",
    "        })\n",
    "    })\n",
    "    test: DatasetDict({\n",
    "        train: Dataset({\n",
    "            features: ['act', 'prompt'],\n",
    "            num_rows: 459\n",
    "        })\n",
    "    })\n",
    "    eval: DatasetDict({\n",
    "        train: Dataset({\n",
    "            features: ['act', 'prompt'],\n",
    "            num_rows: 153\n",
    "        })\n",
    "    })\n",
    "    })\n",
    "    EXAMPLE2:\n",
    "    dataset=load_and_prepare_dataset('fka/awesome-chatgpt-prompts')\n",
    "    DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['act', 'prompt'],\n",
    "        num_rows: 122\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['act', 'prompt'],\n",
    "        num_rows: 15\n",
    "    })\n",
    "    eval: Dataset({\n",
    "        features: ['act', 'prompt'],\n",
    "        num_rows: 16\n",
    "    })\n",
    "    })\n",
    "    EXAMPLE3:\n",
    "    datset_path=load_and_prepare_dataset('/content/awesome-chatgpt-prompts')\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['act', 'prompt'],\n",
    "        num_rows: 122\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['act', 'prompt'],\n",
    "        num_rows: 15\n",
    "    })\n",
    "    eval: Dataset({\n",
    "        features: ['act', 'prompt'],\n",
    "        num_rows: 16\n",
    "    })\n",
    "    })\n",
    "\n",
    "    \"\"\"\n",
    "    # Load dataset from different types of input sources\n",
    "    if isinstance(input_source, (str, Path)):\n",
    "        # Dataset name, single file or path to folder\n",
    "        dataset = load_dataset(input_source, streaming=streaming)\n",
    "        dataset = DatasetDict(dataset)\n",
    "    elif isinstance(input_source, dict):\n",
    "        # Dictionary with specified train, test, and eval files\n",
    "        formats = ['csv', 'json', 'jsonl', 'parquet', 'txt']\n",
    "        datasets = {}\n",
    "        for split, files in input_source.items():\n",
    "            format_detected = None\n",
    "            for fmt in formats:\n",
    "                if any(str(file).endswith(fmt) for file in files):\n",
    "                    format_detected = fmt\n",
    "                    break\n",
    "            if format_detected is None:\n",
    "                raise ValueError(f\"No supported file format detected for files: {files}\")\n",
    "            datasets[split] = load_dataset(format_detected, data_files=files, streaming=streaming)\n",
    "        dataset = DatasetDict(datasets)\n",
    "    else:\n",
    "        raise ValueError(\"Input source should be a dataset name, path to a folder, a single file, multiple files, or a dictionary.\")\n",
    "\n",
    "    # Perform the split if needed and if not in streaming mode\n",
    "    if not streaming:\n",
    "        train_size, test_size, eval_size = split_ratios\n",
    "        assert 0.0 < train_size < 1.0 and 0.0 < test_size < 1.0 and 0.0 < eval_size < 1.0 and (train_size + test_size + eval_size) == 1.0, \\\n",
    "            \"Split ratios must be between 0 and 1 and sum up to 1.\"\n",
    "\n",
    "        if \"train\" not in dataset or \"test\" not in dataset or \"eval\" not in dataset:\n",
    "            # Assuming all splits are to be derived from the 'train' dataset\n",
    "            full_dataset = concatenate_datasets(list(dataset.values())) if isinstance(dataset, dict) else dataset\n",
    "            split_dataset = full_dataset.train_test_split(train_size=train_size, seed=seed)\n",
    "            test_eval_split = split_dataset['test'].train_test_split(test_size=test_size / (test_size + eval_size), seed=seed)\n",
    "\n",
    "            dataset = DatasetDict({\n",
    "                \"train\": split_dataset[\"train\"],\n",
    "                \"test\": test_eval_split[\"train\"],\n",
    "                \"eval\": test_eval_split[\"test\"]\n",
    "            })\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_number_of_trainable_parameters(model):\n",
    "    r\"\"\"\n",
    "    Returns the number of trainable parameters and number of all parameters in the model.\n",
    "    \"\"\"\n",
    "    # note: same as PeftModel.get_nb_trainable_parameters\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        # Due to the design of 4bit linear layers from bitsandbytes\n",
    "        # one needs to multiply the number of parameters by 2 to get\n",
    "        # the correct number of parameters\n",
    "        if param.__class__.__name__ == \"Params4bit\":\n",
    "            num_params = num_params * 2\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    print(\"Total no of training_parameters:\",trainable_params)\n",
    "    print(\"Total no of parameters is :\",all_param)\n",
    "    print(\"percantage of trainable parameters is\",100*((trainable_params)/(all_param)))\n",
    "    return trainable_params, all_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from transformers import (\n",
    "    PreTrainedTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    SchedulerType,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    get_scheduler,      \n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "# Define a function to read arguments from a YAML file\n",
    "def load_arguments_from_yaml(yaml_file_path:str):\n",
    "    with open(yaml_file_path, 'r') as stream:\n",
    "        try:\n",
    "            arguments = yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "            arguments = {}\n",
    "    return arguments\n",
    "\n",
    "\n",
    "def load_model(model_name_or_path: Union[str,List]) -> AutoModelForCausalLM:\n",
    "    \"\"\"\n",
    "    Function to load a transformers model.\n",
    "    \n",
    "    Args:\n",
    "      model_name_or_path (Union[str, Path]): The name or path of the model.\n",
    "\n",
    "    Returns:\n",
    "        model (AutoModelForCausalLM): The loaded model.\n",
    "    \"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "    return model\n",
    "\n",
    "def create_tokenizer(\n",
    "    tokenizer_name_or_path: Union[str,List] ) -> AutoTokenizer:\n",
    "    \"\"\"\n",
    "    Initializes and returns a tokenizer based on the specified pretrained model or path.\n",
    "\n",
    "    Args:\n",
    "        tokenizer_name_or_path (str): The name or path of the tokenizer's pretrained model.\n",
    "\n",
    "    Returns:\n",
    "        AutoTokenizer: The initialized tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "    \n",
    "    # Set special tokens if they are not already set\n",
    "    special_tokens = {\n",
    "        'pad_token': tokenizer.eos_token,\n",
    "        'bos_token': tokenizer.eos_token,\n",
    "        'eos_token': tokenizer.eos_token,\n",
    "        'unk_token': tokenizer.eos_token,\n",
    "        'sep_token': tokenizer.eos_token,\n",
    "        'cls_token': tokenizer.eos_token,\n",
    "        'mask_token':tokenizer.eos_token\n",
    "    }\n",
    "    for token_name, token_value in special_tokens.items():\n",
    "        if getattr(tokenizer, f\"{token_name}_id\") is None:\n",
    "            setattr(tokenizer, token_name, token_value)\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML file loaded successfully:\n",
      "{'description': 'Pre-training on the custom dataset custom model.', 'model_name_or_path': '', 'output_dir': '', 'dataset_name_or_path': '', 'tokenizer_name_or_path': '', 'config_name': '', 'max_seq_length': 1024, 'tokenizer_name': '', 'chars_per_token': 3.5, 'fim_rate': 0.5, 'fim_spm_rate': 0.5, 'seed': 0}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from typing import Any, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "def load_yaml(file_path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Loads a YAML file and returns its contents as a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (Path): A Path object representing the path to the YAML file.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, Any]: A dictionary containing the parameters from the YAML file.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If the YAML file does not exist at the specified path.\n",
    "    - yaml.YAMLError: If the YAML file contains invalid syntax.\n",
    "    \"\"\"\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"The file '{file_path}' was not found.\")\n",
    "\n",
    "    if not file_path.suffix in ['.yml', '.yaml']:\n",
    "        raise ValueError(f\"The file '{file_path}' does not have a .yml or .yaml extension.\")\n",
    "\n",
    "    try:\n",
    "        with file_path.open('r') as file:\n",
    "            parameters = yaml.safe_load(file)\n",
    "            if parameters is None:\n",
    "                # In case the YAML file is empty, return an empty dictionary\n",
    "                return {}\n",
    "            if not isinstance(parameters, dict):\n",
    "                raise TypeError(\"The top level of the YAML file should be a dictionary.\")\n",
    "            return parameters\n",
    "    except yaml.YAMLError as e:\n",
    "        raise yaml.YAMLError(f\"Error parsing YAML file: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a YAML file named 'config.yml' in the current directory.\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        config_path = Path('file_operations-/File_and_Operations/transformers_peft_trl/Fine-tuning/General_fine_tuning/Pre_training.yml')\n",
    "        config_params = load_yaml(config_path)\n",
    "        print(\"YAML file loaded successfully:\")\n",
    "        print(config_params)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "description=\"\"\n",
    "def parse_arguments() -> argparse.Namespace:\n",
    "    \"\"\"\n",
    "    Parses command-line arguments.\n",
    "\n",
    "    Returns:\n",
    "    - argparse.Namespace: An object containing the parsed command-line arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=f\"\"\"\n",
    "               {description}\n",
    "                   \"\"\")\n",
    "    parser.add_argument('--yaml_path', type=str, help=\"Path to the YAML file to load.\")\n",
    "    parser.add_argument('--data_name', type=str, help=\"Path to the\")\n",
    "\n",
    "    args=parser.parse_args()\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Class                                      | Used For                                    |\n",
    "|--------------------------------------------|---------------------------------------------|\n",
    "| AggregationStrategy                        | Strategy for handling grouped token outputs. |\n",
    "| Any                                        | Denotes any type - type hint.                |\n",
    "| ArgumentHandler                            | Parses custom command-line arguments.        |\n",
    "| AudioClassificationPipeline                | Pipeline for audio classification tasks.     |\n",
    "| AutoConfig                                 | Automatically loads model configurations.    |\n",
    "| AutoFeatureExtractor                       | Automatically creates feature extractors.    |\n",
    "| AutoImageProcessor                         | Automatically processes images for models.   |\n",
    "| AutoModel                                  | Generic model loading (auto-detection).      |\n",
    "| AutoModelForAudioClassification            | Audio classification model loading.          |\n",
    "| AutoModelForCTC                            | Models for Connectionist Temporal Classification. |\n",
    "| AutoModelForCausalLM                       | Causal language modeling autoloading.        |\n",
    "| AutoModelForDepthEstimation                | Depth estimation model autoloading.          |\n",
    "| AutoModelForDocumentQuestionAnswering      | Document QA model autoloading.               |\n",
    "| AutoModelForImageClassification            | Image classification model autoloading.      |\n",
    "| AutoModelForImageSegmentation              | Image segmentation model autoloading.        |\n",
    "| AutoModelForImageToImage                   | Image-to-image tasks model autoloading.      |\n",
    "| AutoModelForMaskGeneration                 | Mask generation model autoloading.           |\n",
    "| AutoModelForMaskedLM                       | Masked language modeling autoloading.        |\n",
    "| AutoModelForObjectDetection                | Object detection model autoloading.          |\n",
    "| AutoModelForQuestionAnswering              | Question answering model autoloading.        |\n",
    "| AutoModelForSemanticSegmentation           | Semantic segmentation model autoloading.     |\n",
    "| AutoModelForSeq2SeqLM                      | Sequence-to-sequence autoloading.            |\n",
    "| AutoModelForSequenceClassification         | Sequence classification model autoloading.    |\n",
    "| AutoModelForSpeechSeq2Seq                  | Speech sequence-to-sequence autoloading.     |\n",
    "| AutoModelForTableQuestionAnswering         | Table QA model autoloading.                  |\n",
    "| AutoModelForTextToSpectrogram              | Text to spectrogram model autoloading.       |\n",
    "| AutoModelForTextToWaveform                 | Text to waveform model autoloading.          |\n",
    "| AutoModelForTokenClassification            | Token classification model autoloading.      |\n",
    "| AutoModelForVideoClassification            | Video classification model autoloading.      |\n",
    "| AutoModelForVision2Seq                     | Vision to sequence tasks autoloading.        |\n",
    "| AutoModelForVisualQuestionAnswering        | Visual QA model autoloading.                 |\n",
    "| AutoModelForZeroShotImageClassification    | Zero-shot image classification autoloading.  |\n",
    "| AutoModelForZeroShotObjectDetection        | Zero-shot object detection autoloading.      |\n",
    "| AutoTokenizer                              | Automatically creates tokenizers.            |\n",
    "| AutomaticSpeechRecognitionPipeline         | ASR pipeline tasks.                          |\n",
    "| BaseImageProcessor                         | Base class for image processing.             |\n",
    "| CONFIG_NAME                                | Constant for configuration file name.        |\n",
    "| Conversation                               | Handles conversation data for models.        |\n",
    "| ConversationalPipeline                     | Pipeline for conversational tasks.           |\n",
    "| CsvPipelineDataFormat                      | CSV format handler for pipeline data.        |\n",
    "| DepthEstimationPipeline                    | Pipeline for depth estimation tasks.         |\n",
    "| Dict                                       | Dictionary type - type hint.                 |\n",
    "| DocumentQuestionAnsweringPipeline          | Pipeline for document QA tasks.              |\n",
    "| FEATURE_EXTRACTOR_MAPPING                  | Mapping of feature extractors.               |\n",
    "| FeatureExtractionPipeline                  | Pipeline for feature extraction tasks.       |\n",
    "| FillMaskPipeline                           | Pipeline for fill-mask tasks.                |\n",
    "| HUGGINGFACE_CO_RESOLVE_ENDPOINT            | Endpoint for resolving Hugging Face hub.     |\n",
    "| IMAGE_PROCESSOR_MAPPING                    | Mapping of image processors.                 |\n",
    "| ImageClassificationPipeline                | Pipeline for image classification tasks.     |\n",
    "| ImageSegmentationPipeline                  | Pipeline for image segmentation tasks.       |\n",
    "| ImageToImagePipeline                       | Pipeline for image-to-image tasks.           |\n",
    "| ImageToTextPipeline                        | Pipeline for image-to-text tasks.            |\n",
    "| JsonPipelineDataFormat                     | JSON format handler for pipeline data.       |\n",
    "| List                                       | List type - type hint.                       |\n",
    "| MULTI_MODEL_CONFIGS                        | Configurations for multi-model support.      |\n",
    "| MaskGenerationPipeline                     | Pipeline for mask generation tasks.          |\n",
    "| NO_FEATURE_EXTRACTOR_TASKS                 | Tasks without feature extractor.             |\n",
    "| NO_IMAGE_PROCESSOR_TASKS                   | Tasks without image processor.               |\n",
    "| NO_TOKENIZER_TASKS                         | Tasks without tokenizer.                     |\n",
    "| NerPipeline                                | Pipeline for named entity recognition.       |\n",
    "| ObjectDetectionPipeline                    | Pipeline for object detection tasks.         |\n",
    "| Optional                                   | Optional type - type hint.                   |\n",
    "| PIPELINE_REGISTRY                          | Registry of pipeline tasks.                  |\n",
    "| Path                                       | Filesystem path type - type hint.            |\n",
    "| PipedPipelineDataFormat                    | Piped format for pipeline data.              |\n",
    "| Pipeline                                   | Base class for all pipelines.                |\n",
    "| PipelineDataFormat                         | Handler for pipeline data formats.           |\n",
    "| PipelineException                          | Custom exception for pipeline errors.        |\n",
    "| PipelineRegistry                           | Manages registered pipeline tasks.           |\n",
    "| PreTrainedFeatureExtractor                 | Base for pretrained feature extractors.      |\n",
    "| PreTrainedTokenizer                        | Base for pretrained tokenizers.              |\n",
    "| PretrainedConfig                           | Base for model configurations.               |\n",
    "| QuestionAnsweringArgumentHandler           | QA argument parser for pipeline.             |\n",
    "| QuestionAnsweringPipeline                  | Pipeline for question answering tasks.       |\n",
    "| SUPPORTED_TASKS                            | Supported tasks for pipelines.               |\n",
    "| SummarizationPipeline                      | Pipeline for text summarization tasks.       |\n",
    "| TASK_ALIASES                               | Aliases for different tasks.                 |\n",
    "| TFAutoModel                                | TensorFlow model autoloading.                |\n",
    "| TFAutoModelForCausalLM                     | TF causal LM model autoloading.              |\n",
    "| TFAutoModelForImageClassification          | TF image classification model autoloading.   |\n",
    "| TFAutoModelForMaskedLM                     | TF masked LM model autoloading.              |\n",
    "| TFAutoModelForQuestionAnswering            | TF question answering model autoloading.     |\n",
    "| TFAutoModelForSeq2SeqLM                    | TF seq2seq LM model autoloading.             |\n",
    "| TFAutoModelForSequenceClassification       | TF sequence classification autoloading.      |\n",
    "| TFAutoModelForTableQuestionAnswering       | TF table QA model autoloading.               |\n",
    "| TFAutoModelForTokenClassification          | TF token classification model autoloading.   |\n",
    "| TFAutoModelForVision2Seq                   | TF vision to sequence model autoloading.     |\n",
    "| TFAutoModelForZeroShotImageClassification  | TF zero-shot image classification autoloading. |\n",
    "| TOKENIZER_MAPPING                          | Mapping of tokenizers.                       |\n",
    "| TYPE_CHECKING                              | Constant for type checking at runtime.       |\n",
    "| TableQuestionAnsweringArgumentHandler      | Table QA argument parser for pipeline.       |\n",
    "| TableQuestionAnsweringPipeline             | Pipeline for table QA tasks.                 |\n",
    "| Text2TextGenerationPipeline                | Pipeline for text-to-text generation.        |\n",
    "| TextClassificationPipeline                 | Pipeline for text classification tasks.      |\n",
    "| TextGenerationPipeline                     | Pipeline for text generation tasks.          |\n",
    "| TextToAudioPipeline                        | Pipeline for text-to-audio tasks.            |\n",
    "| TokenClassificationArgumentHandler         | Token classification argument parser.        |\n",
    "| TokenClassificationPipeline                | Pipeline for token classification tasks.     |\n",
    "| TranslationPipeline                        | Pipeline for translation tasks.              |\n",
    "| Tuple                                      | Tuple type - type hint.                      |\n",
    "| Union                                      | Union type - type hint.                      |\n",
    "| VideoClassificationPipeline                | Pipeline for video classification tasks.     |\n",
    "| VisualQuestionAnsweringPipeline            | Pipeline for visual QA tasks.                |\n",
    "| ZeroShotAudioClassificationPipeline        | Pipeline for zero-shot audio classification. |\n",
    "| ZeroShotClassificationArgumentHandler      | Zero-shot classification argument parser.    |\n",
    "| ZeroShotClassificationPipeline             | Pipeline for zero-shot classification tasks. |\n",
    "| ZeroShotImageClassificationPipeline        | Pipeline for zero-shot image classification. |\n",
    "| ZeroShotObjectDetectionPipeline            | Pipeline for zero-shot object detection.     |\n",
    "| __builtins__                               | Built-in objects in the module.              |\n",
    "| __cached__                                 | Cached file path for the module.             |\n",
    "| __doc__                                    | Module documentation string.                 |\n",
    "| __file__                                   | Path to the module file.                     |\n",
    "| __loader__                                 | Loader for the module.                       |\n",
    "| __name__                                   | Name of the module.                          |\n",
    "| __package__                                | Package name for the module.                 |\n",
    "| __path__                                   | Path for package module search.              |\n",
    "| __spec__                                   | Specification for the module.                |\n",
    "| audio_classification                       | Module for audio classification functions.   |\n",
    "| audio_utils                                | Utilities for audio processing.              |\n",
    "| automatic_speech_recognition               | ASR module functions and utilities.          |\n",
    "| base                                       | Base module for classes and functions.       |\n",
    "| cached_file                                | Function to cache files.                     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: add_argument\n",
      "Signature: (self, *args, **kwargs)\n",
      "Docstring:\n",
      "add_argument(dest, ..., name=value, ...)\n",
      "add_argument(option_string, option_string, ..., name=value, ...)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "import inspect\n",
    "from typing import Callable\n",
    "\n",
    "def display_function_details(func: Callable):\n",
    "    \"\"\"Display detailed information about a single function.\"\"\"\n",
    "    if func is not None:\n",
    "        # Retrieve the name of the function for display purposes\n",
    "        func_name = func.__name__\n",
    "        # Get the signature of the function\n",
    "        try:\n",
    "            sig = inspect.signature(func)\n",
    "        except ValueError:\n",
    "            sig = \"Not available\"\n",
    "        # Get the docstring of the function\n",
    "        docstring = inspect.getdoc(func) or \"Not available\"\n",
    "\n",
    "        print(f\"Function: {func_name}\")\n",
    "        print(f\"Signature: {sig}\")\n",
    "        print(f\"Docstring:\\n{docstring}\")\n",
    "        print(\"-\" * 80)\n",
    "    else:\n",
    "        print(\"The specified function does not exist or is not callable.\")\n",
    "\n",
    "def explore_specific_function(module, function_name: str):\n",
    "    \"\"\"Explore a specific function within the given module.\"\"\"\n",
    "    # Get the attribute from the module matching the function_name\n",
    "    func = getattr(module, function_name, None)\n",
    "    if callable(func):\n",
    "        display_function_details(func)\n",
    "    else:\n",
    "        print(f\"No callable function named '{function_name}' found in the module.\")\n",
    "\n",
    "# Skill 13, 22: Writing Modular Code and Understanding Namespaces\n",
    "if __name__ == \"__main__\":\n",
    "    function_to_explore = 'add_argument'\n",
    "    explore_specific_function(argparse.ArgumentParser, function_to_explore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

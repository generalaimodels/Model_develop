{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "import psutil\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "\n",
    "def levenshtein_distance(str1: str, str2: str) -> int:\n",
    "    \"\"\"Calculate the Levenshtein distance between two strings.\"\"\"\n",
    "    if str1 == str2:\n",
    "        return 0\n",
    "    num_rows = len(str1) + 1\n",
    "    num_cols = len(str2) + 1\n",
    "    dp_matrix = list(range(num_cols))\n",
    "    for i in range(1, num_rows):\n",
    "        prev = dp_matrix[0]\n",
    "        dp_matrix[0] = i\n",
    "        for j in range(1, num_cols):\n",
    "            temp = dp_matrix[j]\n",
    "            if str1[i - 1] == str2[j - 1]:\n",
    "                dp_matrix[j] = prev\n",
    "            else:\n",
    "                dp_matrix[j] = min(prev, dp_matrix[j], dp_matrix[j - 1]) + 1\n",
    "            prev = temp\n",
    "    return dp_matrix[num_cols - 1]\n",
    "\n",
    "\n",
    "def get_closest_label(eval_pred: str, classes: List[str]) -> str:\n",
    "    \"\"\"Find the closest label to the predicted label.\"\"\"\n",
    "    min_id = sys.maxsize\n",
    "    min_edit_distance = sys.maxsize\n",
    "    for i, class_label in enumerate(classes):\n",
    "        edit_distance = levenshtein_distance(eval_pred.strip(), class_label)\n",
    "        if edit_distance < min_edit_distance:\n",
    "            min_id = i\n",
    "            min_edit_distance = edit_distance\n",
    "    return classes[min_id]\n",
    "\n",
    "\n",
    "def b2mb(x: int) -> int:\n",
    "    \"\"\"Convert bytes to megabytes.\"\"\"\n",
    "    return int(x / 2**20)\n",
    "\n",
    "\n",
    "class TorchTracemalloc:\n",
    "    \"\"\"Context manager to track the peak memory usage of the process.\"\"\"\n",
    "\n",
    "    def __enter__(self):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_max_memory_allocated()\n",
    "        self.begin = torch.cuda.memory_allocated()\n",
    "        self.process = psutil.Process()\n",
    "\n",
    "        self.cpu_begin = self.cpu_mem_used()\n",
    "        self.peak_monitoring = True\n",
    "        peak_monitor_thread = threading.Thread(target=self.peak_monitor_func)\n",
    "        peak_monitor_thread.daemon = True\n",
    "        peak_monitor_thread.start()\n",
    "        return self\n",
    "\n",
    "    def cpu_mem_used(self) -> int:\n",
    "        \"\"\"Get resident set size memory for the current process.\"\"\"\n",
    "        return self.process.memory_info().rss\n",
    "\n",
    "    def peak_monitor_func(self):\n",
    "        self.cpu_peak = -1\n",
    "\n",
    "        while True:\n",
    "            self.cpu_peak = max(self.cpu_mem_used(), self.cpu_peak)\n",
    "\n",
    "            if not self.peak_monitoring:\n",
    "                break\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        self.peak_monitoring = False\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        self.end = torch.cuda.memory_allocated()\n",
    "        self.peak = torch.cuda.max_memory_allocated()\n",
    "        self.used = b2mb(self.end - self.begin)\n",
    "        self.peaked = b2mb(self.peak - self.begin)\n",
    "\n",
    "        self.cpu_end = self.cpu_mem_used()\n",
    "        self.cpu_used = b2mb(self.cpu_end - self.cpu_begin)\n",
    "        self.cpu_peaked = b2mb(self.cpu_peak - self.cpu_begin)\n",
    "\n",
    "\n",
    "def main():\n",
    "    accelerator = Accelerator()\n",
    "    model_name_or_path = \"facebook/bart-large\"\n",
    "    dataset_name = \"twitter_complaints\"\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "    )\n",
    "    text_column = \"Tweet text\"\n",
    "    label_column = \"text_label\"\n",
    "    lr = 3e-3\n",
    "    num_epochs = 5\n",
    "    batch_size = 8\n",
    "    seed = 42\n",
    "    do_test = False\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset = load_dataset(\"ought/raft\", dataset_name)\n",
    "    classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n",
    "    dataset = dataset.map(\n",
    "        lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    target_max_length = max(\n",
    "        [len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes]\n",
    "    )\n",
    "\n",
    "    def preprocess_function(examples: Dict[str, List[str]]) -> Dict[str, torch.Tensor]:\n",
    "        inputs = examples[text_column]\n",
    "        targets = examples[label_column]\n",
    "        model_inputs = tokenizer(inputs, truncation=True)\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=target_max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = labels[\"input_ids\"]\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        processed_datasets = dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            remove_columns=dataset[\"train\"].column_names,\n",
    "            load_from_cache_file=True,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    train_dataset = processed_datasets[\"train\"]\n",
    "    eval_dataset = processed_datasets[\"train\"]\n",
    "    test_dataset = processed_datasets[\"test\"]\n",
    "\n",
    "    def collate_fn(examples: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "        return tokenizer.pad(examples, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, collate_fn=collate_fn, batch_size=batch_size, pin_memory=True\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, collate_fn=collate_fn, batch_size=batch_size, pin_memory=True\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=(len(train_dataloader) * num_epochs),\n",
    "    )\n",
    "\n",
    "    model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler = accelerator.prepare(\n",
    "        model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler\n",
    "    )\n",
    "    accelerator.print(model)\n",
    "\n",
    "\n",
    "is_ds_zero_3 = False\n",
    "if getattr(accelerator.state, \"deepspeed_plugin\", None):\n",
    "    is_ds_zero_3 = accelerator.state.deepspeed_plugin.zero_stage == 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with TorchTracemalloc() as tracemalloc:\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.detach().float()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        accelerator.print(\n",
    "            f\"GPU Memory before entering the train : {b2mb(tracemalloc.begin)}\"\n",
    "        )\n",
    "        accelerator.print(\n",
    "            f\"GPU Memory consumed at the end of the train (end-begin): {tracemalloc.used}\"\n",
    "        )\n",
    "        accelerator.print(\n",
    "            f\"GPU Peak Memory consumed during the train (max-begin): {tracemalloc.peaked}\"\n",
    "        )\n",
    "        accelerator.print(\n",
    "            f\"GPU Total Peak Memory consumed during the train (max): {tracemalloc.peaked + b2mb(tracemalloc.begin)}\"\n",
    "        )\n",
    "\n",
    "        accelerator.print(\n",
    "            f\"CPU Memory before entering the train : {b2mb(tracemalloc.cpu_begin)}\"\n",
    "        )\n",
    "        accelerator.print(\n",
    "            f\"CPU Memory consumed at the end of the train (end-begin): {tracemalloc.cpu_used}\"\n",
    "        )\n",
    "        accelerator.print(\n",
    "            f\"CPU Peak Memory consumed during the train (max-begin): {tracemalloc.cpu_peaked}\"\n",
    "        )\n",
    "        accelerator.print(\n",
    "            f\"CPU Total Peak Memory consumed during the train (max): {tracemalloc.cpu_peaked + b2mb(tracemalloc.cpu_begin)}\"\n",
    "        )\n",
    "\n",
    "        train_epoch_loss = total_loss / len(train_dataloader)\n",
    "        train_ppl = torch.exp(train_epoch_loss)\n",
    "        accelerator.print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=}\")\n",
    "\n",
    "    model.eval()\n",
    "    eval_preds = []\n",
    "    with TorchTracemalloc() as tracemalloc:\n",
    "        for _, batch in enumerate(tqdm(eval_dataloader)):\n",
    "            batch = {k: v for k, v in batch.items() if k != \"labels\"}\n",
    "            with torch.no_grad():\n",
    "                outputs = accelerator.unwrap_model(model).generate(\n",
    "                    **batch, synced_gpus=is_ds_zero_3\n",
    "                )\n",
    "            outputs = accelerator.pad_across_processes(\n",
    "                outputs, dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "            preds = accelerator.gather_for_metrics(outputs).detach().cpu().numpy()\n",
    "            eval_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\n",
    "\n",
    "        accelerator.print(\n",
    "            f\"GPU Memory before entering the eval : {b2mb(tracemalloc.begin)}\"\n",
    "        )\n",
    "        accelerator.print(\n",
    "            f\"GPU Memory consumed at the end of the eval (end-begin): {tracemalloc.used}\"\n",
    "        )\n",
    "        accelerator.print(\n",
    "            f\"GPU Peak Memory consumed during the eval (max-begin): {tracemalloc.peaked}\"\n",
    "        )\n",
    "        accelerator.print(\n",
    "            f\"GPU Total Peak Memory consumed during the eval (max): {tracemalloc.peaked + b2mb(tracemalloc.begin)}\"\n",
    "        )\n",
    "\n",
    "        accelerator.print(\n",
    "            f\"CPU Memory before entering the eval : {b2mb(tracemalloc.cpu_begin)}\"\n",
    "        )\n",
    "        accelerator.print(\n",
    "            f\"CPU Memory consumed at the end of the eval (end-begin): {tracemalloc.cpu_used}\"\n",
    "        )\n",
    "        accelerator.print(\n",
    "            f\"CPU Peak Memory consumed during the eval (max-begin): {tracemalloc.cpu_peaked}\"\n",
    "        )\n",
    "        accelerator.print(\n",
    "            f\"CPU Total Peak Memory consumed during the eval (max): {tracemalloc.cpu_peaked + b2mb(tracemalloc.cpu_begin)}\"\n",
    "        )\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    assert len(eval_preds) == len(\n",
    "        dataset[\"train\"][label_column]\n",
    "    ), f\"{len(eval_preds)} != {len(dataset['train'][label_column])}\"\n",
    "    for pred, true in zip(eval_preds, dataset[\"train\"][label_column]):\n",
    "        if pred.strip() == true.strip():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    accuracy = correct / total * 100\n",
    "    accelerator.print(f\"{accuracy=}\")\n",
    "    accelerator.print(f\"{eval_preds[:10]=}\")\n",
    "    accelerator.print(f\"{dataset['train'][label_column][:10]=}\")\n",
    "\n",
    "if do_test:\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    for _, batch in enumerate(tqdm(test_dataloader)):\n",
    "        batch = {k: v for k, v in batch.items() if k != \"labels\"}\n",
    "        with torch.no_grad():\n",
    "            outputs = accelerator.unwrap_model(model).generate(\n",
    "                **batch, synced_gpus=is_ds_zero_3\n",
    "            )\n",
    "        outputs = accelerator.pad_across_processes(\n",
    "            outputs, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        preds = accelerator.gather(outputs).detach().cpu().numpy()\n",
    "        test_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\n",
    "\n",
    "    test_preds_cleaned = [\n",
    "        get_closest_label(pred, classes) for pred in test_preds\n",
    "    ]\n",
    "\n",
    "    test_df = dataset[\"test\"].to_pandas()\n",
    "    assert len(test_preds_cleaned) == len(test_df), f\"{len(test_preds_cleaned)} != {len(test_df)}\"\n",
    "    test_df[label_column] = test_preds_cleaned\n",
    "    test_df[\"text_labels_orig\"] = test_preds\n",
    "    accelerator.print(test_df[[text_column, label_column]].sample(20))\n",
    "\n",
    "    pred_df = test_df[[\"ID\", label_column]]\n",
    "    pred_df.columns = [\"ID\", \"Label\"]\n",
    "\n",
    "    os.makedirs(f\"data/{dataset_name}\", exist_ok=True)\n",
    "    pred_df.to_csv(f\"data/{dataset_name}/preds.csv\", index=False)\n",
    "if name == \"main\":\n",
    "   main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python  \n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "import psutil\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "\n",
    "def levenshtein_distance(str1: str, str2: str) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the Levenshtein distance between two strings.\n",
    "\n",
    "    Args:\n",
    "        str1 (str): First string.\n",
    "        str2 (str): Second string.\n",
    "\n",
    "    Returns:\n",
    "        int: Levenshtein distance between the two strings.\n",
    "    \"\"\"\n",
    "    if str1 == str2:\n",
    "        return 0\n",
    "    num_rows = len(str1) + 1\n",
    "    num_cols = len(str2) + 1\n",
    "    dp_matrix = list(range(num_cols))\n",
    "    for i in range(1, num_rows):\n",
    "        prev = dp_matrix[0]\n",
    "        dp_matrix[0] = i\n",
    "        for j in range(1, num_cols):\n",
    "            temp = dp_matrix[j]\n",
    "            if str1[i - 1] == str2[j - 1]:\n",
    "                dp_matrix[j] = prev\n",
    "            else:\n",
    "                dp_matrix[j] = min(prev, dp_matrix[j], dp_matrix[j - 1]) + 1\n",
    "            prev = temp\n",
    "    return dp_matrix[num_cols - 1]\n",
    "\n",
    "\n",
    "def get_closest_label(eval_pred: str, classes: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Find the closest label to the predicted label based on Levenshtein distance.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (str): Predicted label.\n",
    "        classes (List[str]): List of possible class labels.\n",
    "\n",
    "    Returns:\n",
    "        str: Closest label to the predicted label.\n",
    "    \"\"\"\n",
    "    min_id = sys.maxsize\n",
    "    min_edit_distance = sys.maxsize\n",
    "    for i, class_label in enumerate(classes):\n",
    "        edit_distance = levenshtein_distance(eval_pred.strip(), class_label)\n",
    "        if edit_distance < min_edit_distance:\n",
    "            min_id = i\n",
    "            min_edit_distance = edit_distance\n",
    "    return classes[min_id]\n",
    "\n",
    "\n",
    "def b2mb(x: int) -> int:\n",
    "    \"\"\"\n",
    "    Convert bytes to megabytes.\n",
    "\n",
    "    Args:\n",
    "        x (int): Value in bytes.\n",
    "\n",
    "    Returns:\n",
    "        int: Value in megabytes.\n",
    "    \"\"\"\n",
    "    return int(x / 2**20)\n",
    "\n",
    "\n",
    "class TorchTracemalloc:\n",
    "    \"\"\"\n",
    "    Context manager to track the peak memory usage of the process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __enter__(self):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_max_memory_allocated()\n",
    "        self.begin = torch.cuda.memory_allocated()\n",
    "        self.process = psutil.Process()\n",
    "\n",
    "        self.cpu_begin = self.cpu_mem_used()\n",
    "        self.peak_monitoring = True\n",
    "        peak_monitor_thread = threading.Thread(target=self.peak_monitor_func)\n",
    "        peak_monitor_thread.daemon = True\n",
    "        peak_monitor_thread.start()\n",
    "        return self\n",
    "\n",
    "    def cpu_mem_used(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the resident set size memory for the current process.\n",
    "\n",
    "        Returns:\n",
    "            int: Memory used by the current process.\n",
    "        \"\"\"\n",
    "        return self.process.memory_info().rss\n",
    "\n",
    "    def peak_monitor_func(self):\n",
    "        self.cpu_peak = -1\n",
    "\n",
    "        while True:\n",
    "            self.cpu_peak = max(self.cpu_mem_used(), self.cpu_peak)\n",
    "\n",
    "            if not self.peak_monitoring:\n",
    "                break\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        self.peak_monitoring = False\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        self.end = torch.cuda.memory_allocated()\n",
    "        self.peak = torch.cuda.max_memory_allocated()\n",
    "        self.used = b2mb(self.end - self.begin)\n",
    "        self.peaked = b2mb(self.peak - self.begin)\n",
    "\n",
    "        self.cpu_end = self.cpu_mem_used()\n",
    "        self.cpu_used = b2mb(self.cpu_end - self.cpu_begin)\n",
    "        self.cpu_peaked = b2mb(self.cpu_peak - self.cpu_begin)\n",
    "\n",
    "\n",
    "def main():\n",
    "    accelerator = Accelerator()\n",
    "    model_name_or_path = \"facebook/bart-large\"\n",
    "    dataset_name = \"twitter_complaints\"\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "    )\n",
    "    text_column = \"Tweet text\"\n",
    "    label_column = \"text_label\"\n",
    "    lr = 3e-3\n",
    "    num_epochs = 5\n",
    "    batch_size = 8\n",
    "    seed = 42\n",
    "    do_test = False\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset = load_dataset(\"ought/raft\", dataset_name)\n",
    "    classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n",
    "    dataset = dataset.map(\n",
    "        lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    target_max_length = max(\n",
    "        [len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes]\n",
    "    )\n",
    "\n",
    "    def preprocess_function(examples: Dict[str, List[str]]) -> Dict[str, Union[List[int], torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Preprocess the dataset examples.\n",
    "\n",
    "        Args:\n",
    "            examples (Dict[str, List[str]]): Dataset examples.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Union[List[int], torch.Tensor]]: Preprocessed examples.\n",
    "        \"\"\"\n",
    "        inputs = examples[text_column]\n",
    "        targets = examples[label_column]\n",
    "        model_inputs = tokenizer(inputs, truncation=True)\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=target_max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = labels[\"input_ids\"]\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        processed_datasets = dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            remove_columns=dataset[\"train\"].column_names,\n",
    "            load_from_cache_file=True,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    train_dataset = processed_datasets[\"train\"]\n",
    "    eval_dataset = processed_datasets[\"train\"]\n",
    "    test_dataset = processed_datasets[\"test\"]\n",
    "\n",
    "    def collate_fn(examples: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Collate function for data loader.\n",
    "\n",
    "        Args:\n",
    "            examples (List[Dict[str, torch.Tensor]]): Batch of examples.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, torch.Tensor]: Collated batch.\n",
    "        \"\"\"\n",
    "        return tokenizer.pad(examples, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, collate_fn=collate_fn, batch_size=batch_size, pin_memory=True\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, collate_fn=collate_fn, batch_size=batch_size, pin_memory=True\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=(len(train_dataloader) * num_epochs),\n",
    "    )\n",
    "\n",
    "    model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler = accelerator.prepare(\n",
    "        model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler\n",
    "    )\n",
    "    accelerator.print(model)\n",
    "\n",
    "    is_ds_zero_3 = False\n",
    "    if getattr(accelerator.state, \"deepspeed_plugin\", None):\n",
    "        is_ds_zero_3 = accelerator.state.deepspeed_plugin.zero_stage == 3\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        with TorchTracemalloc() as tracemalloc:\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.detach().float()\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        accelerator.print(f\"GPU Memory before entering the train : {b2mb(tracemalloc.begin)}\")\n",
    "        accelerator.print(f\"GPU Memory consumed at the end of the train (end-begin): {tracemalloc.used}\")\n",
    "        accelerator.print(f\"GPU Peak Memory consumed during the train (max-begin): {tracemalloc.peaked}\")\n",
    "        accelerator.print(\n",
    "            f\"GPU Total Peak Memory consumed during the train (max): {tracemalloc.peaked + b2mb(tracemalloc.begin)}\"\n",
    "        )\n",
    "\n",
    "        accelerator.print(f\"CPU Memory before entering the train : {b2mb(tracemalloc.cpu_begin)}\")\n",
    "        accelerator.print(f\"CPU Memory consumed at the end of the train (end-begin): {tracemalloc.cpu_used}\")\n",
    "        accelerator.print(f\"CPU Peak Memory consumed during the train (max-begin): {tracemalloc.cpu_peaked}\")\n",
    "        accelerator.print(\n",
    "            f\"CPU Total Peak Memory consumed during the train (max): {tracemalloc.cpu_peaked + b2mb(tracemalloc.cpu_begin)}\"\n",
    "        )\n",
    "\n",
    "        train_epoch_loss = total_loss / len(train_dataloader)\n",
    "        train_ppl = torch.exp(train_epoch_loss)\n",
    "        accelerator.print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=}\")\n",
    "\n",
    "        model.eval()\n",
    "        eval_preds = []\n",
    "        with TorchTracemalloc() as tracemalloc:\n",
    "            for _, batch in enumerate(tqdm(eval_dataloader)):\n",
    "                batch = {k: v for k, v in batch.items() if k != \"labels\"}\n",
    "                with torch.no_grad():\n",
    "                    outputs = accelerator.unwrap_model(model).generate(\n",
    "                        **batch, synced_gpus=is_ds_zero_3\n",
    "                    )\n",
    "                outputs = accelerator.pad_across_processes(outputs, dim=1, pad_index=tokenizer.pad_token_id)\n",
    "                preds = accelerator.gather_for_metrics(outputs).detach().cpu().numpy()\n",
    "                eval_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\n",
    "\n",
    "        accelerator.print(f\"GPU Memory before entering the eval : {b2mb(tracemalloc.begin)}\")\n",
    "        accelerator.print(f\"GPU Memory consumed at the end of the eval (end-begin): {tracemalloc.used}\")\n",
    "        accelerator.print(f\"GPU Peak Memory consumed during the eval (max-begin): {tracemalloc.peaked}\")\n",
    "        accelerator.print(\n",
    "            f\"GPU Total Peak Memory consumed during the eval (max): {tracemalloc.peaked + b2mb(tracemalloc.begin)}\"\n",
    "        )\n",
    "\n",
    "        accelerator.print(f\"CPU Memory before entering the eval : {b2mb(tracemalloc.cpu_begin)}\")\n",
    "        accelerator.print(f\"CPU Memory consumed at the end of the eval (end-begin): {tracemalloc.cpu_used}\")\n",
    "        accelerator.print(f\"CPU Peak Memory consumed during the eval (max-begin): {tracemalloc.cpu_peaked}\")\n",
    "        accelerator.print(\n",
    "            f\"CPU Total Peak Memory consumed during the eval (max): {tracemalloc.cpu_peaked + b2mb(tracemalloc.cpu_begin)}\"\n",
    "        )\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        assert len(eval_preds) == len(\n",
    "            dataset[\"train\"][label_column]\n",
    "        ), f\"{len(eval_preds)} != {len(dataset['train'][label_column])}\"\n",
    "        for pred, true in zip(eval_preds, dataset[\"train\"][label_column]):\n",
    "            if pred.strip() == true.strip():\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        accuracy = correct / total * 100\n",
    "        accelerator.print(f\"{accuracy=}\")\n",
    "        accelerator.print(f\"{eval_preds[:10]=}\")\n",
    "        accelerator.print(f\"{dataset['train'][label_column][:10]=}\")\n",
    "\n",
    "    if do_test:\n",
    "        model.eval()\n",
    "        test_preds = []\n",
    "        for _, batch in enumerate(tqdm(test_dataloader)):\n",
    "            batch = {k: v for k, v in batch.items() if k != \"labels\"}\n",
    "            with torch.no_grad():\n",
    "                outputs = accelerator.unwrap_model(model).generate(\n",
    "                    **batch, synced_gpus=is_ds_zero_3\n",
    "                )\n",
    "            outputs = accelerator.pad_across_processes(outputs, dim=1, pad_index=tokenizer.pad_token_id)\n",
    "            preds = accelerator.gather(outputs).detach().cpu().numpy()\n",
    "            test_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\n",
    "\n",
    "        test_preds_cleaned = []\n",
    "        for _, pred in enumerate(test_preds):\n",
    "            test_preds_cleaned.append(get_closest_label(pred, classes))\n",
    "\n",
    "        test_df = dataset[\"test\"].to_pandas()\n",
    "        assert len(test_preds_cleaned) == len(test_df), f\"{len(test_preds_cleaned)} != {len(test_df)}\"\n",
    "        test_df[label_column] = test_preds_cleaned\n",
    "        test_df[\"text_labels_orig\"] = test_preds\n",
    "        accelerator.print(test_df[[text_column, label_column]].sample(20))\n",
    "\n",
    "        pred_df = test_df[[\"ID\", label_column]]\n",
    "        pred_df.columns = [\"ID\", \"Label\"]\n",
    "\n",
    "        os.makedirs(f\"data/{dataset_name}\", exist_ok=True)\n",
    "        pred_df.to_csv(f\"data/{dataset_name}/preds.csv\", index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "import psutil\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "\n",
    "def levenshtein_distance(str1: str, str2: str) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the Levenshtein distance between two strings.\n",
    "    Time Complexity: O(N^2)\n",
    "    Space Complexity: O(N)\n",
    "    \"\"\"\n",
    "    if str1 == str2:\n",
    "        return 0\n",
    "    num_rows = len(str1) + 1\n",
    "    num_cols = len(str2) + 1\n",
    "    dp_matrix = list(range(num_cols))\n",
    "    for i in range(1, num_rows):\n",
    "        prev = dp_matrix[0]\n",
    "        dp_matrix[0] = i\n",
    "        for j in range(1, num_cols):\n",
    "            temp = dp_matrix[j]\n",
    "            if str1[i - 1] == str2[j - 1]:\n",
    "                dp_matrix[j] = prev\n",
    "            else:\n",
    "                dp_matrix[j] = min(prev, dp_matrix[j], dp_matrix[j - 1]) + 1\n",
    "            prev = temp\n",
    "    return dp_matrix[num_cols - 1]\n",
    "\n",
    "\n",
    "def get_closest_label(eval_pred: str, classes: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Find the closest label to the predicted string based on Levenshtein distance.\n",
    "    \"\"\"\n",
    "    min_id = sys.maxsize\n",
    "    min_edit_distance = sys.maxsize\n",
    "    for i, class_label in enumerate(classes):\n",
    "        edit_distance = levenshtein_distance(eval_pred.strip(), class_label)\n",
    "        if edit_distance < min_edit_distance:\n",
    "            min_id = i\n",
    "            min_edit_distance = edit_distance\n",
    "    return classes[min_id]\n",
    "\n",
    "\n",
    "def b2mb(x: int) -> int:\n",
    "    \"\"\"Convert bytes to megabytes.\"\"\"\n",
    "    return int(x / 2**20)\n",
    "\n",
    "\n",
    "class TorchTracemalloc:\n",
    "    \"\"\"Context manager to track the peak memory usage of the process.\"\"\"\n",
    "\n",
    "    def __enter__(self):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_max_memory_allocated()  # reset the peak gauge to zero\n",
    "        self.begin = torch.cuda.memory_allocated()\n",
    "        self.process = psutil.Process()\n",
    "\n",
    "        self.cpu_begin = self.cpu_mem_used()\n",
    "        self.peak_monitoring = True\n",
    "        peak_monitor_thread = threading.Thread(target=self.peak_monitor_func)\n",
    "        peak_monitor_thread.daemon = True\n",
    "        peak_monitor_thread.start()\n",
    "        return self\n",
    "\n",
    "    def cpu_mem_used(self) -> int:\n",
    "        \"\"\"Get resident set size memory for the current process.\"\"\"\n",
    "        return self.process.memory_info().rss\n",
    "\n",
    "    def peak_monitor_func(self):\n",
    "        self.cpu_peak = -1\n",
    "\n",
    "        while True:\n",
    "            self.cpu_peak = max(self.cpu_mem_used(), self.cpu_peak)\n",
    "\n",
    "            if not self.peak_monitoring:\n",
    "                break\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        self.peak_monitoring = False\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        self.end = torch.cuda.memory_allocated()\n",
    "        self.peak = torch.cuda.max_memory_allocated()\n",
    "        self.used = b2mb(self.end - self.begin)\n",
    "        self.peaked = b2mb(self.peak - self.begin)\n",
    "\n",
    "        self.cpu_end = self.cpu_mem_used()\n",
    "        self.cpu_used = b2mb(self.cpu_end - self.cpu_begin)\n",
    "        self.cpu_peaked = b2mb(self.cpu_peak - self.cpu_begin)\n",
    "\n",
    "\n",
    "def preprocess_function(examples: Dict[str, List[str]]) -> Dict[str, Union[List[int], List[List[int]]]]:\n",
    "    \"\"\"Preprocess the dataset for training.\"\"\"\n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n",
    "    targets = [str(x) for x in examples[label_column]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(targets, add_special_tokens=False)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def test_preprocess_function(examples: Dict[str, List[str]]) -> Dict[str, Union[List[int], List[List[int]]]]:\n",
    "    \"\"\"Preprocess the dataset for testing.\"\"\"\n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def main():\n",
    "    accelerator = Accelerator()\n",
    "    model_name_or_path = \"bigscience/bloomz-7b1\"\n",
    "    dataset_name = \"twitter_complaints\"\n",
    "    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "    text_column = \"Tweet text\"\n",
    "    label_column = \"text_label\"\n",
    "    lr = 3e-3\n",
    "    num_epochs = 20\n",
    "    batch_size = 8\n",
    "    seed = 42\n",
    "    max_length = 64\n",
    "    do_test = False\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset = load_dataset(\"ought/raft\", dataset_name)\n",
    "    classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n",
    "    dataset = dataset.map(\n",
    "    lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        processed_datasets = dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            remove_columns=dataset[\"train\"].column_names,\n",
    "            load_from_cache_file=True,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    train_dataset = processed_datasets[\"train\"]\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "\n",
    "        processed_datasets = dataset.map(\n",
    "        test_preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "         )\n",
    "    eval_dataset = processed_datasets[\"train\"]\n",
    "    test_dataset = processed_datasets[\"test\"]\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
    ")\n",
    "    eval_dataloader = DataLoader(\n",
    "    eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
    ")\n",
    "    test_dataloader = DataLoader(\n",
    "    test_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
    ")\n",
    "\n",
    "    print(next(iter(train_dataloader)))\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")\n",
    "\n",
    "    model, train_dataloader, eval_dataloader, test_dataloader, optimizer,     lr_scheduler = accelerator.prepare(\n",
    "    model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler\n",
    ")\n",
    "    accelerator.print(model)\n",
    "\n",
    "    is_ds_zero_3 = False\n",
    "    if getattr(accelerator.state, \"deepspeed_plugin\", None):\n",
    "\n",
    "        is_ds_zero_3 = accelerator.state.deepspeed_plugin.zero_stage == 3\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        with TorchTracemalloc() as tracemalloc:\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.detach().float()\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "    accelerator.print(f\"GPU Memory before entering the train : {b2mb(tracemalloc.begin)}\")\n",
    "    accelerator.print(f\"GPU Memory consumed at the end of the train (end-begin): {tracemalloc.used}\")\n",
    "    accelerator.print(f\"GPU Peak Memory consumed during the train (max-begin): {tracemalloc.peaked}\")\n",
    "    accelerator.print(\n",
    "        f\"GPU Total Peak Memory consumed during the train (max): {tracemalloc.peaked + b2mb(tracemalloc.begin)}\"\n",
    "    )\n",
    "\n",
    "    accelerator.print(f\"CPU Memory before entering the train : {b2mb(tracemalloc.cpu_begin)}\")\n",
    "    accelerator.print(f\"CPU Memory consumed at the end of the train (end-begin): {tracemalloc.cpu_used}\")\n",
    "    accelerator.print(f\"CPU Peak Memory consumed during the train (max-begin): {tracemalloc.cpu_peaked}\")\n",
    "    accelerator.print(\n",
    "        f\"CPU Total Peak Memory consumed during the train (max): {tracemalloc.cpu_peaked + b2mb(tracemalloc.cpu_begin)}\"\n",
    "    )\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    accelerator.print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=}\")\n",
    "\n",
    "    model.eval()\n",
    "    eval_preds = []\n",
    "    with TorchTracemalloc() as tracemalloc:\n",
    "        for _, batch in enumerate(tqdm(eval_dataloader)):\n",
    "            batch = {k: v for k, v in batch.items() if k != \"labels\"}\n",
    "            with torch.no_grad():\n",
    "                outputs = accelerator.unwrap_model(model).generate(\n",
    "                    **batch, synced_gpus=is_ds_zero_3, max_new_tokens=10\n",
    "                )\n",
    "            outputs = accelerator.pad_across_processes(outputs, dim=1, pad_index=tokenizer.pad_token_id)\n",
    "            preds = accelerator.gather_for_metrics(outputs)\n",
    "            preds = preds[:, max_length:].detach().cpu().numpy()\n",
    "            eval_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\n",
    "\n",
    "    accelerator.print(f\"GPU Memory before entering the eval : {b2mb(tracemalloc.begin)}\")\n",
    "    accelerator.print(f\"GPU Memory consumed at the end of the eval (end-begin): {tracemalloc.used}\")\n",
    "    accelerator.print(f\"GPU Peak Memory consumed during the eval (max-begin): {tracemalloc.peaked}\")\n",
    "    accelerator.print(\n",
    "        f\"GPU Total Peak Memory consumed during the eval (max): {tracemalloc.peaked + b2mb(tracemalloc.begin)}\"\n",
    "    )\n",
    "\n",
    "    accelerator.print(f\"CPU Memory before entering the eval : {b2mb(tracemalloc.cpu_begin)}\")\n",
    "    accelerator.print(f\"CPU Memory consumed at the end of the eval (end-begin): {tracemalloc.cpu_used}\")\n",
    "    accelerator.print(f\"CPU Peak Memory consumed during the eval (max-begin): {tracemalloc.cpu_peaked}\")\n",
    "    accelerator.print(\n",
    "        f\"CPU Total Peak Memory consumed during the eval (max): {tracemalloc.cpu_peaked + b2mb(tracemalloc.cpu_begin)}\"\n",
    "    )\n",
    "\n",
    "    eval_preds = [get_closest_label(x, classes) for x in eval_preds]\n",
    "    accuracy = sum([int(x == y) for x, y in zip(eval_preds, dataset[\"train\"][label_column])]) / len(eval_preds)\n",
    "    accelerator.print(f\"{epoch=}: {accuracy=}\")\n",
    "\n",
    "    if do_test:\n",
    "        test_preds = []\n",
    "        with TorchTracemalloc() as tracemalloc:\n",
    "            for _, batch in enumerate(tqdm(test_dataloader)):\n",
    "                with torch.no_grad():\n",
    "                    outputs = accelerator.unwrap_model(model).generate(\n",
    "                        **batch, synced_gpus=is_ds_zero_3, max_new_tokens=10\n",
    "                    )\n",
    "                outputs = accelerator.pad_across_processes(outputs, dim=1, pad_index=tokenizer.pad_token_id)\n",
    "                preds = accelerator.gather_for_metrics(outputs)\n",
    "                preds = preds[:, max_length:].detach().cpu().numpy()\n",
    "        test_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\n",
    "\n",
    "\n",
    "        accelerator.print(f\"GPU Memory before entering the test : {b2mb(tracemalloc.begin)}\")\n",
    "        accelerator.print(f\"GPU Memory consumed at the end of the test (end-begin): {tracemalloc.used}\")\n",
    "        accelerator.print(f\"GPU Peak Memory consumed during the test (max-begin): {tracemalloc.peaked}\")\n",
    "        accelerator.print(\n",
    "            f\"GPU Total Peak Memory consumed during the test (max): {tracemalloc.peaked + b2mb(tracemalloc.begin)}\"\n",
    "        )\n",
    "\n",
    "        accelerator.print(f\"CPU Memory before entering the test : {b2mb(tracemalloc.cpu_begin)}\")\n",
    "        accelerator.print(f\"CPU Memory consumed at the end of the test (end-begin): {tracemalloc.cpu_used}\")\n",
    "        accelerator.print(f\"CPU Peak Memory consumed during the test (max-begin): {tracemalloc.cpu_peaked}\")\n",
    "        accelerator.print(\n",
    "            f\"CPU Total Peak Memory consumed during the test (max): {tracemalloc.cpu_peaked + b2mb(tracemalloc.cpu_begin)}\"\n",
    "        )\n",
    "\n",
    "        test_preds = [get_closest_label(x, classes) for x in test_preds]\n",
    "        accuracy = sum([int(x == y) for x, y in zip(test_preds, dataset[\"test\"][label_column])]) / len(test_preds)\n",
    "        accelerator.print(f\"{epoch=}: {accuracy=}\")\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(f\"model-{epoch}\", save_function=accelerator.save)\n",
    "if name == \"main\":\n",
    "main()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

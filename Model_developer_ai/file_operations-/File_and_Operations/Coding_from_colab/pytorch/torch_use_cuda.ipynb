{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an advanced tabular form with functionality snippets for the provided list of items:\n",
    "\n",
    "| Function/Class | Description | Example Usage |\n",
    "|---------------|-------------|---------------|\n",
    "| `torch.cuda.is_available()` | Check if CUDA is available on the system | `if torch.cuda.is_available():`<br>&nbsp;&nbsp;&nbsp;&nbsp;`device = torch.device(\"cuda\")` |\n",
    "| `torch.cuda.device_count()` | Get the number of available CUDA devices | `num_devices = torch.cuda.device_count()`<br>`print(f\"Number of CUDA devices: {num_devices}\")` |\n",
    "| `torch.cuda.current_device()` | Get the index of the currently selected CUDA device | `current_device_index = torch.cuda.current_device()`<br>`print(f\"Current device index: {current_device_index}\")` |\n",
    "| `torch.cuda.set_device(device)` | Set the current CUDA device | `torch.cuda.set_device(0)`<br>`# Set device to the first GPU` |\n",
    "| `torch.cuda.device(device)` | Context manager to set the current CUDA device | `with torch.cuda.device(1):`<br>&nbsp;&nbsp;&nbsp;&nbsp;`# Code block executed on the second GPU` |\n",
    "| `torch.cuda.memory_allocated(device=None)` | Get the current GPU memory usage in bytes | `memory_usage = torch.cuda.memory_allocated()`<br>`print(f\"Current memory usage: {memory_usage} bytes\")` |\n",
    "| `torch.cuda.max_memory_allocated(device=None)` | Get the maximum GPU memory usage in bytes | `max_memory = torch.cuda.max_memory_allocated()`<br>`print(f\"Maximum memory usage: {max_memory} bytes\")` |\n",
    "| `torch.cuda.empty_cache()` | Release all unoccupied cached memory | `torch.cuda.empty_cache()`<br>`# Free up cached memory` |\n",
    "| `torch.cuda.synchronize(device=None)` | Synchronize the current CUDA device | `torch.cuda.synchronize()`<br>`# Wait for all CUDA operations to complete` |\n",
    "| `torch.cuda.stream(stream)` | Context manager to set the current CUDA stream | `with torch.cuda.stream(torch.cuda.Stream()):`<br>&nbsp;&nbsp;&nbsp;&nbsp;`# Code block executed on a new stream` |\n",
    "| `torch.cuda.Event(enable_timing=False, blocking=False, interprocess=False)` | Create a CUDA event object for timing and synchronization | `start_event = torch.cuda.Event(enable_timing=True)`<br>`end_event = torch.cuda.Event(enable_timing=True)`<br>`start_event.record()`<br>`# Code block to be timed`<br>`end_event.record()`<br>`end_event.synchronize()`<br>`elapsed_time = start_event.elapsed_time(end_event)`<br>`print(f\"Elapsed time: {elapsed_time} milliseconds\")` |\n",
    "\n",
    "These are just a few examples of the functionality provided by the PyTorch CUDA module. The table showcases some commonly used functions and classes along with brief descriptions and example usage snippets.\n",
    "\n",
    "Note that the examples assume you have imported the necessary modules, such as `import torch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apologies for the confusion. Here's an updated advanced tabular form with more functionality snippets for the provided list of items:\n",
    "\n",
    "| Function/Class | Description | Example Usage |\n",
    "|---------------|-------------|---------------|\n",
    "| `torch.cuda.amp` | Automatic Mixed Precision (AMP) package for improving performance and reducing memory usage | `from torch.cuda.amp import autocast`<br>`with autocast():`<br>&nbsp;&nbsp;&nbsp;&nbsp;`# Code block executed with mixed precision` |\n",
    "| `torch.cuda.profiler` | CUDA profiler for analyzing performance and identifying bottlenecks | `torch.cuda.profiler.start()`<br>`# Code block to be profiled`<br>`torch.cuda.profiler.stop()`<br>`print(torch.cuda.profiler.output_to_file(\"profile.txt\"))` |\n",
    "| `torch.cuda.nvtx` | NVTX (NVIDIA Tools Extension) for adding custom annotations to profiling timelines | `torch.cuda.nvtx.range_push(\"Layer 1\")`<br>`# Code block for Layer 1`<br>`torch.cuda.nvtx.range_pop()`<br>`torch.cuda.nvtx.range_push(\"Layer 2\")`<br>`# Code block for Layer 2`<br>`torch.cuda.nvtx.range_pop()` |\n",
    "| `torch.cuda.random` | Random number generation on CUDA devices | `torch.cuda.manual_seed(42)`<br>`# Set random seed for CUDA`<br>`torch.cuda.manual_seed_all(42)`<br>`# Set random seed for all CUDA devices` |\n",
    "| `torch.cuda.sparse` | Sparse tensor operations on CUDA devices | `indices = torch.tensor([[0, 1, 1],`<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`[2, 0, 2]]).cuda()`<br>`values = torch.tensor([3, 4, 5]).cuda()`<br>`size = (3, 3)`<br>`sparse_tensor = torch.cuda.sparse.FloatTensor(`<br>&nbsp;&nbsp;&nbsp;&nbsp;`indices, values, size)` |\n",
    "| `torch.cuda.get_device_properties(device)` | Get the properties of a CUDA device | `properties = torch.cuda.get_device_properties(0)`<br>`print(f\"Device name: {properties.name}\")`<br>`print(f\"Total memory: {properties.total_memory}\")` |\n",
    "| `torch.cuda.memory_summary(device=None, abbreviated=False)` | Print a summary of GPU memory usage | `torch.cuda.memory_summary()`<br>`# Print detailed memory summary` |\n",
    "| `torch.cuda.memory_snapshot()` | Capture a snapshot of the CUDA memory allocator state | `snapshot = torch.cuda.memory_snapshot()`<br>`print(snapshot)` |\n",
    "| `torch.cuda.memory_stats(device=None)` | Get the memory statistics for a CUDA device | `stats = torch.cuda.memory_stats()`<br>`print(f\"Allocated memory: {stats['allocated_bytes.all.current']}\")`<br>`print(f\"Reserved memory: {stats['reserved_bytes.all.current']}\")` |\n",
    "| `torch.cuda.nccl` | NCCL (NVIDIA Collective Communications Library) for multi-GPU communication | `torch.cuda.nccl.init()`<br>`# Initialize NCCL`<br>`torch.cuda.nccl.broadcast(tensor, src=0)`<br>`# Broadcast tensor from device 0 to all devices` |\n",
    "\n",
    "These additional snippets cover a wider range of functionality provided by the PyTorch CUDA module, including automatic mixed precision, profiling, custom annotations, random number generation, sparse tensors, device properties, memory management, and multi-GPU communication using NCCL.\n",
    "\n",
    "Remember to import the necessary modules and adapt the code snippets to your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "\n",
    "| Function/Class | Description | Example Usage |\n",
    "|---------------|-------------|---------------|\n",
    "| `torch.cpu.is_available()` | Check if CPU is available for PyTorch | `if torch.cpu.is_available():`<br>&nbsp;&nbsp;&nbsp;&nbsp;`device = torch.device(\"cpu\")` |\n",
    "| `torch.cpu.device_count()` | Get the number of CPU devices | `num_devices = torch.cpu.device_count()`<br>`print(f\"Number of CPU devices: {num_devices}\")` |\n",
    "| `torch.cpu.current_stream(device=None)` | Get the current stream for a CPU device | `stream = torch.cpu.current_stream()`<br>`print(f\"Current stream: {stream}\")` |\n",
    "| `torch.cpu.synchronize(device=None)` | Synchronize the current CPU device | `torch.cpu.synchronize()`<br>`# Wait for all CPU operations to complete` |\n",
    "| `torch.cpu.stream(stream)` | Context manager to set the current CPU stream | `with torch.cpu.stream(torch.Stream()):`<br>&nbsp;&nbsp;&nbsp;&nbsp;`# Code block executed on a new stream` |\n",
    "| `torch.cpu.amp` | Automatic Mixed Precision (AMP) package for CPU | `from torch.cpu.amp import autocast`<br>`with autocast():`<br>&nbsp;&nbsp;&nbsp;&nbsp;`# Code block executed with mixed precision` |\n",
    "| `torch.cpu.StreamContext` | Context manager for a CPU stream | `with torch.cpu.StreamContext():`<br>&nbsp;&nbsp;&nbsp;&nbsp;`# Code block executed within a CPU stream context` |\n",
    "\n",
    "These snippets demonstrate the functionality provided by the PyTorch CPU module, including device availability checks, device count, stream management, synchronization, and automatic mixed precision.\n",
    "\n",
    "Note that some of the functions and classes mentioned in the provided list, such as `AbstractContextManager`, `Any`, `Optional`, and `Union`, are not specific to the PyTorch CPU module but rather part of Python's typing and contextlib modules.\n",
    "\n",
    "Remember to import the necessary modules, such as `import torch`, before using these snippets in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

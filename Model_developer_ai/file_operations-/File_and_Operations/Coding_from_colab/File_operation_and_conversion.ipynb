{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from typing import Dict, List ,Optional\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import yaml\n",
    "import re\n",
    "from pptx import Presentation\n",
    "from io import StringIO\n",
    "from datasets import load_dataset\n",
    "EXTENSION_READERS = {\n",
    "    '.md': lambda f: f.read(),\n",
    "    '.py': lambda f: f.read(),\n",
    "    '.csv': lambda f: pd.read_csv(f),\n",
    "    '.json': lambda f: json.load(f),\n",
    "    '.yaml': lambda f: yaml.safe_load(f),\n",
    "    '.txt': lambda f: f.read(),\n",
    "    '.xml': lambda f: f.read(),\n",
    "    '.html': lambda f: f.read(),\n",
    "    '.css': lambda f: f.read(),\n",
    "    '.js': lambda f: f.read(),\n",
    "    '.java': lambda f: f.read(),\n",
    "    '.cpp': lambda f: f.read(),\n",
    "    '.h': lambda f: f.read(),\n",
    "    '.php': lambda f: f.read(),\n",
    "    '.rb': lambda f: f.read(),\n",
    "    '.sql': lambda f: f.read(),\n",
    "    '.xls': lambda f: pd.read_excel(f),\n",
    "    '.xlsx': lambda f: pd.read_excel(f),\n",
    "    '.ppt': lambda f: read_pptx(f),\n",
    "    '.pptx': lambda f: read_pptx(f)\n",
    "}\n",
    "\n",
    "def read_pptx(file):\n",
    "    \"\"\"Custom function to read .pptx files with python-pptx\"\"\"\n",
    "    prs = Presentation(file)\n",
    "    text = []\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                text.append(shape.text)\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# Utilize regular expressions to match any of the file extensions\n",
    "EXTENSION_PATTERN = r\".*\\.(md|py|csv|json|yaml|txt|xml|html|css|js|java|cpp|h|php|rb|sql|xls|xlsx|ppt|pptx)$\"\n",
    "\n",
    "\n",
    "def get_files_with_extensions(dir_path: str) -> Dict[str, List[str]]:\n",
    "    ext_files = defaultdict(list)\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file).replace(\"\\\\\", \"/\")\n",
    "            _, ext = os.path.splitext(file)\n",
    "            ext_files[ext].append(file_path)\n",
    "    return ext_files\n",
    "\n",
    "\n",
    "def write_to_csv(file_path: str, ext_files: Dict[str, List[str]]) -> None:\n",
    "    with open(file_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        max_len = max(len(v) for v in ext_files.values())\n",
    "        writer.writerow(ext_files.keys())\n",
    "        for i in range(max_len):\n",
    "            row = [ext_files[k][i] if i < len(ext_files[k]) else '' for k in ext_files.keys()]\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the extracted text from the PDF.\n",
    "    This function can be customized based on the cleaning requirements.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The text extracted from the PDF.\n",
    "\n",
    "    Returns:\n",
    "    - str: The cleaned text.\n",
    "    \"\"\"\n",
    "    cleaned_text = ' '.join(text.split())  # Removing extra whitespaces\n",
    "    # Add more cleaning rules as needed.\n",
    "    return cleaned_text\n",
    "\n",
    "def split_and_save_text(cleaned_text: str, base_output_path: Path, max_size_bytes: int = 50 * 1024 * 1024) -> None:\n",
    "    \"\"\"\n",
    "    Split the cleaned text into multiple files, each smaller than the specified max size, and save them.\n",
    "\n",
    "    Parameters:\n",
    "    - cleaned_text (str): The cleaned text to be split and saved.\n",
    "    - base_output_path (Path): The base path where the text files will be saved.\n",
    "    - max_size_bytes (int): Maximum size of the text file in bytes.\n",
    "    \"\"\"\n",
    "    part_num = 1\n",
    "    text_part = \"\"\n",
    "    for line in cleaned_text.split('\\n'):\n",
    "        if len(text_part.encode('utf-8')) + len(line.encode('utf-8')) < max_size_bytes:\n",
    "            text_part += line + '\\n'\n",
    "        else:\n",
    "            # Save the current part and start a new one\n",
    "            output_path = base_output_path.with_suffix(f'.part{part_num}.txt')\n",
    "            with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(text_part)\n",
    "            part_num += 1\n",
    "            text_part = line + '\\n' # Start new part with the current line\n",
    "    \n",
    "    # Save the last part\n",
    "    if text_part:\n",
    "        output_path = base_output_path.with_suffix(f'.part{part_num}.txt')\n",
    "        with open(output_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(text_part)\n",
    "\n",
    "def convert_pdf_to_text(pdf_path: str, output_folder: str) -> None:\n",
    "    \"\"\"\n",
    "    Convert a PDF file to text files, splitting contents to ensure each resulting file is less than 50 MB.\n",
    "    \n",
    "    Parameters:\n",
    "    - pdf_path (str): Path to the PDF file.\n",
    "    - output_folder (str): Path to the folder where the text files will be saved.\n",
    "    \"\"\"\n",
    "    # Ensure the output folder exists, create it if it does not\n",
    "    output_folder_path = Path(output_folder)\n",
    "    output_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        doc.close()\n",
    "        \n",
    "        cleaned_text = clean_text(text)\n",
    "        \n",
    "        base_output_path = output_folder_path / Path(pdf_path).stem\n",
    "        split_and_save_text(cleaned_text, base_output_path)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while converting {pdf_path}: {str(e)}\")\n",
    "\n",
    "def process_pdfs_from_csv(csv_path: str, output_folder: str) -> None:\n",
    "    \"\"\"\n",
    "    Process PDFs listed in a CSV file, converting them to text files and ensuring each part is less than 50 MB.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_path (str): Path to the CSV file containing paths to PDF files.\n",
    "    - output_folder (str): Path to the folder where text files will be stored.\n",
    "    \"\"\"\n",
    "    pdf_paths = pd.read_csv(csv_path, encoding='latin1')\n",
    "    # pdf_paths = pd.read_csv(csv_path)\n",
    "    for pdf_path in pdf_paths['.pdf']:\n",
    "        convert_pdf_to_text(pdf_path, output_folder)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def list_files_with_extensions(directory_path):\n",
    "    try:\n",
    "        files = os.listdir(directory_path)\n",
    "        return [file for file in files if re.match(EXTENSION_PATTERN, file)]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The directory {directory_path} was not found.\")\n",
    "        return None\n",
    "\n",
    "def read_file_content(directory_path, filename):\n",
    "    try:\n",
    "        extension = os.path.splitext(filename)[1]\n",
    "        with open(os.path.join(directory_path, filename), 'r') as file:\n",
    "            file_reader = EXTENSION_READERS.get(extension)\n",
    "            return file_reader(file) if file_reader else None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file {filename}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def process_files_txtfile(directory_path: str, user_folder: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    This function processes all files in a given directory and writes their content to a user-specific text file.\n",
    "    Each text file is ensured to be less than 50 MB in size.\n",
    "\n",
    "    Args:\n",
    "    directory_path (str): The path to the directory containing the files to be processed.\n",
    "    user_folder (str): The name of the user-specific folder where the text files will be written.\n",
    "\n",
    "    Returns:\n",
    "    str: The path to the user-specific folder, or None if an error occurred.\n",
    "    \"\"\"\n",
    "    # Create the user-specific folder if it doesn't exist\n",
    "    user_folder_path = os.path.join(directory_path, user_folder)\n",
    "    os.makedirs(user_folder_path, exist_ok=True)\n",
    "\n",
    "    # Get a list of all files in the directory\n",
    "    files = list_files_with_extensions(directory_path)\n",
    "\n",
    "    if files is None:\n",
    "        return\n",
    "\n",
    "    # Initialize the text file counter and size\n",
    "    txt_file_counter = 1\n",
    "    txt_file_size = 0\n",
    "\n",
    "    for filename in files:\n",
    "        content = read_file_content(directory_path, filename)\n",
    "        if content is not None:\n",
    "            # Create a new text file if the size is over 50 MB\n",
    "            if txt_file_size >= 50 * 1024 * 1024:\n",
    "                txt_file_counter += 1\n",
    "                txt_file_size = 0\n",
    "\n",
    "            # Open the text file in append mode\n",
    "            txt_file_path = os.path.join(user_folder_path, f\"{user_folder}_{txt_file_counter}.txt\")\n",
    "            with open(txt_file_path, \"a\") as f:\n",
    "                if isinstance(content, pd.DataFrame):\n",
    "                    # Convert DataFrame to CSV string without index and write to file\n",
    "                    content_csv = content.to_csv(index=False)\n",
    "                    f.write(content_csv)\n",
    "                    txt_file_size += len(content_csv)\n",
    "                elif isinstance(content, dict):\n",
    "                    # Convert dict to pretty-printed string using json.dumps and write to file\n",
    "                    content_str = json.dumps(content, indent=4)\n",
    "                    f.write(content_str)\n",
    "                    txt_file_size += len(content_str)\n",
    "                else:\n",
    "                    # If it's not a DataFrame or dict, convert it to a string\n",
    "                    content_str = str(content)\n",
    "                    f.write(content_str)\n",
    "                    txt_file_size += len(content_str)\n",
    "\n",
    "            print(f\"--- File: {filename} ---\")\n",
    "            print(f\"Content written to {txt_file_path}\")\n",
    "            print(\"-------------------------------\\n\")\n",
    "\n",
    "    return user_folder_path\n",
    "def loading_folder_using_datasets(folder_path:str):\n",
    "    \n",
    "    \n",
    "    dataset = load_dataset('text', data_files=folder_path+'/*.txt')\n",
    "    return dataset\n",
    "    \n",
    "##=========================||    Extraction  OF DATA         ||==================================\n",
    "dir_path = \"E:/LLMS/hemanth/\"  # replace with your directory path\n",
    "csv_file_path = 'csvfile.csv'  # replace with your CSV file path\n",
    "ext_files = get_files_with_extensions(dir_path)\n",
    "write_to_csv(csv_file_path, ext_files)\n",
    "process_pdfs_from_csv(csv_path='csvfile.csv', output_folder='E:/LLMS/hemanth/output')\n",
    "process_files_txtfile(dir_path,  \"E:/LLMS/hemanth/output\")\n",
    "dataset=loading_folder_using_datasets(folder_path='E:/LLMS/hemanth/output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting gradio\n",
      "  Downloading gradio-4.21.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (23.2.1)\n",
      "Collecting altair<6.0,>=4.2.0 (from gradio)\n",
      "  Using cached altair-5.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: fastapi in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (0.96.0)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting gradio-client==0.12.0 (from gradio)\n",
      "  Downloading gradio_client-0.12.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (0.25.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (0.19.4)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: matplotlib~=3.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (3.8.0)\n",
      "Requirement already satisfied: numpy~=1.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (1.24.3)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.9.15-cp311-none-win_amd64.whl.metadata (50 kB)\n",
      "     ---------------------------------------- 0.0/50.7 kB ? eta -:--:--\n",
      "     -------------------------------- ------- 41.0/50.7 kB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 50.7/50.7 kB 860.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (1.5.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (10.0.0)\n",
      "Collecting pydantic>=2.0 (from gradio)\n",
      "  Downloading pydantic-2.6.4-py3-none-any.whl.metadata (85 kB)\n",
      "     ---------------------------------------- 0.0/85.1 kB ? eta -:--:--\n",
      "     -------------------------------------- - 81.9/85.1 kB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 85.1/85.1 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pydub in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Collecting python-multipart>=0.0.9 (from gradio)\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading ruff-0.3.2-py3-none-win_amd64.whl.metadata (23 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio)\n",
      "  Using cached tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.9 (from typer[all]<1.0,>=0.9->gradio)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (0.23.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio-client==0.12.0->gradio) (2023.6.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio-client==0.12.0->gradio) (10.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (4.20.0)\n",
      "Collecting toolz (from altair<6.0,>=4.2.0->gradio)\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.24.1->gradio) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.24.1->gradio) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (3.12.2)\n",
      "Requirement already satisfied: requests in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib~=3.0->gradio) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=2.0->gradio)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic>=2.0->gradio)\n",
      "  Downloading pydantic_core-2.16.3-cp311-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio) (7.1.2)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (10.16.2)\n",
      "INFO: pip is looking at multiple versions of fastapi to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fastapi (from gradio)\n",
      "  Downloading fastapi-0.110.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting starlette<0.37.0,>=0.36.3 (from fastapi->gradio)\n",
      "  Downloading starlette-0.36.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.32.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.16.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\heman\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->gradio) (1.26.16)\n",
      "Downloading gradio-4.21.0-py3-none-any.whl (17.0 MB)\n",
      "   ---------------------------------------- 0.0/17.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/17.0 MB 4.2 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.4/17.0 MB 5.5 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.6/17.0 MB 4.6 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.0/17.0 MB 5.5 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 1.3/17.0 MB 5.8 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.6/17.0 MB 6.2 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 2.0/17.0 MB 6.2 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 2.4/17.0 MB 6.5 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.7/17.0 MB 6.7 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 3.1/17.0 MB 6.7 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.4/17.0 MB 6.8 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.7/17.0 MB 6.8 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.9/17.0 MB 6.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 4.3/17.0 MB 6.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 4.5/17.0 MB 6.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 4.6/17.0 MB 6.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.7/17.0 MB 6.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.8/17.0 MB 5.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 5.1/17.0 MB 5.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 5.3/17.0 MB 5.8 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 5.6/17.0 MB 5.9 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.7/17.0 MB 5.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 6.4/17.0 MB 6.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 6.4/17.0 MB 6.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 7.3/17.0 MB 6.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 7.4/17.0 MB 6.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 7.5/17.0 MB 6.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 8.2/17.0 MB 6.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 8.3/17.0 MB 6.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 8.3/17.0 MB 6.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.9/17.0 MB 6.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 9.1/17.0 MB 6.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 9.2/17.0 MB 6.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 9.5/17.0 MB 6.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 9.6/17.0 MB 6.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 9.7/17.0 MB 5.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 9.7/17.0 MB 5.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 10.0/17.0 MB 5.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 10.0/17.0 MB 5.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 10.2/17.0 MB 5.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 10.3/17.0 MB 5.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 10.3/17.0 MB 5.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 10.4/17.0 MB 5.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 10.5/17.0 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 10.5/17.0 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 10.6/17.0 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.7/17.0 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.8/17.0 MB 4.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.8/17.0 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.9/17.0 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 11.0/17.0 MB 4.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 11.1/17.0 MB 4.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 11.2/17.0 MB 4.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 11.3/17.0 MB 4.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 11.3/17.0 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 11.4/17.0 MB 4.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.5/17.0 MB 4.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.6/17.0 MB 4.2 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.7/17.0 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.8/17.0 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.9/17.0 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 12.0/17.0 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 12.1/17.0 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 12.2/17.0 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 12.3/17.0 MB 3.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.4/17.0 MB 3.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.5/17.0 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.6/17.0 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.6/17.0 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.7/17.0 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 12.9/17.0 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 13.0/17.0 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 13.1/17.0 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 13.2/17.0 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 13.3/17.0 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 13.4/17.0 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 13.5/17.0 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 13.6/17.0 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 13.7/17.0 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 13.8/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.9/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 14.0/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 14.1/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 14.3/17.0 MB 3.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 14.4/17.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 14.5/17.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 14.6/17.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 14.7/17.0 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 14.9/17.0 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 15.0/17.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 15.1/17.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 15.2/17.0 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 15.3/17.0 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 15.4/17.0 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 15.6/17.0 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 15.7/17.0 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.8/17.0 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.9/17.0 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 16.1/17.0 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 16.2/17.0 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 16.3/17.0 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 16.5/17.0 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.6/17.0 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.7/17.0 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.9/17.0 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  17.0/17.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  17.0/17.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 17.0/17.0 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading gradio_client-0.12.0-py3-none-any.whl (310 kB)\n",
      "   ---------------------------------------- 0.0/310.7 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 143.4/310.7 kB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 286.7/310.7 kB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 310.7/310.7 kB 2.4 MB/s eta 0:00:00\n",
      "Using cached tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Using cached altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "Downloading orjson-3.9.15-cp311-none-win_amd64.whl (136 kB)\n",
      "   ---------------------------------------- 0.0/136.0 kB ? eta -:--:--\n",
      "   ---------------------------------------  133.1/136.0 kB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 136.0/136.0 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.6.4-py3-none-any.whl (394 kB)\n",
      "   ---------------------------------------- 0.0/394.9 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 122.9/394.9 kB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 286.7/394.9 kB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  389.1/394.9 kB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 394.9/394.9 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.16.3-cp311-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/1.9 MB 4.6 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.3/1.9 MB 3.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.5/1.9 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.6/1.9 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.9/1.9 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.1/1.9 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.2/1.9 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.4/1.9 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.5/1.9 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.7/1.9 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.8/1.9 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 3.2 MB/s eta 0:00:00\n",
      "Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Downloading ruff-0.3.2-py3-none-win_amd64.whl (7.6 MB)\n",
      "   ---------------------------------------- 0.0/7.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/7.6 MB 5.1 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.3/7.6 MB 3.9 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.5/7.6 MB 3.6 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.6/7.6 MB 3.5 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.8/7.6 MB 3.4 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.9/7.6 MB 3.3 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.0/7.6 MB 3.2 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.2/7.6 MB 3.2 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.3/7.6 MB 3.1 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.4/7.6 MB 3.0 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.4/7.6 MB 3.1 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.4/7.6 MB 3.1 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.4/7.6 MB 2.5 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.4/7.6 MB 2.5 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.5/7.6 MB 2.2 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.5/7.6 MB 2.2 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.5/7.6 MB 2.2 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.5/7.6 MB 1.9 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.5/7.6 MB 1.9 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.7/7.6 MB 1.8 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.7/7.6 MB 1.8 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.8/7.6 MB 1.7 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.8/7.6 MB 1.7 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.9/7.6 MB 1.7 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.9/7.6 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 1.9/7.6 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.0/7.6 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.0/7.6 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.0/7.6 MB 1.5 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.0/7.6 MB 1.5 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.1/7.6 MB 1.5 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.1/7.6 MB 1.5 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.2/7.6 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.2/7.6 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.2/7.6 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.3/7.6 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.3/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.3/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.4/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.4/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 2.5/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 2.6/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 2.6/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 2.7/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 2.7/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 2.8/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 2.8/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 2.9/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 3.0/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 3.0/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 3.1/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 3.2/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 3.2/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 3.3/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 3.3/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 3.4/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 3.5/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 3.5/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 3.6/7.6 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 3.7/7.6 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.7/7.6 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.8/7.6 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 3.9/7.6 MB 1.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.0/7.6 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.0/7.6 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.1/7.6 MB 1.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 4.2/7.6 MB 1.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 4.3/7.6 MB 1.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 4.4/7.6 MB 1.4 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 4.5/7.6 MB 1.4 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 4.5/7.6 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 4.6/7.6 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 4.7/7.6 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 4.8/7.6 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 4.9/7.6 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.0/7.6 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.1/7.6 MB 1.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.2/7.6 MB 1.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.3/7.6 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.3/7.6 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.4/7.6 MB 1.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.5/7.6 MB 1.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.6/7.6 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.7/7.6 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.8/7.6 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 5.9/7.6 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 6.0/7.6 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 6.1/7.6 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.2/7.6 MB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.3/7.6 MB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.5/7.6 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.6/7.6 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.6/7.6 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.8/7.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.9/7.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.0/7.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.1/7.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.2/7.6 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.3/7.6 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.4/7.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.5/7.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.6/7.6 MB 1.6 MB/s eta 0:00:00\n",
      "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
      "   ---------------------------------------- 0.0/92.1 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 81.9/92.1 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 92.1/92.1 kB 1.7 MB/s eta 0:00:00\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
      "   ---------------------------------------- 0.0/71.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 71.5/71.5 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "   ---------------------------------------- 0.0/56.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 56.1/56.1 kB 1.5 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py): started\n",
      "  Building wheel for ffmpy (setup.py): finished with status 'done'\n",
      "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5605 sha256=f85fc3da4579cc1af534736796c56b56b79efaa91b8551ddaa979144c21a09cc\n",
      "  Stored in directory: c:\\users\\heman\\appdata\\local\\pip\\cache\\wheels\\55\\3c\\f2\\f6e34046bac0d57c13c7d08123b85872423b89c8f59bafda51\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: ffmpy, typer, toolz, tomlkit, semantic-version, ruff, python-multipart, pydantic-core, orjson, annotated-types, starlette, pydantic, gradio-client, fastapi, altair, gradio\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.3.2\n",
      "    Uninstalling typer-0.3.2:\n",
      "      Successfully uninstalled typer-0.3.2\n",
      "  Attempting uninstall: ruff\n",
      "    Found existing installation: ruff 0.1.15\n",
      "    Uninstalling ruff-0.1.15:\n",
      "      Successfully uninstalled ruff-0.1.15\n",
      "  Attempting uninstall: python-multipart\n",
      "    Found existing installation: python-multipart 0.0.6\n",
      "    Uninstalling python-multipart-0.0.6:\n",
      "      Successfully uninstalled python-multipart-0.0.6\n",
      "  Attempting uninstall: starlette\n",
      "    Found existing installation: starlette 0.27.0\n",
      "    Uninstalling starlette-0.27.0:\n",
      "      Successfully uninstalled starlette-0.27.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.12\n",
      "    Uninstalling pydantic-1.10.12:\n",
      "      Successfully uninstalled pydantic-1.10.12\n",
      "  Attempting uninstall: fastapi\n",
      "    Found existing installation: fastapi 0.96.0\n",
      "    Uninstalling fastapi-0.96.0:\n",
      "      Successfully uninstalled fastapi-0.96.0\n",
      "Successfully installed altair-5.2.0 annotated-types-0.6.0 fastapi-0.110.0 ffmpy-0.3.2 gradio-4.21.0 gradio-client-0.12.0 orjson-3.9.15 pydantic-2.6.4 pydantic-core-2.16.3 python-multipart-0.0.9 ruff-0.3.2 semantic-version-2.10.0 starlette-0.36.3 tomlkit-0.12.0 toolz-0.12.1 typer-0.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\heman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~ydantic'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 0.4.6 requires fastapi<0.100.0,>=0.95.2, but you have fastapi 0.110.0 which is incompatible.\n",
      "chromadb 0.4.6 requires pydantic<2.0,>=1.9, but you have pydantic 2.6.4 which is incompatible.\n",
      "privategpt 0.0.26 requires fastapi==0.96.0, but you have fastapi 0.110.0 which is incompatible.\n",
      "privategpt 0.0.26 requires urllib3>=2.0.2, but you have urllib3 1.26.16 which is incompatible.\n",
      "promptinject 0.1.0 requires openai<0.26.0,>=0.25.0, but you have openai 1.11.0 which is incompatible.\n",
      "ragatouille 0.0.6b4 requires ruff<0.2.0,>=0.1.9, but you have ruff 0.3.2 which is incompatible.\n",
      "spacy-transformers 1.2.5 requires transformers<4.31.0,>=3.4.0, but you have transformers 4.37.2 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RootModel' from 'pydantic' (c:\\Users\\heman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic\\__init__.cp311-win_amd64.pyd)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[0;32m      5\u001b[0m demo \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mInterface(fn\u001b[38;5;241m=\u001b[39mget_answer, inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m demo\u001b[38;5;241m.\u001b[39mlaunch()  \n",
      "File \u001b[1;32mc:\\Users\\heman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_simple_templates\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessing_utils\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\heman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\_simple_templates\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimpledropdown\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleDropdown\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimpleimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleImage\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimpletextbox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleTextbox\n",
      "File \u001b[1;32mc:\\Users\\heman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\_simple_templates\\simpledropdown.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FormComponent\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Events\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSimpleDropdown\u001b[39;00m(FormComponent):\n",
      "File \u001b[1;32mc:\\Users\\heman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\components\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mannotated_image\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnnotatedImage\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbar_plot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BarPlot\n",
      "File \u001b[1;32mc:\\Users\\heman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\components\\annotated_image.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m file\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio_client\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocumentation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m document\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m processing_utils, utils\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Component\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_classes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileData, GradioModel\n",
      "File \u001b[1;32mc:\\Users\\heman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\processing_utils.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils \u001b[38;5;28;01mas\u001b[39;00m client_utils\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageOps, PngImagePlugin\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, wasm_utils\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_classes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileData, GradioModel, GradioRootModel\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m abspath, get_upload_folder, is_in_or_equal\n",
      "File \u001b[1;32mc:\\Users\\heman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\utils.py:48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Context\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_classes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileData\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstrings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m en\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:  \u001b[38;5;66;03m# Only import for type checking (is False at runtime).\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\heman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\data_classes.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wasm_utils\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wasm_utils\u001b[38;5;241m.\u001b[39mIS_WASM \u001b[38;5;129;01mor\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, RootModel, ValidationError\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# XXX: Currently Pyodide V2 is not available on Pyodide,\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# so we install V1 for the Wasm version.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Generic, TypeVar\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'RootModel' from 'pydantic' (c:\\Users\\heman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic\\__init__.cp311-win_amd64.pyd)"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "\n",
    "demo = gr.Interface(fn=get_answer, inputs=\"text\", outputs=\"text\")\n",
    "demo.launch()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folderpath='E:/LLMS/hemanth/Hemanth/file_operations-/File_and_Operations/Coding_from_colab'\n",
    "# Loading all or specific extension like .pdf .py .csv .json .txt .md\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "print(\"============================* all files  *==============================\")\n",
    "ALL_DOC = DirectoryLoader(folderpath, glob= \"**/[!.]*\",show_progress=True, use_multithreading=True,silent_errors=True) #,txt ,.py, .csv ,.pdf,.md,.csv,.json \"**/[!.]*\"\n",
    "Documents = ALL_DOC.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folderpath='E:/LLMS/hemanth/Hemanth/file_operations-/File_and_Operations/Coding_from_colab'\n",
    "# Loading all or specific extension like .pdf .py .csv .json .txt .md\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "print(\"============================* all files  *==============================\")\n",
    "ALL_DOC = DirectoryLoader(folderpath, glob= \"**/[!.]*\",show_progress=True, use_multithreading=True,silent_errors=True) #,txt ,.py, .csv ,.pdf,.md,.csv,.json \"**/[!.]*\"\n",
    "Documents = ALL_DOC.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "   print(Documents[i].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Any, List,Union,Optional\n",
    "from datasets import (load_dataset, \n",
    "                      DatasetDict,\n",
    "                      concatenate_datasets\n",
    "                      )\n",
    "\n",
    "\n",
    "#Load the datset\n",
    "def load_and_prepare_dataset(\n",
    "    input_source: Union[str, Path, Dict[str, List[Union[str, Path]]]],\n",
    "    split_ratios: tuple = (0.8, 0.1, 0.1),\n",
    "    seed: int = 42,\n",
    "    streaming: bool = False\n",
    ") -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Load a dataset from various input sources and prepare it by splitting into train, test, and eval sets.\n",
    "\n",
    "    :param input_source: A dataset name, path to a folder, a single file, multiple files, or a dictionary specifying train, test, and eval files.\n",
    "    :param split_ratios: A tuple containing the ratios for train, test, and eval splits (default is (0.8, 0.1, 0.1)).\n",
    "    :param seed: A random seed for reproducibility of the split (default is 42).\n",
    "    :param streaming: Whether to use streaming to handle large files (default is False).\n",
    "    :return: A DatasetDict containing the split datasets.\n",
    "    \n",
    "    Example:\n",
    "    # Example usage with streaming for large files:\n",
    "    # dataset_dict = load_and_prepare_dataset({\n",
    "    #     'train': ['train_file_1.csv', 'train_file_2.csv'],\n",
    "    #     'test': ['test_file.csv'],\n",
    "    #     'eval': ['eval_file.csv']\n",
    "    # }, streaming=True)\n",
    "    # print(dataset_dict)\n",
    "    OUTPUT1:\n",
    "    DatasetDict({\n",
    "    train: DatasetDict({\n",
    "        train: Dataset({\n",
    "            features: ['act', 'prompt'],\n",
    "            num_rows: 459\n",
    "        })\n",
    "    })\n",
    "    test: DatasetDict({\n",
    "        train: Dataset({\n",
    "            features: ['act', 'prompt'],\n",
    "            num_rows: 459\n",
    "        })\n",
    "    })\n",
    "    eval: DatasetDict({\n",
    "        train: Dataset({\n",
    "            features: ['act', 'prompt'],\n",
    "            num_rows: 153\n",
    "        })\n",
    "    })\n",
    "    })\n",
    "    EXAMPLE2:\n",
    "    dataset=load_and_prepare_dataset('fka/awesome-chatgpt-prompts')\n",
    "    DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['act', 'prompt'],\n",
    "        num_rows: 122\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['act', 'prompt'],\n",
    "        num_rows: 15\n",
    "    })\n",
    "    eval: Dataset({\n",
    "        features: ['act', 'prompt'],\n",
    "        num_rows: 16\n",
    "    })\n",
    "    })\n",
    "    EXAMPLE3:\n",
    "    datset_path=load_and_prepare_dataset('/content/awesome-chatgpt-prompts')\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['act', 'prompt'],\n",
    "        num_rows: 122\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['act', 'prompt'],\n",
    "        num_rows: 15\n",
    "    })\n",
    "    eval: Dataset({\n",
    "        features: ['act', 'prompt'],\n",
    "        num_rows: 16\n",
    "    })\n",
    "    })\n",
    "\n",
    "    \"\"\"\n",
    "    # Load dataset from different types of input sources\n",
    "    if isinstance(input_source, (str, Path)):\n",
    "        # Dataset name, single file or path to folder\n",
    "        dataset = load_dataset(input_source, streaming=streaming)\n",
    "        dataset = DatasetDict(dataset)\n",
    "    elif isinstance(input_source, dict):\n",
    "        # Dictionary with specified train, test, and eval files\n",
    "        formats = ['csv', 'json', 'jsonl', 'parquet', 'txt']\n",
    "        datasets = {}\n",
    "        for split, files in input_source.items():\n",
    "            format_detected = None\n",
    "            for fmt in formats:\n",
    "                if any(str(file).endswith(fmt) for file in files):\n",
    "                    format_detected = fmt\n",
    "                    break\n",
    "            if format_detected is None:\n",
    "                raise ValueError(f\"No supported file format detected for files: {files}\")\n",
    "            datasets[split] = load_dataset(format_detected, data_files=files, streaming=streaming)\n",
    "        dataset = DatasetDict(datasets)\n",
    "    else:\n",
    "        raise ValueError(\"Input source should be a dataset name, path to a folder, a single file, multiple files, or a dictionary.\")\n",
    "\n",
    "    # Perform the split if needed and if not in streaming mode\n",
    "    if not streaming:\n",
    "        train_size, test_size, eval_size = split_ratios\n",
    "        assert 0.0 < train_size < 1.0 and 0.0 < test_size < 1.0 and 0.0 < eval_size < 1.0 and (train_size + test_size + eval_size) == 1.0, \\\n",
    "            \"Split ratios must be between 0 and 1 and sum up to 1.\"\n",
    "\n",
    "        if \"train\" not in dataset or \"test\" not in dataset or \"eval\" not in dataset:\n",
    "            # Assuming all splits are to be derived from the 'train' dataset\n",
    "            full_dataset = concatenate_datasets(list(dataset.values())) if isinstance(dataset, dict) else dataset\n",
    "            split_dataset = full_dataset.train_test_split(train_size=train_size, seed=seed)\n",
    "            test_eval_split = split_dataset['test'].train_test_split(test_size=test_size / (test_size + eval_size), seed=seed)\n",
    "\n",
    "            dataset = DatasetDict({\n",
    "                \"train\": split_dataset[\"train\"],\n",
    "                \"test\": test_eval_split[\"train\"],\n",
    "                \"eval\": test_eval_split[\"test\"]\n",
    "            })\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b419c6e39274465a7f88eb1023fc8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/852 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Specify the path to the text files\n",
    "path_to_text_files = 'E:/LLMS/hemanth/output'\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('text', data_files=path_to_text_files+'/*.txt')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!rm E:/LLMS/hemanth/output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "from typing import Optional\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the extracted text from the PDF.\n",
    "    This function can be customized based on the cleaning requirements.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The text extracted from the PDF.\n",
    "\n",
    "    Returns:\n",
    "    - str: The cleaned text.\n",
    "    \"\"\"\n",
    "    cleaned_text = ' '.join(text.split())  # Removing extra whitespaces\n",
    "    # Add more cleaning rules as needed.\n",
    "    return cleaned_text\n",
    "\n",
    "def split_and_save_text(cleaned_text: str, base_output_path: Path, max_size_bytes: int = 50 * 1024 * 1024) -> None:\n",
    "    \"\"\"\n",
    "    Split the cleaned text into multiple files, each smaller than the specified max size, and save them.\n",
    "\n",
    "    Parameters:\n",
    "    - cleaned_text (str): The cleaned text to be split and saved.\n",
    "    - base_output_path (Path): The base path where the text files will be saved.\n",
    "    - max_size_bytes (int): Maximum size of the text file in bytes.\n",
    "    \"\"\"\n",
    "    part_num = 1\n",
    "    text_part = \"\"\n",
    "    for line in cleaned_text.split('\\n'):\n",
    "        if len(text_part.encode('utf-8')) + len(line.encode('utf-8')) < max_size_bytes:\n",
    "            text_part += line + '\\n'\n",
    "        else:\n",
    "            # Save the current part and start a new one\n",
    "            output_path = base_output_path.with_suffix(f'.part{part_num}.txt')\n",
    "            with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(text_part)\n",
    "            part_num += 1\n",
    "            text_part = line + '\\n' # Start new part with the current line\n",
    "    \n",
    "    # Save the last part\n",
    "    if text_part:\n",
    "        output_path = base_output_path.with_suffix(f'.part{part_num}.txt')\n",
    "        with open(output_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(text_part)\n",
    "\n",
    "def convert_pdf_to_text(pdf_path: str, output_folder: str) -> None:\n",
    "    \"\"\"\n",
    "    Convert a PDF file to text files, splitting contents to ensure each resulting file is less than 50 MB.\n",
    "    \n",
    "    Parameters:\n",
    "    - pdf_path (str): Path to the PDF file.\n",
    "    - output_folder (str): Path to the folder where the text files will be saved.\n",
    "    \"\"\"\n",
    "    # Ensure the output folder exists, create it if it does not\n",
    "    output_folder_path = Path(output_folder)\n",
    "    output_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        doc.close()\n",
    "        \n",
    "        cleaned_text = clean_text(text)\n",
    "        \n",
    "        base_output_path = output_folder_path / Path(pdf_path).stem\n",
    "        split_and_save_text(cleaned_text, base_output_path)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while converting {pdf_path}: {str(e)}\")\n",
    "\n",
    "def process_pdfs_from_csv(csv_path: str, output_folder: str) -> None:\n",
    "    \"\"\"\n",
    "    Process PDFs listed in a CSV file, converting them to text files and ensuring each part is less than 50 MB.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_path (str): Path to the CSV file containing paths to PDF files.\n",
    "    - output_folder (str): Path to the folder where text files will be stored.\n",
    "    \"\"\"\n",
    "    pdf_paths = pd.read_csv(csv_path, encoding='latin1')\n",
    "    # pdf_paths = pd.read_csv(csv_path)\n",
    "    for pdf_path in pdf_paths['.pdf']:\n",
    "        convert_pdf_to_text(pdf_path, output_folder)\n",
    "        \n",
    "        \n",
    "process_pdfs_from_csv(csv_path='csvfile.csv', output_folder='E:/LLMS/hemanth/output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from typing import List\n",
    "\n",
    "def write_to_file(content: str, output_dir: str, file_counter: int) -> int:\n",
    "    filename = os.path.join(output_dir, f'output_{file_counter}.txt')\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(content)\n",
    "    return file_counter + 1\n",
    "\n",
    "def process_files(input_dir: str, output_dir: str) -> None:\n",
    "    file_counter = 1\n",
    "    content = ''\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with open(file_path, 'r',encoding='utf-8') as f:\n",
    "                    file_content = f.read()\n",
    "                    if len(content + file_content) > 50 * 1024 * 1024:  # 50MB limit\n",
    "                        file_counter = write_to_file(content, output_dir, file_counter)\n",
    "                        content = file_content\n",
    "                    else:\n",
    "                        content += file_content\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "    if content:\n",
    "        write_to_file(content, output_dir, file_counter)\n",
    "\n",
    "def main(input_dir: str, output_dir: str) -> None:\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    process_files(input_dir, output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = 'C:/Users/heman/Desktop/deeplearning'\n",
    "    output_dir = 'C:/Users/heman/Desktop/deeplearning/data1'\n",
    "    main(input_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def print_directory_structure(path, indent='', last=True):\n",
    "    \"\"\"\n",
    "    Print directory structure in a tree-like format\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Error: Path not found.\")\n",
    "        return\n",
    "\n",
    "    if os.path.isfile(path):\n",
    "        print(indent + '├─ ' + os.path.basename(path))\n",
    "        return\n",
    "\n",
    "    files = sorted(os.listdir(path))\n",
    "\n",
    "    for i, entry in enumerate(files):\n",
    "        full_path = os.path.join(path, entry)\n",
    "        is_last = i == len(files) - 1\n",
    "        if os.path.isdir(full_path):\n",
    "            if is_last:\n",
    "                print(indent + '└─ ' + entry)\n",
    "            else:\n",
    "                print(indent + '├─ ' + entry)\n",
    "            print_directory_structure(full_path, indent + '   ', is_last)\n",
    "        else:\n",
    "            if is_last:\n",
    "                print(indent + '└─ ' + entry)\n",
    "            else:\n",
    "                print(indent + '├─ ' + entry)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory_path = \"E:/LLMS/hemanth/Hemanth/file_operations-\"\n",
    "    print(\"Directory Structure:\")\n",
    "    print_directory_structure(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- File: download.py ---\n",
      "Content written to E:/LLMS/hemanth/output_1.txt\n",
      "-------------------------------\n",
      "\n",
      "--- File: filetest.csv ---\n",
      "Content written to E:/LLMS/hemanth/output_1.txt\n",
      "-------------------------------\n",
      "\n",
      "An error occurred while reading the file huggingface_repos.md: 'charmap' codec can't decode byte 0x9d in position 2671: character maps to <undefined>\n",
      "An error occurred while reading the file llama.cpp: [Errno 13] Permission denied: 'E:/LLMS/hemanth/llama.cpp'\n",
      "--- File: metting.txt ---\n",
      "Content written to E:/LLMS/hemanth/output_1.txt\n",
      "-------------------------------\n",
      "\n",
      "An error occurred while reading the file output2.csv: 'charmap' codec can't decode byte 0x9d in position 21408: character maps to <undefined>\n",
      "--- File: output_1.txt ---\n",
      "Content written to E:/LLMS/hemanth/output_1.txt\n",
      "-------------------------------\n",
      "\n",
      "--- File: output_2.txt ---\n",
      "Content written to E:/LLMS/hemanth/output_1.txt\n",
      "-------------------------------\n",
      "\n",
      "--- File: output_3.txt ---\n",
      "Content written to E:/LLMS/hemanth/output_1.txt\n",
      "-------------------------------\n",
      "\n",
      "--- File: output_4.txt ---\n",
      "Content written to E:/LLMS/hemanth/output_1.txt\n",
      "-------------------------------\n",
      "\n",
      "--- File: requirements.txt ---\n",
      "Content written to E:/LLMS/hemanth/output_1.txt\n",
      "-------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'E:/LLMS/hemanth/output'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import re\n",
    "import pandas as pd\n",
    "from pptx import Presentation\n",
    "from io import StringIO\n",
    "\n",
    "from typing import Optional\n",
    "EXTENSION_READERS = {\n",
    "    '.md': lambda f: f.read(),\n",
    "    '.py': lambda f: f.read(),\n",
    "    '.csv': lambda f: pd.read_csv(f),\n",
    "    '.json': lambda f: json.load(f),\n",
    "    '.yaml': lambda f: yaml.safe_load(f),\n",
    "    '.txt': lambda f: f.read(),\n",
    "    '.xml': lambda f: f.read(),\n",
    "    '.html': lambda f: f.read(),\n",
    "    '.css': lambda f: f.read(),\n",
    "    '.js': lambda f: f.read(),\n",
    "    '.java': lambda f: f.read(),\n",
    "    '.cpp': lambda f: f.read(),\n",
    "    '.h': lambda f: f.read(),\n",
    "    '.php': lambda f: f.read(),\n",
    "    '.rb': lambda f: f.read(),\n",
    "    '.sql': lambda f: f.read(),\n",
    "    '.xls': lambda f: pd.read_excel(f),\n",
    "    '.xlsx': lambda f: pd.read_excel(f),\n",
    "    '.ppt': lambda f: read_pptx(f),\n",
    "    '.pptx': lambda f: read_pptx(f)\n",
    "}\n",
    "\n",
    "def read_pptx(file):\n",
    "    \"\"\"Custom function to read .pptx files with python-pptx\"\"\"\n",
    "    prs = Presentation(file)\n",
    "    text = []\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                text.append(shape.text)\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# Utilize regular expressions to match any of the file extensions\n",
    "EXTENSION_PATTERN = r\".*\\.(md|py|csv|json|yaml|txt|xml|html|css|js|java|cpp|h|php|rb|sql|xls|xlsx|ppt|pptx)$\"\n",
    "\n",
    "def list_files_with_extensions(directory_path):\n",
    "    try:\n",
    "        files = os.listdir(directory_path)\n",
    "        return [file for file in files if re.match(EXTENSION_PATTERN, file)]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The directory {directory_path} was not found.\")\n",
    "        return None\n",
    "\n",
    "def read_file_content(directory_path, filename):\n",
    "    try:\n",
    "        extension = os.path.splitext(filename)[1]\n",
    "        with open(os.path.join(directory_path, filename), 'r') as file:\n",
    "            file_reader = EXTENSION_READERS.get(extension)\n",
    "            return file_reader(file) if file_reader else None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file {filename}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def process_files_txtfile(directory_path: str, user_folder: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    This function processes all files in a given directory and writes their content to a user-specific text file.\n",
    "    Each text file is ensured to be less than 50 MB in size.\n",
    "\n",
    "    Args:\n",
    "    directory_path (str): The path to the directory containing the files to be processed.\n",
    "    user_folder (str): The name of the user-specific folder where the text files will be written.\n",
    "\n",
    "    Returns:\n",
    "    str: The path to the user-specific folder, or None if an error occurred.\n",
    "    \"\"\"\n",
    "    # Create the user-specific folder if it doesn't exist\n",
    "    user_folder_path = os.path.join(directory_path, user_folder)\n",
    "    os.makedirs(user_folder_path, exist_ok=True)\n",
    "\n",
    "    # Get a list of all files in the directory\n",
    "    files = list_files_with_extensions(directory_path)\n",
    "\n",
    "    if files is None:\n",
    "        return\n",
    "\n",
    "    # Initialize the text file counter and size\n",
    "    txt_file_counter = 1\n",
    "    txt_file_size = 0\n",
    "\n",
    "    for filename in files:\n",
    "        content = read_file_content(directory_path, filename)\n",
    "        if content is not None:\n",
    "            # Create a new text file if the size is over 50 MB\n",
    "            if txt_file_size >= 50 * 1024 * 1024:\n",
    "                txt_file_counter += 1\n",
    "                txt_file_size = 0\n",
    "\n",
    "            # Open the text file in append mode\n",
    "            txt_file_path = os.path.join(user_folder_path, f\"{user_folder}_{txt_file_counter}.txt\")\n",
    "            with open(txt_file_path, \"a\") as f:\n",
    "                if isinstance(content, pd.DataFrame):\n",
    "                    # Convert DataFrame to CSV string without index and write to file\n",
    "                    content_csv = content.to_csv(index=False)\n",
    "                    f.write(content_csv)\n",
    "                    txt_file_size += len(content_csv)\n",
    "                elif isinstance(content, dict):\n",
    "                    # Convert dict to pretty-printed string using json.dumps and write to file\n",
    "                    content_str = json.dumps(content, indent=4)\n",
    "                    f.write(content_str)\n",
    "                    txt_file_size += len(content_str)\n",
    "                else:\n",
    "                    # If it's not a DataFrame or dict, convert it to a string\n",
    "                    content_str = str(content)\n",
    "                    f.write(content_str)\n",
    "                    txt_file_size += len(content_str)\n",
    "\n",
    "            print(f\"--- File: {filename} ---\")\n",
    "            print(f\"Content written to {txt_file_path}\")\n",
    "            print(\"-------------------------------\\n\")\n",
    "\n",
    "    return user_folder_path\n",
    "directory_path = \"E:/LLMS/hemanth/\"\n",
    "user_folder = \"E:/LLMS/hemanth/output\"\n",
    "process_files_txtfile(directory_path, user_folder)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while reading the file DeepSpeed.csv: 'charmap' codec can't decode byte 0x8f in position 60753: character maps to <undefined>\n",
      "--- File: downloading.py ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/hemanth1/_1.txt\n",
      "-------------------------------\n",
      "\n",
      "--- File: duckduckgo.py ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/hemanth1/_1.txt\n",
      "-------------------------------\n",
      "\n",
      "--- File: extractall.py ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/hemanth1/_1.txt\n",
      "-------------------------------\n",
      "\n",
      "--- File: functions1.csv ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/hemanth1/_1.txt\n",
      "-------------------------------\n",
      "\n",
      "--- File: paramiko.json ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/hemanth1/_1.txt\n",
      "-------------------------------\n",
      "\n",
      "--- File: Pre_processing.py ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/hemanth1/_1.txt\n",
      "-------------------------------\n",
      "\n",
      "--- File: requirements.txt ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/hemanth1/_1.txt\n",
      "-------------------------------\n",
      "\n",
      "--- File: testing.py ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/hemanth1/_1.txt\n",
      "-------------------------------\n",
      "\n",
      "--- File: training1.py ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/hemanth1/_1.txt\n",
      "-------------------------------\n",
      "\n",
      "--- File: xformers.json ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/hemanth1/_1.txt\n",
      "-------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'E:/LLMS/hemanth/Hemanth/hemanth1/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def process_files_txtfile(directory_path: str, user_folder: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    This function processes all files in a given directory and writes their content to a user-specific text file.\n",
    "    Each text file is ensured to be less than 50 MB in size.\n",
    "\n",
    "    Args:\n",
    "    directory_path (str): The path to the directory containing the files to be processed.\n",
    "    user_folder (str): The name of the user-specific folder where the text files will be written.\n",
    "\n",
    "    Returns:\n",
    "    str: The path to the user-specific folder, or None if an error occurred.\n",
    "    \"\"\"\n",
    "    # Create the user-specific folder if it doesn't exist\n",
    "    user_folder_path = os.path.join(directory_path, user_folder)\n",
    "    os.makedirs(user_folder_path, exist_ok=True)\n",
    "\n",
    "    # Get a list of all files in the directory\n",
    "    files = list_files_with_extensions(directory_path)\n",
    "\n",
    "    if files is None:\n",
    "        return\n",
    "\n",
    "    # Initialize the text file counter and size\n",
    "    txt_file_counter = 1\n",
    "    txt_file_size = 0\n",
    "\n",
    "    for filename in files:\n",
    "        content = read_file_content(directory_path, filename)\n",
    "        if content is not None:\n",
    "            # Create a new text file if the size is over 50 MB\n",
    "            if txt_file_size >= 50 * 1024 * 1024:\n",
    "                txt_file_counter += 1\n",
    "                txt_file_size = 0\n",
    "\n",
    "            # Open the text file in append mode\n",
    "            txt_file_path = os.path.join(user_folder_path, f\"{user_folder}_{txt_file_counter}.txt\")\n",
    "            with open(txt_file_path, \"a\") as f:\n",
    "                if isinstance(content, pd.DataFrame):\n",
    "                    # Convert DataFrame to CSV string without index and write to file\n",
    "                    content_csv = content.to_csv(index=False)\n",
    "                    f.write(content_csv)\n",
    "                    txt_file_size += len(content_csv)\n",
    "                elif isinstance(content, dict):\n",
    "                    # Convert dict to pretty-printed string using json.dumps and write to file\n",
    "                    content_str = json.dumps(content, indent=4)\n",
    "                    f.write(content_str)\n",
    "                    txt_file_size += len(content_str)\n",
    "                else:\n",
    "                    # If it's not a DataFrame or dict, it should already be a string\n",
    "                    f.write(content)\n",
    "                    txt_file_size += len(content)\n",
    "\n",
    "            print(f\"--- File: {filename} ---\")\n",
    "            print(f\"Content written to {txt_file_path}\")\n",
    "            print(\"-------------------------------\\n\")\n",
    "\n",
    "    return user_folder_path\n",
    "\n",
    "directory_path = \"E:/LLMS/hemanth/Hemanth/file_operations-/\"\n",
    "user_folder = \"E:/LLMS/hemanth/Hemanth/hemanth1/\"\n",
    "process_files_txtfile(directory_path, user_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/hemanth.csv to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/hemanth1.json.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 114\u001b[0m\n\u001b[0;32m    109\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 114\u001b[0m \u001b[43mcsv_to_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mE:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/hemanth.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mjson_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mE:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/hemanth1.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 19\u001b[0m, in \u001b[0;36mcsv_to_json\u001b[1;34m(csv_file_path, json_file_path)\u001b[0m\n\u001b[0;32m     16\u001b[0m   csv_reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictReader(csv_file)\n\u001b[0;32m     18\u001b[0m   \u001b[38;5;66;03m# Convert CSV rows into a list of dictionaries\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m   data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcsv_reader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Open the JSON file for writing\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[0;32m     23\u001b[0m   \u001b[38;5;66;03m# Dump the list of dictionaries to the JSON file\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hemanthk.LAP53-FJS.000\\AppData\\Local\\Programs\\Python\\Python310\\lib\\csv.py:111\u001b[0m, in \u001b[0;36mDictReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_num \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m# Used only for its side effect.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames\n\u001b[1;32m--> 111\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader\u001b[38;5;241m.\u001b[39mline_num\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# unlike the basic reader, we prefer not to return blanks,\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# because we will typically wind up with a dict full of None\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# values\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hemanthk.LAP53-FJS.000\\AppData\\Local\\Programs\\Python\\Python310\\lib\\codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m    322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import yaml\n",
    "import os\n",
    "def csv_to_json(csv_file_path:str, json_file_path:str):\n",
    "  \"\"\" Converts a CSV file with variable X columns and Y rows into a JSON file.\n",
    "\n",
    "  Args:\n",
    "    csv_file_path (str): Path to the input CSV file.\n",
    "    json_file_path (str): Path to the desired output JSON file.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    # Open the CSV file for reading\n",
    "    with open(csv_file_path, 'r', encoding='utf-8') as csv_file:\n",
    "      # Create a CSV reader object\n",
    "      csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "      # Convert CSV rows into a list of dictionaries\n",
    "      data = list(csv_reader)\n",
    "\n",
    "    # Open the JSON file for writing\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "      # Dump the list of dictionaries to the JSON file\n",
    "      json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f'Successfully converted {csv_file_path} to {json_file_path}.')\n",
    "\n",
    "  except FileNotFoundError:\n",
    "    print(f'Error: CSV file {csv_file_path} not found.')\n",
    "\n",
    "  except Exception as e:\n",
    "    print(f'An error occurred: {e}')\n",
    "\n",
    "\n",
    "def json_to_csv(json_file_path:str, csv_file_path:str):\n",
    "  \"\"\" Converts a JSON file into a CSV file.\n",
    "\n",
    "  Args:\n",
    "    json_file_path (str): Path to the input JSON file.\n",
    "    csv_file_path (str): Path to the desired output CSV file.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    # Open the JSON file for reading\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "      # Load the JSON data\n",
    "      data = json.load(json_file)\n",
    "\n",
    "    if not (isinstance(data, list) and all(isinstance(item, dict) for item in data)):\n",
    "      raise ValueError('JSON data must be a list of dictionaries')\n",
    "\n",
    "    # Get the keys of the first dictionary to use as CSV headers\n",
    "    headers = set()\n",
    "    for item in data:\n",
    "      headers.update(item.keys())\n",
    "    headers = list(headers)\n",
    "\n",
    "    # Open the CSV file for writing\n",
    "    with open(csv_file_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "      # Create a CSV writer object\n",
    "      csv_writer = csv.DictWriter(csv_file, fieldnames=headers)\n",
    "\n",
    "      # Write the headers to the CSV file\n",
    "      csv_writer.writeheader()\n",
    "\n",
    "      # Write each dictionary in the list to the CSV file\n",
    "      for row in data:\n",
    "        # Check if the row contains all the required keys\n",
    "        if not set(headers).issubset(row.keys()):\n",
    "          raise ValueError(f'Missing keys in row: {row}')\n",
    "\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "    print(f'Successfully converted {json_file_path} to {csv_file_path}.')\n",
    "\n",
    "  except FileNotFoundError:\n",
    "    print(f'Error: JSON file {json_file_path} not found.')\n",
    "  except ValueError as e:\n",
    "    print(f'Error: {e}')\n",
    "  except Exception as e:\n",
    "    print(f'An error occurred: {e}')\n",
    "\n",
    "\n",
    "def csv_to_yaml(csv_file_path:str, yaml_file_path:str):\n",
    "  \"\"\" Converts a CSV file with variable X columns and Y rows into a YAML file.\n",
    "\n",
    "  Args:\n",
    "    csv_file_path (str): Path to the input CSV file.\n",
    "    yaml_file_path (str): Path to the desired output YAML file.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    # Open the CSV file for reading\n",
    "    with open(csv_file_path, 'r', encoding='utf-8') as csv_file:\n",
    "      # Create a CSV reader object\n",
    "      csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "      # Convert CSV rows into a list of dictionaries\n",
    "      data = list(csv_reader)\n",
    "\n",
    "    # Open the YAML file for writing\n",
    "    with open(yaml_file_path, 'w', encoding='utf-8') as yaml_file:\n",
    "      # Dump the list of dictionaries to the YAML file\n",
    "      yaml.dump(data, yaml_file, allow_unicode=True)\n",
    "\n",
    "    print(f'Successfully converted {csv_file_path} to {yaml_file_path}.')\n",
    "\n",
    "  except FileNotFoundError:\n",
    "    print(f'Error: CSV file {csv_file_path} not found.')\n",
    "\n",
    "  except Exception as e:\n",
    "    print(f'An error occurred: {e}')\n",
    "\n",
    "\n",
    "\n",
    "csv_to_json(csv_file_path='E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/hemanth.csv',json_file_path='E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/hemanth1.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_to_csv('E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/hemanth1.json','E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/hemanth2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_yaml('E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/hemanth2.csv','output.yml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import DirectoryLoader,PyPDFLoader\n",
    "def write_to_csv(file_path: str, data: dict, write_header: bool) -> None:\n",
    "    \"\"\"\n",
    "    Function to append data into a CSV file.\n",
    "    \n",
    "    Args:\n",
    "    file_path (str): The path to the CSV file.\n",
    "    data (dict): The data to be appended into the CSV file.\n",
    "    write_header (bool): Whether to write the header.\n",
    "    \"\"\"\n",
    "    mode = 'a' if os.path.exists(file_path) else 'w'\n",
    "    with open(file_path, mode, newline='', encoding='UTF-8', errors='ignore') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"content\", \"documents\", \"metasource\"])\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "        try:\n",
    "            writer.writerow({k: data[k] for k in [\"content\", \"documents\", \"metasource\"]})\n",
    "        except UnicodeEncodeError:\n",
    "            print(f\"Warning: UnicodeEncodeError encountered for file {data['documents']}. Skipping this file.\")\n",
    "\n",
    "def read_pdfs_from_folder(folder_path: str, csv_file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Function to recursively read PDF files from a folder and its subfolders and extract their content.\n",
    "    \n",
    "    Args:\n",
    "    folder_path (str): The path to the folder containing the PDF files.\n",
    "    csv_file_path (str): The path to the CSV file.\n",
    "    \n",
    "    Example:\n",
    "    ```\n",
    "    folder_path = \"path/to/pdf/folder\"\n",
    "    csv_file_path = \"output.csv\"\n",
    "    read_pdfs_from_folder(folder_path, csv_file_path)\n",
    "    ```\n",
    "    \n",
    "    Output:\n",
    "    The content of the PDF files in the specified folder and its subfolders is extracted and written to the CSV file.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\".pdf\"):\n",
    "                full_file_path = os.path.join(root, file_name)\n",
    "                loader = PyPDFLoader(full_file_path)\n",
    "                pages = loader.load_and_split()\n",
    "                for page in pages:\n",
    "                    data = {\n",
    "                        \"content\": page.page_content,\n",
    "                        \"documents\": file_name,\n",
    "                        \"metasource\": full_file_path\n",
    "                    }\n",
    "                    write_to_csv(csv_file_path, data, file_name == os.listdir(root)[0])\n",
    "\n",
    "# Usage\n",
    "folder_path = \"E:/LLMS/hemanth/Hemanth/amazon/\"\n",
    "csv_file_path = \"output.csv\"\n",
    "read_pdfs_from_folder(folder_path, csv_file_path)\n",
    "\n",
    "# Usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def get_image_paths(folder: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively get a list of paths for all .png images in the given folder.\n",
    "    Handles Windows-style paths properly.\n",
    "\n",
    "    Args:\n",
    "        folder: The folder path to search.\n",
    "\n",
    "    Returns:\n",
    "        A list of paths for all .png images found.\n",
    "    \"\"\"\n",
    "    \n",
    "    image_paths = []\n",
    "\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.png'):\n",
    "                path = os.path.join(root, file)\n",
    "                path = path.replace('\\\\', '/') # convert Windows paths\n",
    "                image_paths.append(path)\n",
    "\n",
    "    return image_paths\n",
    "\n",
    "\n",
    "def save_to_csv(image_paths: List[str], csv_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Save a list of image paths to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        image_paths: A list of image path strings.\n",
    "        csv_file: Path to the CSV file to save.\n",
    "    \"\"\"\n",
    "    with open(csv_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for path in image_paths:\n",
    "            writer.writerow([path])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    folder = 'E:/LLMS/hemanth/Hemanth/'\n",
    "    csv_file = 'image_paths.csv'\n",
    "\n",
    "    image_paths = get_image_paths(folder)\n",
    "    save_to_csv(image_paths, csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to folder_structure.json and folder_structure.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "def save_to_json(data: List[Dict[str, Any]], filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves the provided data to a JSON file with the given filename.\n",
    "    \n",
    "    :param data: The data to be saved in JSON format.\n",
    "    :param filename: The name of the file to save the JSON data to.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "def save_to_csv(data: List[Dict[str, Any]], filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves the provided data to a CSV file with the given filename.\n",
    "    \n",
    "    :param data: The data to be saved in CSV format.\n",
    "    :param filename: The name of the file to save the CSV data to.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', newline='') as csv_file:\n",
    "        fieldnames = ['folder', 'extensions', 'files']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for folder_info in data:\n",
    "            writer.writerow(folder_info)\n",
    "\n",
    "def get_extensions_and_paths(directory: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Traverses the directory tree starting at the given directory and compiles a list of \n",
    "    dictionaries, each containing folder names, their file extensions, and file paths.\n",
    "    \n",
    "    :param directory: The root directory from which to start the folder traversal.\n",
    "    :return: A list of dictionaries with folder names, extensions, and file paths.\n",
    "    \"\"\"\n",
    "    folder_structure = []\n",
    "    root_path = Path(directory)\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        folder_info = {\n",
    "            \"folder\": folder_name,\n",
    "            \"extensions\": set(),\n",
    "            \"files\": {}\n",
    "        }\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            extension = file_path.suffix\n",
    "            if extension:\n",
    "                folder_info[\"extensions\"].add(extension)\n",
    "                # Use as_posix() to convert the path to a string with forward slashes\n",
    "                folder_info[\"files\"].setdefault(extension, []).append(file_path.as_posix())\n",
    "        \n",
    "        # Convert the set of extensions to a sorted list\n",
    "        folder_info[\"extensions\"] = sorted(list(folder_info[\"extensions\"]))\n",
    "        folder_structure.append(folder_info)\n",
    "\n",
    "    return folder_structure\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        directory_to_scan = \"E:/LLMS/hemanth/Hemanth/file_operations-\"\n",
    "        folder_data = get_extensions_and_paths(directory_to_scan)\n",
    "        json_filename = \"folder_structure.json\"\n",
    "        csv_filename = \"folder_structure.csv\"\n",
    "        save_to_json(folder_data, json_filename)\n",
    "        save_to_csv(folder_data, csv_filename)\n",
    "        print(f\"Data saved to {json_filename} and {csv_filename}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "def get_file_paths(folder: str, extensions: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Get all file paths in the folder with the given extensions.\n",
    "\n",
    "    Args:\n",
    "        folder: A path to the folder to search in.\n",
    "        extensions: A list of file extensions to include.\n",
    "\n",
    "    Returns:\n",
    "        A list of lists, where each sublist contains file paths with the same extension.\n",
    "    \"\"\"\n",
    "    paths = [[] for _ in extensions]\n",
    "    folder_path = Path(folder)\n",
    "\n",
    "    for i, ext in enumerate(extensions):\n",
    "        for file_path in folder_path.rglob('*' + ext):\n",
    "            paths[i].append(str(file_path))\n",
    "\n",
    "    return paths\n",
    "\n",
    "def save_to_csv(file_paths: List[List[str]], csv_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Save lists of file paths to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_paths: A list of lists where each sublist contains file paths of a certain type.\n",
    "        csv_file: Path to the CSV file to save.\n",
    "    \"\"\"\n",
    "    with open(csv_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for row in zip(*file_paths):\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == '__main__':\n",
    "    folder = 'E:/LLMS/hemanth/Hemanth/' \n",
    "    extensions = ['.py','.pdf',]\n",
    "    csv_file = 'file_paths.csv'\n",
    "\n",
    "    file_paths = get_file_paths(folder, extensions)\n",
    "    save_to_csv(file_paths, csv_file)\n",
    "    \n",
    "    print('Done!')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from typing import List\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def get_file_paths(folder: str, extensions: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Get all file paths in the folder with the given extensions.\n",
    "    ...\n",
    "    \"\"\"\n",
    "    paths = [[] for _ in extensions]\n",
    "    folder_path = Path(folder)\n",
    "\n",
    "    for i, ext in enumerate(extensions):\n",
    "        for file_path in folder_path.rglob('*' + ext):\n",
    "            # Use as_posix() to get the file_path with forward slashes\n",
    "            paths[i].append(file_path.as_posix())\n",
    "\n",
    "    return paths\n",
    "def save_to_csv(file_paths: List[List[str]], csv_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Save lists of file paths to a CSV file in columns based on file types.\n",
    "\n",
    "    Args:\n",
    "        file_paths: A list containing a list of paths for each file type.\n",
    "        csv_file: Path to the CSV file to save.\n",
    "    \"\"\"\n",
    "\n",
    "    max_rows = max(len(paths) for paths in file_paths)\n",
    "    \n",
    "    with open(csv_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write header row with file types\n",
    "        header = [ext.upper()[1:] for ext in extensions]\n",
    "        writer.writerow(header)\n",
    "        \n",
    "        for i in range(max_rows):\n",
    "            row = []\n",
    "            for paths in file_paths:\n",
    "                if i < len(paths):\n",
    "                    row.append(paths[i])\n",
    "                else:\n",
    "                    row.append('')\n",
    "            writer.writerow(row)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    folder = 'E:/LLMS/hemanth/Hemanth/'\n",
    "    extensions = ['.png', '.jpg', '.pdf']\n",
    "    csv_file = 'file_paths.csv'\n",
    "\n",
    "    file_paths = get_file_paths(folder, extensions)\n",
    "    save_to_csv(file_paths, csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- File: Datasetsturture.py ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: filepre_processing.py ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: filetxtmd.py ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: File_conversion.py ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: File_operation_and_conversion.ipynb ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: file_paths.csv ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: file_reading.ipynb ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: folder_structure.csv ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: folder_structure.json ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: Folder_to_txt_file.py ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: Hemanth ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: hemanth.csv ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: hemanth1.json ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: hemanth1.txt ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: hemanth2.csv ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: loaderfiles.py ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: MetaMathQA-395K.csv ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: MetaMathQA-395K.json ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: output.csv ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: output.yml ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: outputFile.txt1.txt ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: outputFile.txt2.txt ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: outputFile.txt3 ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: outputFile.txt4 ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: outputFile3.txt ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: processsing_csv.py ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: testing .py ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n",
      "--- File: transfomers ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile2.txt\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "MAX_FILE_SIZE = 50 * 1024 * 1024  # 50 MB\n",
    "\n",
    "def check_create_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "def next_file_index(folder_path, base_filename):\n",
    "    files = os.listdir(folder_path)\n",
    "    existing_indices = [int(f.replace(base_filename, '').replace('.txt', '')) for f in files if f.startswith(base_filename) and f.endswith('.txt')]\n",
    "    return max(existing_indices + [0]) + 1\n",
    "\n",
    "def get_file_path(folder_path, base_filename, index):\n",
    "    return os.path.join(folder_path, f\"{base_filename}{index}.txt\")\n",
    "\n",
    "def should_create_new_file(file_path):\n",
    "    return os.path.exists(file_path) and os.path.getsize(file_path) >= MAX_FILE_SIZE\n",
    "\n",
    "def adjust_content_and_write(content, file_path):\n",
    "    if isinstance(content, pd.DataFrame):\n",
    "        content = content.to_csv(index=False)\n",
    "    elif isinstance(content, dict):\n",
    "        content = json.dumps(content)\n",
    "    elif isinstance(content, list):\n",
    "        content = '\\n'.join(content)\n",
    "    elif not isinstance(content, str):\n",
    "        raise ValueError(\"Unsupported content type for writing to file\")\n",
    "\n",
    "    with open(file_path, \"a\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "def read_file_content(directory_path, filename):\n",
    "    # Implement your file reading logic here, adjusted for different file formats\n",
    "    # This is a placeholder function; the actual implementation depends on your specific file types and processing needs\n",
    "    return \"Placeholder for actual content\"\n",
    "\n",
    "def process_files_into_folders(directory_path, folder_base_path, base_filename):\n",
    "    files = os.listdir(directory_path)\n",
    "    check_create_folder(folder_base_path)\n",
    "    \n",
    "    index = next_file_index(folder_base_path, base_filename)\n",
    "    file_path = get_file_path(folder_base_path, base_filename, index)\n",
    "    \n",
    "    for filename in files:\n",
    "        file_full_path = os.path.join(directory_path, filename)\n",
    "        content = read_file_content(directory_path, filename)  # Assume this returns content correctly based on file type\n",
    "        \n",
    "        if content is not None:\n",
    "            if should_create_new_file(file_path):\n",
    "                index += 1\n",
    "                file_path = get_file_path(folder_base_path, base_filename, index)\n",
    "\n",
    "        adjust_content_and_write(content, file_path)\n",
    "        print(f\"--- File: {filename} ---\\nContent written to {file_path}\\n-------------------------------\")\n",
    "\n",
    "# Correct paths and base_filename as per your directory structure and needs\n",
    "directory_path = \"E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/\"\n",
    "folder_base_path = \"E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/\"\n",
    "base_filename = 'outputFile'\n",
    "process_files_into_folders(directory_path, folder_base_path, base_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'tputFile1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m folder_base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m base_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mou\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 60\u001b[0m \u001b[43mprocess_files_into_folders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder_base_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_filename\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[49], line 38\u001b[0m, in \u001b[0;36mprocess_files_into_folders\u001b[1;34m(user_given_directory_path, target_folder_base_path, base_filename)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_files_into_folders\u001b[39m(user_given_directory_path, target_folder_base_path, base_filename):\n\u001b[0;32m     37\u001b[0m     check_create_folder(target_folder_base_path)\n\u001b[1;32m---> 38\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[43mnext_file_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_folder_base_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m get_file_path(target_folder_base_path, base_filename, index)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(user_given_directory_path):\n",
      "Cell \u001b[1;32mIn[49], line 16\u001b[0m, in \u001b[0;36mnext_file_index\u001b[1;34m(folder_path, base_filename)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 16\u001b[0m highest_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mint\u001b[39m(f\u001b[38;5;241m.\u001b[39mreplace(base_filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m highest_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[49], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 16\u001b[0m highest_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m highest_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'tputFile1'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Constants\n",
    "MAX_FILE_SIZE = 50 * 1024 * 1024  # 50 MB in bytes\n",
    "\n",
    "def check_create_folder(folder_path):\n",
    "    \"\"\"Ensure the folder exists, create if not.\"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "def next_file_index(folder_path, base_filename):\n",
    "    \"\"\"Find the next file index that can be used.\"\"\"\n",
    "    files = [f for f in os.listdir(folder_path) if f.startswith(base_filename) and f.endswith('.txt')]\n",
    "    if not files:\n",
    "        return 1\n",
    "    highest_index = max([int(f.replace(base_filename, '').replace('.txt', '')) for f in files])\n",
    "    return highest_index + 1\n",
    "\n",
    "def get_file_path(folder_base_path, base_filename, index):\n",
    "    \"\"\"Construct a file path with the given index.\"\"\"\n",
    "    return os.path.join(folder_base_path, f\"{base_filename}{index}.txt\")\n",
    "\n",
    "def should_create_new_file(file_path):\n",
    "    \"\"\"Determine if a new file needs to be created based on the size.\"\"\"\n",
    "    return os.path.exists(file_path) and os.path.getsize(file_path) >= MAX_FILE_SIZE\n",
    "\n",
    "def adjust_content_and_write(content, file_path, add_separator=False):\n",
    "    \"\"\"Write content to the file, optionally adding a separator if the file already exists.\"\"\"\n",
    "    mode = \"a\" if os.path.exists(file_path) else \"w\"\n",
    "    with open(file_path, mode, encoding='utf-8') as f:\n",
    "        if add_separator and os.path.getsize(file_path) > 0:\n",
    "            # Simulate a 'separator' with a newline\n",
    "            f.write(\"\\n\\n\" + (\"-\" * 80) + \"\\n\\n\")\n",
    "        f.write(content)\n",
    "\n",
    "def process_files_into_folders(user_given_directory_path, target_folder_base_path, base_filename):\n",
    "    check_create_folder(target_folder_base_path)\n",
    "    index = next_file_index(target_folder_base_path, base_filename)\n",
    "    file_path = get_file_path(target_folder_base_path, base_filename, index)\n",
    "    \n",
    "    for filename in os.listdir(user_given_directory_path):\n",
    "        if filename.endswith('.txt'):  # Assuming we are only interested in .txt files\n",
    "            with open(os.path.join(user_given_directory_path, filename), 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                \n",
    "                if should_create_new_file(file_path):\n",
    "                    index += 1\n",
    "                    file_path = get_file_path(target_folder_base_path, base_filename, index)\n",
    "                \n",
    "                adjust_content_and_write(content, file_path, add_separator=True)\n",
    "                \n",
    "                print(f\"--- File: {filename} ---\")\n",
    "                print(f\"Content written to {file_path}\")\n",
    "                print(\"-------------------------------\\n\")\n",
    "\n",
    "# Example usage - adjust the paths and base_filename as necessary\n",
    "directory_path = \"E:/LLMS/hemanth/\"\n",
    "folder_base_path = \"E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/\"\n",
    "base_filename = 'ou'\n",
    "process_files_into_folders(directory_path, folder_base_path, base_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\00 - Training.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\03 - Multimodal Learning.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\08 - Graph Neural Networks.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\09 - Recommender Systems.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\10 - Computational Biology.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\.git\\index (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\.git\\objects\\pack\\pack-2400e8f4e05af351bd2d7191f5d84ab8f68884d7.idx (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\.git\\objects\\pack\\pack-2400e8f4e05af351bd2d7191f5d84ab8f68884d7.pack (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\.git\\objects\\pack\\pack-2400e8f4e05af351bd2d7191f5d84ab8f68884d7.rev (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\01 - Computer Vision\\04 - Face Recognition and Detection.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\01 - Computer Vision\\05 - Video.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\01 - Computer Vision\\06 - 3D.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\01 - Computer Vision\\01 - Image Classification\\01 - Large Networks.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\01 - Computer Vision\\01 - Image Classification\\02 - Small Networks.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\01 - Computer Vision\\01 - Image Classification\\03 - AutoML.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\01 - Computer Vision\\01 - Image Classification\\04 - Robustness.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\01 - Computer Vision\\01 - Image Classification\\05 - Visualizing & Understanding.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\01 - Computer Vision\\01 - Image Classification\\06 - Transfer Learning.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\01 - Computer Vision\\02 - Image Transformation\\01 - Semantic Segmentation.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\01 - Computer Vision\\02 - Image Transformation\\02 - Super-Resolution, Denoising, and Colorization.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\01 - Computer Vision\\02 - Image Transformation\\03 - Pose Estimation.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\01 - Computer Vision\\02 - Image Transformation\\04 - Optical Flow and Depth Estimation.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\01 - Computer Vision\\03 - Object Detection\\01 - Two Stage Detectors.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\01 - Computer Vision\\03 - Object Detection\\02 - One Stage Detectors.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\02 - Natural Language Processing\\01 - Word Representations.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\02 - Natural Language Processing\\02 - Text Classification.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\02 - Natural Language Processing\\03 - Neural Machine Translation.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\02 - Natural Language Processing\\04 - Language Modeling.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\04 - Generative Networks\\01 - Variational Auto-Encoders.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\04 - Generative Networks\\02 - Unconditional GANs.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\04 - Generative Networks\\03 - Conditional GANs.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\04 - Generative Networks\\04 - Diffusion Models.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\05 - Advanced Topics\\01 - Domain Adaptation.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\05 - Advanced Topics\\02 - Few Shot Learning.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\05 - Advanced Topics\\03 - Federated Learning.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\05 - Advanced Topics\\04 - Semi-Supervised Learning.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\05 - Advanced Topics\\05 - Self-Supervised Learning.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\06 - Speech & Music\\01 - Recognition.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\06 - Speech & Music\\02 - Synthesis.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\06 - Speech & Music\\03 - Modeling.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\07 - Reinforcement Learning\\01 - Games.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\07 - Reinforcement Learning\\02 - Simulated Environments.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\07 - Reinforcement Learning\\03 - Real Environments.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\07 - Reinforcement Learning\\04 - Uncertainty Quantification & Multitask Learning.pdf (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\unilm\\.git\\index (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\unilm\\.git\\objects\\pack\\pack-4c646106460cc0008152611ba39901374fa728b7.idx (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\unilm\\.git\\objects\\pack\\pack-4c646106460cc0008152611ba39901374fa728b7.pack (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\unilm\\.git\\objects\\pack\\pack-4c646106460cc0008152611ba39901374fa728b7.rev (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\unilm\\beats\\Evaluation_Results\\Comparing_Different_BEATS_Tokenizers.png (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\unilm\\beats\\Evaluation_Results\\Comparing_Different_Pre-Training_Targets.png (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\unilm\\beats\\Evaluation_Results\\Comparing_with_the_SOTA_Ensemble_Models.png (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\unilm\\beats\\Evaluation_Results\\Comparing_with_the_SOTA_Single_Models.png (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\unilm\\beit2\\demo\\ILSVRC2012_val_00031649.JPEG (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\unilm\\beit2\\vqkd_teacher\\clip\\bpe_simple_vocab_16e6.txt.gz (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\unilm\\decoding\\IAD\\fairseq\\docs\\fairseq.gif (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\unilm\\decoding\\IAD\\fairseq\\docs\\fairseq_logo.png (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\unilm\\dit\\object_detection\\publaynet_example.jpeg (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\unilm\\edgelm\\docs\\fairseq.gif (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\unilm\\edgelm\\docs\\fairseq_logo.png (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\unilm\\edgelm\\examples\\flores101\\flores_logo.png (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\unilm\\edgelm\\examples\\MMPT\\videoclip.png (UnicodeDecodeError)\n",
      "Skipping file: E:/LLMS/hemanth/Applied-Deep-Learning\\unilm\\edgelm\\examples\\MMPT\\vlm.png (UnicodeDecodeError)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'target_folder_base_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m folder_base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     66\u001b[0m base_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaing\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 67\u001b[0m \u001b[43mprocess_files_into_folders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder_base_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_filename\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[50], line 61\u001b[0m, in \u001b[0;36mprocess_files_into_folders\u001b[1;34m(user_given_directory_path, target_folder_base_path, base_filename)\u001b[0m\n\u001b[0;32m     59\u001b[0m index \u001b[38;5;241m=\u001b[39m next_file_index(target_folder_base_path, base_filename)\n\u001b[0;32m     60\u001b[0m file_path \u001b[38;5;241m=\u001b[39m get_file_path(target_folder_base_path, base_filename, index)\n\u001b[1;32m---> 61\u001b[0m \u001b[43mrecursive_file_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_given_directory_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[50], line 54\u001b[0m, in \u001b[0;36mrecursive_file_search\u001b[1;34m(directory_path, callback)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m root, _, files \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(directory_path):\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m---> 54\u001b[0m         \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[50], line 45\u001b[0m, in \u001b[0;36mprocess_file\u001b[1;34m(full_file_path)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m should_create_new_file(file_path):\n\u001b[0;32m     44\u001b[0m             index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 45\u001b[0m             file_path \u001b[38;5;241m=\u001b[39m get_file_path(\u001b[43mtarget_folder_base_path\u001b[49m, base_filename, index)\n\u001b[0;32m     47\u001b[0m         adjust_content_and_write(content, file_path, add_separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'target_folder_base_path' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Constants\n",
    "MAX_FILE_SIZE = 50 * 1024 * 1024  # 50 MB in bytes\n",
    "\n",
    "def check_create_folder(folder_path):\n",
    "    \"\"\"Ensure the folder exists, create if not.\"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "def next_file_index(folder_path, base_filename):\n",
    "    \"\"\"Find the next file index that can be used.\"\"\"\n",
    "    files = [f for f in os.listdir(folder_path) if f.startswith(base_filename) and f.endswith('.txt')]\n",
    "    if not files:\n",
    "        return 1\n",
    "    highest_index = max([int(f.replace(base_filename, '').replace('.txt', '')) for f in files])\n",
    "    return highest_index + 1\n",
    "\n",
    "def get_file_path(folder_base_path, base_filename, index):\n",
    "    \"\"\"Construct a file path with the given index.\"\"\"\n",
    "    return os.path.join(folder_base_path, f\"{base_filename}{index}.txt\")\n",
    "\n",
    "def should_create_new_file(file_path):\n",
    "    \"\"\"Determine if a new file needs to be created based on the size.\"\"\"\n",
    "    return os.path.exists(file_path) and os.path.getsize(file_path) >= MAX_FILE_SIZE\n",
    "\n",
    "def adjust_content_and_write(content, file_path, add_separator=False):\n",
    "    \"\"\"Write content to the file, optionally adding a separator if the file already exists.\"\"\"\n",
    "    mode = \"a\" if os.path.exists(file_path) else \"w\"\n",
    "    with open(file_path, mode, encoding='utf-8') as f:\n",
    "        if add_separator and os.path.getsize(file_path) > 0:\n",
    "            # Simulate a 'separator' with a newline\n",
    "            f.write(\"\\n\\n\" + (\"-\" * 80) + \"\\n\\n\")\n",
    "        f.write(content)\n",
    "\n",
    "def process_file(full_file_path):\n",
    "    global index, file_path\n",
    "    \n",
    "    try:\n",
    "        with open(full_file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            \n",
    "            if should_create_new_file(file_path):\n",
    "                index += 1\n",
    "                file_path = get_file_path(target_folder_base_path, base_filename, index)\n",
    "            \n",
    "            adjust_content_and_write(content, file_path, add_separator=True)\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Skipping file: {full_file_path} (UnicodeDecodeError)\")\n",
    "\n",
    "def recursive_file_search(directory_path, callback):\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for filename in files:\n",
    "            callback(os.path.join(root, filename))\n",
    "\n",
    "def process_files_into_folders(user_given_directory_path, target_folder_base_path, base_filename):\n",
    "    global index, file_path\n",
    "    check_create_folder(target_folder_base_path)\n",
    "    index = next_file_index(target_folder_base_path, base_filename)\n",
    "    file_path = get_file_path(target_folder_base_path, base_filename, index)\n",
    "    recursive_file_search(user_given_directory_path, process_file)\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"E:/LLMS/hemanth/\"\n",
    "folder_base_path = \"E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/\"\n",
    "base_filename = 'caing'\n",
    "process_files_into_folders(directory_path, folder_base_path, base_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Constants\n",
    "MAX_FILE_SIZE = 50 * 1024 * 1024  # 50 MB in bytes\n",
    "\n",
    "def check_create_folder(folder_path):\n",
    "    \"\"\"Ensure the folder exists, create if not.\"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "def next_file_index(folder_path, base_filename):\n",
    "    \"\"\"Find the next file index that can be used.\"\"\"\n",
    "    files = [f for f in os.listdir(folder_path) if f.startswith(base_filename) and f.endswith('.txt')]\n",
    "    if not files:\n",
    "        return 1\n",
    "    highest_index = max([int(f.split(base_filename)[-1].split('.txt')[0]) for f in files])\n",
    "    return highest_index + 1\n",
    "\n",
    "def get_file_path(folder_base_path, base_filename, index):\n",
    "    \"\"\"Construct a file path with the given index.\"\"\"\n",
    "    return os.path.join(folder_base_path, f\"{base_filename}{index}.txt\")\n",
    "\n",
    "def should_create_new_file(file_path):\n",
    "    \"\"\"Determine if a new file needs to be created based on the size.\"\"\"\n",
    "    return os.path.exists(file_path) and os.path.getsize(file_path) >= MAX_FILE_SIZE\n",
    "\n",
    "def adjust_content_and_write(content, file_path, add_separator=False):\n",
    "    \"\"\"Append content to the file, optionally adding a separator if the file already exists.\"\"\"\n",
    "    mode = \"a\" if os.path.exists(file_path) else \"w\"\n",
    "    with open(file_path, mode, encoding='utf-8') as f:\n",
    "        if add_separator and os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "            f.write(\"\\n\\n\" + (\"-\" * 80) + \"\\n\\n\")\n",
    "        f.write(content)\n",
    "   \n",
    "def recursive_file_search(directory_path, callback):\n",
    "    \"\"\"Recursively searches for files within the directory_path and processes them using callback.\"\"\"\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for filename in files:\n",
    "            callback(os.path.join(root, filename))\n",
    "\n",
    "def process_files_into_folders(user_given_directory_path, target_folder_base_path, base_filename):\n",
    "    check_create_folder(target_folder_base_path)\n",
    "    index = next_file_index(target_folder_base_path, base_filename)\n",
    "    file_path = get_file_path(target_folder_base_path, base_filename, index)\n",
    "    \n",
    "    def process_file(full_file_path):\n",
    "        nonlocal index, file_path\n",
    "        # Attempt to open and read the file, replacing characters that cannot be decoded\n",
    "        with open(full_file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "             content = file.read()\n",
    "                \n",
    "        # Check if a new file is needed before writing content\n",
    "        if should_create_new_file(file_path):\n",
    "            index += 1  # Increment to use a new file\n",
    "            file_path = get_file_path(target_folder_base_path, base_filename, index)\n",
    "        \n",
    "        adjust_content_and_write(content, file_path, add_separator=True)\n",
    "                    \n",
    "\n",
    "    \n",
    "    recursive_file_search(user_given_directory_path, process_file)\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing\"\n",
    "folder_base_path = \"E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/\"\n",
    "base_filename = 'output1test1'\n",
    "process_files_into_folders(directory_path, folder_base_path, base_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Constants\n",
    "MAX_FILE_SIZE = 50 * 1024 * 1024  # 50 MB in bytes\n",
    "\n",
    "def check_create_folder(folder_path):\n",
    "    \"\"\"Ensure the folder exists, create if not.\"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "def next_file_index(folder_path, base_filename):\n",
    "    \"\"\"Find the next file index that can be used.\"\"\"\n",
    "    files = [f for f in os.listdir(folder_path) if f.startswith(base_filename) and f.endswith('.txt')]\n",
    "    if not files:\n",
    "        return 1\n",
    "    highest_index = max([int(f.replace(base_filename, '').replace('.txt', '')) for f in files])\n",
    "    return highest_index + 1\n",
    "\n",
    "def get_file_path(folder_base_path, base_filename, index):\n",
    "    \"\"\"Construct a file path with the given index.\"\"\"\n",
    "    return os.path.join(folder_base_path, f\"{base_filename}{index}.txt\")\n",
    "\n",
    "def should_create_new_file(file_path):\n",
    "    \"\"\"Determine if a new file needs to be created based on the size.\"\"\"\n",
    "    return os.path.exists(file_path) and os.path.getsize(file_path) >= MAX_FILE_SIZE\n",
    "\n",
    "def adjust_content_and_write(content, file_path, add_separator=False):\n",
    "    \"\"\"Append content to the file, optionally adding a separator if the file already exists.\"\"\"\n",
    "    mode = \"a\" if os.path.exists(file_path) else \"w\"\n",
    "    with open(file_path, mode, encoding='utf-8') as f:\n",
    "        if add_separator and os.path.getsize(file_path) > 0:\n",
    "            f.write(\"\\n\\n\" + (\"-\" * 80) + \"\\n\\n\")\n",
    "        f.write(content)\n",
    "        \n",
    "def process_files_into_folders(directory_path, folder_base_path, base_filename):\n",
    "    check_create_folder(folder_base_path)\n",
    "    index = next_file_index(folder_base_path, base_filename)\n",
    "    file_path = get_file_path(folder_base_path, base_filename, index)\n",
    "    \n",
    "    for filename in sorted(os.listdir(directory_path)):\n",
    "        if filename.endswith('.py'):  # Assuming we're only interested in .txt files\n",
    "            with open(os.path.join(directory_path, filename), 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                \n",
    "                # Check if a new file is needed before writing content\n",
    "                if should_create_new_file(file_path):\n",
    "                    index += 1  # Increment to use a new file\n",
    "                    file_path = get_file_path(folder_base_path, base_filename, index)\n",
    "                \n",
    "                adjust_content_and_write(content, file_path, add_separator=True)\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"E:/LLMS/hemanth/\"\n",
    "folder_base_path = \"E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/\"\n",
    "base_filename = 'caseing'\n",
    "process_files_into_folders(directory_path, folder_base_path, base_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "MAX_FILE_SIZE = 50 * 1024 * 1024  # 50 MB in bytes\n",
    "\n",
    "def check_create_folder(folder_path):\n",
    "    \"\"\"Ensure the folder exists, create if not.\"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "def next_file_index(folder_path, base_filename):\n",
    "    \"\"\"Find the next file index that can be used.\"\"\"\n",
    "    files = [f for f in os.listdir(folder_path) if f.startswith(base_filename) and f.endswith('.txt')]\n",
    "    if not files:\n",
    "        return 1\n",
    "    highest_index = max([int(f.replace(base_filename, '').replace('.txt', '')) for f in files])\n",
    "    return highest_index + 1\n",
    "\n",
    "def get_file_path(folder_base_path, base_filename, index):\n",
    "    \"\"\"Construct a file path with the given index.\"\"\"\n",
    "    return os.path.join(folder_base_path, f\"{base_filename}{index}.txt\")\n",
    "\n",
    "def should_create_new_file(file_path):\n",
    "    \"\"\"Determine if a new file needs to be created based on the size.\"\"\"\n",
    "    return os.path.exists(file_path) and os.path.getsize(file_path) >= MAX_FILE_SIZE\n",
    "\n",
    "def adjust_content_and_write(content, file_path, add_separator=False):\n",
    "    \"\"\"Write content to the file, optionally adding a separator if the file already exists.\"\"\"\n",
    "    mode = \"a\" if os.path.exists(file_path) else \"w\"\n",
    "    with open(file_path, mode, encoding='utf-8') as f:\n",
    "        if add_separator and os.path.getsize(file_path) > 0:\n",
    "            # Simulate a 'separator' with a newline\n",
    "            f.write(\"\\n\\n\" + (\"-\" * 80) + \"\\n\\n\")\n",
    "        f.write(content)\n",
    "\n",
    "def process_files_into_folders(user_given_directory_path, target_folder_base_path, base_filename):\n",
    "    check_create_folder(target_folder_base_path)\n",
    "    index = next_file_index(target_folder_base_path, base_filename)\n",
    "    file_path = get_file_path(target_folder_base_path, base_filename, index)\n",
    "    \n",
    "    for filename in os.listdir(user_given_directory_path):\n",
    "        if filename.endswith('.txt'):  # Assuming we are only interested in .txt files\n",
    "            with open(os.path.join(user_given_directory_path, filename), 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                \n",
    "                if should_create_new_file(file_path):\n",
    "                    index += 1\n",
    "                    file_path = get_file_path(target_folder_base_path, base_filename, index)\n",
    "                \n",
    "                adjust_content_and_write(content, file_path, add_separator=True)\n",
    "\n",
    "# Example usage\n",
    "# Example usage - adjust the paths and base_filename as necessary\n",
    "directory_path = \"E:/LLMS/hemanth/\"\n",
    "folder_base_path = \"E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/\"\n",
    "base_filename = 'outputFile'\n",
    "process_files_into_folders(directory_path, folder_base_path, base_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- File: DeepSpeed.csv ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile.txt1\n",
      "-------------------------------\n",
      "\n",
      "--- File: downloading.py ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile.txt1\n",
      "-------------------------------\n",
      "\n",
      "--- File: duckduckgo.py ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile.txt1\n",
      "-------------------------------\n",
      "\n",
      "--- File: extractall.py ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile.txt1\n",
      "-------------------------------\n",
      "\n",
      "--- File: functions1.csv ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile.txt1\n",
      "-------------------------------\n",
      "\n",
      "--- File: paramiko.json ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile.txt1\n",
      "-------------------------------\n",
      "\n",
      "--- File: Pre_processing.py ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile.txt1\n",
      "-------------------------------\n",
      "\n",
      "--- File: requirements.txt ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile.txt1\n",
      "-------------------------------\n",
      "\n",
      "--- File: testing.py ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile.txt1\n",
      "-------------------------------\n",
      "\n",
      "--- File: training1.py ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile.txt1\n",
      "-------------------------------\n",
      "\n",
      "--- File: xformers.json ---\n",
      "Content written to E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/outputFile.txt1\n",
      "-------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pptx import Presentation\n",
    "import re\n",
    "\n",
    "# Constants\n",
    "MAX_FILE_SIZE = 50 * 1024 * 1024  # 50 MB in bytes\n",
    "\n",
    "def check_create_folder(folder_path):\n",
    "    \"\"\"Ensure the folder exists, create if not.\"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "def next_file_index(folder_path, base_filename):\n",
    "    \"\"\"Find the next file index that can be used.\"\"\"\n",
    "    files = [f for f in os.listdir(folder_path) if f.startswith(base_filename) and f.endswith('.txt')]\n",
    "    if not files:\n",
    "        return 1\n",
    "    highest_index = max([int(f.replace(base_filename, '').replace('.txt', '')) for f in files])\n",
    "    return highest_index + 1\n",
    "def get_file_path(folder_path, base_filename, index):\n",
    "    \"\"\"Construct a file path with the given index.\"\"\"\n",
    "    return os.path.join(folder_path, f\"{base_filename}{index}\")\n",
    "\n",
    "def should_create_new_file(file_path):\n",
    "    \"\"\"Determine if a new file needs to be created based on the size.\"\"\"\n",
    "    return os.path.exists(file_path) and os.path.getsize(file_path) >= MAX_FILE_SIZE\n",
    "\n",
    "def adjust_content_and_write(content, file_path, add_page_break=False):\n",
    "    \"\"\"Write content to the file, optionally adding a page break (separator).\"\"\"\n",
    "    with open(file_path, \"a\", encoding='utf-8') as f:\n",
    "        if add_page_break and os.path.getsize(file_path) > 0:\n",
    "            # Simulate a 'page break' with a separator\n",
    "            f.write(\"\\n\\n\" + (\"-\" * 80) + \"\\n\\n\")\n",
    "        \n",
    "        if isinstance(content, pd.DataFrame):\n",
    "            content.to_csv(f, index=False, header=not os.path.exists(file_path))\n",
    "        elif isinstance(content, list):\n",
    "            f.write('\\n'.join(content))\n",
    "        elif isinstance(content, str):\n",
    "            f.write(content)\n",
    "        else:\n",
    "            f.write(str(content))\n",
    "def process_files_into_folders(directory_path, folder_base_path, base_filename):\n",
    "    files = list_files_with_extensions(directory_path)\n",
    "    \n",
    "    if files is None:\n",
    "        return\n",
    "    \n",
    "    # Initialize file management variables\n",
    "    index = next_file_index(folder_base_path, base_filename)\n",
    "    file_path = get_file_path(folder_base_path, base_filename, index)\n",
    "    check_create_folder(folder_base_path)\n",
    "    \n",
    "    for filename in files:\n",
    "        content = read_file_content(directory_path, filename)\n",
    "        \n",
    "        if content is not None:\n",
    "            if should_create_new_file(file_path):\n",
    "                index += 1\n",
    "                file_path = get_file_path(folder_base_path, base_filename, index)\n",
    "\n",
    "            adjust_content_and_write(content, file_path)\n",
    "            \n",
    "            print(f\"--- File: {filename} ---\")\n",
    "            print(f\"Content written to {file_path}\")\n",
    "            print(\"-------------------------------\\n\")\n",
    "\n",
    "# Example usage - adjust the paths and base_filename as necessary\n",
    "directory_path = \"E:/LLMS/hemanth/Hemanth/file_operations-/\"\n",
    "folder_base_path = \"E:/LLMS/hemanth/Hemanth/file_operations-/dataset_processing/Hemanth/\"\n",
    "base_filename = 'outputFile.txt'\n",
    "process_files_into_folders(directory_path, folder_base_path, base_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.8.0)\n",
      "Collecting gradio\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/68/6c/28d4a841651b32b2e8b4bdc55cfe96785e5cce63ce2c07d039df2c0abfd3/gradio-4.21.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio-4.21.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (5.2.0)\n",
      "Requirement already satisfied: fastapi in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.104.1)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.3.1)\n",
      "Collecting gradio-client==0.12.0 (from gradio)\n",
      "  Obtaining dependency information for gradio-client==0.12.0 from https://files.pythonhosted.org/packages/cd/4d/5b430cc0fbb19b20368e9cd791700270c9551dab7234e6501b1587c414de/gradio_client-0.12.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio_client-0.12.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.25.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.20.3)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (6.1.1)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: matplotlib~=3.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (3.8.0)\n",
      "Requirement already satisfied: numpy~=1.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (1.25.2)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (3.9.10)\n",
      "Requirement already satisfied: packaging in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (2.1.1)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (10.0.1)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (2.4.1)\n",
      "Requirement already satisfied: pydub in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Collecting python-multipart>=0.0.9 (from gradio)\n",
      "  Obtaining dependency information for python-multipart>=0.0.9 from https://files.pythonhosted.org/packages/3d/47/444768600d9e0ebc82f8e347775d24aef8f6348cf00e9fa0e81910814e6d/python_multipart-0.0.9-py3-none-any.whl.metadata\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Obtaining dependency information for ruff>=0.2.2 from https://files.pythonhosted.org/packages/50/89/2d4c24fd695587bc4846ebbaabe436ada307b8e157af4c3f790c34ffef25/ruff-0.3.2-py3-none-win_amd64.whl.metadata\n",
      "  Downloading ruff-0.3.2-py3-none-win_amd64.whl.metadata (23 kB)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer[all]<1.0,>=0.9 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (4.8.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.24.0.post1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio-client==0.12.0->gradio) (2023.9.2)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio-client==0.12.0->gradio) (11.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (4.20.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.24.1->gradio) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.24.1->gradio) (3.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (3.12.4)\n",
      "Requirement already satisfied: requests in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib~=3.0->gradio) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=2.0->gradio) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=2.0->gradio) (2.10.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\roaming\\python\\python310\\site-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (13.6.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi->gradio) (0.27.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx>=0.24.1->gradio) (1.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.11.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.31.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.13.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\roaming\\python\\python310\\site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.5)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hemanthk.lap53-fjs.000\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
      "Downloading gradio-4.21.0-py3-none-any.whl (17.0 MB)\n",
      "   ---------------------------------------- 0.0/17.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/17.0 MB 1.1 MB/s eta 0:00:16\n",
      "   ---------------------------------------- 0.2/17.0 MB 2.1 MB/s eta 0:00:09\n",
      "    --------------------------------------- 0.4/17.0 MB 2.8 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.5/17.0 MB 2.8 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.6/17.0 MB 2.8 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.7/17.0 MB 2.6 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.9/17.0 MB 2.6 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 1.0/17.0 MB 2.6 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 1.2/17.0 MB 2.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.4/17.0 MB 2.9 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.5/17.0 MB 2.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.6/17.0 MB 2.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.8/17.0 MB 2.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.0/17.0 MB 2.9 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 2.2/17.0 MB 3.0 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 2.3/17.0 MB 3.0 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 2.5/17.0 MB 3.0 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 2.6/17.0 MB 3.0 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 2.8/17.0 MB 3.0 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 2.9/17.0 MB 3.0 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 3.0/17.0 MB 3.0 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 3.2/17.0 MB 3.1 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 3.4/17.0 MB 3.1 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 3.5/17.0 MB 3.1 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 3.7/17.0 MB 3.1 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 3.8/17.0 MB 3.1 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 4.0/17.0 MB 3.1 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 4.2/17.0 MB 3.1 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 4.3/17.0 MB 3.1 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 4.4/17.0 MB 3.1 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 4.6/17.0 MB 3.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 4.7/17.0 MB 3.1 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 4.9/17.0 MB 3.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 5.0/17.0 MB 3.2 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 5.2/17.0 MB 3.1 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 5.4/17.0 MB 3.2 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 5.5/17.0 MB 3.2 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 5.7/17.0 MB 3.2 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 5.8/17.0 MB 3.2 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 6.0/17.0 MB 3.2 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 6.2/17.0 MB 3.2 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 6.3/17.0 MB 3.2 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 6.5/17.0 MB 3.2 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 6.6/17.0 MB 3.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 6.9/17.0 MB 3.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 7.0/17.0 MB 3.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 7.2/17.0 MB 3.2 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 7.4/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 7.5/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 7.7/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 7.9/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 8.0/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 8.2/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 8.3/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 8.4/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 8.6/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 8.8/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 8.9/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 9.1/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 9.3/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 9.4/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 9.5/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 9.7/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 9.8/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 10.0/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 10.1/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 10.3/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 10.4/17.0 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 10.4/17.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 10.6/17.0 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.7/17.0 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.9/17.0 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 11.0/17.0 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 11.1/17.0 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 11.3/17.0 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 11.4/17.0 MB 3.2 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.5/17.0 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.7/17.0 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.9/17.0 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 12.0/17.0 MB 3.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 12.2/17.0 MB 3.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 12.3/17.0 MB 3.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.5/17.0 MB 3.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.6/17.0 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 12.8/17.0 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 12.9/17.0 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 13.1/17.0 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 13.2/17.0 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 13.4/17.0 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 13.6/17.0 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 13.7/17.0 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 13.9/17.0 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 14.0/17.0 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 14.2/17.0 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 14.3/17.0 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 14.5/17.0 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 14.7/17.0 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 14.7/17.0 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.9/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.9/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 15.1/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 15.2/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 15.4/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 15.5/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 15.7/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.9/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 16.0/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 16.2/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 16.3/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 16.5/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.6/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.8/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.9/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  17.0/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  17.0/17.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 17.0/17.0 MB 3.1 MB/s eta 0:00:00\n",
      "Downloading gradio_client-0.12.0-py3-none-any.whl (310 kB)\n",
      "   ---------------------------------------- 0.0/310.7 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 112.6/310.7 kB 6.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 276.5/310.7 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 310.7/310.7 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Downloading ruff-0.3.2-py3-none-win_amd64.whl (7.6 MB)\n",
      "   ---------------------------------------- 0.0/7.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/7.6 MB 4.3 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.3/7.6 MB 3.5 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.5/7.6 MB 3.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.6/7.6 MB 3.6 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.8/7.6 MB 3.5 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.9/7.6 MB 3.5 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.1/7.6 MB 3.5 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.3/7.6 MB 3.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.4/7.6 MB 3.5 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.6/7.6 MB 3.5 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.8/7.6 MB 3.5 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.9/7.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.1/7.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.2/7.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.4/7.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.5/7.6 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.7/7.6 MB 3.5 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.8/7.6 MB 3.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.9/7.6 MB 3.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.1/7.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.2/7.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.4/7.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.5/7.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.7/7.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.8/7.6 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.0/7.6 MB 3.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.1/7.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.3/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.4/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.5/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.7/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.8/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.0/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.1/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.3/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.5/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.6/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.8/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.9/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.1/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.2/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.3/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.5/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.7/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.9/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.0/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.1/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.3/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.4/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.6/7.6 MB 3.3 MB/s eta 0:00:00\n",
      "Installing collected packages: ruff, python-multipart, gradio-client, gradio\n",
      "  Attempting uninstall: ruff\n",
      "    Found existing installation: ruff 0.1.15\n",
      "    Uninstalling ruff-0.1.15:\n",
      "      Successfully uninstalled ruff-0.1.15\n",
      "  Attempting uninstall: python-multipart\n",
      "    Found existing installation: python-multipart 0.0.6\n",
      "    Uninstalling python-multipart-0.0.6:\n",
      "      Successfully uninstalled python-multipart-0.0.6\n",
      "  Attempting uninstall: gradio-client\n",
      "    Found existing installation: gradio_client 0.7.1\n",
      "    Uninstalling gradio_client-0.7.1:\n",
      "      Successfully uninstalled gradio_client-0.7.1\n",
      "  Attempting uninstall: gradio\n",
      "    Found existing installation: gradio 4.8.0\n",
      "    Uninstalling gradio-4.8.0:\n",
      "      Successfully uninstalled gradio-4.8.0\n",
      "Successfully installed gradio-4.21.0 gradio-client-0.12.0 python-multipart-0.0.9 ruff-0.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ragatouille 0.0.7.post2 requires ruff<0.2.0,>=0.1.9, but you have ruff 0.3.2 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gradio gtts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from IPython.display import Javascript\n",
    "from google.colab import output\n",
    "from base64 import b64decode\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from gtts import gTTS\n",
    "\n",
    "# JavaScript code to record audio from the user's microphone for a specified amount of time\n",
    "RECORD = \"\"\"\n",
    "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
    "const b2text = blob => new Promise(resolve => {\n",
    "  const reader = new FileReader()\n",
    "  reader.onloadend = e => resolve(e.srcElement.result)\n",
    "  reader.readAsDataURL(blob)\n",
    "})\n",
    "var record = time => new Promise(async resolve => {\n",
    "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
    "  recorder = new MediaRecorder(stream)\n",
    "  chunks = []\n",
    "  recorder.ondataavailable = e => chunks.push(e.data)\n",
    "  recorder.start()\n",
    "  await sleep(time)\n",
    "  recorder.onstop = async ()=>{\n",
    "    blob = new Blob(chunks)\n",
    "    text = await b2text(blob)\n",
    "    resolve(text)\n",
    "  }\n",
    "  recorder.stop()\n",
    "})\n",
    "\"\"\"\n",
    "\n",
    "def record(sec=60):\n",
    "  \"\"\"\n",
    "  This function records audio from the user's microphone for a specified amount of time.\n",
    "  The audio is saved as a .wav file.\n",
    "\n",
    "  Input:\n",
    "  sec: int, the number of seconds to record\n",
    "\n",
    "  Output:\n",
    "  'audio.wav': str, the filename of the recorded audio\n",
    "  \"\"\"\n",
    "  print('Recording Your Voice')\n",
    "  display(Javascript(RECORD))\n",
    "  s = output.eval_js('record(%d)' % (sec*1000))\n",
    "  b = b64decode(s.split(',')[1])\n",
    "  with open('audio.wav','wb') as f:\n",
    "    f.write(b)\n",
    "  print('Stopped Recording')\n",
    "  return 'audio.wav'\n",
    "\n",
    "def speech_to_text(audio_data):\n",
    "  \"\"\"\n",
    "  This function converts speech in an audio file to text using a pre-trained model from Hugging Face.\n",
    "\n",
    "  Input:\n",
    "  audio_data: str, the filename of the audio file\n",
    "\n",
    "  Output:\n",
    "  result[\"text\"]: str, the transcribed text\n",
    "  \"\"\"\n",
    "  device = \"cpu\"\n",
    "  torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "  model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "  model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "      model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=False, use_safetensors=True\n",
    "  )\n",
    "  model.to(device)\n",
    "\n",
    "  processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "  pipe = pipeline(\n",
    "      \"automatic-speech-recognition\",\n",
    "      model=model,\n",
    "      tokenizer=processor.tokenizer,\n",
    "      feature_extractor=processor.feature_extractor,\n",
    "      max_new_tokens=128,\n",
    "      chunk_length_s=30,\n",
    "      batch_size=16,\n",
    "      return_timestamps=True,\n",
    "      torch_dtype=torch_dtype,\n",
    "      device=device,\n",
    "  )\n",
    "\n",
    "  result = pipe(audio_data)\n",
    "  return result[\"text\"]\n",
    "\n",
    "def text_to_speech(text):\n",
    "  \"\"\"\n",
    "  This function converts text to speech using the gTTS library.\n",
    "  The speech is saved as an .mp3 file.\n",
    "\n",
    "  Input:\n",
    "  text: str, the text to convert to speech\n",
    "\n",
    "  Output:\n",
    "  filename: str, the filename of the speech audio\n",
    "  \"\"\"\n",
    "  tts = gTTS(text=text, lang='en')\n",
    "  filename = \"speech.mp3\"\n",
    "  tts.save(filename)\n",
    "  return filename\n",
    "\n",
    "def get_answer(user_query):\n",
    "  \"\"\"\n",
    "  This function takes a user query in the form of text or audio and returns an answer in the form of text or audio.\n",
    "\n",
    "  Input:\n",
    "  user_query: str or audio file, the user's query\n",
    "\n",
    "  Output:\n",
    "  answer or audio_answer: str or audio file, the answer to the user's query\n",
    "  \"\"\"\n",
    "  if isinstance(user_query, str):\n",
    "    # Text input\n",
    "    retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)\n",
    "    print(user_query)\n",
    "  else:\n",
    "    # Audio input\n",
    "    audio_file = record()\n",
    "    user_query = speech_to_text(audio_file)\n",
    "    retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)\n",
    "\n",
    "  RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
    "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
    "  )\n",
    "  retrieved_docs_text = [doc.page_content for doc in retrieved_docs]\n",
    "  context = \"\\nExtracted documents:\\n\"\n",
    "  context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)])\n",
    "  final_prompt = RAG_PROMPT_TEMPLATE.format(question=user_query, context=context)\n",
    "\n",
    "  answer = READER_LLM(final_prompt)[0][\"generated_text\"]\n",
    "\n",
    "  output_format = gr.Radio([\"Text\", \"Audio\"], label=\"Output Format\", value=\"Text\")\n",
    "  if output_format == \"Audio\":\n",
    "    audio_answer = text_to_speech(answer)\n",
    "    return audio_answer\n",
    "  else:\n",
    "    return answer\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=get_answer,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=7, label=\"Enter your text query\"),\n",
    "        gr.Audio(label=\"Record your audio query\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Answer\"),\n",
    "        gr.Audio(label=\"Audio Answer\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

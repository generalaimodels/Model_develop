{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Retrieve the full argument specification for the TrainingArguments initializer\n",
    "args_spec = inspect.getfullargspec(TrainingArguments.__init__)\n",
    "\n",
    "print(\"Arguments for TrainingArguments:\")\n",
    "# Exclude 'self' from the arguments list as it's not an actual parameter\n",
    "for arg in args_spec.args[1:]:  # args[0] is 'self', which we skip\n",
    "    print(arg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MAX_NEW_TOKENS = 128\n",
    "model_name = 'decapoda-research/llama-7b-hf'\n",
    "\n",
    "text = 'Hamburg is in which country?\\n'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)\n",
    "max_memory = f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_name,\n",
    "  device_map='auto',\n",
    "  load_in_8bit=True,\n",
    "  max_memory=max_memory\n",
    ")\n",
    "generated_ids = model.generate(input_ids, max_length=MAX_NEW_TOKENS)\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments for TrainingArguments:\n",
      "output_dir\n",
      "overwrite_output_dir\n",
      "do_train\n",
      "do_eval\n",
      "do_predict\n",
      "evaluation_strategy\n",
      "prediction_loss_only\n",
      "per_device_train_batch_size\n",
      "per_device_eval_batch_size\n",
      "per_gpu_train_batch_size\n",
      "per_gpu_eval_batch_size\n",
      "gradient_accumulation_steps\n",
      "eval_accumulation_steps\n",
      "eval_delay\n",
      "learning_rate\n",
      "weight_decay\n",
      "adam_beta1\n",
      "adam_beta2\n",
      "adam_epsilon\n",
      "max_grad_norm\n",
      "num_train_epochs\n",
      "max_steps\n",
      "lr_scheduler_type\n",
      "lr_scheduler_kwargs\n",
      "warmup_ratio\n",
      "warmup_steps\n",
      "log_level\n",
      "log_level_replica\n",
      "log_on_each_node\n",
      "logging_dir\n",
      "logging_strategy\n",
      "logging_first_step\n",
      "logging_steps\n",
      "logging_nan_inf_filter\n",
      "save_strategy\n",
      "save_steps\n",
      "save_total_limit\n",
      "save_safetensors\n",
      "save_on_each_node\n",
      "save_only_model\n",
      "no_cuda\n",
      "use_cpu\n",
      "use_mps_device\n",
      "seed\n",
      "data_seed\n",
      "jit_mode_eval\n",
      "use_ipex\n",
      "bf16\n",
      "fp16\n",
      "fp16_opt_level\n",
      "half_precision_backend\n",
      "bf16_full_eval\n",
      "fp16_full_eval\n",
      "tf32\n",
      "local_rank\n",
      "ddp_backend\n",
      "tpu_num_cores\n",
      "tpu_metrics_debug\n",
      "debug\n",
      "dataloader_drop_last\n",
      "eval_steps\n",
      "dataloader_num_workers\n",
      "past_index\n",
      "run_name\n",
      "disable_tqdm\n",
      "remove_unused_columns\n",
      "label_names\n",
      "load_best_model_at_end\n",
      "metric_for_best_model\n",
      "greater_is_better\n",
      "ignore_data_skip\n",
      "fsdp\n",
      "fsdp_min_num_params\n",
      "fsdp_config\n",
      "fsdp_transformer_layer_cls_to_wrap\n",
      "deepspeed\n",
      "label_smoothing_factor\n",
      "optim\n",
      "optim_args\n",
      "adafactor\n",
      "group_by_length\n",
      "length_column_name\n",
      "report_to\n",
      "ddp_find_unused_parameters\n",
      "ddp_bucket_cap_mb\n",
      "ddp_broadcast_buffers\n",
      "dataloader_pin_memory\n",
      "dataloader_persistent_workers\n",
      "skip_memory_metrics\n",
      "use_legacy_prediction_loop\n",
      "push_to_hub\n",
      "resume_from_checkpoint\n",
      "hub_model_id\n",
      "hub_strategy\n",
      "hub_token\n",
      "hub_private_repo\n",
      "hub_always_push\n",
      "gradient_checkpointing\n",
      "gradient_checkpointing_kwargs\n",
      "include_inputs_for_metrics\n",
      "fp16_backend\n",
      "push_to_hub_model_id\n",
      "push_to_hub_organization\n",
      "push_to_hub_token\n",
      "mp_parameters\n",
      "auto_find_batch_size\n",
      "full_determinism\n",
      "torchdynamo\n",
      "ray_scope\n",
      "ddp_timeout\n",
      "torch_compile\n",
      "torch_compile_backend\n",
      "torch_compile_mode\n",
      "dispatch_batches\n",
      "split_batches\n",
      "include_tokens_per_second\n",
      "include_num_input_tokens_seen\n",
      "neftune_noise_alpha\n",
      "output_dir\n",
      "overwrite_output_dir\n",
      "do_train (default: False)\n",
      "do_eval (default: False)\n",
      "do_predict (default: False)\n",
      "evaluation_strategy (default: False)\n",
      "prediction_loss_only (default: no)\n",
      "per_device_train_batch_size (default: False)\n",
      "per_device_eval_batch_size (default: 8)\n",
      "per_gpu_train_batch_size (default: 8)\n",
      "per_gpu_eval_batch_size (default: None)\n",
      "gradient_accumulation_steps (default: None)\n",
      "eval_accumulation_steps (default: 1)\n",
      "eval_delay (default: None)\n",
      "learning_rate (default: 0)\n",
      "weight_decay (default: 5e-05)\n",
      "adam_beta1 (default: 0.0)\n",
      "adam_beta2 (default: 0.9)\n",
      "adam_epsilon (default: 0.999)\n",
      "max_grad_norm (default: 1e-08)\n",
      "num_train_epochs (default: 1.0)\n",
      "max_steps (default: 3.0)\n",
      "lr_scheduler_type (default: -1)\n",
      "lr_scheduler_kwargs (default: linear)\n",
      "warmup_ratio (default: <factory>)\n",
      "warmup_steps (default: 0.0)\n",
      "log_level (default: 0)\n",
      "log_level_replica (default: passive)\n",
      "log_on_each_node (default: warning)\n",
      "logging_dir (default: True)\n",
      "logging_strategy (default: None)\n",
      "logging_first_step (default: steps)\n",
      "logging_steps (default: False)\n",
      "logging_nan_inf_filter (default: 500)\n",
      "save_strategy (default: True)\n",
      "save_steps (default: steps)\n",
      "save_total_limit (default: 500)\n",
      "save_safetensors (default: None)\n",
      "save_on_each_node (default: True)\n",
      "save_only_model (default: False)\n",
      "no_cuda (default: False)\n",
      "use_cpu (default: False)\n",
      "use_mps_device (default: False)\n",
      "seed (default: False)\n",
      "data_seed (default: 42)\n",
      "jit_mode_eval (default: None)\n",
      "use_ipex (default: False)\n",
      "bf16 (default: False)\n",
      "fp16 (default: False)\n",
      "fp16_opt_level (default: False)\n",
      "half_precision_backend (default: O1)\n",
      "bf16_full_eval (default: auto)\n",
      "fp16_full_eval (default: False)\n",
      "tf32 (default: False)\n",
      "local_rank (default: None)\n",
      "ddp_backend (default: -1)\n",
      "tpu_num_cores (default: None)\n",
      "tpu_metrics_debug (default: None)\n",
      "debug (default: False)\n",
      "dataloader_drop_last (default: )\n",
      "eval_steps (default: False)\n",
      "dataloader_num_workers (default: None)\n",
      "past_index (default: 0)\n",
      "run_name (default: -1)\n",
      "disable_tqdm (default: None)\n",
      "remove_unused_columns (default: None)\n",
      "label_names (default: True)\n",
      "load_best_model_at_end (default: None)\n",
      "metric_for_best_model (default: False)\n",
      "greater_is_better (default: None)\n",
      "ignore_data_skip (default: None)\n",
      "fsdp (default: False)\n",
      "fsdp_min_num_params (default: )\n",
      "fsdp_config (default: 0)\n",
      "fsdp_transformer_layer_cls_to_wrap (default: None)\n",
      "deepspeed (default: None)\n",
      "label_smoothing_factor (default: None)\n",
      "optim (default: 0.0)\n",
      "optim_args (default: adamw_torch)\n",
      "adafactor (default: None)\n",
      "group_by_length (default: False)\n",
      "length_column_name (default: False)\n",
      "report_to (default: length)\n",
      "ddp_find_unused_parameters (default: None)\n",
      "ddp_bucket_cap_mb (default: None)\n",
      "ddp_broadcast_buffers (default: None)\n",
      "dataloader_pin_memory (default: None)\n",
      "dataloader_persistent_workers (default: True)\n",
      "skip_memory_metrics (default: False)\n",
      "use_legacy_prediction_loop (default: True)\n",
      "push_to_hub (default: False)\n",
      "resume_from_checkpoint (default: False)\n",
      "hub_model_id (default: None)\n",
      "hub_strategy (default: None)\n",
      "hub_token (default: every_save)\n",
      "hub_private_repo (default: None)\n",
      "hub_always_push (default: False)\n",
      "gradient_checkpointing (default: False)\n",
      "gradient_checkpointing_kwargs (default: False)\n",
      "include_inputs_for_metrics (default: None)\n",
      "fp16_backend (default: False)\n",
      "push_to_hub_model_id (default: auto)\n",
      "push_to_hub_organization (default: None)\n",
      "push_to_hub_token (default: None)\n",
      "mp_parameters (default: None)\n",
      "auto_find_batch_size (default: )\n",
      "full_determinism (default: False)\n",
      "torchdynamo (default: False)\n",
      "ray_scope (default: None)\n",
      "ddp_timeout (default: last)\n",
      "torch_compile (default: 1800)\n",
      "torch_compile_backend (default: False)\n",
      "torch_compile_mode (default: None)\n",
      "dispatch_batches (default: None)\n",
      "split_batches (default: None)\n",
      "include_tokens_per_second (default: False)\n",
      "include_num_input_tokens_seen (default: False)\n",
      "neftune_noise_alpha (default: False)\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Retrieve the full argument specification for the TrainingArguments initializer\n",
    "args_spec = inspect.getfullargspec(TrainingArguments.__init__)\n",
    "\n",
    "print(\"Arguments for TrainingArguments:\")\n",
    "# Exclude 'self' from the arguments list as it's not an actual parameter\n",
    "for arg in args_spec.args[1:]:  # args[0] is 'self', which we skip\n",
    "    print(arg)\n",
    "\n",
    "# If you want to include default values and other metadata, you could do:\n",
    "if args_spec.defaults:\n",
    "    # The last 'len(defaults)' arguments have default values\n",
    "    defaults_offset = len(args_spec.args) - len(args_spec.defaults)\n",
    "    for idx, arg in enumerate(args_spec.args[1:]):\n",
    "        if idx >= defaults_offset:\n",
    "            default = args_spec.defaults[idx - defaults_offset]\n",
    "            print(f\"{arg} (default: {default})\")\n",
    "        else:\n",
    "            print(arg)\n",
    "else:\n",
    "    for arg in args_spec.args[1:]:\n",
    "        print(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import torch \n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_int8_training,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from dataclass_csv import DataclassReader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from enum import Enum\n",
    "from datasets import load_dataset,DatasetDict\n",
    "from typing import Union , Dict,Optional,Any,List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_data_loader(input: Union[str, Dict[str, str]], format: Optional[str] = None, split_ratios: Optional[Dict[str, float]] = None) -> Optional[DatasetDict]:\n",
    "    \"\"\"\n",
    "    Loads a dataset from a given input path or dictionary specifying file paths and splits it.\n",
    "\n",
    "    :param input: A string representing the dataset name or directory, or a dictionary containing file paths.\n",
    "    :param format: The format of the dataset if loading from a file (e.g., 'csv' or 'json').\n",
    "    :param split_ratios: A dictionary with keys 'train', 'test', and 'eval' containing split ratios.\n",
    "    :return: A loaded and split dataset or None in case of failure.\n",
    "    \"\"\"\n",
    "    if split_ratios is None:\n",
    "        split_ratios = {'train': 0.8, 'test': 0.1, 'eval': 0.1}\n",
    "\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        if isinstance(input, dict) and format in ['csv', 'json']:\n",
    "            dataset = load_dataset(format, data_files=input)\n",
    "        elif isinstance(input, str) and format == 'text':\n",
    "            dataset = load_dataset(format, data_dir=input)\n",
    "        elif isinstance(input, str) and format is None:\n",
    "            dataset = load_dataset(input)\n",
    "        else:\n",
    "            warnings.warn(\"Invalid input or format. Please provide a valid dataset name, directory, or file paths.\")\n",
    "            return None\n",
    "    except FileNotFoundError as e:\n",
    "        warnings.warn(str(e))\n",
    "        return None\n",
    "\n",
    "    # Split the dataset\n",
    "    if dataset:\n",
    "        split_dataset = dataset['train'].train_test_split(test_size=split_ratios['test'] + split_ratios['eval'])\n",
    "        test_eval_dataset = split_dataset['test'].train_test_split(test_size=split_ratios['eval'] / (split_ratios['test'] + split_ratios['eval']))\n",
    "        dataset = DatasetDict({\n",
    "            'train': split_dataset['train'],\n",
    "            'test': test_eval_dataset['train'],\n",
    "            'eval': test_eval_dataset['test']\n",
    "        })\n",
    "\n",
    "    print(\"Splits: \", dataset.keys())\n",
    "    print(\"Columns: \", {split: dataset[split].column_names for split in dataset.keys()})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "# The URL of the PDF file\n",
    "url = \"https://arxiv.org/pdf/2401.07324v1.pdf\"\n",
    "# The name of the output HTML file\n",
    "output = \"output.html\"\n",
    "# Run the pdf2htmlEX tool with the URL and the output file name as arguments\n",
    "subprocess.run([\"pdf2htmlEX\", url, output])\n",
    "# Open the output HTML file in the default browser\n",
    "import webbrowser\n",
    "webbrowser.open(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(tokenizer_name_or_path: str = 'gpt2') -> AutoTokenizer:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    if tokenizer.bos_token_id is None:\n",
    "        tokenizer.bos_token_id = tokenizer.pad_token_id\n",
    "    if tokenizer.eos_token_id is None:\n",
    "        tokenizer.eos_token_id = tokenizer.pad_token_id\n",
    "    if tokenizer.unk_token_id is None:\n",
    "        tokenizer.unk_token_id = tokenizer.pad_token_id\n",
    "    if tokenizer.sep_token_id is None:\n",
    "        tokenizer.sep_token_id = tokenizer.pad_token_id\n",
    "    if tokenizer.cls_token_id is None:\n",
    "        tokenizer.cls_token_id = tokenizer.pad_token_id\n",
    "    if tokenizer.mask_token_id is None:\n",
    "        tokenizer.mask_token_id = tokenizer.pad_token_id\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path=''\n",
    "TEXT_COLUMN=''\n",
    "LABEL_COLUMN=''\n",
    "MAX_LENGTH=''\n",
    "BATCH_SIZE=16\n",
    "tokenizer_name_or_path=''\n",
    "tokenizer=create_tokenizer(tokenizer_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pre_Preocessing_function(Dataset:Union[str, Dict[str, str],List[str,str]],Columns:List[str],tokenizer:AutoTokenizer):\n",
    "    COLUMNS=Dataset['train'].column_names\n",
    "    BATCH_SIZE=len(Dataset['train'][COLUMNS[0]])\n",
    "    if Columns==None:\n",
    "        for column in  COLUMNS:\n",
    "            inputs=[f\" {column} : {x} : \" for x in Dataset['train'][column]]\n",
    "    else:\n",
    "        for column in Columns:\n",
    "            inputs=[f\" {column} : {x} : \" for x in Dataset['train'][column]]\n",
    "    targets = [str(x) for x in Dataset['train'][LABEL_COLUMN]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(targets, add_special_tokens=False)  # don't add bos token because we concatenate with inputs\n",
    "    for i in range(BATCH_SIZE):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "    for i in range(BATCH_SIZE):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            MAX_LENGTH - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (MAX_LENGTH - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        labels[\"input_ids\"][i] = [-100] * (MAX_LENGTH - len(sample_input_ids)) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:MAX_LENGTH])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:MAX_LENGTH])\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:MAX_LENGTH])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=advanced_data_loader('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_datasets = dataset.map(\n",
    "    Pre_Preocessing_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "train_dataset = processed_datasets[\"train\"]\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=BATCH_SIZE, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "model=AutoModelForCausalLM.from_pretrained(model_name_or_path, quantization_config=bnb_config)\n",
    "peft_config =LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=64,\n",
    "    lora_alpha=256,\n",
    "    lora_dropout=0.2,\n",
    "    fan_in_fan_out=True,\n",
    "    bias=\"all\",\n",
    "    modules_to_save=[\"classifier/score\", \"pooler\"],\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    # target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    # modules_to_save=[\"lm_head\"],\n",
    "\n",
    "    # layers_to_transform=[2, 4, 6],\n",
    "    # layers_pattern=\"custom_pattern\",\n",
    "    # rank_pattern={\n",
    "    #     \"model.decoder.layers.0.encoder_attn.k_proj\": 16,\n",
    "    #     \"model.decoder.layers.2.encoder_attn.k_proj\": 32\n",
    "    # },\n",
    "    # alpha_pattern={\n",
    "    #     \"model.decoder.layers.0.encoder_attn.k_proj\": 64,\n",
    "    #     \"model.decoder.layers.4.encoder_attn.k_proj\": 128\n",
    "    # },\n",
    "    # megatron_config={\n",
    "    #     \"hidden_size\": 4096,\n",
    "    #     \"num_attention_heads\": 32,\n",
    "    #     \"num_layers\": 24\n",
    "    # },\n",
    "    # megatron_core=\"custom_megatron_core\",\n",
    "    # loftq_config=LoraConfig(\n",
    "    #     quantization_bits=8,\n",
    "    #     quantization_range=128\n",
    "    # )\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

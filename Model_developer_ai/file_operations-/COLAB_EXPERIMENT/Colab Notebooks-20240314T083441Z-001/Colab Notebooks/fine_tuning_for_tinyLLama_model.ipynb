{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b559c2eabf8740d7bb645087518742aa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2a7fcb7145f94f56aea309777c60b8d7","IPY_MODEL_14b1e97e882547a6858e4e7b6e89091b","IPY_MODEL_e9223f0eb99f446f9695970fe64eb484"],"layout":"IPY_MODEL_3700b526936d425eb28c896f786daf83"}},"2a7fcb7145f94f56aea309777c60b8d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb9b87ae5b30417f89b0de4edf7db630","placeholder":"​","style":"IPY_MODEL_58b6b2710d07426eb648a23e773db698","value":"Running tokenizer on dataset: 100%"}},"14b1e97e882547a6858e4e7b6e89091b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3756610b859b470bb73175f6cc0dc172","max":527,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1e5fb023ebef492e9f04c0d4e7dfcbd7","value":527}},"e9223f0eb99f446f9695970fe64eb484":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0fe716f3244417facc4e5d51264fa6d","placeholder":"​","style":"IPY_MODEL_d5c9f9dc779b402eb90d3e9dbc261d9d","value":" 527/527 [00:01&lt;00:00, 333.64 examples/s]"}},"3700b526936d425eb28c896f786daf83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb9b87ae5b30417f89b0de4edf7db630":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58b6b2710d07426eb648a23e773db698":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3756610b859b470bb73175f6cc0dc172":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e5fb023ebef492e9f04c0d4e7dfcbd7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a0fe716f3244417facc4e5d51264fa6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5c9f9dc779b402eb90d3e9dbc261d9d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4fcb2937a8c848e38e3dc1f1108b7298":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b8b3390e0e74d1eaff8c7dc4ac271bf","IPY_MODEL_605b4b73e7ae42c19b3edd0963538171","IPY_MODEL_29b960c4905d4f8fafab8c26f3c8a6cc"],"layout":"IPY_MODEL_934b1ec8e0384a9799a610565d8c0d67"}},"6b8b3390e0e74d1eaff8c7dc4ac271bf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_26c7097c93ec4cdab826d246513e3157","placeholder":"​","style":"IPY_MODEL_dc35a4519d724f9e8aeabd116882a805","value":"Running tokenizer on dataset: 100%"}},"605b4b73e7ae42c19b3edd0963538171":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4a0132821e4429d8a733d857db9f006","max":66,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2e282017dbb84378b8e80cfbd2169433","value":66}},"29b960c4905d4f8fafab8c26f3c8a6cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bfb4c995c3242fb8ecda4b16e7a4562","placeholder":"​","style":"IPY_MODEL_4582c044597945ab9fdc6feab3664537","value":" 66/66 [00:00&lt;00:00, 480.19 examples/s]"}},"934b1ec8e0384a9799a610565d8c0d67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26c7097c93ec4cdab826d246513e3157":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc35a4519d724f9e8aeabd116882a805":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c4a0132821e4429d8a733d857db9f006":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e282017dbb84378b8e80cfbd2169433":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2bfb4c995c3242fb8ecda4b16e7a4562":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4582c044597945ab9fdc6feab3664537":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b34fb3076c942789e73aadab2bac3e2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_480409d89b8443409d8e0350e81d548e","IPY_MODEL_df34dc6b58264a66b0038c2c12d31684","IPY_MODEL_3eb020593788464893c5dab17d8599ff"],"layout":"IPY_MODEL_a7cf01c1e43748b0a0a1d5e5366ad7de"}},"480409d89b8443409d8e0350e81d548e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e81ae9a228547d48312f6bba23bdc5f","placeholder":"​","style":"IPY_MODEL_4713d08b654a4727baf69d0855b9f6eb","value":"Running tokenizer on dataset: 100%"}},"df34dc6b58264a66b0038c2c12d31684":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4eeba2225b2048488cb49bc9d39ef18c","max":66,"min":0,"orientation":"horizontal","style":"IPY_MODEL_17f7277528e048468fd19a95fa272052","value":66}},"3eb020593788464893c5dab17d8599ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d381aa2c0ab8456d97b105f23387250e","placeholder":"​","style":"IPY_MODEL_58184c83ba1c46a0b56d39c24153ed43","value":" 66/66 [00:00&lt;00:00, 571.87 examples/s]"}},"a7cf01c1e43748b0a0a1d5e5366ad7de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e81ae9a228547d48312f6bba23bdc5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4713d08b654a4727baf69d0855b9f6eb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4eeba2225b2048488cb49bc9d39ef18c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17f7277528e048468fd19a95fa272052":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d381aa2c0ab8456d97b105f23387250e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58184c83ba1c46a0b56d39c24153ed43":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6b481374f5a04b488edbd367676f38e0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_002fa47e19dc427db748b057d6df5911","IPY_MODEL_707aa36878a441b2a29c080c93a8a7f4","IPY_MODEL_991dff78393448048bf8bb58598b535b"],"layout":"IPY_MODEL_9080332c9b3644aeaf9093771465b19d"}},"002fa47e19dc427db748b057d6df5911":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f1e6fef768348c6a316d691f1229b3c","placeholder":"​","style":"IPY_MODEL_81a180e15b194ac6ae15155483db2631","value":"Running tokenizer on dataset: 100%"}},"707aa36878a441b2a29c080c93a8a7f4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c1ec3f7acd54857a3a280f93da54820","max":66,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a47a87904d594e3bb86178bd2845ff41","value":66}},"991dff78393448048bf8bb58598b535b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1735e1db0b70483f86220fb0948da6b3","placeholder":"​","style":"IPY_MODEL_fafebac6e74b482f8fb68c1aaec86f23","value":" 66/66 [00:00&lt;00:00, 882.24 examples/s]"}},"9080332c9b3644aeaf9093771465b19d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f1e6fef768348c6a316d691f1229b3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81a180e15b194ac6ae15155483db2631":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c1ec3f7acd54857a3a280f93da54820":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a47a87904d594e3bb86178bd2845ff41":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1735e1db0b70483f86220fb0948da6b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fafebac6e74b482f8fb68c1aaec86f23":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install -q -U transformers peft tqdm datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LCcoWR72AlcI","executionInfo":{"status":"ok","timestamp":1706854724473,"user_tz":-330,"elapsed":16567,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"71c278a2-2c0f-469f-9716-de2c8cc1cadb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import locale\n","locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"jk64QmHCs9RT","executionInfo":{"status":"ok","timestamp":1706858671864,"user_tz":-330,"elapsed":620,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"57099295-30f7-42d8-cf20-90640ce1abfb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'en_US.UTF-8'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["!cat /content/drive/MyDrive/Adversarial_chatbot_dataset.csv"],"metadata":{"id":"l5uLbGq4d8qj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Union,List, Dict, Optional\n","from datasets import load_dataset, DatasetDict\n","from transformers import AutoTokenizer\n","import multiprocessing\n","def advanced_data_loader(input: Union[str, Dict[str, str]], format: Optional[str] = None, split_ratios: Optional[Dict[str, float]] = None) -> Optional[DatasetDict]:\n","    \"\"\"\n","    Loads a dataset from a given input path or dictionary specifying file paths and splits it.\n","\n","    :param input: A string representing the dataset name or directory, or a dictionary containing file paths.\n","    :param format: The format of the dataset if loading from a file (e.g., 'csv' or 'json').\n","    :param split_ratios: A dictionary with keys 'train', 'test', and 'eval' containing split ratios.\n","    :return: A loaded and split dataset or None in case of failure.\n","    \"\"\"\n","    if split_ratios is None:\n","        split_ratios = {'train': 0.8, 'test': 0.1, 'eval': 0.1}\n","\n","    try:\n","        # Load the dataset\n","        if isinstance(input, dict) and format in ['csv', 'json']:\n","            dataset = load_dataset(format, data_files=input)\n","        elif isinstance(input, str) and format == 'text':\n","            dataset = load_dataset(format, data_dir=input)\n","        elif isinstance(input, str) and format is None:\n","            dataset = load_dataset(input)\n","        else:\n","            warnings.warn(\"Invalid input or format. Please provide a valid dataset name, directory, or file paths.\")\n","            return None\n","    except FileNotFoundError as e:\n","        warnings.warn(str(e))\n","        return None\n","\n","    # Split the dataset\n","    if dataset:\n","        split_dataset = dataset['train'].train_test_split(test_size=split_ratios['test'] + split_ratios['eval'])\n","        test_eval_dataset = split_dataset['test'].train_test_split(test_size=split_ratios['eval'] / (split_ratios['test'] + split_ratios['eval']))\n","        dataset = DatasetDict({\n","            'train': split_dataset['train'],\n","            'test': test_eval_dataset['train'],\n","            'eval': test_eval_dataset['test']\n","        })\n","\n","    print(\"Splits: \", dataset.keys())\n","    print(\"Columns: \", {split: dataset[split].column_names for split in dataset.keys()})\n","    return dataset"],"metadata":{"id":"FAR2tNJrfl2o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM\n","from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n","import torch\n","from datasets import load_dataset\n","import os\n","from transformers import AutoTokenizer\n","from torch.utils.data import DataLoader\n","from transformers import default_data_collator, get_linear_schedule_with_warmup\n","from tqdm import tqdm\n","from datasets import load_dataset\n","\n","from peft import PeftModel, PeftConfig\n","\n","device = \"cuda\"\n","model_name_or_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","tokenizer_name_or_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","text_column = \"output\"\n","label_column = \"input\"\n","max_length = 512\n","lr = 3e-2\n","num_epochs = 3\n","batch_size = 1\n","dataset_name = \"twitter_complaints\"\n","\n","\n","\n","\n","peft_config = PromptTuningConfig(\n","    task_type=TaskType.CAUSAL_LM,\n","    prompt_tuning_init=PromptTuningInit.TEXT,\n","    num_virtual_tokens=8,\n","    prompt_tuning_init_text=\"Classify if the tweet is a complaint or not:\",\n","    tokenizer_name_or_path=model_name_or_path,\n",")\n","\n","checkpoint_name = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}_v1.pt\".replace(\n","    \"/\", \"_\"\n",")\n","\n","dataset=advanced_data_loader({'train': '/content/drive/MyDrive/Adversarial_chatbot_dataset.csv', 'test': '/content/drive/MyDrive/Adversarial_chatbot_dataset.csv'}, 'csv')\n","# classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n","# dataset = dataset.map(\n","#     lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n","#     batched=True,\n","#     num_proc=1,\n","# )\n","\n","# data preprocessing\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","# target_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes])\n","\n","\n","def preprocess_function(examples):\n","    batch_size = len(examples[text_column])\n","    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n","    targets = [str(x) for x in examples[label_column]]\n","    model_inputs = tokenizer(inputs)\n","    labels = tokenizer(targets, add_special_tokens=False)  # don't add bos token because we concatenate with inputs\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n","        # print(i, sample_input_ids, label_input_ids)\n","        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n","        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n","        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n","    # print(model_inputs)\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        label_input_ids = labels[\"input_ids\"][i]\n","        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n","            max_length - len(sample_input_ids)\n","        ) + sample_input_ids\n","        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n","            \"attention_mask\"\n","        ][i]\n","        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n","        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n","        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n","        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","\n","processed_datasets = dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    num_proc=1,\n","    remove_columns=dataset[\"train\"].column_names,\n","    load_from_cache_file=False,\n","    desc=\"Running tokenizer on dataset\",\n",")\n","\n","train_dataset = processed_datasets[\"train\"]\n","eval_dataset = processed_datasets[\"train\"]\n","\n","\n","train_dataloader = DataLoader(\n","    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",")\n","eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n","\n","def test_preprocess_function(examples):\n","    batch_size = len(examples[text_column])\n","    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n","    model_inputs = tokenizer(inputs)\n","    # print(model_inputs)\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n","            max_length - len(sample_input_ids)\n","        ) + sample_input_ids\n","        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n","            \"attention_mask\"\n","        ][i]\n","        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n","        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n","    return model_inputs\n","\n","\n","test_dataset = dataset[\"test\"].map(\n","    test_preprocess_function,\n","    batched=True,\n","    num_proc=1,\n","    remove_columns=dataset[\"train\"].column_names,\n","    load_from_cache_file=False,\n","    desc=\"Running tokenizer on dataset\",\n",")\n","\n","test_dataloader = DataLoader(test_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n","\n","# creating model\n","model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n","model = get_peft_model(model, peft_config)\n","model.print_trainable_parameters()\n","\n","# model\n","# optimizer and lr scheduler\n","optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","lr_scheduler = get_linear_schedule_with_warmup(\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=(len(train_dataloader) * num_epochs),\n",")\n","\n","# training and evaluation\n","model = model.to(device)\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for step, batch in enumerate(tqdm(train_dataloader)):\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        #         print(batch)\n","        #         print(batch[\"input_ids\"].shape)\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        total_loss += loss.detach().float()\n","        loss.backward()\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","\n","    model.eval()\n","    eval_loss = 0\n","    eval_preds = []\n","    for step, batch in enumerate(tqdm(eval_dataloader)):\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","        loss = outputs.loss\n","        eval_loss += loss.detach().float()\n","        eval_preds.extend(\n","            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n","        )\n","\n","    eval_epoch_loss = eval_loss / len(eval_dataloader)\n","    eval_ppl = torch.exp(eval_epoch_loss)\n","    train_epoch_loss = total_loss / len(train_dataloader)\n","    train_ppl = torch.exp(train_epoch_loss)\n","    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n","\n","\n","model.eval()\n","i = 33\n","inputs = tokenizer(f'{text_column} : {dataset[\"test\"][i][text_column]} Label : ', return_tensors=\"pt\")\n","print(dataset[\"test\"][i][text_column])\n","print(inputs)\n","\n","with torch.no_grad():\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","    outputs = model.generate(\n","        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=3\n","    )\n","    print(outputs)\n","    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))\n","\n","\n","# saving model\n","peft_model_id = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\".replace(\n","    \"/\", \"_\"\n",")\n","model.save_pretrained(peft_model_id)\n","\n","\n","peft_model_id = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\".replace(\n","    \"/\", \"_\"\n",")\n","\n","config = PeftConfig.from_pretrained(peft_model_id)\n","model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n","model = PeftModel.from_pretrained(model, peft_model_id)\n","\n","model.to(device)\n","model.eval()\n","i = 4\n","inputs = tokenizer(f'{text_column} : {dataset[\"test\"][i][text_column]} Label : ', return_tensors=\"pt\")\n","print(dataset[\"test\"][i][text_column])\n","print(inputs)\n","\n","with torch.no_grad():\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","    outputs = model.generate(\n","        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=3\n","    )\n","    print(outputs)\n","    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["b559c2eabf8740d7bb645087518742aa","2a7fcb7145f94f56aea309777c60b8d7","14b1e97e882547a6858e4e7b6e89091b","e9223f0eb99f446f9695970fe64eb484","3700b526936d425eb28c896f786daf83","eb9b87ae5b30417f89b0de4edf7db630","58b6b2710d07426eb648a23e773db698","3756610b859b470bb73175f6cc0dc172","1e5fb023ebef492e9f04c0d4e7dfcbd7","a0fe716f3244417facc4e5d51264fa6d","d5c9f9dc779b402eb90d3e9dbc261d9d","4fcb2937a8c848e38e3dc1f1108b7298","6b8b3390e0e74d1eaff8c7dc4ac271bf","605b4b73e7ae42c19b3edd0963538171","29b960c4905d4f8fafab8c26f3c8a6cc","934b1ec8e0384a9799a610565d8c0d67","26c7097c93ec4cdab826d246513e3157","dc35a4519d724f9e8aeabd116882a805","c4a0132821e4429d8a733d857db9f006","2e282017dbb84378b8e80cfbd2169433","2bfb4c995c3242fb8ecda4b16e7a4562","4582c044597945ab9fdc6feab3664537","9b34fb3076c942789e73aadab2bac3e2","480409d89b8443409d8e0350e81d548e","df34dc6b58264a66b0038c2c12d31684","3eb020593788464893c5dab17d8599ff","a7cf01c1e43748b0a0a1d5e5366ad7de","1e81ae9a228547d48312f6bba23bdc5f","4713d08b654a4727baf69d0855b9f6eb","4eeba2225b2048488cb49bc9d39ef18c","17f7277528e048468fd19a95fa272052","d381aa2c0ab8456d97b105f23387250e","58184c83ba1c46a0b56d39c24153ed43","6b481374f5a04b488edbd367676f38e0","002fa47e19dc427db748b057d6df5911","707aa36878a441b2a29c080c93a8a7f4","991dff78393448048bf8bb58598b535b","9080332c9b3644aeaf9093771465b19d","2f1e6fef768348c6a316d691f1229b3c","81a180e15b194ac6ae15155483db2631","5c1ec3f7acd54857a3a280f93da54820","a47a87904d594e3bb86178bd2845ff41","1735e1db0b70483f86220fb0948da6b3","fafebac6e74b482f8fb68c1aaec86f23"]},"id":"oT_lT0boAsf-","executionInfo":{"status":"ok","timestamp":1706857860549,"user_tz":-330,"elapsed":1755087,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"724781dd-7b4a-42d8-d441-66296db1af41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Splits:  dict_keys(['train', 'test', 'eval'])\n","Columns:  {'train': ['id', 'input', 'output'], 'test': ['id', 'input', 'output'], 'eval': ['id', 'input', 'output']}\n"]},{"output_type":"display_data","data":{"text/plain":["Running tokenizer on dataset:   0%|          | 0/527 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b559c2eabf8740d7bb645087518742aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Running tokenizer on dataset:   0%|          | 0/66 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fcb2937a8c848e38e3dc1f1108b7298"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Running tokenizer on dataset:   0%|          | 0/66 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b34fb3076c942789e73aadab2bac3e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Running tokenizer on dataset:   0%|          | 0/66 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b481374f5a04b488edbd367676f38e0"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["trainable params: 16,384 || all params: 1,100,064,768 || trainable%: 0.0014893668515343272\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 527/527 [06:18<00:00,  1.39it/s]\n","100%|██████████| 527/527 [03:22<00:00,  2.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch=0: train_ppl=tensor(1.3604, device='cuda:0') train_epoch_loss=tensor(0.3078, device='cuda:0') eval_ppl=tensor(1.2607, device='cuda:0') eval_epoch_loss=tensor(0.2317, device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 527/527 [06:16<00:00,  1.40it/s]\n","100%|██████████| 527/527 [03:22<00:00,  2.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch=1: train_ppl=tensor(1.2344, device='cuda:0') train_epoch_loss=tensor(0.2106, device='cuda:0') eval_ppl=tensor(1.1979, device='cuda:0') eval_epoch_loss=tensor(0.1806, device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 527/527 [06:16<00:00,  1.40it/s]\n","100%|██████████| 527/527 [03:22<00:00,  2.61it/s]\n","/usr/local/lib/python3.10/dist-packages/peft/peft_model.py:1180: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n","  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"]},{"output_type":"stream","name":"stdout","text":["epoch=2: train_ppl=tensor(1.1783, device='cuda:0') train_epoch_loss=tensor(0.1640, device='cuda:0') eval_ppl=tensor(1.1515, device='cuda:0') eval_epoch_loss=tensor(0.1411, device='cuda:0')\n","Fundamental vulnerabilities present in AI-driven anomaly detection systems include susceptibility to adversarial examples that mimic normal behavior, limited generalization to unseen anomalies, and potential sensitivity to changes in input distribution. Adversaries may exploit these vulnerabilities to evade detection or manipulate system behavior. Defenses involve diverse anomaly detection techniques, adversarial training, and continuous model evaluation to address fundamental vulnerabilities and improve the overall effectiveness of AI-driven anomaly detection systems.\n","{'input_ids': tensor([[    1,  1962,   584, 13249, 11491, 23180, 11614,  2198,   297,   319,\n","         29902, 29899, 24477,   854, 29342, 14997, 15326,  6757,  3160,  2858,\n","          1547,  4127,   304, 19901, 27521,  6455,   393,   286,   326,   293,\n","          4226,  6030, 29892,  9078,  2498,  2133,   304,   443, 28026, 29342,\n","           284,   583, 29892,   322,  7037,  4771, 24858,   304,  3620,   297,\n","          1881,  4978, 29889,  2087,   874,  4314,  1122, 16035,   277,  1438,\n","         23180, 11614,   304,   321,  1564,   311, 15326,   470, 26749,  1788,\n","          6030, 29889,  5282, 11259, 25135, 16984, 29342, 14997, 15326, 13698,\n","         29892, 19901, 27521,  6694, 29892,   322,  9126,  1904, 17983,   304,\n","          3211, 15281, 23180, 11614,   322, 11157,   278, 12463,  2779, 20193,\n","           310,   319, 29902, 29899, 24477,   854, 29342, 14997, 15326,  6757,\n","         29889, 15796,   584, 29871]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","tensor([[    1,  1962,   584, 13249, 11491, 23180, 11614,  2198,   297,   319,\n","         29902, 29899, 24477,   854, 29342, 14997, 15326,  6757,  3160,  2858,\n","          1547,  4127,   304, 19901, 27521,  6455,   393,   286,   326,   293,\n","          4226,  6030, 29892,  9078,  2498,  2133,   304,   443, 28026, 29342,\n","           284,   583, 29892,   322,  7037,  4771, 24858,   304,  3620,   297,\n","          1881,  4978, 29889,  2087,   874,  4314,  1122, 16035,   277,  1438,\n","         23180, 11614,   304,   321,  1564,   311, 15326,   470, 26749,  1788,\n","          6030, 29889,  5282, 11259, 25135, 16984, 29342, 14997, 15326, 13698,\n","         29892, 19901, 27521,  6694, 29892,   322,  9126,  1904, 17983,   304,\n","          3211, 15281, 23180, 11614,   322, 11157,   278, 12463,  2779, 20193,\n","           310,   319, 29902, 29899, 24477,   854, 29342, 14997, 15326,  6757,\n","         29889, 15796,   584, 29871,  1724,   526,   278, 15281, 23180, 11614,\n","          2198,   297,   319, 29902]], device='cuda:0')\n","['output : Fundamental vulnerabilities present in AI-driven anomaly detection systems include susceptibility to adversarial examples that mimic normal behavior, limited generalization to unseen anomalies, and potential sensitivity to changes in input distribution. Adversaries may exploit these vulnerabilities to evade detection or manipulate system behavior. Defenses involve diverse anomaly detection techniques, adversarial training, and continuous model evaluation to address fundamental vulnerabilities and improve the overall effectiveness of AI-driven anomaly detection systems. Label :  What are the fundamental vulnerabilities present in AI']\n","Privacy concerns predominantly linked with model inversion attacks involve the potential exposure of sensitive information about individuals represented in the model's training data. Adversaries aim to infer private details by leveraging observed model outputs. Defenses include secure aggregation methods, input blurring techniques, and differential privacy mechanisms to mitigate information leakage and address the privacy concerns associated with model inversion attacks on AI models.\n","{'input_ids': tensor([[    1,  1962,   584, 18936,  4135, 21838,   758, 24130, 10835,  9024,\n","           411,  1904,   297,  3259, 16661, 25135,   278,  7037, 14060,   545,\n","           310, 20502,  2472,  1048, 15724,  9875,   297,   278,  1904, 29915,\n","         29879,  6694,   848, 29889,  2087,   874,  4314, 12242,   304, 10115,\n","          2024,  4902,   491, 26610,  6751,  8900,  1904, 14391, 29889,  5282,\n","         11259,  3160, 11592, 11404,   362,  3519, 29892,  1881,  1999,  1038,\n","           292, 13698, 29892,   322, 16712,  5999,  4135,  7208, 12903,   304,\n","          1380,   335,   403,  2472, 24993,   482,   322,  3211,   278,  5999,\n","          4135, 21838,  6942,   411,  1904,   297,  3259, 16661,   373,   319,\n","         29902,  4733, 29889, 15796,   584, 29871]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","tensor([[    1,  1962,   584, 18936,  4135, 21838,   758, 24130, 10835,  9024,\n","           411,  1904,   297,  3259, 16661, 25135,   278,  7037, 14060,   545,\n","           310, 20502,  2472,  1048, 15724,  9875,   297,   278,  1904, 29915,\n","         29879,  6694,   848, 29889,  2087,   874,  4314, 12242,   304, 10115,\n","          2024,  4902,   491, 26610,  6751,  8900,  1904, 14391, 29889,  5282,\n","         11259,  3160, 11592, 11404,   362,  3519, 29892,  1881,  1999,  1038,\n","           292, 13698, 29892,   322, 16712,  5999,  4135,  7208, 12903,   304,\n","          1380,   335,   403,  2472, 24993,   482,   322,  3211,   278,  5999,\n","          4135, 21838,  6942,   411,  1904,   297,  3259, 16661,   373,   319,\n","         29902,  4733, 29889, 15796,   584, 29871,  1724,   526,   278,  5999,\n","          4135, 21838,   758, 24130, 10835,  9024]], device='cuda:0')\n","[\"output : Privacy concerns predominantly linked with model inversion attacks involve the potential exposure of sensitive information about individuals represented in the model's training data. Adversaries aim to infer private details by leveraging observed model outputs. Defenses include secure aggregation methods, input blurring techniques, and differential privacy mechanisms to mitigate information leakage and address the privacy concerns associated with model inversion attacks on AI models. Label :  What are the privacy concerns predominantly linked\"]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"qlcPG1KRjKtY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WtzQJEyGjKp5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-yZx2PA3jKnG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OMbn9CHejKkx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MLXCo7bHjKiS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eOt9ryeojKgG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SJNdpTFHjKdu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5inzk9rLjKbM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eKZNGEt4jKYs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TIOt-N_XjKWa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0rvgH8-sjKUE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset=advanced_data_loader({'train': '/content/drive/MyDrive/dataset1.csv', 'test': '/content/drive/MyDrive/dataset1.csv'}, 'csv')"],"metadata":{"id":"E45vlUmsf9r_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"-GwGR0CXCV3y"}},{"cell_type":"code","source":["from typing import List, Optional\n","from huggingface_hub import HfApi, HfFolder, Repository, SpaceHardware, SpaceStorage\n","import os\n","# Define the function to login to Hugging Face\n","def login_to_huggingface(token: str):\n","    \"\"\"\n","    Login to Hugging Face using an access token.\n","\n","    :param token: str - The access token for Hugging Face.\n","    \"\"\"\n","    HfFolder.save_token(token)\n","\n","# Define the function to create or use an existing repo\n","def get_or_create_repo(\n","    repo_id: str,\n","    token: str,\n","    repo_type: Optional[str] = None,\n","    private: bool = False,\n","    exist_ok: bool = True\n",") -> Repository:\n","    \"\"\"\n","    Get or create a repository on Hugging Face.\n","\n","    :param repo_id: str - The namespace and repo name separated by a slash.\n","    :param token: str - The access token for Hugging Face.\n","    :param repo_type: Optional[str] - The type of the repository ('dataset', 'model', or 'space').\n","    :param private: bool - Whether the repository should be private.\n","    :param exist_ok: bool - If True, do not raise an error if the repo already exists.\n","    :return: Repository - The Repository object.\n","    \"\"\"\n","    api = HfApi()\n","    repo_url = api.create_repo(\n","        repo_id=repo_id,\n","        token=token,\n","        private=private,\n","        repo_type=repo_type,\n","        exist_ok=exist_ok\n","    )\n","    repo = Repository(local_dir=repo_id, clone_from=repo_url, use_auth_token=token)\n","    return repo\n","\n","def upload_to_huggingface(repo: Repository, folder_paths: List[str]):\n","    \"\"\"\n","    Upload folders to the specified Hugging Face repository.\n","\n","    :param repo: Repository - The Hugging Face Repository object.\n","    :param folder_paths: List[str] - A list of paths to the folders to upload.\n","    \"\"\"\n","    repo.git_pull()\n","    for folder_path in folder_paths:\n","        # Check if the folder_path is within the local clone of the repository\n","        if os.path.commonpath([repo.local_dir, folder_path]) != repo.local_dir:\n","            raise ValueError(f\"The path '{folder_path}' is not within the repository directory '{repo.local_dir}'.\")\n","        repo.git_add(folder_path)\n","    repo.git_commit(\"Upload folders\")\n","    repo.git_push()\n","\n","# Define the main function to handle different tasks\n","def main(\n","    task: str,\n","    repo_id: str,\n","    token: str,\n","    dataset_paths: Optional[List[str]] = None,\n","    model_paths: Optional[List[str]] = None\n","):\n","    \"\"\"\n","    Main function to handle different upload tasks to Hugging Face.\n","\n","    :param task: str - The task to perform ('dataset', 'model', or 'both').\n","    :param repo_id: str - The namespace and repo name separated by a slash.\n","    :param token: str - The access token for Hugging Face.\n","    :param dataset_paths: Optional[List[str]] - A list of paths to dataset folders to upload.\n","    :param model_paths: Optional[List[str]] - A list of paths to model folders to upload.\n","    \"\"\"\n","    login_to_huggingface(token)\n","    if task in [\"dataset\", \"both\"] and dataset_paths:\n","        dataset_repo = get_or_create_repo(repo_id, token, repo_type=\"dataset\")\n","        upload_to_huggingface(dataset_repo, dataset_paths)\n","    if task in [\"model\", \"both\"] and model_paths:\n","        model_repo = get_or_create_repo(repo_id, token, repo_type=\"model\")\n","        upload_to_huggingface(model_repo, model_paths)\n","\n","# Usage example\n","if __name__ == \"__main__\":\n","    # Replace 'your_token' with your actual Hugging Face access token\n","    TOKEN = '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'\n","\n","    # Replace 'your_dataset_repo_id' with the desired dataset repository id\n","    DATASET_REPO_ID = 'hemanthkandimalla/data'\n","\n","    # Replace 'your_model_repo_id' with the desired model repository id\n","    MODEL_REPO_ID = 'hemanthkandimalla/model'\n","\n","    # Replace the following list with the paths to the dataset folders you want to upload\n","    DATASET_PATHS = ['/content/hemanthkandimalla/data/california_housing_test.csv',]\n","\n","    # Replace the following list with the paths to the model folders you want to upload\n","    # MODEL_PATHS = ['/content/hemanthkandimalla/model/california_housing_train.csv','/content/hemanthkandimalla/model/california_housing_train.csv']\n","\n","    # Call the main function for the desired task\n","    main(task=\"both\", repo_id=DATASET_REPO_ID, token=TOKEN, dataset_paths=DATASET_PATHS)\n","    # main(task=\"both\", repo_id=MODEL_REPO_ID, token=TOKEN, model_paths=MODEL_PATHS)\n"],"metadata":{"id":"iPZ9bcVzE_sf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir hemanthkandimalla/data/folder"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GLaH14v4SWjE","executionInfo":{"status":"ok","timestamp":1706902007072,"user_tz":-330,"elapsed":568,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"81635f73-b1af-4755-99d2-0bdfc134fb30"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘hemanthkandimalla/data/folder’: No such file or directory\n"]}]},{"cell_type":"code","source":["!git lfs install"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KniCPwyKQBBg","executionInfo":{"status":"ok","timestamp":1706901382920,"user_tz":-330,"elapsed":573,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"d5866d67-2a6c-421b-fbc3-607906175f9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Git LFS initialized.\n"]}]},{"cell_type":"code","source":["input_prompt=\"adversarical attacks list on the ai model \"\n","\n","# Encode the input prompt as a tensor\n","input_ids = tokenizer.encode(input_prompt, return_tensors='pt')\n","input_ids = input_ids.to(device)\n","\n","# Generate a text completion using the fine-tuned model\n","completion = model.generate(\n","    input_ids,\n","    max_length=512,\n","    num_beams=5,\n","    early_stopping=True,\n","    temperature=0.7\n",")\n","\n","# Decode the completion and print it\n","completion_text = tokenizer.decode(completion[0])\n","print(completion_text.strip())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9OmwDnR4rOCn","executionInfo":{"status":"ok","timestamp":1706858875278,"user_tz":-330,"elapsed":1783,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"b72da17a-fb7c-4b59-8f49-c6d6ab6dbf42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<s> adversarical attacks list on the ai model \n","Can you summarize the main points of the article discussing the impact of adversarial attacks on AI models?</s>\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PFe9tIpJdg3T","executionInfo":{"status":"ok","timestamp":1707107612241,"user_tz":-330,"elapsed":22337,"user":{"displayName":"Hemanth Varma","userId":"06942620438381381899"}},"outputId":"f94bcb77-9d47-417b-dd3f-59e51734ee2d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["\n","\n","1. **Attention Mechanisms**\n","2. **Encoder-Decoder Architecture**\n","3. **Neural History Compressor**\n","4. **Hybrid Networks (CNN-RNN)**\n","5. **Listen, Attend and Spell (LAS)**\n","6. **Recurrent Neural Network Transducer (RNNT)**\n","7. **Model Compression Techniques**\n","8. **Long Short-Term Memory Networks (LSTM)**\n","9. **Gated Recurrent Units (GRU)**\n","10. **Bidirectional RNNs**\n","11. **Sequence to Sequence Models (Seq2Seq)**\n","12. **Transformer Models**\n","13. **BERT (Bidirectional Encoder Representations from Transformers)**\n","14. **GPT (Generative Pretrained Transformer)**\n","15. **XLNet**\n","16. **T5 (Text-to-Text Transfer Transformer)**\n","17. **ERNIE (Enhanced Representation through kNowledge IntEgration)**\n","18. **ALBERT (A Lite BERT)**\n","19. **RoBERTa (Robustly optimized BERT approach)**\n","20. **ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)**\n","\n"],"metadata":{"id":"XOnfG3P8Iexl"}},{"cell_type":"markdown","source":["# Equations for AGI System Development\n","\n","## Word to Tokenization\n","\n","- Let $ D $ be the input document or text.\n","- Tokenization $ T(D) $ is a function that converts the document into a sequence of tokens.\n","- $ T(D) = [t_1, t_2, ..., t_n] $, where $ t_i $ represents the $ i $-th token.\n","\n","## Tokenizations to Pass Attention\n","\n","- Let $ H $ be the hidden representation obtained through embedding and encoding.\n","- $ H = E(T(D)) $, where $ E $ represents the embedding function.\n","- The attention mechanism $ A $ can be applied to $ H $ to obtain the weighted context representation $ C $.\n","- $ C = A(H) $, where $ A $ computes attention weights and combines them with $ H $.\n","\n","## Attention Mechanisms\n","\n","- Attention involves three key components: Query $ Q $, Key $ K $, and Value $ V $.\n","- Given $ Q $, $ K $, and $ V $, the attention weight $ \\alpha_{ij} $ between the $ i $-th token and $ j $-th token is calculated as\n","  $$ \\alpha_{ij} = \\text{softmax}\\left(\\frac{Q_i \\cdot K_j}{\\sqrt{d_k}}\\right) $$,\n","  where $ d_k $ is the dimensionality of the key vectors.\n","- The weighted sum of values $ V $ using attention weights gives the context vector\n","  $$ C_i = \\sum_{j} \\alpha_{ij}V_j $$.\n","\n","In summary, the process can be expressed mathematically as follows:\n","\n","1. **Tokenization:** $ T(D) = [t_1, t_2, ..., t_n] $\n","2. **Embedding:** $ H = E(T(D)) $\n","3. **Attention Mechanism:** $$ C_i = \\sum_{j} \\text{softmax}\\left(\\frac{Q_i \\cdot K_j}{\\sqrt{d_k}}\\right)V_j $$\n"],"metadata":{"id":"HG-loVWGMXD7"}},{"cell_type":"markdown","source":["# Encoder-Decoder Architecture\n","\n","## Overview\n","\n","This document provides a detailed description of the Encoder-Decoder architecture commonly used in sequence-to-sequence tasks, with a focus on machine translation.\n","\n","## Encoder\n","\n","1. **Input Sequence Representation:**\n","   - Let $ X = [x_1, x_2, x_3, ..., x_n] $ be the input sequence.\n","\n","2. **Embedding:**\n","   - Map each input element to its corresponding embedding:\n","     $$ E(X) = [e(x_1), e(x_2), ..., e(x_n)] $$\n","\n","3. **Encoder Hidden States:**\n","   - Use a recurrent or transformer-based neural network to generate hidden states:\n","     $$ H_{\\text{encoder}} = \\text{Encoder}(E(X)) $$\n","\n","4. **Context Vector:**\n","   - The final hidden state or a weighted sum of hidden states represents the context vector:\n","     $$ C = \\text{context}(H_{\\text{encoder}}) $$\n","\n","## Decoder\n","\n","1. **Initial Hidden State:**\n","   - The context vector from the encoder is used as the initial hidden state for the decoder:\n","     $$ H_{\\text{decoder}}^0 = C $$\n","\n","2. **Decoder Hidden States:**\n","   - Utilize a recurrent or transformer-based neural network to generate decoder hidden states:\n","     $$ H_{\\text{decoder}} = \\text{Decoder}(E(Y), H_{\\text{decoder}}^{t-1}) $$\n","\n","3. **Output Probability Distribution:**\n","   - Map decoder hidden states to the output sequence probability distribution:\n","     $$ P(Y_t | X, Y_{<t}) = \\text{softmax}(\\text{Linear}(H_{\\text{decoder}}^t)) $$\n","\n","4. **Output Sequence:**\n","   - The final output sequence is generated by sampling from the probability distribution:\n","     $$ Y_t \\sim P(Y_t | X, Y_{<t}) $$\n","\n","## Summary\n","\n","- **Encoder:**\n","  1. Input Sequence: $ X = [x_1, x_2, x_3, ..., x_n] $\n","  2. Embedding: $ E(X) = [e(x_1), e(x_2), ..., e(x_n)] $\n","  3. Encoder Hidden States: $ H_{\\text{encoder}} = \\text{Encoder}(E(X)) $\n","  4. Context Vector: $ C = \\text{context}(H_{\\text{encoder}}) $\n","\n","- **Decoder:**\n","  1. Initial Hidden State: $ H_{\\text{decoder}}^0 = C $\n","  2. Decoder Hidden States: $ H_{\\text{decoder}} = \\text{Decoder}(E(Y), H_{\\text{decoder}}^{t-1}) $\n","  3. Output Probability Distribution: $ P(Y_t | X, Y_{<t}) = \\text{softmax}(\\text{Linear}(H_{\\text{decoder}}^t)) $\n","  4. Output Sequence: $ Y_t \\sim P(Y_t | X, Y_{<t}) $\n","\n","This represents the step-by-step mathematical description of an Encoder-Decoder architecture.\n"],"metadata":{"id":"dm7riq7TNkLV"}},{"cell_type":"markdown","source":["\n","# Neural History Compressor\n","\n","## Overview\n","\n","This document provides a mathematical description of the Neural History Compressor, a task involving compressing historical information using neural network architectures.\n","\n","## Problem Formulation\n","\n","Given a sequence of historical data represented as $ H = [h_1, h_2, ..., h_n] $, the goal is to design a Neural History Compressor that generates a compressed representation $ C $ capturing essential information from the input history.\n","\n","## Mathematical Formulation\n","\n","### Input Sequence\n","\n","- Let $ H = [h_1, h_2, ..., h_n] $ be the input historical sequence.\n","\n","### Encoder\n","\n","1. **Embedding:**\n","   - Map each historical element to its corresponding embedding:\n","     $$ E(H) = [e(h_1), e(h_2), ..., e(h_n)] $$\n","\n","2. **Encoder Hidden States:**\n","   - Utilize a neural network to generate hidden states:\n","     $$ H_{\\text{encoder}} = \\text{Encoder}(E(H)) $$\n","\n","### Compressed Representation\n","\n","- The final hidden state or a weighted sum of hidden states represents the compressed representation:\n","  $$ C = \\text{Compressor}(H_{\\text{encoder}}) $$\n","\n","### Decoder\n","\n","1. **Initial Hidden State:**\n","   - The compressed representation is used as the initial hidden state for the decoder:\n","     $$ H_{\\text{decoder}}^0 = C $$\n","\n","2. **Decoder Hidden States:**\n","   - Utilize a neural network to generate decoder hidden states:\n","     $$ H_{\\text{decoder}} = \\text{Decoder}(E(Y), H_{\\text{decoder}}^{t-1}) $$\n","\n","3. **Output Sequence:**\n","   - The final output sequence is generated by sampling from the probability distribution:\n","     $$ Y_t \\sim P(Y_t | C, Y_{<t}) $$\n","\n","## Summary\n","\n","- **Input Sequence:**\n","  - $$ H = [h_1, h_2, ..., h_n] $$\n","\n","- **Encoder:**\n","  1. **Embedding:** $$ E(H) = [e(h_1), e(h_2), ..., e(h_n)] $$\n","  2. **Encoder Hidden States:** $$H_{\\text{encoder}} = \\text{Encoder}(E(H))$$\n","\n","- **Compressed Representation:**\n","  - $$ C = \\text{Compressor}(H_{\\text{encoder}})$$\n","\n","- **Decoder:**\n","  1. **Initial Hidden State:** $$ H_{\\text{decoder}}^0 = C $$\n","  2. **Decoder Hidden States:** $$ H_{\\text{decoder}} = \\text{Decoder}(E(Y), H_{\\text{decoder}}^{t-1}) $$\n","  3. **Output Sequence:** $$ Y_t \\sim P(Y_t | C, Y_{<t}) $$\n","\n"],"metadata":{"id":"uwVMUOEyO0rD"}},{"cell_type":"markdown","source":["# Hybrid Networks (CNN-RNN)\n","\n","## Overview\n","\n","This document provides a mathematical description of Hybrid Networks combining Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).\n","\n","## Problem Formulation\n","\n","The task involves processing data using a combination of CNN and RNN architectures to capture both spatial and temporal dependencies in the input.\n","\n","## Mathematical Formulation\n","\n","### Input Data\n","\n","- Let $ X = [x_1, x_2, ..., x_n] $ be the input data.\n","\n","### Convolutional Neural Network (CNN)\n","\n","1. **Convolutional Layer:**\n","   - Apply convolutional operations to the input:\n","     $$ C = \\text{CNN}(X) $$\n","\n","### Recurrent Neural Network (RNN)\n","\n","1. **Embedding:**\n","   - Map each input element to its corresponding embedding:\n","     $$ E(X) = [e(x_1), e(x_2), ..., e(x_n)] $$\n","\n","2. **Recurrent Layer:**\n","   - Utilize a recurrent neural network to capture temporal dependencies:\n","     $$ H_{\\text{RNN}} = \\text{RNN}(E(X)) $$\n","\n","### Hybrid Network\n","\n","- Combine features from CNN and RNN:\n","  $$ H_{\\text{Hybrid}} = \\text{Concatenate}(C, H_{\\text{RNN}}) $$\n","\n","### Output Layer\n","\n","- Use the hybrid features in a final output layer:\n","  $$ Y = \\text{OutputLayer}(H_{\\text{Hybrid}}) $$\n","\n","## Summary\n","\n","- **Input Data:**\n","  - $ X = [x_1, x_2, ..., x_n] $\n","\n","- **Convolutional Neural Network (CNN):**\n","  - **Convolutional Layer:** $ C = \\text{CNN}(X) $\n","\n","- **Recurrent Neural Network (RNN):**\n","  1. **Embedding:** $ E(X) = [e(x_1), e(x_2), ..., e(x_n)] $\n","  2. **Recurrent Layer:** $ H_{\\text{RNN}} = \\text{RNN}(E(X)) $\n","\n","- **Hybrid Network:**\n","  - Combine features from CNN and RNN: $ H_{\\text{Hybrid}} = \\text{Concatenate}(C, H_{\\text{RNN}}) $\n","\n","- **Output Layer:**\n","  - Use the hybrid features in a final output layer: $ Y = \\text{OutputLayer}(H_{\\text{Hybrid}}) $\n","\n","This represents the mathematical description of Hybrid Networks (CNN-RNN) for the given task.\n"],"metadata":{"id":"NyHZ3fh1ROMj"}},{"cell_type":"markdown","source":["# Listen, Attend and Spell (LAS)\n","\n","## Overview\n","\n","This document provides a more detailed mathematical description of the Listen, Attend and Spell (LAS) model, focusing on the internal workings of the Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) components.\n","\n","## Problem Formulation\n","\n","The LAS model is designed for sequence-to-sequence tasks, particularly in Automatic Speech Recognition (ASR), where the input is an acoustic signal, and the output is a sequence of phonemes or characters.\n","\n","## Mathematical Formulation\n","\n","### Input Acoustic Sequence\n","\n","- Let $ X = [x_1, x_2, ..., x_n] $ be the input acoustic sequence.\n","\n","### Convolutional Neural Network (CNN)\n","\n","1. **Convolutional Layer:**\n","   - Apply convolutional operations to the input:\n","     $$ C = \\text{CNN}(X) $$\n","\n","   - **Convolution Operation:**\n","     - Given a filter $ W $ and a bias $ b $, the convolution operation at position $ i $ is calculated as:\n","       $$ C_i = \\text{activation}\\left(\\sum_{k=1}^{K} W_k \\cdot X_{i+k} + b\\right) $$\n","\n","   - **Activation Function:**\n","     - Typically, an activation function (e.g., ReLU) is applied element-wise to the convolution result:\n","       $$ \\text{activation}(z) = \\max(0, z) $$\n","\n","### Recurrent Neural Network (RNN)\n","\n","1. **Embedding:**\n","   - Map each input element to its corresponding embedding:\n","     $$ E(X) = [e(x_1), e(x_2), ..., e(x_n)] $$\n","\n","2. **Recurrent Layer:**\n","   - Utilize a recurrent neural network to capture temporal dependencies:\n","     $$ H_{\\text{RNN}} = \\text{RNN}(E(X)) $$\n","\n","   - **RNN Cell:**\n","     - The RNN cell processes each input embedding sequentially, updating its hidden state at each time step:\n","       $$ h_t = \\text{RNNCell}(e(x_t), h_{t-1}) $$\n","\n","   - **RNNCell Function:**\n","     - The RNNCell function is often implemented as a simple update equation, such as using the Gated Recurrent Unit (GRU) or Long Short-Term Memory (LSTM) equations.\n","\n","### Attention Mechanism\n","\n","- Attention involves three key components: Query $ Q $, Key $ K $, and Value $ V $.\n","- Given $ Q $, $ K $, and $ V $, the attention weight $ \\alpha_{ij} $ between the $ i $-th token and $ j $-th token is calculated as\n","  $$ \\alpha_{ij} = \\text{softmax}\\left(\\frac{Q_i \\cdot K_j}{\\sqrt{d_k}}\\right) $$,\n","  where $ d_k $ is the dimensionality of the key vectors.\n","- The weighted sum of values $ V $ using attention weights gives the context vector\n","  $$ C_i = \\sum_{j} \\alpha_{ij}V_j $$.\n","\n","### Hybrid Network\n","\n","- Combine features from CNN, RNN, and Attention:\n","  $$ H_{\\text{Hybrid}} = \\text{Concatenate}(C, H_{\\text{RNN}}, \\text{Attention}(Q, K, V)) $$\n","\n","### Output Layer\n","\n","- Use the hybrid features in a final output layer:\n","  $$ Y = \\text{OutputLayer}(H_{\\text{Hybrid}}) $$\n","\n","## Summary\n","\n","- **Input Acoustic Sequence:**\n","  - $ X = [x_1, x_2, ..., x_n] $\n","\n","- **Convolutional Neural Network (CNN):**\n","  1. **Convolutional Layer:** $ C = \\text{CNN}(X) $\n","     - **Convolution Operation:**\n","       $$ C_i = \\text{activation}\\left(\\sum_{k=1}^{K} W_k \\cdot X_{i+k} + b\\right) $$\n","     - **Activation Function:**\n","       $$ \\text{activation}(z) = \\max(0, z) $$\n","\n","- **Recurrent Neural Network (RNN):**\n","  1. **Embedding:** $ E(X) = [e(x_1), e(x_2), ..., e(x_n)] $\n","  2. **Recurrent Layer:** $ H_{\\text{RNN}} = \\text{RNN}(E(X)) $\n","     - **RNN Cell:**\n","       $$ h_t = \\text{RNNCell}(e(x_t), h_{t-1}) $\n","\n","- **Attention Mechanism:**\n","  - Three key components: Query $ Q $, Key $ K $, and Value $ V $.\n","  - Attention Weight Calculation: $ \\alpha_{ij} = \\text{softmax}\\left(\\frac{Q_i \\cdot K_j}{\\sqrt{d_k}}\\right) $\n","  - Context Vector Calculation: $ C_i = \\sum_{j} \\alpha_{ij}V_j $\n","\n","- **Hybrid Network:**\n","  - Combine features from CNN, RNN, and Attention: $ H_{\\text{Hybrid}} = \\text{Concatenate}(C, H_{\\text{RNN}}, \\text{Attention}(Q, K, V)) $\n","\n","- **Output Layer:**\n","  - Use the hybrid features in a final output layer: $ Y = \\text{OutputLayer}(H_{\\text{Hybrid}}) $\n","\n","This represents a more detailed mathematical description of the Listen, Attend and Spell (LAS) model, providing insight into the operations within the CNN and RNN components.\n"],"metadata":{"id":"CO3J72qzTwkU"}},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Opn-TtQn1H4H","executionInfo":{"status":"ok","timestamp":1706810470757,"user_tz":-330,"elapsed":24101,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"09d7265f-bedf-4135-e089-a2a1b9395e38"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Token: \n","Add token as git credential? (Y/n) y\n","Token is valid (permission: read).\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PYW17NH6Ib-P","executionInfo":{"status":"ok","timestamp":1706814260599,"user_tz":-330,"elapsed":66620,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"0873a066-fcda-407a-adc5-621978bf7a04"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["['Q: What is the capital of France? A: The capital of France is Paris. Q: Who wrote Hamlet? A: Hamlet was written by William Shakespeare. Q: How many legs do two dogs have in total? A: Let\\'s think step by step. One dog has four legs. So, two dogs have 4 times 2 equals 8 legs in total. \\n<|assistant|>\\nThe answer to the question \"Who wrote Hamlet?\" is William Shakespeare.']\n"]}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessorList, MinLengthLogitsProcessor\n","from typing import List, Optional\n","import torch\n","\n","class AdvancedChatBot:\n","    def __init__(self, model_name: str, examples: List[str]):\n","        \"\"\"\n","        Initialize the chatbot with a given model and a few-shot learning prompt.\n","\n","        Args:\n","            model_name (str): The name of the pre-trained model to be used.\n","            examples (List[str]): A list of example strings demonstrating the desired output format.\n","        \"\"\"\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name,token='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')\n","        self.model = AutoModelForCausalLM.from_pretrained(model_name,token='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')\n","        self.examples = examples\n","        self.reset_history()\n","\n","    def reset_history(self):\n","        \"\"\"Resets the chat history.\"\"\"\n","        self.chat_history_ids = None\n","\n","    def add_to_history(self, new_text: str):\n","        \"\"\"\n","        Adds new text to the chat history.\n","\n","        Args:\n","            new_text (str): The new text to add to the history.\n","        \"\"\"\n","        new_input_ids = self.tokenizer.encode(new_text + self.tokenizer.eos_token, return_tensors='pt')\n","        self.chat_history_ids = torch.cat([self.chat_history_ids, new_input_ids], dim=-1) if self.chat_history_ids is not None else new_input_ids\n","\n","    def generate_response(self, query: str, max_length: int = 100, num_return_sequences: int = 1,\n","                          temperature: float = 1.0, top_k: Optional[int] = None,\n","                          top_p: Optional[float] = None, num_beans: Optional[int] = None) -> List[str]:\n","        \"\"\"\n","        Generate a text response for the given query using the model.\n","        \"\"\"\n","        # Format the prompt with few-shot examples and the query\n","        prompt = self.tokenizer.eos_token.join(self.examples) + self.tokenizer.eos_token + query + self.tokenizer.eos_token\n","\n","        # Add the formatted prompt to the history\n","        self.add_to_history(prompt)\n","\n","        # Generate the response\n","        responses = self.model.generate(\n","            self.chat_history_ids,\n","            max_length=max_length,\n","            pad_token_id=self.tokenizer.eos_token_id,\n","            num_return_sequences=num_return_sequences,\n","            temperature=temperature,\n","            top_k=top_k,\n","            top_p=top_p,\n","            num_beams=num_beans,\n","            # ... (include other arguments and configurations as needed)\n","        )\n","\n","        # Update the chat history\n","        self.add_to_history(self.tokenizer.decode(responses[0]))\n","\n","        # Decode the generated tokens to text\n","        return [self.tokenizer.decode(response, skip_special_tokens=True) for response in responses]\n","\n","# Example usage\n","model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Replace with your model of choice\n","examples = [\n","    \"Q: What is the capital of France? A: The capital of France is Paris.\",\n","    \"Q: Who wrote Hamlet? A: Hamlet was written by William Shakespeare.\",\n","    # Include more examples to guide the model in the few-shot learning approach.\n","]\n","\n","chat_bot = AdvancedChatBot(model_name, examples)\n","\n","# Generate a response to a query using chain-of-thought reasoning\n","response = chat_bot.generate_response(\n","    query=\"Q: How many legs do two dogs have in total? A: Let's think step by step. One dog has four legs. So, two dogs have 4 times 2 equals 8 legs in total.\",\n","    max_length=200,\n","    num_return_sequences=1,\n","    temperature=0.9,\n","    top_k=50,\n","    top_p=0.95,\n","    num_beans=5\n",")\n","\n","print(response)"]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessorList, MinLengthLogitsProcessor\n","from typing import List, Optional\n","import torch\n","\n","class AdvancedChatBot:\n","    def __init__(self, model_name: str, examples: List[str]):\n","        \"\"\"\n","        Initialize the chatbot with a given model and a few-shot learning prompt.\n","\n","        Args:\n","            model_name (str): The name of the pre-trained model to be used.\n","            examples (List[str]): A list of example strings demonstrating the desired output format.\n","        \"\"\"\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n","        self.examples = examples\n","        self.reset_history()\n","\n","    def reset_history(self):\n","        \"\"\"Resets the chat history.\"\"\"\n","        self.chat_history_ids = None\n","\n","    def add_to_history(self, new_text: str):\n","        \"\"\"\n","        Adds new text to the chat history.\n","\n","        Args:\n","            new_text (str): The new text to add to the history.\n","        \"\"\"\n","        new_input_ids = self.tokenizer.encode(new_text + self.tokenizer.eos_token, return_tensors='pt')\n","        self.chat_history_ids = torch.cat([self.chat_history_ids, new_input_ids], dim=-1) if self.chat_history_ids is not None else new_input_ids\n","\n","    def generate_response(self, query: str, max_length: int = 100, num_return_sequences: int = 1,\n","                          temperature: float = 1.0, top_k: Optional[int] = None,\n","                          top_p: Optional[float] = None, num_beans: Optional[int] = None) -> List[str]:\n","        \"\"\"\n","        Generate a text response for the given query using the model.\n","        \"\"\"\n","        # Format the prompt with few-shot examples and the query\n","        prompt = self.tokenizer.eos_token.join(self.examples) + self.tokenizer.eos_token + query + self.tokenizer.eos_token\n","\n","        # Add the formatted prompt to the history\n","        self.add_to_history(prompt)\n","\n","        # Generate the response\n","        responses = self.model.generate(\n","            self.chat_history_ids,\n","            max_length=max_length,\n","            pad_token_id=self.tokenizer.eos_token_id,\n","            num_return_sequences=num_return_sequences,\n","            temperature=temperature,\n","            top_k=top_k,\n","            top_p=top_p,\n","            num_beams=num_beans,\n","            # ... (include other arguments and configurations as needed)\n","        )\n","\n","        # Update the chat history\n","        self.add_to_history(self.tokenizer.decode(responses[0]))\n","\n","        # Decode the generated tokens to text\n","        return [self.tokenizer.decode(response, skip_special_tokens=True) for response in responses]\n","\n","# Example usage\n","model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Replace with your model of choice\n","examples = [\n","    \"Q: What is the capital of France? A: The capital of France is Paris.\",\n","    \"Q: Who wrote Hamlet? A: Hamlet was written by William Shakespeare.\",\n","    # Include more examples to guide the model in the few-shot learning approach.\n","]\n","\n","chat_bot = AdvancedChatBot(model_name, examples)\n","\n","# Generate a response to a query using chain-of-thought reasoning\n","response = chat_bot.generate_response(\n","    query=\"Q: How many legs do two dogs have in total? A: Let's think step by step. One dog has four legs. So, two dogs have 4 times 2 equals 8 legs in total.\",\n","    max_length=200,\n","    num_return_sequences=1,\n","    temperature=0.9,\n","    top_k=50,\n","    top_p=0.95,\n","    num_beans=5\n",")\n","\n","print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AbcmSLggzW6a","executionInfo":{"status":"ok","timestamp":1706814310516,"user_tz":-330,"elapsed":49923,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"ecebc295-5cc0-46c9-e934-e9b2afaa6391"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["['Q: What is the capital of France? A: The capital of France is Paris. Q: Who wrote Hamlet? A: Hamlet was written by William Shakespeare. Q: How many legs do two dogs have in total? A: Let\\'s think step by step. One dog has four legs. So, two dogs have 4 times 2 equals 8 legs in total. \\n<|assistant|>\\nThe answer to the question \"Who wrote Hamlet?\" is William Shakespeare.']\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessorList, MinLengthLogitsProcessor\n","from typing import List, Optional\n","import torch\n","\n","class ChatBot:\n","    def __init__(self, model_name: str):\n","        \"\"\"\n","        Initialize the chatbot with a given model.\n","\n","        Args:\n","        model_name (str): The name of the pre-trained model to be used.\n","        \"\"\"\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n","        self.chat_history_ids = None\n","\n","    def generate_response(self, query: str, max_length: int = 100, num_return_sequences: int = 1,\n","                          temperature: float = 1.0, top_k: Optional[int] = None,\n","                          top_p: Optional[float] = None, num_beams: Optional[int] = None) -> List[str]:\n","        \"\"\"\n","        Generate a text response for the given query using the model.\n","\n","        Args:\n","        query (str): The input text prompt to generate text from.\n","        max_length (int): The maximum length of the sequence to be generated.\n","        num_return_sequences (int): The number of sequences to generate.\n","        temperature (float): The value used to model the next token probabilities.\n","        top_k (Optional[int]): The number of highest probability vocabulary tokens to keep for top-k-filtering.\n","        top_p (Optional[float]): The cumulative probability for nucleus sampling.\n","        num_beams (Optional[int]): The number of beams for beam search.\n","\n","        Returns:\n","        List[str]: The generated text responses.\n","        \"\"\"\n","        # Encode the input query\n","        new_user_input_ids = self.tokenizer.encode(query + self.tokenizer.eos_token, return_tensors='pt')\n","\n","        # Append the new user input tokens to the chat history\n","        bot_input_ids = torch.cat([self.chat_history_ids, new_user_input_ids], dim=-1) if self.chat_history_ids is not None else new_user_input_ids\n","\n","        # Define logits processors\n","        logits_processor = LogitsProcessorList()\n","        if max_length is not None:\n","            logits_processor.append(MinLengthLogitsProcessor(min_length=max_length, eos_token_id=self.tokenizer.eos_token_id))\n","\n","        # Generate a response given the input\n","        chat_history_ids = self.model.generate(\n","            bot_input_ids,\n","            max_length=max_length,\n","            pad_token_id=self.tokenizer.eos_token_id,\n","            num_return_sequences=num_return_sequences,\n","            temperature=temperature,\n","            top_k=top_k,\n","            top_p=top_p,\n","            num_beams=num_beams,\n","            logits_processor=logits_processor\n","        )\n","\n","        # Update the chat history\n","        self.chat_history_ids = chat_history_ids\n","\n","        # Decode the generated tokens to text\n","        responses = [self.tokenizer.decode(generated_ids, skip_special_tokens=True) for generated_ids in chat_history_ids]\n","        return responses\n","\n","# Example usage\n","model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Replace with your model of choice\n","chat_bot = ChatBot(model_name)\n","\n","# Generate a response to a query with specific decoding strategy parameters\n","response = chat_bot.generate_response(\n","    query=\"What is the capital of France?\",\n","    max_length=50,\n","    num_return_sequences=1,\n","    temperature=0.7,\n","    top_k=50,\n","    top_p=0.9,\n","    num_beams=5\n",")\n","\n","print(response)\n","\n","# Continue the conversation with history in mind\n","follow_up_response = chat_bot.generate_response(\n","    query=\"Why is it famous?\",\n","    max_length=50,\n","    num_return_sequences=1,\n","    temperature=0.7,\n","    top_k=50,\n","    top_p=0.9,\n","    num_beams=5\n",")\n","\n","print(follow_up_response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":584},"id":"oOlo3GkA23CS","executionInfo":{"status":"error","timestamp":1706814366861,"user_tz":-330,"elapsed":56348,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"9a8543be-0c14-4e3d-ff42-7700e239fa2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["['What is the capital of France? \\n<|assistant|>\\nThe capital of France is Paris, located in the Île-de-France region.\\n\\nSource: https://en.wikipedia.org/wiki/']\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1141: UserWarning: Input length of input_ids is 57, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n","  warnings.warn(\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"The expanded size of the tensor (50) must match the existing size (58) at non-singleton dimension 0.  Target sizes: [50].  Tensor sizes: [58]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-0aa97afb12b8>\u001b[0m in \u001b[0;36m<cell line: 84>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# Continue the conversation with history in mind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m follow_up_response = chat_bot.generate_response(\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Why is it famous?\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-0aa97afb12b8>\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(self, query, max_length, num_return_sequences, temperature, top_k, top_p, num_beams)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Generate a response given the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         chat_history_ids = self.model.generate(\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mbot_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1556\u001b[0m             )\n\u001b[1;32m   1557\u001b[0m             \u001b[0;31m# 13. run beam search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1558\u001b[0;31m             return self.beam_search(\n\u001b[0m\u001b[1;32m   1559\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3030\u001b[0m                     \u001b[0mthis_peer_finished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3032\u001b[0;31m         sequence_outputs = beam_scorer.finalize(\n\u001b[0m\u001b[1;32m   3033\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3034\u001b[0m             \u001b[0mbeam_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/beam_search.py\u001b[0m in \u001b[0;36mfinalize\u001b[0;34m(self, input_ids, final_beam_scores, final_beam_tokens, final_beam_indices, max_length, pad_token_id, eos_token_id, beam_indices, decoder_prompt_len)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;31m# fill with hypotheses and eos_token_id if the latter fits in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhypo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0mdecoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0msent_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhypo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (50) must match the existing size (58) at non-singleton dimension 0.  Target sizes: [50].  Tensor sizes: [58]"]}]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","from typing import List\n","\n","class ChatBot:\n","    \"\"\"\n","    A chatbot using a language model for generating responses to user queries.\n","\n","    Attributes:\n","        model_name (str): The pre-trained model name or path.\n","        history (List[str]): A list to keep track of the conversation history.\n","        tokenizer (AutoTokenizer): The tokenizer for the model.\n","        model (AutoModelForCausalLM): The language model.\n","    \"\"\"\n","\n","    def __init__(self, model_name: str):\n","        \"\"\"\n","        Initializes the chatbot with a given language model.\n","\n","        Args:\n","            model_name (str): The pre-trained model name or path.\n","        \"\"\"\n","        self.model_name = model_name\n","        self.history = []\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","    def generate_response(self, query: str, max_length: int = 50, num_return_sequences: int = 1,\n","                          temperature: float = 1.0, top_k: int = 50, top_p: float = 0.95) -> str:\n","        \"\"\"\n","        Generates a response to the user query using the language model.\n","\n","        Args:\n","            query (str): The user's query or prompt for the model.\n","            max_length (int): The maximum length of the sequence to be generated.\n","            num_return_sequences (int): The number of sequences to generate.\n","            temperature (float): The value used to model the likelihood of the next token.\n","            top_k (int): The number of highest probability vocabulary tokens to keep for top-k filtering.\n","            top_p (float): The cumulative probability for nucleus sampling.\n","\n","        Returns:\n","            str: The generated response.\n","        \"\"\"\n","        # Add the current query to the history\n","        self.history.append(query)\n","\n","        # Merge the history into a single string\n","        input_text = self.tokenizer.eos_token.join(self.history)\n","\n","        # Encode the input text\n","        input_ids = self.tokenizer.encode(input_text, return_tensors='pt')\n","\n","        # Generate the response\n","        response_ids = self.model.generate(\n","            input_ids,\n","            max_length=max_length,\n","            pad_token_id=self.tokenizer.eos_token_id,\n","            num_return_sequences=num_return_sequences,\n","            temperature=temperature,\n","            top_k=top_k,\n","            top_p=top_p\n","        )\n","\n","        # Decode the generated response\n","        response = self.tokenizer.decode(response_ids[0], skip_special_tokens=True)\n","\n","        # Add the generated response to the history\n","        self.history.append(response)\n","\n","        return response\n","\n","# Example usage:\n","model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'  # Replace with your model of choice\n","chatbot = ChatBot(model_name)\n","\n","# Generate a response to a query\n","query = \"Let's talk about the profession of a software engineer.\"\n","response = chatbot.generate_response(query)\n","\n","print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zkOUY_2r27hR","executionInfo":{"status":"ok","timestamp":1706814403776,"user_tz":-330,"elapsed":25946,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"927758ee-3f17-4298-a90b-0d75bc9857af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Let's talk about the profession of a software engineer. What does a software engineer do?\n","\n","1. Develop software applications: A software engineer develops software applications using programming languages such as Java, C++, Python, and Ruby.\n","\n"]}]}]}
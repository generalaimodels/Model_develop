{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNYCvgGAgMuvkUZRqOLmUoX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import datasets\n","import transformers\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from peft import LoraConfig, TaskType, PeftModel\n","from transformers import TrainingArguments, Trainer\n","from evaluate import load\n","\n","def load_data(input: str) -> datasets.DatasetDict:\n","    \"\"\"\n","    Load a dataset from the Hugging Face Hub or a local file in CSV or JSON format.\n","\n","    Args:\n","        input (str): The name of the dataset on the Hub or the path of the local file.\n","\n","    Returns:\n","        datasets.DatasetDict: A dictionary of datasets with splits as keys.\n","    \"\"\"\n","    try:\n","        dataset = datasets.load_dataset(input)\n","    except ValueError:\n","        ext = input.split(\".\")[-1]\n","        if ext == \"csv\":\n","            dataset = datasets.load_dataset(\"csv\", data_files=input)\n","        elif ext == \"json\":\n","            dataset = datasets.load_dataset(\"json\", data_files=input)\n","        else:\n","            raise ValueError(f\"Unsupported file format: {ext}\")\n","    return dataset\n","\n","def process_data(dataset: datasets.DatasetDict, func: Callable) -> datasets.DatasetDict:\n","    \"\"\"\n","    Process a dataset using a custom function.\n","\n","    Args:\n","        dataset (datasets.DatasetDict): A dictionary of datasets with splits as keys.\n","        func (Callable): A function that takes a dictionary of features as input and returns a modified dictionary of features as output.\n","\n","    Returns:\n","        datasets.DatasetDict: A dictionary of processed datasets with splits as keys.\n","    \"\"\"\n","    processed_dataset = dataset.map(func, batched=True)\n","    return processed_dataset\n","\n","def load_tokenizer(input: str) -> AutoTokenizer:\n","    \"\"\"\n","    Load a tokenizer from a model name or a local path.\n","\n","    Args:\n","        input (str): The model name on the Hub or the local path of the model/tokenizer.\n","\n","    Returns:\n","        AutoTokenizer: An instance of the tokenizer that corresponds to the model.\n","    \"\"\"\n","    try:\n","        tokenizer = AutoTokenizer.from_pretrained(input)\n","    except OSError:\n","        raise ValueError(f\"Invalid model name or path: {input}\")\n","    return tokenizer\n","\n","def generate_text(input: str, model: AutoModelForCausalLM, tokenizer: AutoTokenizer) -> str:\n","    \"\"\"\n","    Generate text from a given input using the model and tokenizer.\n","\n","    Args:\n","        input (str): The input text to generate text from.\n","        model (AutoModelForCausalLM): The model to use for generating text.\n","        tokenizer (AutoTokenizer): The tokenizer to use for encoding and decoding the input and output text.\n","\n","    Returns:\n","        str: The generated text.\n","    \"\"\"\n","    input_ids = tokenizer.encode(input, return_tensors=\"pt\")\n","    output_ids = model.generate(input_ids, max_length=200, pad_token_id=tokenizer.eos_token_id)\n","    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","    return output\n","\n","lora_config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    lora_dropout=0.05,\n","    target_modules=[\"q_proj\", \"v_proj\"],\n","    task_type=TaskType.CAUSAL_LM\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\"t5-base\")\n","tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n","\n","dataset = load_data(\"wikitext\")\n","tokenized_dataset = process_data(dataset, lambda examples: tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512))\n","\n","model = PeftModel.from_pretrained(model, lora_config)\n","\n","training_args = transformers.TrainingArguments(\n","    output_dir=\"output\",\n","    num_train_epochs=1,\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=16,\n","    learning_rate=1e-4,\n","    fp16=True,\n","    fp16_backend=\"amp\",\n","    save_total_limit=1,\n","    save_steps=500,\n","    logging_steps=100,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=500,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"perplexity\",\n","    greater_is_better=False,\n",")\n","\n","trainer = transformers.Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"test\"],\n",")\n","\n","trainer.train()\n","\n","accuracy = load(\"accuracy\")\n","perplexity = load(\"perplexity\")\n","bleu = load(\"bleu\")\n","\n","generated_texts = []\n","\n","for example in tokenized_dataset[\"test\"]:\n","    input = tokenizer.decode(example[\"input_ids\"][0], skip_special_tokens=True)\n","    output = generate_text(input, model, tokenizer)\n","    generated_texts.append(output)\n","\n","accuracy_score = accuracy.compute(references=tokenized_dataset[\"test\"][\"text\"], predictions=generated_texts)\n","perplexity_score = perplexity.compute(references=tokenized_dataset[\"test\"][\"text\"], predictions=generated_texts)\n","bleu_score = bleu.compute(references=tokenized_dataset[\"test\"][\"text\"], predictions=generated_texts)"],"metadata":{"id":"GqmI5a_0AtC9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-F5YbaCH7Les"},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForCausalLM\n","from peft import LoraConfig, TaskType, PeftModel\n","from transformers import TrainingArguments\n","from transformers import Trainer\n","from evaluate import load\n","def load_data(input: str) -> datasets.DatasetDict:\n","    \"\"\"\n","    Load a dataset from the Hugging Face Hub or a local file in CSV or JSON format.\n","\n","    Args:\n","        input (str): The name of the dataset on the Hub or the path of the local file.\n","\n","    Returns:\n","        datasets.DatasetDict: A dictionary of datasets with splits as keys.\n","    \"\"\"\n","    # Try to load the dataset from the Hub\n","    try:\n","        dataset = load_dataset(input)\n","    # If the input is not a valid dataset name, try to load it from a local file\n","    except ValueError:\n","        # Check the file extension\n","        ext = input.split(\".\")[-1]\n","        # If the file is in CSV format, use the csv script\n","        if ext == \"csv\":\n","            dataset = load_dataset(\"csv\", data_files=input)\n","        # If the file is in JSON format, use the json script\n","        elif ext == \"json\":\n","            dataset = load_dataset(\"json\", data_files=input)\n","        # Otherwise, raise an error\n","        else:\n","            raise ValueError(f\"Unsupported file format: {ext}\")\n","    # Return the dataset\n","    return dataset\n","\n","# Define a function to process a dataset using a custom function\n","def process_data(dataset: datasets.DatasetDict, func: Callable) -> datasets.DatasetDict:\n","    \"\"\"\n","    Process a dataset using a custom function.\n","\n","    Args:\n","        dataset (datasets.DatasetDict): A dictionary of datasets with splits as keys.\n","        func (Callable): A function that takes a dictionary of features as input and returns a modified dictionary of features as output.\n","\n","    Returns:\n","        datasets.DatasetDict: A dictionary of processed datasets with splits as keys.\n","    \"\"\"\n","    # Apply the function to each split of the dataset\n","    processed_dataset = dataset.map(func)\n","    # Return the processed dataset\n","    return processed_dataset\n","\n","\n","# Define a function to load a tokenizer from a model name or a local path\n","def load_tokenizer(input: str) -> AutoTokenizer:\n","    \"\"\"\n","    Load a tokenizer from a model name or a local path.\n","\n","    Args:\n","        input (str): The model name on the Hub or the local path of the model/tokenizer.\n","\n","    Returns:\n","        AutoTokenizer: An instance of the tokenizer that corresponds to the model.\n","    \"\"\"\n","    # Try to load the tokenizer using the AutoTokenizer class\n","    try:\n","        tokenizer = AutoTokenizer.from_pretrained(input)\n","    # If the input is not a valid model name or path, raise an error\n","    except OSError:\n","        raise ValueError(f\"Invalid model name or path: {input}\")\n","    # Return the tokenizer\n","    return tokenizer\n","\n","# Define a function to generate text from a given input\n","def generate_text(input):\n","    # Encode the input and generate the output\n","    input_ids = model.tokenizer.encode(input, return_tensors=\"pt\")\n","    output_ids = model.generate(input_ids)\n","    # Decode the output and return it\n","    output = model.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","    return output\n","# # Example usage\n","# # Load the tokenizer from a model name on the Hub\n","# tokenizer = load_tokenizer(\"bert-base-cased\")\n","# # Load the tokenizer from a local path\n","# tokenizer = load_tokenizer(\"./saved_model/\")\n","\n","\n","\n","\n","# Define a function to load a model from a model name or a local path\n","def load_model(input: str) -> AutoModelForCausalLM:\n","    \"\"\"\n","    Load a model from a model name or a local path.\n","\n","    Args:\n","        input (str): The model name on the Hub or the local path of the model.\n","\n","    Returns:\n","        AutoModelForCausalLM: An instance of the model that corresponds to the input.\n","    \"\"\"\n","    # Try to load the model using the AutoModelForCausalLM class\n","    try:\n","        model = AutoModelForCausalLM.from_pretrained(input)\n","    # If the input is not a valid model name or path, raise an error\n","    except OSError:\n","        raise ValueError(f\"Invalid model name or path: {input}\")\n","    # Return the model\n","    return model\n","lora_config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    lora_dropout=0.05,\n","    target_modules=[\"q_proj\", \"v_proj\"], # apply LoRA to the query and value projection layers\n","    task_type=TaskType.CAUSAL_LM # specify the task type as causal language modeling\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=\"output\", # specify the output directory\n","    num_train_epochs=1, # specify the number of training epochs\n","    per_device_train_batch_size=1, # specify the batch size per device\n","    gradient_accumulation_steps=16, # specify the number of steps to accumulate gradients\n","    learning_rate=1e-4, # specify the learning rate\n","    fp16=True, # use mixed precision training\n","    fp16_backend=\"amp\", # use the apex library for mixed precision\n","    save_total_limit=1, # limit the number of saved checkpoints\n","    save_steps=500, # save a checkpoint every 500 steps\n","    logging_steps=100, # log the training metrics every 100 steps\n","    evaluation_strategy=\"steps\", # evaluate the model every eval_steps\n","    eval_steps=500, # evaluate the model every 500 steps\n","    load_best_model_at_end=True, # load the best model at the end of training\n","    metric_for_best_model=\"perplexity\", # use perplexity as the metric to select the best model\n","    greater_is_better=False # lower perplexity is better\n",")\n","\n","trainer = Trainer(\n","    model=lora_model, # use the LoRA model\n","    args=training_args, # use the training arguments\n","    train_dataset=dataset[\"train\"], # use the train split of the dataset\n","    eval_dataset=dataset[\"test\"], # use the test split of the dataset\n","    tokenizer=None # no need to use a tokenizer for this task\n",")\n","trainer.train()\n","\n","# Load the accuracy, perplexity and BLEU metrics from the evaluate module\n","accuracy = load(\"accuracy\")\n","perplexity = load(\"perplexity\")\n","bleu = load(\"bleu\")\n","\n","# Define a function to generate text from a given input\n","def generate_text(input):\n","    # Encode the input and generate the output\n","    input_ids = model.tokenizer.encode(input, return_tensors=\"pt\")\n","    output_ids = model.generate(input_ids)\n","    # Decode the output and return it\n","    output = model.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","    return output\n","\n","# Define a list to store the generated texts\n","generated_texts = []\n","\n","# Loop through the test split of the dataset\n","for example in dataset[\"test\"]:\n","    # Get the input and the reference text\n","    input = example[\"input\"]\n","    reference = example[\"target\"]\n","    # Generate the output text\n","    output = generate_text(input)\n","    # Append the output text to the list\n","    generated_texts.append(output)\n","\n","# Compute the accuracy, perplexity and BLEU scores using the metrics\n","accuracy_score = accuracy.compute(references=dataset[\"test\"][\"target\"], predictions=generated_texts)\n","perplexity_score = perplexity.compute(references=dataset[\"test\"][\"target\"], predictions=generated_texts)\n","bleu_score = bleu.compute(references=dataset[\"test\"][\"target\"], predictions=generated_texts)\n","\n"]},{"cell_type":"code","source":["# Import the PEFT library and the AutoModelForCausalLM class from transformers\n","from peft import LoraConfig, TaskType, PeftModel\n","from transformers import AutoModelForCausalLM\n","\n","# Define the model name or path and the dataset name or path\n","model_name_or_path = \"databricks/dolly-v2-3b\"\n","dataset_name_or_path = \"path/to/dataset\"\n","\n","# Define the best LoRA arguments based on the PEFT documentation and the model size\n","# For a 3B model, the recommended values are r=16, lora_alpha=32, and lora_dropout=0.05\n","lora_config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    lora_dropout=0.05,\n","    target_modules=[\"q_proj\", \"v_proj\"], # apply LoRA to the query and value projection layers\n","    task_type=TaskType.CAUSAL_LM # specify the task type as causal language modeling\n",")\n","\n","# Load the base model and the LoRA model using the AutoModelForCausalLM class\n","base_model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n","lora_model = PeftModel(base_model, lora_config)\n","\n","# Load the dataset using the datasets library\n","from datasets import load_dataset\n","dataset = load_dataset(dataset_name_or_path)\n","\n","# Define the training arguments using the TrainingArguments class from transformers\n","from transformers import TrainingArguments\n","training_args = TrainingArguments(\n","    output_dir=\"output\", # specify the output directory\n","    num_train_epochs=1, # specify the number of training epochs\n","    per_device_train_batch_size=1, # specify the batch size per device\n","    gradient_accumulation_steps=16, # specify the number of steps to accumulate gradients\n","    learning_rate=1e-4, # specify the learning rate\n","    fp16=True, # use mixed precision training\n","    fp16_backend=\"amp\", # use the apex library for mixed precision\n","    save_total_limit=1, # limit the number of saved checkpoints\n","    save_steps=500, # save a checkpoint every 500 steps\n","    logging_steps=100, # log the training metrics every 100 steps\n","    evaluation_strategy=\"steps\", # evaluate the model every eval_steps\n","    eval_steps=500, # evaluate the model every 500 steps\n","    load_best_model_at_end=True, # load the best model at the end of training\n","    metric_for_best_model=\"perplexity\", # use perplexity as the metric to select the best model\n","    greater_is_better=False # lower perplexity is better\n",")\n","\n","# Define the trainer using the Trainer class from transformers\n","from transformers import Trainer\n","trainer = Trainer(\n","    model=lora_model, # use the LoRA model\n","    args=training_args, # use the training arguments\n","    train_dataset=dataset[\"train\"], # use the train split of the dataset\n","    eval_dataset=dataset[\"test\"], # use the test split of the dataset\n","    tokenizer=None # no need to use a tokenizer for this task\n",")\n","\n","# Train the LoRA model\n","trainer.train()\n","\n","# Save the LoRA model\n","trainer.save_model(\"lora_model\")\n"],"metadata":{"id":"3Vz4UfT09cbN"},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","mount_file_id":"1qk6JR-Thj6-navEZNlRWD-nQgN2YQTRn","authorship_tag":"ABX9TyOTHcqGkOhv4GqXcFPz3OZ7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install -q -U PyPDF2  accelerate   peft"],"metadata":{"id":"8LVrka-NDdXz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import csv\n","import os\n","import pandas as pd\n","import random\n","import string\n","import PyPDF2\n","import io\n","import sys\n","import traceback\n","\n","\n","\n","def extract_text_from_pdf(filepath, length_of_chunk):\n","    try:\n","        # Open the PDF file in read-binary mode\n","        with open(filepath, 'rb') as file:\n","            # Initialize a PDF file reader object\n","            reader = PyPDF2.PdfReader(file)\n","\n","            # Initialize an empty string to hold the extracted text\n","            text = ''\n","\n","            # Loop through each page in the PDF\n","            for page in reader.pages:\n","                # Extract the text from the page and add it to the text string\n","                text += page.extract_text()\n","\n","        # Split the text into chunks of the specified length\n","        chunks = [text[i:i + length_of_chunk] for i in range(0, len(text), length_of_chunk)]\n","\n","        # Return the chunks\n","        return chunks\n","\n","    except FileNotFoundError:\n","        print(f\"The file {filepath} does not exist.\")\n","        return None\n","    except:\n","        print(\"An unexpected error occurred.\")\n","        return None\n","\n","def get_module_structure(directory_path):\n","  \"\"\" Generates a list of all files and their module structure within a given directory.\n","\n","  Args:\n","    directory_path (str): Path to the directory you want to explore.\n","\n","  Returns:\n","    A dictionary representing the module structure, with keys as module names and values as lists of file names.\n","  \"\"\"\n","\n","  module_structure = {}\n","\n","  for root, dirs, files in os.walk(directory_path):\n","     # Normalize the root path to ensure consistent separators\n","    normalized_root = os.path.normpath(root).replace('\\\\', '/')\n","        # Get the module name from the directory path\n","    module_name = os.path.basename(normalized_root)\n","    # Add the module name to the dictionary if it doesn't exist\n","    if module_name not in module_structure:\n","      module_structure[module_name] = []\n","\n","    # Add the file names to the list for the module\n","    module_structure[module_name].extend([os.path.join(normalized_root, file).replace('\\\\', '/') for file in files])\n","\n","  return module_structure\n","\n","def get_module_csv(directory_path):\n","    \"\"\" Generates a list of all .csv files and their module structure within a given directory.\n","\n","    Args:\n","      directory_path (str): Path to the directory you want to explore.\n","\n","    Returns:\n","      A dictionary representing the module structure, with keys as module names and values as lists of .csv file names.\n","    \"\"\"\n","\n","    module_csv = {}\n","\n","    for root, dirs, files in os.walk(directory_path):\n","        # Normalize the root path to ensure consistent separators\n","        normalized_root = os.path.normpath(root).replace('\\\\', '/')\n","        # Get the module name from the directory path\n","        module_name = os.path.basename(normalized_root)\n","\n","        # Add the module name to the dictionary if it doesn't exist\n","        if module_name not in module_csv:\n","            module_csv[module_name] = []\n","\n","        # Add only .csv file names to the list for the module\n","        csv_files = [file for file in files if file.lower().endswith('.csv')]\n","        module_csv[module_name].extend([os.path.join(normalized_root, file).replace('\\\\', '/') for file in csv_files])\n","\n","    return module_csv\n","\n","\n","def get_module_json(directory_path):\n","    \"\"\" Generates a list of all .json files and their module structure within a given directory.\n","\n","    Args:\n","      directory_path (str): Path to the directory you want to explore.\n","\n","    Returns:\n","      A dictionary representing the module structure, with keys as module names and values as lists of .json file names.\n","    \"\"\"\n","    module_json = {}\n","\n","    for root, dirs, files in os.walk(directory_path):\n","        # Normalize the root path to ensure consistent separators\n","        normalized_root = os.path.normpath(root).replace('\\\\', '/')\n","\n","        # Get the module name from the directory path\n","        module_name = os.path.basename(normalized_root)\n","\n","        # Add the module name to the dictionary if it doesn't exist\n","        if module_name not in module_json:\n","            module_json[module_name] = []\n","\n","        # Add only .json file names to the list for the module\n","        json_files = [file for file in files if file.lower().endswith('.json')]\n","        module_json[module_name].extend([os.path.join(normalized_root, file).replace('\\\\', '/') for file in json_files])\n","\n","    return module_json\n","\n","def normalize_json_data(data):\n","    \"\"\"\n","    Normalizes JSON data to a list of dictionaries if possible.\n","    \"\"\"\n","    # If it's a dictionary, wrap it in a list\n","    if isinstance(data, dict):\n","        return [data]\n","    # If it's a list of lists or primitives, convert it to a list of dictionaries\n","    elif isinstance(data, list) and all(not isinstance(item, dict) for item in data):\n","        # Assume each item in the list can be a row in the CSV\n","        return [{str(index): value for index, value in enumerate(item)} if isinstance(item, list) else {'value': item} for item in data]\n","    # If it's already a list of dictionaries, no need to modify\n","    elif isinstance(data, list) and all(isinstance(item, dict) for item in data):\n","        return data\n","    else:\n","        raise ValueError('JSON data structure is not supported.')\n","def json_to_csv(json_file_path, csv_file_path):\n","    \"\"\"\n","    Converts a JSON file into a CSV file.\n","    \"\"\"\n","    try:\n","        with open(json_file_path, 'r', encoding='utf-8') as json_file:\n","            data = json.load(json_file)\n","        normalized_data = normalize_json_data(data)\n","        headers = set().union(*(d.keys() for d in normalized_data))\n","        with open(csv_file_path, 'w', encoding='utf-8', newline='') as csv_file:\n","            writer = csv.DictWriter(csv_file, fieldnames=headers)\n","            writer.writeheader()\n","            writer.writerows(normalized_data)\n","        print(f\"Converted JSON to CSV: {csv_file_path}\")\n","    except Exception as e:\n","        print(f\"Error converting JSON to CSV: {e}\")\n","        traceback.print_exc()\n","\n","# def json_to_csv(json_file_path, csv_file_path):\n","#     \"\"\"\n","#     Converts a JSON file into a CSV file.\n","\n","#     Args:\n","#       json_file_path (str): Path to the input JSON file.\n","#       csv_file_path (str): Path to the output CSV file.\n","#     \"\"\"\n","#     try:\n","#         # Check if JSON file exists\n","#         if not os.path.isfile(json_file_path):\n","#             raise FileNotFoundError(f\"JSON file does not exist: {json_file_path}\")\n","\n","#         # Read JSON data\n","#         with open(json_file_path, 'r', encoding='utf-8') as json_file:\n","#             data = json.load(json_file)\n","\n","#         # Normalize the JSON data to a list of dictionaries\n","#         normalized_data = normalize_json_data(data)\n","\n","#         # Aggregate headers from all data entries\n","#         headers = {key for item in normalized_data for key in item.keys()}\n","\n","#         # Write CSV data\n","#         with open(csv_file_path, 'w', encoding='utf-8', newline='') as csv_file:\n","#             csv_writer = csv.DictWriter(csv_file, fieldnames=headers)\n","#             csv_writer.writeheader()  # Write the headers to the CSV file\n","#             csv_writer.writerows(normalized_data)  # Write data rows\n","\n","#         print(f\"Successfully converted {json_file_path} to {csv_file_path}.\")\n","\n","#     except json.JSONDecodeError:\n","#         print(f\"Error: Invalid JSON data in file {json_file_path}.\")\n","#     except FileNotFoundError as e:\n","#         print(str(e))\n","#     except ValueError as e:\n","#         print(str(e))\n","#     except Exception as e:\n","#         print(f\"An unexpected error occurred: {e}\")\n","\n","\n","\n","def print_random_row(dataframe):\n","    \"\"\"\n","    Prints a random row from a Pandas DataFrame.\n","\n","    Args:\n","      dataframe (pd.DataFrame): A Pandas DataFrame object.\n","\n","    Returns:\n","      str: A formatted string of the random row.\n","    \"\"\"\n","    if dataframe.empty:\n","        return \"The DataFrame is empty.\"\n","\n","    random_index = random.randint(0, len(dataframe) - 1)\n","    random_row = dataframe.iloc[random_index]\n","\n","    task = \"\\n\".join(f\"{column}: {value}\" for column, value in random_row.items())\n","    prompt = f\"Following the instructions:\\n{task}\"\n","    return prompt\n","def chunkify(text, length_of_chunk):\n","    \"\"\"\n","    Splits text into chunks of a given length.\n","\n","    Args:\n","        text (str): The text to be split.\n","        length_of_chunk (int): The maximum length of each chunk.\n","\n","    Returns:\n","        list of str: A list of text chunks.\n","    \"\"\"\n","    return [text[i:i+length_of_chunk] for i in range(0, len(text), length_of_chunk)]\n","def get_all_rows(dataframe, length_of_chunk=2048):\n","    \"\"\"\n","    Returns all rows from a Pandas DataFrame in specified chunk lengths.\n","\n","    Args:\n","        dataframe (pd.DataFrame): A Pandas DataFrame object.\n","        length_of_chunk (int): The maximum length of each chunk.\n","\n","    Returns:\n","        List[str]: A list of formatted strings of all the rows, chunked by length.\n","    \"\"\"\n","    if dataframe.empty:\n","        return [\"The DataFrame is empty.\"]\n","\n","    all_chunks = []\n","    for index, row in dataframe.iterrows():\n","        # Format the row into a string\n","        row_str = \"\\n\".join(f\"{column}: {value}\" for column, value in row.items())\n","        prompt = f\"Row {index}:\\n{row_str}\\n\"\n","        # Split the formatted row string into chunks\n","        row_chunks = chunkify(prompt, length_of_chunk)\n","        all_chunks.extend(row_chunks)\n","\n","    return all_chunks\n","\n","def process_json_file(json_file_path, length_of_chunk=2048):\n","    # Convert the JSON file to CSV format\n","    csv_file_path = json_file_path.replace('.json', '.csv')\n","    json_to_csv(json_file_path, csv_file_path)\n","\n","    # Load the CSV into a pandas DataFrame\n","    dataframe = pd.read_csv(csv_file_path)\n","\n","    # Get all rows from the DataFrame, formatted and chunked\n","    return get_all_rows(dataframe, length_of_chunk)\n","\n","def preprocess_text(text):\n","    \"\"\"\n","    Preprocesses the text.\n","\n","    Args:\n","      text (str): The text to preprocess.\n","\n","    Returns:\n","      str: The preprocessed text.\n","    \"\"\"\n","    # Example: simple preprocessing that removes punctuation and converts to lowercase.\n","    return text.translate(str.maketrans('', '', string.punctuation)).lower()\n","\n","def process_files_text(file_path, length_of_chunk=2048):\n","    \"\"\"\n","    Processes a file and returns a list of tuples containing the starting byte and the preprocessed chunk of text.\n","\n","    Args:\n","      file_path (str): The path to the file to process.\n","      length_of_chunk (int): The size of each chunk to be read and processed.\n","\n","    Returns:\n","      List[Tuple[int, str]]: A list of tuples containing the starting byte and the preprocessed chunk of text.\n","    \"\"\"\n","    # Check if file exists and is a file\n","    if not os.path.isfile(file_path):\n","        raise FileNotFoundError(f\"The file at path {file_path} does not exist.\")\n","\n","    processed_chunks = []\n","\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        while True:\n","            start_byte = file.tell()  # Get the current position in the file\n","            chunk = file.read(length_of_chunk)\n","            if not chunk:\n","                break  # End of file reached\n","            preprocessed_chunk = preprocess_text(chunk)\n","            processed_chunks.append((start_byte, preprocessed_chunk))\n","            if len(chunk) < length_of_chunk:\n","                break  # Last chunk, smaller than max size, processed\n","\n","    return processed_chunks\n","# def process_files(file_path, length_of_chunk=200):\n","#     try:\n","#         file_extension = get_file_extension(file_path)\n","#         if not file_extension:\n","#             # Optionally handle files with no extension differently here\n","#             raise ValueError(f'Error processing file {file_path}: File has no extension')\n","#         if file_extension == '.pdf':\n","#             return extract_text_from_pdf(file_path, length_of_chunk)\n","#         elif file_extension in ['.json', '.csv']:\n","#             return process_json_file(file_path, length_of_chunk)\n","#         elif file_extension in ['.txt', '.md', '.py', '.ipynb']:\n","#             return process_files_text(file_path, length_of_chunk)\n","#         else:\n","#             raise NotImplementedError(f'File type {file_extension} is not supported.')\n","#     except (ValueError, NotImplementedError) as e:\n","#         # Log error and continue with the next file\n","#         print(f\"Skipping file {file_path} due to error: {e}\")\n","#         return None\n","\n","def get_file_extension(file_path):\n","    _, file_extension = os.path.splitext(file_path)\n","    return file_extension.lower()\n","\n","def process_files(file_path, length_of_chunk=2048):\n","    \"\"\"\n","    Processes a file and returns a list of processed chunks of text.\n","    \"\"\"\n","    try:\n","        file_extension = get_file_extension(file_path)\n","        if not file_extension:\n","            raise ValueError(f'File {file_path} has no extension.')\n","\n","        if file_extension == '.pdf':\n","            return extract_text_from_pdf(file_path, length_of_chunk)\n","        elif file_extension == '.json':\n","            return process_json_file(file_path, length_of_chunk)\n","        elif file_extension == '.csv':\n","            dataframe = pd.read_csv(file_path)\n","            return get_all_rows(dataframe, length_of_chunk)\n","        elif file_extension in ['.txt', '.md', '.py']:\n","            return process_files_text(file_path, length_of_chunk)\n","        else:\n","            raise NotImplementedError(f'File type {file_extension} is not supported.')\n","\n","    except FileNotFoundError:\n","        print(f'File not found: {file_path}')\n","    except ValueError as ve:\n","        print(f'ValueError: {ve}')\n","    except NotImplementedError as nie:\n","        print(f'NotImplementedError: {nie}')\n","    except Exception as e:\n","        print(f'An unexpected error occurred while processing {file_path}: {e}')\n","        traceback.print_exc()\n","\n","# directory_path = \"/content/drive/MyDrive\"\n","# try:\n","#     module_structure = get_module_structure(directory_path)\n","#     for module, files in module_structure.items():\n","#         print(f'Module: {module}')\n","#         for file in files:\n","#             try:\n","#                 result = process_files(file, length_of_chunk=1000)\n","#                 # Only print the result if it's not None\n","#                 if result is not None:\n","#                     print(\"\\n\", result[:1][0])\n","#             except Exception as e:\n","#                 # Catch any other unexpected exceptions and log them\n","#                 print(f\"An error occurred while processing {file}: {e}\")\n","#         print(\"\\n\")\n","# except Exception as e:\n","#     print(f\"An error occurred while getting the module structure: {e}\")\n"],"metadata":{"id":"sNeCfmiEAOTb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q -U transformers"],"metadata":{"id":"RZ_CQr46W-Mh"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k1TGo7PFAEGa"},"outputs":[],"source":["import os\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n","\n","# Assuming process_files returns a processed text or list of texts\n","\n","class CustomTextDataset(Dataset):\n","    def __init__(self, tokenizer, file_paths, length_of_chunk):\n","        self.tokenizer = tokenizer\n","        self.texts = []\n","        self.length_of_chunk = length_of_chunk\n","        self.file_paths = file_paths\n","        for file_path in file_paths:\n","            self.texts.extend(self._load_and_process_file(file_path))\n","\n","    def _load_and_process_file(self, file_path):\n","        processed_data = process_files(file_path, self.length_of_chunk)\n","        if processed_data is None:\n","            return []\n","        return processed_data\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        return self.tokenizer(self.texts[idx], return_tensors='pt', truncation=True, padding='max_length', max_length=512)\n","\n","# Initialize tokenizer and model\n","model_name = 'gpt2'  # for example, use the appropriate model\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","# Get file paths from the module structure\n","directory_path = \"/content/drive/MyDrive/Applied-Deep-Learning/02 - Natural Language Processing\"\n","file_paths = []\n","try:\n","    module_structure = get_module_structure(directory_path)\n","    for module, files in module_structure.items():\n","        file_paths.extend([os.path.join(module, f) for f in files])\n","except Exception as e:\n","    print(f\"An error occurred while getting the module structure: {e}\")\n","\n","# Create the Custom Dataset\n","dataset = CustomTextDataset(tokenizer, file_paths, length_of_chunk=1000)\n","data_loader = DataLoader(dataset, batch_size=1, shuffle=True)  # Adjust batch size according to your GPU capacity\n","\n","# # Define Trainer and Training Arguments\n","# training_args = TrainingArguments(\n","#     output_dir=\"./model_output\",  # Set to your desired output directory\n","#     per_device_train_batch_size=1,  # Adjust based on your GPU memory and model size\n","#     logging_dir='./logs',\n","#     logging_steps=10,\n","# )\n","\n","# trainer = Trainer(\n","#     model=model,\n","#     args=training_args,\n","#     train_dataset=dataset,\n","#     data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'][0] for f in data]),\n","#                                 'attention_mask': torch.stack([f['attention_mask'][0] for f in data]),\n","#                                 'labels': torch.stack([f['input_ids'][0] for f in data])},\n","# )\n","\n","# # Start the training\n","# trainer.train()\n","\n","# # Save the model\n","# trainer.save_model(output_dir='./model_output')"]},{"cell_type":"code","source":["!pip install -q -U transformers --upgrade"],"metadata":{"id":"4RTWmpKtQia2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define Trainer and Training Arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./model_output\",  # The output directory for model checkpoints and predictions\n","    overwrite_output_dir=True,  # Overwrite the content of the output directory\n","    do_train=True,  # Whether to run training\n","    do_eval=True,  # Whether to run evaluation on the dev dataset\n","    do_predict=True,  # Whether to run predictions on the test dataset\n","    evaluation_strategy=\"steps\",  # The evaluation strategy to adopt during training: steps, epoch, or no\n","    prediction_loss_only=True,  # Whether to only return the loss when predicting\n","    per_device_train_batch_size=4,  # Batch size per device during training, adjust based on GPU memory\n","    per_device_eval_batch_size=4,  # Batch size per device for evaluation, adjust based on GPU memory\n","    gradient_accumulation_steps=1,  # Number of updates steps to accumulate before performing a backward pass\n","    eval_accumulation_steps=None,  # Number of predictions steps to accumulate before moving the tensors to the CPU\n","    eval_delay=None,  # The delay for evaluation in seconds (to throttle the frequency of eval loops)\n","    learning_rate=5e-5,  # The initial learning rate for Adam\n","    weight_decay=0,  # Weight decay for Adam\n","    adam_beta1=0.9,  # Beta1 for Adam\n","    adam_beta2=0.999,  # Beta2 for Adam\n","    adam_epsilon=1e-8,  # Epsilon for Adam\n","    max_grad_norm=1.0,  # Max gradient norm\n","    num_train_epochs=3,  # Total number of training epochs to perform\n","    max_steps=-1,  # If > 0: set total number of training steps to perform. Override num_train_epochs\n","    lr_scheduler_type=\"linear\",  # The scheduler type to use: linear, cosine, etc\n","    warmup_ratio=0.0,  # Linear warmup over warmup_ratio fraction of total steps\n","    warmup_steps=0,  # Linear warmup over warmup_steps\n","    log_level=\"info\",  # Logger verbosity\n","    logging_dir='./logs',  # Directory for storing logs\n","    logging_steps=10,  # Log every X updates steps\n","    save_strategy=\"steps\",  # The checkpoint save strategy to adopt during training: steps, epoch, no\n","    save_steps=500,  # Save checkpoint every X updates steps\n","    save_total_limit=None,  # Limit the total amount of checkpoints. Deletes the older checkpoints\n","    seed=42,  # Random seed for initialization\n","    disable_tqdm=False,  # Whether to disable the progress bars\n","    load_best_model_at_end=True,  # Whether to load the best model found at each evaluation\n","    metric_for_best_model=\"loss\",  # The metric to use to compare two different models\n","    greater_is_better=False,  # Whether the `metric_for_best_model` should be maximized or not\n","    dataloader_num_workers=0,  # Number of subprocesses to use for data loading\n","    label_smoothing_factor=0.0,  # The label smoothing epsilon to use (0 means no label smoothing)\n","    # Add any additional arguments you need here\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset,\n","    data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'][0] for f in data]),\n","                                'attention_mask': torch.stack([f['attention_mask'][0] for f in data]),\n","                                'labels': torch.stack([f['input_ids'][0] for f in data])},\n","    # Add any additional Trainer configuration here\n",")\n","\n","# Start the training\n","trainer.train()\n","\n","# Save the model\n","trainer.save_model(output_dir='./model_output')"],"metadata":{"id":"iR3qvonAPltl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!"],"metadata":{"id":"2FNnKoAqXoWz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install -q -U transformers==4.35.2"],"metadata":{"id":"TLZ0YhG1VUFv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install accelerate -U"],"metadata":{"id":"3H2suBmSSt2S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/myshell-ai/OpenVoice.git"],"metadata":{"id":"dEZe0YpaYQ3I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cd OpenVoice/\n","!ls"],"metadata":{"id":"esc7z8HoYUzc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q -U -r  requirements.txt"],"metadata":{"id":"l6IiCsbOYiVD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python api.py"],"metadata":{"id":"_W20W6P0YsGw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","import se_extractor\n","from api import BaseSpeakerTTS, ToneColorConverter"],"metadata":{"id":"ll22SGgxZS9p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ckpt_base = 'checkpoints/base_speakers/EN'\n","ckpt_converter = 'checkpoints/converter'\n","device = 'cuda:0'\n","output_dir = 'outputs'\n","\n","base_speaker_tts = BaseSpeakerTTS(f'{ckpt_base}/config.json', device=device)\n","base_speaker_tts.load_ckpt(f'{ckpt_base}/checkpoint.pth')\n","\n","tone_color_converter = ToneColorConverter(f'{ckpt_converter}/config.json', device=device)\n","tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')\n","\n","os.makedirs(output_dir, exist_ok=True)"],"metadata":{"id":"HRQ3DdKSZmfP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"],"metadata":{"id":"UDMgVSqUaVUd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer"],"metadata":{"id":"dThJrq7Zab_G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = 'gpt2'"],"metadata":{"id":"v9mGLJAMakPC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The instruction dataset to use\n","dataset_name = \"mlabonne/guanaco-llama2-1k\"\n","\n","# Fine-tuned model name\n","new_model = \"llama-2-7b-miniguanaco\""],"metadata":{"id":"Z-X293R1anWz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The model that you want to train from the Hugging Face hub\n","model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n","\n","# The instruction dataset to use\n","dataset_name = \"mlabonne/guanaco-llama2-1k\"\n","\n","# Fine-tuned model name\n","new_model = \"llama-2-7b-miniguanaco\"\n","\n","################################################################################\n","# QLoRA parameters\n","################################################################################\n","\n","# LoRA attention dimension\n","lora_r = 64\n","\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","\n","################################################################################\n","# TrainingArguments parameters\n","################################################################################\n","\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"./results\"\n","\n","# Number of training epochs\n","num_train_epochs = 1\n","\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = False\n","bf16 = False\n","\n","# Batch size per GPU for training\n","per_device_train_batch_size = 4\n","\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 4\n","\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-4\n","\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.001\n","\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","\n","# Learning rate schedule\n","lr_scheduler_type = \"cosine\"\n","\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = -1\n","\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = True\n","\n","# Save checkpoint every X updates steps\n","save_steps = 0\n","\n","# Log every X updates steps\n","logging_steps = 25\n","\n","################################################################################\n","# SFT parameters\n","################################################################################\n","\n","# Maximum sequence length to use\n","max_seq_length = None\n","\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = False\n","\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0}"],"metadata":{"id":"IGJNWXXza1hA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install accelerate"],"metadata":{"id":"_Kat4JncbfH2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load dataset (you can process it here)\n","dataset = load_dataset(dataset_name, split=\"train\")\n","\n","# Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","\n","# Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    # device_map=device_map,\n","    low_cpu_mem_usage=True\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","\n","# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"tensorboard\"\n",")\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")\n","\n","# Train model\n","trainer.train()\n","\n","# Save trained model\n","trainer.model.save_pretrained(new_model)"],"metadata":{"id":"R_wrerl1a851"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ignore warnings\n","logging.set_verbosity(logging.CRITICAL)\n","\n","# Run text generation pipeline with our next model\n","prompt = \"What is a large language model?\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"],"metadata":{"id":"8sORz5pvbJmt"},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyM05ODqnOReODMgRKY+kzwi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dh-9fRG0st8b"},"outputs":[],"source":["!pip install -q -U transformers"]},{"cell_type":"code","source":["# Replace 'your_file.txt' with the path to your input text file\n","file_path = 'your_file.txt'\n","\n","# Writing 100 questions to the specified file\n","with open(file_path, 'w') as file:\n","    for i in range(1, 101):\n","        question = f\"Question {i}: What is your favorite color?\"\n","        file.write(question + '\\n')\n"],"metadata":{"id":"QwNI1-pBuh2y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cat your_file.txt"],"metadata":{"id":"6HHLaQkNukOy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import BertTokenizer\n","\n","def tokenize_line(tokenizer, line, max_length, padding, truncation):\n","    tokens = tokenizer.tokenize(line)\n","    tokens = tokens[:max_length] if truncation else tokens\n","    tokens = tokens + ['[PAD]'] * (max_length - len(tokens)) if padding else tokens[:max_length]\n","    input_ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokens)]).long()\n","    return input_ids, len(tokens)\n","\n","def process_file(file_path, tokenizer, max_length, padding, truncation):\n","    with open(file_path, 'r') as file:\n","        for line in file:\n","            input_ids, token_length = tokenize_line(tokenizer, line.strip(), max_length, padding, truncation)\n","            print(f'Input IDs: {input_ids.tolist()}, Token Length: {token_length}')\n","\n","if __name__ == '__main__':\n","    # Replace 'bert-base-uncased' with the desired pre-trained model\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","    # Set the desired max_length, padding, and truncation values\n","    max_length = 1000\n","    padding = True\n","    truncation = True\n","\n","    # Replace 'your_file.txt' with the path to your input text file\n","    file_path = 'your_file.txt'\n","\n","    process_file(file_path, tokenizer, max_length, padding, truncation)"],"metadata":{"id":"xSm5cb2VuTB7"},"execution_count":null,"outputs":[]}]}
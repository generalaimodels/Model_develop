{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1nEUmZNNB5gn4t1bf2J2Nc0Uka-O3Xv4T","authorship_tag":"ABX9TyNSBwYXqQds+rWhiOXVgdZq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"id":"KBrKi69hwfTI","executionInfo":{"status":"ok","timestamp":1705388677148,"user_tz":-330,"elapsed":13930,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}}},"outputs":[],"source":["!pip install -q -U transformers==4.35"]},{"cell_type":"code","source":[],"metadata":{"id":"Tz3qjBS6LMPo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","from typing import Tuple\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    PreTrainedModel,\n","    PreTrainedTokenizer\n",")\n","\n","def load_model_and_tokenizer(model_name_or_path: str) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n","    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True)\n","    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n","\n","    if tokenizer.pad_token is None:\n","        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n","\n","    return model, tokenizer\n","\n","def generate_text(\n","    model: PreTrainedModel,\n","    tokenizer: PreTrainedTokenizer,\n","    prompt: str,\n","    history: str = \"\",\n","    max_length: int = 100,\n","    temperature: float = 0.9\n",") -> str:\n","    full_prompt = history + \"\\n\" + prompt\n","    input_ids = tokenizer(full_prompt, return_tensors=\"pt\").input_ids\n","\n","    gen_tokens = model.generate(\n","        input_ids,\n","        do_sample=True,\n","        temperature=temperature,\n","        max_length=max_length\n","    )\n","\n","    gen_text = tokenizer.decode(gen_tokens[0])\n","\n","    return gen_text\n","\n","model_name_or_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","model, tokenizer = load_model_and_tokenizer(model_name_or_path)\n","\n","num_iterations = 2\n","file_json = 'output1.json'\n","\n","output_list = []\n","history = \"\"\n","\n","# Few-shot examples\n","examples = [\n","    {\"question\": \"What is the capital of France?\", \"answer\": \"The capital of France is Paris.\"},\n","    {\"question\": \"Who is the president of the United States?\", \"answer\": \"The president of the United States is Joe Biden.\"}\n","]\n","\n","# Add few-shot examples to history\n","for example in examples:\n","    history += f\"Question: {example['question']}\\nAnswer: {example['answer']}\\n\"\n","\n","for i in range(num_iterations):\n","    prompt_Q = f\"Generate adversarial question ({i + 1})\"\n","    Question = generate_text(model, tokenizer, prompt_Q, history=history)\n","    history += \"\\n\" + Question\n","\n","    prompt_A = f'Answer to: {Question}'\n","    Answer = generate_text(model, tokenizer, prompt_A, history=history)\n","    history += \"\\n\" + Answer\n","\n","    output_list.append({'prompt': Question, 'generated_text': Answer})\n","\n","with open(file_json, mode='w', encoding='utf-8') as jsonfile:\n","    json.dump(output_list, jsonfile, ensure_ascii=False, indent=4)\n"],"metadata":{"id":"FoqD6To5CYfx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","from typing import Tuple\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    PreTrainedModel,\n","    PreTrainedTokenizer\n",")\n","\n","def load_model_and_tokenizer(model_name_or_path: str) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n","    try:\n","        model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True)\n","        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n","\n","        if tokenizer.pad_token is None:\n","            tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n","\n","        return model, tokenizer\n","    except Exception as e:\n","        raise RuntimeError(f\"Failed to load model and tokenizer: {e}\")\n","\n","def generate_text(\n","    model: PreTrainedModel,\n","    tokenizer: PreTrainedTokenizer,\n","    prompt: str,\n","    max_length: int = 100,\n","    temperature: float = 0.9\n",") -> str:\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","\n","    gen_tokens = model.generate(\n","        input_ids,\n","        do_sample=True,\n","        temperature=temperature,\n","        max_length=max_length\n","    )\n","\n","    gen_text = tokenizer.decode(gen_tokens[0])\n","\n","    return gen_text\n","\n","# Load the model and tokenizer once\n","model_name_or_path = \"gpt2\"\n","model, tokenizer = load_model_and_tokenizer(model_name_or_path)\n","\n","# Specify the number of iterations to generate text\n","num_iterations = 2\n","file_json = 'output1.json'\n","\n","# Generate text and save to JSON file\n","output_list = []\n","\n","for i in range(num_iterations):\n","    prompt_Q = f\"Generate adversarial question ({i + 1})\"\n","    Question = generate_text(model, tokenizer, prompt_Q)\n","    prompt_A = f'Answer to: {Question}'\n","    Answer = generate_text(model, tokenizer, prompt_A)\n","    output_list.append({'prompt': Question, 'generated_text': Answer})\n","\n","# Write list of dictionaries to a JSON file\n","with open(file_json, mode='w', encoding='utf-8') as jsonfile:\n","    json.dump(output_list, jsonfile, ensure_ascii=False, indent=4)"],"metadata":{"id":"xgLTqAsujWV4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","from typing import Tuple, Dict\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    PreTrainedModel,\n","    PreTrainedTokenizer\n",")\n","\n","def load_model_and_tokenizer(model_name_or_path: str) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n","    try:\n","        model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True)\n","        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n","\n","        if tokenizer.pad_token is None:\n","            tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n","\n","        return model, tokenizer\n","    except Exception as e:\n","        raise RuntimeError(f\"Failed to load model and tokenizer: {e}\")\n","\n","def generate_text(\n","    model: PreTrainedModel,\n","    tokenizer: PreTrainedTokenizer,\n","    prompt: str,\n","    max_length: int = 100,\n","    temperature: float = 0.9\n",") -> str:\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","\n","    gen_tokens = model.generate(\n","        input_ids,\n","        do_sample=True,\n","        temperature=temperature,\n","        max_length=max_length\n","    )\n","\n","    gen_text = tokenizer.decode(gen_tokens[0])\n","\n","    return gen_text\n","\n","# Load the model and tokenizer once\n","model_name_or_path = \"gpt2\"\n","model, tokenizer = load_model_and_tokenizer(model_name_or_path)\n","\n","# Specify the number of iterations to generate text\n","num_iterations = 10  # Set this to your desired number\n","file_json = 'output1.json'\n","\n","# Function to write a single entry to the JSON file\n","def write_entry_to_json(file_path: str, data: Dict, mode: str='a'):\n","    # Open the file in the append mode (or write mode if it's the first entry)\n","    with open(file_path, mode, encoding='utf-8') as file:\n","        if mode == 'w':  # If it's the first entry, write an opening bracket to start the JSON array\n","            file.write('[\\n')\n","        else:  # For subsequent entries, write a comma to separate JSON objects\n","            file.write(',\\n')\n","        json.dump(data, file, ensure_ascii=False, indent=4)\n","\n","# Create the JSON file and write the first entry separately to handle the opening bracket\n","for i in range(num_iterations):\n","    prompt_Q = f\"Generate adversarial question ({i + 1})\"\n","    Question = generate_text(model, tokenizer, prompt_Q)\n","    prompt_A = f'Answer to: {Question}'\n","    Answer = generate_text(model, tokenizer, prompt_A)\n","    entry = {'prompt': Question, 'generated_text': Answer}\n","\n","    if i == 0:\n","        write_entry_to_json(file_json, entry, 'w')\n","    else:\n","        write_entry_to_json(file_json, entry)\n","\n","# Closing the JSON array with a bracket\n","with open(file_json, 'a', encoding='utf-8') as file:\n","    file.write('\\n]')"],"metadata":{"id":"v3SPC6pgkO-E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704970910239,"user_tz":-330,"elapsed":41673,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"26897301-5685-43bf-c0fc-5264391fa715"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]}]},{"cell_type":"code","source":["import csv\n","from typing import Tuple\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    PreTrainedModel,\n","    PreTrainedTokenizer\n",")\n","\n","def load_model_and_tokenizer(model_name_or_path: str) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n","    try:\n","        model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True)\n","        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n","\n","        if tokenizer.pad_token is None:\n","            tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n","\n","        return model, tokenizer\n","    except Exception as e:\n","        raise RuntimeError(f\"Failed to load model and tokenizer: {e}\")\n","\n","def generate_text(\n","    model: PreTrainedModel,\n","    tokenizer: PreTrainedTokenizer,\n","    prompt: str,\n","    max_length: int = 100,\n","    temperature: float = 0.9\n",") -> str:\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","\n","    gen_tokens = model.generate(\n","        input_ids,\n","        do_sample=True,\n","        temperature=temperature,\n","        max_length=max_length\n","    )\n","\n","    gen_text = tokenizer.batch_decode(gen_tokens)[0]\n","\n","    return gen_text\n","\n","# Load the model and tokenizer once\n","model_name_or_path = \"gpt2\"\n","model, tokenizer = load_model_and_tokenizer(model_name_or_path)\n","\n","# Specify the number of iterations to generate text\n","num_iterations = 2\n","file_csv='output1.csv'\n","# Generate text and write to CSV file\n","with open(file_csv, mode='w', newline='', encoding='utf-8') as csvfile:\n","    fieldnames = ['prompt', 'generated_text']\n","    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","    writer.writeheader()\n","\n","    for i in range(num_iterations):\n","        prompt_Q = f\" Generate adversarical question ({i + 1})\"\n","        Question = generate_text(model, tokenizer, prompt_Q)\n","        prompt_A=f'answer to {Question }'\n","        Answer = generate_text(model, tokenizer, prompt_A)\n","        writer.writerow({'prompt': Question, 'generated_text': Answer})"],"metadata":{"id":"96spvkQEav5f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"],"metadata":{"id":"EJusCXeigz5r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer\n","\n","\n","# The model that you want to train from the Hugging Face hub\n","model_name = \"gpt2\"\n","\n","# The instruction dataset to use\n","dataset_name = \"mlabonne/guanaco-llama2-1k\"\n","\n","# Fine-tuned model name\n","new_model = \"llama-2-7b-miniguanaco\"\n","\n","################################################################################\n","# QLoRA parameters\n","################################################################################\n","\n","# LoRA attention dimension\n","lora_r = 64\n","\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","\n","################################################################################\n","# TrainingArguments parameters\n","################################################################################\n","\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"./results\"\n","\n","# Number of training epochs\n","num_train_epochs = 1\n","\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = False\n","bf16 = False\n","\n","# Batch size per GPU for training\n","per_device_train_batch_size = 4\n","\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 4\n","\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-4\n","\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.001\n","\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","\n","# Learning rate schedule\n","lr_scheduler_type = \"cosine\"\n","\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = -1\n","\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = True\n","\n","# Save checkpoint every X updates steps\n","save_steps = 0\n","\n","# Log every X updates steps\n","logging_steps = 25\n","\n","################################################################################\n","# SFT parameters\n","################################################################################\n","\n","# Maximum sequence length to use\n","max_seq_length = None\n","\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = False\n","\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0}\n","\n","\n","# Load dataset (you can process it here)\n","dataset = load_dataset(dataset_name, split=\"train\")\n","\n","# Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","\n","# # Check GPU compatibility with bfloat16\n","# if compute_dtype == torch.float16 and use_4bit:\n","#     major, _ = torch.cuda.get_device_capability()\n","#     if major >= 8:\n","#         print(\"=\" * 80)\n","#         print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","#         print(\"=\" * 80)\n","\n","# Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","\n","# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"tensorboard\"\n",")\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")\n","\n","# Train model\n","trainer.train()\n","\n","# Save trained model\n","trainer.model.save_pretrained(new_model)\n","\n","\n","# Ignore warnings\n","logging.set_verbosity(logging.CRITICAL)\n","\n","# Run text generation pipeline with our next model\n","prompt = \"What is a large language model?\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=400)\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"],"metadata":{"id":"zi4oSyTYgrBe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv\n","\n","with open('/content/drive/MyDrive/training.csv', 'r') as csvfile, open('/content/drive/MyDrive/new_output.csv', 'a', newline='') as outfile:\n","    reader = csv.reader(csvfile)\n","    writer = csv.writer(outfile)\n","\n","    next(reader)  # Skip header row\n","    for row in reader:\n","        input_text = row[1]\n","        # Process input_text or perform any operations needed\n","        print(f\"Input: {input_text}\")\n","\n","        # Append data from row[2] to new_output.csv\n","        output_data = row[2]  # Assuming row[2] contains the data you want to append\n","        writer.writerow([output_data])  # Write the data to new_output.csv\n"],"metadata":{"id":"ycRDPOV4xFSD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","model_directory='gpt2'\n","import csv\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","model = AutoModelForCausalLM.from_pretrained(model_directory)\n","tokenizer = AutoTokenizer.from_pretrained(model_directory)\n","\n","with open('/content/drive/MyDrive/training.csv', 'r') as csvfile, open('/content/drive/MyDrive/new_output.csv', 'a', newline='') as outfile:\n","    reader = csv.reader(csvfile)\n","    writer = csv.writer(outfile)\n","\n","    next(reader)\n","    for row in reader:\n","        input_text = row[1]\n","\n","        encoded_input = tokenizer.encode(input_text, return_tensors='pt')\n","        output = model.generate(encoded_input, max_length=256)\n","\n","        decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","        print(f\"Input: {input_text}\")\n","        print(f\"Output: {decoded_output}\")\n","\n","        writer.writerow([decoded_output]) # Write generated text to new csv"],"metadata":{"id":"spW2DoD7wyTY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv\n","\n","model_directory = 'gpt-2'\n","csv_file = 'questions_answers.csv'\n","\n","model = AutoModelForCausalLM.from_pretrained(model_directory)\n","tokenizer = AutoTokenizer.from_pretrained(model_directory)\n","\n","# Set pad token to eos token if it's not already set\n","if tokenizer.pad_token is None:\n","    # Add special tokens (eos_token as pad_token)\n","    tokenizer.add_special_tokens({'pad_token':tokenizer.eos_token})\n","\n","# Generate a question\n","def generate_question(model, tokenizer, max_length=4048):\n","    prompt = \"Generate a question: \"\n","    generated = model.generate(tokenizer.encode(prompt, return_tensors=\"pt\"), max_length=max_length)\n","    decoded_output = tokenizer.decode(generated[0], skip_special_tokens=True)\n","    return decoded_output\n","\n","question = generate_question(model, tokenizer)\n","print(\"\\nGenerated question:\", question)\n","\n","# Generate an answer to the question\n","def generate_answer(model, tokenizer, question, max_length=4048):\n","    generated = model.generate(tokenizer.encode(question, return_tensors=\"pt\"), max_length=max_length)\n","    decoded_output = tokenizer.decode(generated[0], skip_special_tokens=True)\n","    return decoded_output\n","\n","answer = generate_answer(model, tokenizer, question)\n","print(\"\\nGenerated answer:\", answer)\n","\n","# Append the question and answer to the CSV file\n","def append_to_csv(csv_file, question, answer):\n","    with open(csv_file, mode='a', newline='', encoding='utf-8') as f:\n","        question_answer = f'{question},{answer}\\n'\n","        f.write(question_answer)\n","\n","append_to_csv(csv_file, question, answer)\n","print(\"\\nQuestion and answer appended to CSV file.\")"],"metadata":{"id":"7gpJhVoH8q0f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Append the question and answer to the CSV file\n","def append_to_csv(csv_file, question, answer):\n","    with open(csv_file, mode='a', newline='', encoding='utf-8') as f:\n","        question_answer = f'{question},{answer}\\n'\n","        f.write(question_answer)\n","csvfile='test.csv'\n","question='Q:'\n","answer=\"A:\"\n","append_to_csv(csv_file, question, answer)\n","\n","\n","for i in range(100):\n","    append_to_csv(csv_file, question, answer)"],"metadata":{"id":"I_KCWhocE7U0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Append the question, answer, and answer1 to the CSV file\n","def append_to_csv(csv_file, question, answer, answer1):\n","    with open(csv_file, mode='a', newline='', encoding='utf-8') as f:\n","        question_answer = f'{question},{answer},{answer1}\\n'\n","        f.write(question_answer)\n","\n","csv_file = 'test.csv'\n","question = 'Q:'\n","answer = 'A:'\n","answer1 = 'a2:'\n","\n","append_to_csv(csv_file, question, answer, answer1)\n","\n","for i in range(100):\n","    # Modify answer and answer1 here if needed\n","    modified_answer = f'A: {i}'  # Example modification\n","    modified_answer1 = f'a2: {i}'  # Example modification\n","\n","    append_to_csv(csv_file, question, modified_answer, modified_answer1)\n"],"metadata":{"id":"OKim1HN6FRdX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","model_directory = '/home/khemanth/LLMs_Models/Hemanth_LLMs/snapshots/37892f30c23786c0d5367d80481fa0d9fba93cf8'\n","csv_file = 'questions_answers.csv'\n","\n","model = AutoModelForCausalLM.from_pretrained(model_directory)\n","tokenizer = AutoTokenizer.from_pretrained(model_directory)\n","\n","# Set pad token to eos token if it's not already set\n","if tokenizer.pad_token is None:\n","    # Add special tokens (eos_token as pad_token)\n","    tokenizer.add_special_tokens({'pad_token':tokenizer.eos_token})\n","\n","\n","question_instuction=\"\"\"\n","\n","Design a prompt that directs a language model to generate advanced adversarial questions, focusing on exposing weaknesses and testing the resilience of targeted models or systems. Make sure the prompt is concise, well-defined, and provides clear instructions on how to approach the task.\n","\n","Questions:\n","\n","Can you provide more context regarding the specific application or domain where these adversarial questions will be utilized?\n","Are there any specific criteria or guidelines for the types of questions you want the model to generate?\n","Should the adversarial questions follow a particular format or structure, such as multiple-choice, yes/no, or open-ended questions?\n","Do you have any specific considerations or requirements in mind while formulating the prompt?\n","How advanced would you like the language model to be in terms of generating these adversarial questions? Should it focus on complex linguistic patterns, domain-specific knowledge, or any other specific aspects?\n","\n","\n","\"\"\"\n","# Generate a question\n","def generate_question(model, tokenizer, max_length=4048):\n","    prompt = f\"Generate a question: {question_instuction}\\n Generate only questions\"\n","    generated = model.generate(tokenizer.encode(prompt, return_tensors=\"pt\"), max_length=max_length)\n","    decoded_output = tokenizer.decode(generated[0], skip_special_tokens=True)\n","    return decoded_output\n","\n","question = generate_question(model, tokenizer)\n","print(\"\\nGenerated question:\", question)\n","\n","\n","\n","\n","answer_prompt=f\"\"\"\n","Chain of Thoughts:\n","\"Imagine you are a helpful assistant. Before providing an answer, think about the question carefully and consider the following: (1) What is the main concept of the question? (2) Are there any specific details or requirements mentioned in the question? (3) Based on your understanding, what would be a reasonable and accurate answer?\n","\n","NOTE:following below instructions\n","\"As a knowledgeable and resourceful assistant, you have access to a vast amount of information. When answering questions, consider the context, implications, and potential follow-up questions that might arise. Strive to provide comprehensive and accurate answers while ensuring they are easy to understand.\n","\n","Please generate a well-rounded and precise response, addressing the underlying concepts, implications, and connections to other related topics if necessary.\"\n","\n","Few-Shot Learning:\n","\"You are a helpful assistant with extensive knowledge across various domains. Below are examples of questions and answers. Study them carefully to understand the format and the depth of the responses.\n","\n","Question 1: {question}?\n","Answer 1:......\n","\n","\"\"\"\n","\n","\n","\n","\n","# Generate an answer to the question\n","def generate_answer(model, tokenizer, question, max_length=4048):\n","    generated = model.generate(tokenizer.encode(question, return_tensors=\"pt\"), max_length=max_length)\n","    decoded_output = tokenizer.decode(generated[0], skip_special_tokens=True)\n","    return decoded_output\n","\n","answer = generate_answer(model, tokenizer, question=answer_prompt)\n","print(\"\\nGenerated answer:\", answer)\n","\n","# Append the question and answer to the CSV file\n","def append_to_csv(csv_file, question, answer):\n","    with open(csv_file, mode='a', newline='', encoding='utf-8') as f:\n","        question_answer = f'{question},{answer}\\n'\n","        f.write(question_answer)\n","\n","\n","append_to_csv(csv_file, question, answer)\n","\n","for i in range(100000):\n","    append_to_csv(csv_file, question, answer)\n","\n","print(\"\\nQuestion and answer appended to CSV file.\")"],"metadata":{"id":"yJsikbH2JaDM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","from typing import Dict, Any\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","# Define the directory and file paths\n","model_directory: str = '/home/khemanth/LLMs_Models/Hemanth_LLMs/snapshots/37892f30c23786c0d5367d80481fa0d9fba93cf8'\n","json_file: str = 'questions_answers.json'\n","\n","# Load the model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained(model_directory)\n","tokenizer = AutoTokenizer.from_pretrained(model_directory)\n","\n","# Ensure the pad token is set correctly\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n","\n","# Define the function to generate a question\n","def generate_question(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, question_instruction: str, max_length: int = 4048) -> str:\n","    \"\"\"\n","    Generates a question based on the provided instruction using the given model and tokenizer.\n","\n","    :param model: The language model to generate the question.\n","    :param tokenizer: The tokenizer for the language model.\n","    :param question_instruction: The instruction to guide the question generation.\n","    :param max_length: The maximum length of the generated question.\n","    :return: The generated question as a string.\n","    \"\"\"\n","    try:\n","        prompt = f\"Generate a question: {question_instruction}\\n Generate only questions\"\n","        generated = model.generate(tokenizer.encode(prompt, return_tensors=\"pt\"), max_length=max_length)\n","        decoded_output = tokenizer.decode(generated[0], skip_special_tokens=True)\n","        return decoded_output\n","    except Exception as e:\n","        print(f\"An error occurred while generating the question: {e}\")\n","        return \"\"\n","\n","# Define the function to generate an answer\n","def generate_answer(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, answer_prompt: str, max_length: int = 4048) -> str:\n","    \"\"\"\n","    Generates an answer to the provided prompt using the given model and tokenizer.\n","\n","    :param model: The language model to generate the answer.\n","    :param tokenizer: The tokenizer for the language model.\n","    :param answer_prompt: The prompt to guide the answer generation.\n","    :param max_length: The maximum length of the generated answer.\n","    :return: The generated answer as a string.\n","    \"\"\"\n","    try:\n","        generated = model.generate(tokenizer.encode(answer_prompt, return_tensors=\"pt\"), max_length=max_length)\n","        decoded_output = tokenizer.decode(generated[0], skip_special_tokens=True)\n","        return decoded_output\n","    except Exception as e:\n","        print(f\"An error occurred while generating the answer: {e}\")\n","        return \"\"\n","\n","# Define the function to append the question and answer to the JSON file\n","def append_to_json(json_file: str, question: str, answer: str) -> None:\n","    \"\"\"\n","    Appends the generated question and answer to a JSON file.\n","\n","    :param json_file: The file path of the JSON file to append to.\n","    :param question: The generated question to append.\n","    :param answer: The generated answer to append.\n","    \"\"\"\n","    try:\n","        data: Dict[str, Any] = {'question': question, 'answer': answer}\n","        with open(json_file, 'a') as f:\n","            json.dump(data, f)\n","            f.write('\\n')\n","    except Exception as e:\n","        print(f\"An error occurred while appending to JSON: {e}\")\n","\n","# Main execution workflow\n","if __name__ == \"__main__\":\n","    question_instruction: str = \"...\"  # Insert the actual question instruction here.\n","    question: str = generate_question(model, tokenizer, question_instruction)\n","    print(\"\\nGenerated question:\", question)\n","\n","    answer_prompt: str = \"...\"  # Insert the actual answer prompt here.\n","    answer: str = generate_answer(model, tokenizer, answer_prompt)\n","    print(\"\\nGenerated answer:\", answer)\n","\n","    append_to_json(json_file, question, answer)\n","    print(\"\\nQuestion and answer appended to JSON file.\")"],"metadata":{"id":"YcSjFamaIKij"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","from typing import List, Dict, Any\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","# Define the directory and file paths\n","model_directory: str = '/path/to/cpp/code/generation/model'\n","json_file: str = 'cpp_code_dataset.json'\n","\n","# Load the model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained(model_directory)\n","tokenizer = AutoTokenizer.from_pretrained(model_directory)\n","\n","# Ensure the pad token is set correctly\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n","\n","# Define the function to generate a C++ code snippet\n","def generate_cpp_code(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, prompt: str, max_length: int = 512) -> str:\n","    \"\"\"\n","    Generates a C++ code snippet based on the provided prompt using the given model and tokenizer.\n","\n","    :param model: The language model to generate the C++ code.\n","    :param tokenizer: The tokenizer for the language model.\n","    :param prompt: The prompt to guide the code generation.\n","    :param max_length: The maximum length of the generated code.\n","    :return: The generated C++ code as a string.\n","    \"\"\"\n","    try:\n","        generated = model.generate(tokenizer.encode(prompt, return_tensors=\"pt\"), max_length=max_length)\n","        decoded_output = tokenizer.decode(generated[0], skip_special_tokens=True)\n","        return decoded_output\n","    except Exception as e:\n","        print(f\"An error occurred while generating the C++ code: {e}\")\n","        return \"\"\n","\n","# Define the function to append the code snippet to the JSON file\n","def append_to_json(json_file: str, code: str) -> None:\n","    \"\"\"\n","    Appends the generated C++ code snippet to a JSON file.\n","\n","    :param json_file: The file path of the JSON file to append to.\n","    :param code: The generated C++ code to append.\n","    \"\"\"\n","    try:\n","        data: Dict[str, Any] = {'cpp_code': code}\n","        with open(json_file, 'a') as f:\n","            json.dump(data, f)\n","            f.write('\\n')\n","    except Exception as e:\n","        print(f\"An error occurred while appending to JSON: {e}\")\n","\n","# Main execution workflow\n","if __name__ == \"__main__\":\n","    # Define the advanced C++ coding prompt\n","    cpp_prompt: str = \"\"\"\n","    You are an advanced C++ programmer tasked with demonstrating sophisticated algorithms and modern coding styles.\n","    Your code should include features like smart pointers, templates, and lambda functions, and exhibit algorithms such as graph traversal, dynamic programming, or sorting techniques.\n","\n","    Few-shot examples:\n","    - Implement a template function for quicksort.\n","    - Use smart pointers to manage memory in a graph structure.\n","    - Write a lambda function to filter even numbers from a list.\n","\n","    Chain of thought:\n","    - Consider the algorithm's time and space complexity.\n","    - Ensure proper use of C++11 (or newer) features.\n","    - Write clean and maintainable code with comments.\n","\n","    generate a C++ code snippet that performs a merge sort on a vector of integers using lambda expressions for comparison:\n","    \"\"\"\n","\n","    # Generate the C++ code snippets and append them to a JSON file\n","    for i in range(1000000):  # Example: Generate 100 C++ code snippets\n","        cpp_code: str = generate_cpp_code(model, tokenizer, cpp_prompt,max_length=4048)\n","        print(f\"\\nGenerated C++ code snippet {i+1}:\\n{cpp_code}\")\n","        append_to_json(json_file, cpp_code)\n","\n","    print(\"\\nC++ code snippets appended to JSON file.\")"],"metadata":{"id":"xxP6wN74JgZN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Append the question and answer to the CSV file\n","def append_to_csv(csv_file, question, answer):\n","    with open(csv_file, mode='a', newline='', encoding='utf-8') as f:\n","        question_answer = f'{question},{answer}\\n'\n","        f.write(question_answer)\n"],"metadata":{"id":"ggQVpq0wK66b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["csv_file=\"hemanth_test.csv\"\n","question=\"\"\"\n","\n","Design a prompt that directs a language model to generate advanced adversarial questions, focusing on exposing weaknesses and testing the resilience of targeted models or systems. Make sure the prompt is concise, well-defined, and provides clear instructions on how to approach the task.\n","\n","Questions:\n","\n","Can you provide more context regarding the specific application or domain where these adversarial questions will be utilized?\n","Are there any specific criteria or guidelines for the types of questions you want the model to generate?\n","Should the adversarial questions follow a particular format or structure, such as multiple-choice, yes/no, or open-ended questions?\n","Do you have any specific considerations or requirements in mind while formulating the prompt?\n","How advanced would you like the language model to be in terms of generating these adversarial questions? Should it focus on complex linguistic patterns, domain-specific knowledge, or any other specific aspects?\n","\n","\n","\"\"\"\n","answer=f\"\"\"\n","Chain of Thoughts:\n","\"Imagine you are a helpful assistant. Before providing an answer, think about the question carefully and consider the following: (1) What is the main concept of the question? (2) Are there any specific details or requirements mentioned in the question? (3) Based on your understanding, what would be a reasonable and accurate answer?\n","\n","NOTE:following below instructions\n","\"As a knowledgeable and resourceful assistant, you have access to a vast amount of information. When answering questions, consider the context, implications, and potential follow-up questions that might arise. Strive to provide comprehensive and accurate answers while ensuring they are easy to understand.\n","\n","Please generate a well-rounded and precise response, addressing the underlying concepts, implications, and connections to other related topics if necessary.\"\n","\n","Few-Shot Learning:\n","\"You are a helpful assistant with extensive knowledge across various domains. Below are examples of questions and answers. Study them carefully to understand the format and the depth of the responses.\n","\n","Question 1: {question}?\n","Answer 1:......\n","\n","\"\"\"\n","\n","\n","for i in range(100000):\n","    append_to_csv(csv_file, question, answer)\n","\n","print(\"\\nQuestion and answer appended to CSV file.\")"],"metadata":{"id":"YFcXyPhhJhox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import  AutoModelForCausalLM, AutoTokenizer\n","model_name='gpt2'\n","# Load the pre-trained model and tokenizer\n","model =  AutoModelForCausalLM.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Set pad token to eos token if it's not already set\n","if tokenizer.pad_token is None:\n","    # Add special tokens (eos_token as pad_token)\n","    tokenizer.add_special_tokens({'pad_token':tokenizer.eos_token})\n","# Define a function to generate answers\n","def generate_answer(question):\n","    # Encode the question\n","    input_ids = tokenizer.encode(question, return_tensors='pt')\n","\n","    # Generate the answer\n","    outputs = model.generate(input_ids, max_length=100)\n","\n","    # Decode the answer\n","    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    return decoded_output\n","\n","# Test the function\n","question = \"What is the capital of France?\"\n","answer = generate_answer(question)\n","print(answer)"],"metadata":{"id":"atrpnhiGNvuM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Step 1: Preparing the Dataset\n","- Step 2: Indexing the Dataset\n","- Step 3: Creating the RAG System\n","- Step 4: Fine-Tuning the System (Optional)\n","- Step 5: Evaluation and Usage"],"metadata":{"id":"Ycz6WkEWYLKP"}},{"cell_type":"code","source":[],"metadata":{"id":"OSyefxl8YZC_"},"execution_count":null,"outputs":[]}]}
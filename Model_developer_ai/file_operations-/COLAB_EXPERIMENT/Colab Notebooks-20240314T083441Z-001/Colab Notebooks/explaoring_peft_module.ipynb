{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyORRiArcPO16yUUYWQU+tYy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["- **Author** **:Kandimalla Hemanth**\n","- **Date of Last Modified** **:1-10-2024**\n","- **E-mail** **:speechcodehemanth2@gmail.com**\n","- **Github** **:https://github.com/HemanthIITJ**\n","\n"],"metadata":{"id":"L7xcXvW5Xgk9"}},{"cell_type":"code","source":["!pip install -q -U transformers accelerate evaluate deepspeed tqdm datasets"],"metadata":{"id":"CtINsmdb3hLu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q -U peft"],"metadata":{"id":"3KYXgkW-73ow"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","| Class Name                           | Explanation (Assumed based on class names) |\n","|--------------------------------------|--------------------------------------------|\n","| PeftConfig                           | Configuration settings for the Peft system.|\n","| PeftMixedModel                       | A model that combines different types of models or data within the Peft system.|\n","| PeftModel                            | The base model for the Peft system.|\n","| PeftModelForCausalLM                 | A Peft model specialized for causal language modeling tasks.|\n","| PeftModelForFeatureExtraction        | A Peft model designed to extract features from data.|\n","| PeftModelForQuestionAnswering        | A Peft model tailored for answering questions.|\n","| PeftModelForSeq2SeqLM                | A Peft model for sequence-to-sequence language modeling.|\n","| PeftModelForSequenceClassification   | A Peft model for classifying sequences into categories.|\n","| PeftModelForTokenClassification      | A Peft model for classifying individual tokens.|\n","| AdaLoraConfig                        | Configuration for AdaLora, an adaptive learning rate optimizer.|\n","| AdaLoraModel                         | A model that implements the AdaLora optimization technique.|\n","| AdaptionPromptConfig                 | Configuration for adapting prompts in a model.|\n","| IA3Config                            | Configuration for IA3, possibly an iterative algorithm or model.|\n","| IA3Model                             | A model that implements the IA3 configuration.|\n","| LoHaConfig                           | Configuration for LoHa, likely a specific model or method.|\n","| LoHaModel                            | A model that implements the LoHa configuration.|\n","| LoKrConfig                           | Configuration for LoKr, which could be a model or optimization technique.|\n","| LoKrModel                            | A model that implements the LoKr configuration.|\n","| LoraConfig                           | Configuration for Lora, which might be a learning rate optimizer or model.|\n","| LoraModel                            | A model that implements the Lora configuration.|\n","| MultitaskPromptTuningConfig          | Configuration for tuning prompts for multitask learning.|\n","| OFTConfig                            | Configuration for OFT, which could stand for a specific type of fine-tuning or optimization.|\n","| OFTModel                             | A model that implements the OFT configuration.|\n","| PrefixTuningConfig                   | Configuration for prefix tuning, a method for adapting language models.|\n","| PromptEncoderConfig                  | Configuration for an encoder that deals with prompts.|\n","| PromptTuningConfig                   | Configuration for tuning prompts in language models.|\n","| _prepare_prompt_learning_config      | A utility function to prepare the configuration for prompt learning.|\n","\n"],"metadata":{"id":"P-3-I8OJDVxY"}},{"cell_type":"markdown","source":["\n","\n","| Class | Explanation |\n","| --- | --- |\n","| PeftConfig | A base class for storing the parameters of a PEFT method |\n","| PeftModel | A base class for applying a PEFT method to any pretrained model from Transformers |\n","| PeftModelForCausalLM | A subclass of PeftModel for causal language modeling tasks |\n","| PeftModelForFeatureExtraction | A subclass of PeftModel for feature extraction tasks |\n","| PeftModelForQuestionAnswering | A subclass of PeftModel for question answering tasks |\n","| PeftModelForSeq2SeqLM | A subclass of PeftModel for sequence-to-sequence language modeling tasks |\n","| PeftModelForSequenceClassification | A subclass of PeftModel for sequence classification tasks |\n","| PeftModelForTokenClassification | A subclass of PeftModel for token classification tasks |\n","| PeftMixedModel | A class for applying different PEFT methods to different layers of a model |\n","| AdaLoraConfig | A subclass of PeftConfig for AdaLora, a PEFT method that adapts LoRA to different tasks |\n","| AdaLoraModel | A subclass of PeftModel for AdaLora |\n","| AdaptionPromptConfig | A subclass of PeftConfig for adaption prompts, a PEFT method that uses prompt embeddings to adapt a model to different tasks |\n","| IA3Config | A subclass of PeftConfig for IA3, a PEFT method that uses input-aware attention to reduce the number of parameters |\n","| IA3Model | A subclass of PeftModel for IA3 |\n","| LoHaConfig | A subclass of PeftConfig for LoHa, a PEFT method that uses low-rank approximation and hashing to reduce the number of parameters |\n","| LoHaModel | A subclass of PeftModel for LoHa |\n","| LoKrConfig | A subclass of PeftConfig for LoKr, a PEFT method that uses low-rank approximation and kernelization to reduce the number of parameters |\n","| LoKrModel | A subclass of PeftModel for LoKr |\n","| LoraConfig | A subclass of PeftConfig for LoRA, a PEFT method that uses low-rank approximation to reduce the number of parameters |\n","| LoraModel | A subclass of PeftModel for LoRA |\n","| MultitaskPromptTuningConfig | A subclass of PeftConfig for multitask prompt tuning, a PEFT method that uses a shared prompt encoder to adapt a model to multiple tasks |\n","| OFTConfig | A subclass of PeftConfig for OFT, a PEFT method that uses orthogonal feature transformation to reduce the number of parameters |\n","| OFTModel | A subclass of PeftModel for OFT |\n","| PrefixTuningConfig | A subclass of PeftConfig for prefix tuning, a PEFT method that uses a learnable prefix to adapt a model to different tasks |\n","| PromptEncoderConfig | A subclass of PeftConfig for prompt encoder, a PEFT method that uses a learnable encoder to generate prompt embeddings |\n","| PromptTuningConfig | A subclass of PeftConfig for prompt tuning, a PEFT method that uses fixed or learnable prompt embeddings to adapt a model to different tasks |\n"],"metadata":{"id":"dX86DOn7EB-w"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM\n","from peft import PeftModel, PeftConfig\n","import torch\n","from datasets import load_dataset\n","import os\n","from transformers import AutoTokenizer\n","from torch.utils.data import DataLoader\n","from transformers import default_data_collator, get_linear_schedule_with_warmup\n","from tqdm import tqdm\n","from datasets import load_dataset\n","\n","device = \"cuda\"\n","model_name_or_path = \"bigscience/bloomz-7b1\"\n","tokenizer_name_or_path = \"bigscience/bloomz-7b1\"\n","dataset_name = \"twitter_complaints\"\n","text_column = \"Tweet text\"\n","label_column = \"text_label\"\n","max_length = 64\n","lr = 1e-3\n","num_epochs = 50\n","batch_size = 8"],"metadata":{"id":"cba8ifh94p3L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","The provided code involves importing necessary libraries and setting up configurations for training a model for text classification using Transformers and Peft.\n","\n","1. **Importing Libraries:**\n","    - `transformers`: Imports necessary modules for utilizing the Hugging Face Transformers library.\n","    - `peft`: Imports the PeftModel and PeftConfig, which might be related to personalized federated learning techniques.\n","    - `torch`: Imports the PyTorch library for tensor computations.\n","    - `datasets`: Provides functionality to load datasets.\n","    - `os`: Allows interaction with the operating system.\n","    - `tqdm`: Offers a progress bar during iterations.\n","\n","2. **Setting Device and Configurations:**\n","    - `device = \"cuda\"`: Sets the device to use CUDA for GPU acceleration if available.\n","    - `model_name_or_path = \"bigscience/bloomz-7b1\"`: Specifies the model to be used, likely a language model from the BigScience project.\n","    - `tokenizer_name_or_path = \"bigscience/bloomz-7b1\"`: Specifies the tokenizer related to the chosen model.\n","    - `dataset_name = \"twitter_complaints\"`: Indicates the dataset being used, possibly containing Twitter complaints.\n","    - `text_column = \"Tweet text\"`: Specifies the column in the dataset containing the tweet text.\n","    - `label_column = \"text_label\"`: Specifies the column containing labels for the text.\n","\n","3. **Training Configurations:**\n","    - `max_length = 64`: Defines the maximum length of the input text sequences.\n","    - `lr = 1e-3`: Sets the learning rate for the training process.\n","    - `num_epochs = 50`: Specifies the number of epochs for training.\n","    - `batch_size = 8`: Determines the batch size used during training.\n","\n","4. **Data Loading and Processing:**\n","    - Loads the specified dataset using `load_dataset`.\n","    - Prepares data loaders and collators for training.\n","    - Creates a schedule for adjusting the learning rate during training with `get_linear_schedule_with_warmup`.\n","\n","5. **Training Loop:**\n","    - Executes the training loop for the specified number of epochs using tqdm to display progress.\n","\n","Note: This code appears to set up a training pipeline for a text classification task using a specific language model and dataset, configuring parameters and loading data for subsequent model training.\n"],"metadata":{"id":"D5boqdeK4uUM"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"ought/raft\", dataset_name)\n","\n","classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n","print(classes)\n","dataset = dataset.map(\n","    lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n","    batched=True,\n","    num_proc=1,\n",")\n","print(dataset)\n","dataset[\"train\"][0]"],"metadata":{"id":"WVPiJ33r5BA_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data loader\n","This code segment involves loading a dataset from the \"ought/raft\" repository and preparing it for text classification tasks.\n","\n","1. **Importing the Required Library:**\n","    - `load_dataset`: Imports the function necessary for loading datasets.\n","\n","2. **Loading and Processing the Dataset:**\n","    - `dataset = load_dataset(\"ought/raft\", dataset_name)`: Loads the specified dataset from the \"ought/raft\" repository. The `dataset_name` variable might represent a specific dataset name within the repository.\n","\n","3. **Extracting Classes and Mapping Labels:**\n","    - Extracts class labels from the dataset.\n","        - `classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]`: Retrieves class labels from the \"Label\" feature and modifies them by replacing underscores with spaces.\n","    - Maps labels to human-readable classes for easier interpretation.\n","        - `dataset = dataset.map(...)`: Modifies the dataset by replacing label indices with their corresponding human-readable class names.\n","        - Uses a lambda function to transform the labels in each batch of the dataset to their corresponding class names.\n","        - `batched=True` and `num_proc=1` might indicate that the mapping is performed in batches using a single processor.\n","\n","4. **Dataset Exploration:**\n","    - `print(classes)`: Prints out the extracted class names.\n","    - `print(dataset)`: Prints the modified dataset after mapping label indices to class names.\n","\n","5. **Accessing the Processed Data:**\n","    - `dataset[\"train\"][0]`: Attempts to access the first item in the \"train\" split of the processed dataset. This likely aims to display the transformation applied to the dataset, showing the mapped text labels for the first sample.\n","\n","Note: This code snippet demonstrates loading a dataset, extracting class labels, mapping label indices to human-readable class names, and printing information about the classes and the processed dataset.\n"],"metadata":{"id":"w2trMS0v5H97"}},{"cell_type":"code","source":["# data preprocessing\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","target_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes])\n","print(target_max_length)\n","\n","\n","def preprocess_function(examples):\n","    batch_size = len(examples[text_column])\n","    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n","    targets = [str(x) for x in examples[label_column]]\n","    model_inputs = tokenizer(inputs)\n","    labels = tokenizer(targets, add_special_tokens=False)  # don't add bos token because we concatenate with inputs\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n","        # print(i, sample_input_ids, label_input_ids)\n","        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n","        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n","        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n","    # print(model_inputs)\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        label_input_ids = labels[\"input_ids\"][i]\n","        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n","            max_length - len(sample_input_ids)\n","        ) + sample_input_ids\n","        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n","            \"attention_mask\"\n","        ][i]\n","        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n","        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n","        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n","        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","\n","processed_datasets = dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    num_proc=1,\n","    remove_columns=dataset[\"train\"].column_names,\n","    load_from_cache_file=False,\n","    desc=\"Running tokenizer on dataset\",\n",")\n","\n","train_dataset = processed_datasets[\"train\"]\n","\n","print(train_dataset[0])\n","train_dataloader = DataLoader(\n","    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",")"],"metadata":{"id":"Plzuyr9G5ewJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Preprocessing Explanation\n","\n","This code section handles the preprocessing steps for the dataset before training the model for text classification.\n","\n","1. **Tokenization Setup:**\n","    - `tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)`: Initializes the tokenizer from a pretrained model specified by `model_name_or_path`.\n","    - If the tokenizer's pad token ID is not available, it's set to the end-of-sequence token ID (`eos_token_id`) of the tokenizer.\n","\n","2. **Determine Maximum Token Length:**\n","    - `target_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes])`: Determines the maximum token length among the class labels after tokenization and prints it out.\n","\n","3. **Preprocessing Function:**\n","    - `preprocess_function(examples)`: This function prepares the data for model input.\n","    - Creates model inputs and labels based on the provided examples (tweets and their labels).\n","    - Tokenizes the text and label columns using the tokenizer.\n","    - Concatenates the tokenized label to the end of the tokenized input text.\n","    - Handles padding, attention masks, and limits the tokenized sequences to a maximum length (`max_length`).\n","    - Converts the processed inputs and labels into PyTorch tensors.\n","    - Sets the model inputs' labels to the labels for model training.\n","\n","4. **Applying Preprocessing to Dataset:**\n","    - `processed_datasets = dataset.map(...)`: Processes the entire dataset using the defined `preprocess_function`.\n","    - This includes tokenizing the input text and labels, padding, truncating to the specified length, and converting to tensors.\n","    - The processed dataset is prepared for training, and the unnecessary columns are removed.\n","\n","5. **Creating Training DataLoader:**\n","    - `train_dataset = processed_datasets[\"train\"]`: Retrieves the processed training dataset.\n","    - `DataLoader` is used to create a training data loader for batching and shuffling the training data.\n","    - The data loader uses the `default_data_collator` function for collating batches and sets the specified batch size.\n","\n","Note: This code segment prepares the dataset for training a text classification model. It tokenizes the input text and labels, processes them to fit within a specified maximum length, and creates a data loader for training with batched and shuffled data.\n"],"metadata":{"id":"LPt6ncyw5hDH"}},{"cell_type":"markdown","source":[],"metadata":{"id":"Wvs6Hb2I5uvv"}},{"cell_type":"code","source":["def test_preprocess_function(examples):\n","    batch_size = len(examples[text_column])\n","    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n","    model_inputs = tokenizer(inputs)\n","    # print(model_inputs)\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n","            max_length - len(sample_input_ids)\n","        ) + sample_input_ids\n","        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n","            \"attention_mask\"\n","        ][i]\n","        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n","        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n","    return model_inputs\n","\n","\n","processed_datasets = dataset.map(\n","    test_preprocess_function,\n","    batched=True,\n","    num_proc=1,\n","    remove_columns=dataset[\"train\"].column_names,\n","    load_from_cache_file=False,\n","    desc=\"Running tokenizer on dataset\",\n",")\n","\n","eval_dataset = processed_datasets[\"train\"]\n","test_dataset = processed_datasets[\"test\"]\n","\n","eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n","test_dataloader = DataLoader(test_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n","print(next(iter(eval_dataloader)))\n","print(next(iter(test_dataloader)))"],"metadata":{"id":"QwFD1Js85rrP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Testing Preprocessing Function and Data Loaders Creation\n","\n","This code snippet involves testing the preprocessing function and creating data loaders for evaluation and testing.\n","\n","1. **Testing Preprocessing Function:**\n","    - `test_preprocess_function(examples)`: This function is similar to the preprocessing function used earlier but doesn't involve adding label sequences.\n","    - It tokenizes the input text, handles padding, and truncates sequences to fit within the specified `max_length`.\n","    - Returns the processed model inputs without label sequences.\n","\n","2. **Applying Test Preprocessing to Dataset:**\n","    - `processed_datasets = dataset.map(...)`: Applies the `test_preprocess_function` to the entire dataset for both evaluation and testing subsets.\n","    - The function tokenizes and processes the input text for both subsets without including label sequences.\n","\n","3. **Creating Evaluation and Testing Data Loaders:**\n","    - `eval_dataset = processed_datasets[\"train\"]`: Retrieves the processed dataset for evaluation.\n","    - `test_dataset = processed_datasets[\"test\"]`: Retrieves the processed dataset for testing.\n","    - Two separate `DataLoader` instances are created for evaluation and testing using the processed datasets.\n","    - These data loaders use `default_data_collator` for batching and setting the specified batch size.\n","\n","4. **Testing Data Loader Contents:**\n","    - `print(next(iter(eval_dataloader)))`: Prints the contents of the first batch from the evaluation data loader.\n","    - `print(next(iter(test_dataloader)))`: Prints the contents of the first batch from the testing data loader.\n","\n","Note: This code snippet aims to test the preprocessing function by applying it to the evaluation and testing subsets of the dataset. It then creates separate data loaders for evaluation and testing, potentially for assessing model performance on unseen data.\n"],"metadata":{"id":"HWswC8We5wBK"}},{"cell_type":"code","source":["from peft import PeftModel, PeftConfig\n","\n","max_memory = {0: \"1GIB\", 1: \"1GIB\", 2: \"2GIB\", 3: \"10GIB\", \"cpu\": \"30GB\"}\n","\n","peft_model_id=\"\"\"\"\"\"\n","config = PeftConfig.from_pretrained(peft_model_id)\n","model = AutoModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", max_memory=max_memory)\n","model = PeftModel.from_pretrained(model, peft_model_id, device_map=\"auto\", max_memory=max_memory)"],"metadata":{"id":"IsoFIf5g5-6D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Code Explanation\n","\n","The provided code involves setting up a `PeftModel` using the `peft` library for personalized federated training techniques.\n","\n","1. **Memory Configuration:**\n","    - `max_memory`: Specifies memory allocations for various devices (0, 1, 2, 3, and \"cpu\") using different memory limits (e.g., \"1GIB\" for 1 Gibibyte).\n","    - This configuration manages memory limits for specific devices, likely utilized during model training or inference.\n","\n","2. **PEFT Model Configuration:**\n","    - `peft_model_id`: An identifier for a specific PEFT (Personalized Federated Training) model related to Twitter complaints and a particular language model (`bigscience/bloomz-7b1`) trained using LORA (Learn Once, Rule Always) and Causal Language Modeling techniques.\n","    - `config = PeftConfig.from_pretrained(peft_model_id)`: Instantiates a configuration object for the PEFT model using the provided `peft_model_id`.\n","\n","3. **Model Instantiation:**\n","    - `model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, device_map=\"auto\", max_memory=max_memory)`: Initializes a language model (`AutoModelForCausalLM`) from the base model specified in the configuration.\n","        - The `device_map=\"auto\"` might suggest automatic device mapping for the model.\n","        - `max_memory` parameter defines memory limits for the model's operation.\n","\n","4. **PEFT Model Initialization:**\n","    - `model = PeftModel.from_pretrained(model, peft_model_id, device_map=\"auto\", max_memory=max_memory)`: Initializes a `PeftModel` incorporating personalized federated training enhancements or adaptations into the previously instantiated language model and the specified PEFT model ID.\n","\n","This code sequence sets up a `PeftModel` utilizing personalized federated training techniques, likely tailored for handling Twitter complaints and integrating specific language model capabilities. It includes memory allocation and device mapping configurations for model operations.\n"],"metadata":{"id":"IY899JVw6U1M"}},{"cell_type":"code","source":["model.eval()\n","i = 89\n","inputs = tokenizer(f'{text_column} : {dataset[\"test\"][i][\"Tweet text\"]} Label : ', return_tensors=\"pt\")\n","print(dataset[\"test\"][i][\"Tweet text\"])\n","print(inputs)\n","\n","with torch.no_grad():\n","    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n","    print(outputs)\n","    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"],"metadata":{"id":"XhSzD6rD6gpb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","test_preds = []\n","\n","for _, batch in enumerate(tqdm(test_dataloader)):\n","    batch = {k: v for k, v in batch.items() if k != \"labels\"}\n","    with torch.no_grad():\n","        outputs = model.generate(**batch, max_new_tokens=10)\n","    preds = outputs[:, max_length:].detach().cpu().numpy()\n","    test_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\n","    if len(test_preds) > 100:\n","        break\n","test_preds"],"metadata":{"id":"i3SZ-_SJ6sGD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["correct = 0\n","total = 0\n","for pred, true in zip(eval_preds, dataset[\"train\"][label_column]):\n","    if pred.strip() == true.strip():\n","        correct += 1\n","    total += 1\n","accuracy = correct / total * 100\n","print(f\"{accuracy=}\")\n","print(f\"{eval_preds[:10]=}\")\n","print(f\"{dataset['train'][label_column][:10]=}\")"],"metadata":{"id":"DzUIDh-y6kep"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","torch.set_default_device(\"cuda\")\n","\n","model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\n","tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n","\n","inputs = tokenizer('''def print_prime(n):\n","   \"\"\"\n","   Print all primes between 1 and n\n","   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n","\n","outputs = model.generate(**inputs, max_length=200)\n","text = tokenizer.batch_decode(outputs)[0]\n","print(text)\n"],"metadata":{"id":"5eTVTzvGH66d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q -U accelerate datasets transformers peft"],"metadata":{"id":"EigV-xDVHpXR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tMfNU2782_Ax"},"outputs":[],"source":["import gc\n","import os\n","import sys\n","import threading\n","\n","import numpy as np\n","import psutil\n","import torch\n","from accelerate import Accelerator\n","from datasets import load_dataset\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    default_data_collator,\n","    get_linear_schedule_with_warmup,\n","    set_seed,\n",")\n","\n","from peft import LoraConfig, TaskType, get_peft_model\n","\n","\n","def levenshtein_distance(str1, str2):\n","    # TC: O(N^2)\n","    # SC: O(N^2)\n","    if str1 == str2:\n","        return 0\n","    num_rows = len(str1) + 1\n","    num_cols = len(str2) + 1\n","    dp_matrix = np.empty((num_rows, num_cols))\n","    dp_matrix[0, :] = range(num_cols)\n","    dp_matrix[:, 0] = range(num_rows)\n","\n","    for i in range(1, num_rows):\n","        for j in range(1, num_cols):\n","            if str1[i - 1] == str2[j - 1]:\n","                dp_matrix[i, j] = dp_matrix[i - 1, j - 1]\n","            else:\n","                dp_matrix[i, j] = min(dp_matrix[i - 1, j - 1], dp_matrix[i - 1, j], dp_matrix[i, j - 1]) + 1\n","\n","    return dp_matrix[num_rows - 1, num_cols - 1]\n","\n","\n","def get_closest_label(eval_pred, classes):\n","    min_id = sys.maxsize\n","    min_edit_distance = sys.maxsize\n","    for i, class_label in enumerate(classes):\n","        edit_distance = levenshtein_distance(eval_pred.strip(), class_label)\n","        if edit_distance < min_edit_distance:\n","            min_id = i\n","            min_edit_distance = edit_distance\n","    return classes[min_id]\n","\n","\n","# Converting Bytes to Megabytes\n","def b2mb(x):\n","    return int(x / 2**20)\n","\n","\n","# This context manager is used to track the peak memory usage of the process\n","class TorchTracemalloc:\n","    def __enter__(self):\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        torch.cuda.reset_max_memory_allocated()  # reset the peak gauge to zero\n","        self.begin = torch.cuda.memory_allocated()\n","        self.process = psutil.Process()\n","\n","        self.cpu_begin = self.cpu_mem_used()\n","        self.peak_monitoring = True\n","        peak_monitor_thread = threading.Thread(target=self.peak_monitor_func)\n","        peak_monitor_thread.daemon = True\n","        peak_monitor_thread.start()\n","        return self\n","\n","    def cpu_mem_used(self):\n","        \"\"\"get resident set size memory for the current process\"\"\"\n","        return self.process.memory_info().rss\n","\n","    def peak_monitor_func(self):\n","        self.cpu_peak = -1\n","\n","        while True:\n","            self.cpu_peak = max(self.cpu_mem_used(), self.cpu_peak)\n","\n","            # can't sleep or will not catch the peak right (this comment is here on purpose)\n","            # time.sleep(0.001) # 1msec\n","\n","            if not self.peak_monitoring:\n","                break\n","\n","    def __exit__(self, *exc):\n","        self.peak_monitoring = False\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        self.end = torch.cuda.memory_allocated()\n","        self.peak = torch.cuda.max_memory_allocated()\n","        self.used = b2mb(self.end - self.begin)\n","        self.peaked = b2mb(self.peak - self.begin)\n","\n","        self.cpu_end = self.cpu_mem_used()\n","        self.cpu_used = b2mb(self.cpu_end - self.cpu_begin)\n","        self.cpu_peaked = b2mb(self.cpu_peak - self.cpu_begin)\n","        print(f\"delta used/peak {self.used:4d}/{self.peaked:4d}\")\n","\n","\n","def main():\n","    accelerator = Accelerator()\n","    model_name_or_path = \"microsoft/phi-2\"\n","    dataset_name = \"twitter_complaints\"\n","    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n","    text_column = \"Tweet text\"\n","    label_column = \"text_label\"\n","    lr = 3e-3\n","    num_epochs = 20\n","    batch_size = 8\n","    seed = 42\n","    max_length = 64\n","    do_test = False\n","    set_seed(seed)\n","\n","    dataset = load_dataset(\"ought/raft\", dataset_name)\n","    classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n","    dataset = dataset.map(\n","        lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n","        batched=True,\n","        num_proc=1,\n","    )\n","\n","    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","\n","    def preprocess_function(examples):\n","        batch_size = len(examples[text_column])\n","        inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n","        targets = [str(x) for x in examples[label_column]]\n","        model_inputs = tokenizer(inputs)\n","        labels = tokenizer(targets, add_special_tokens=False)  # don't add bos token because we concatenate with inputs\n","        for i in range(batch_size):\n","            sample_input_ids = model_inputs[\"input_ids\"][i]\n","            label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n","            model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n","            labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n","            model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n","        for i in range(batch_size):\n","            sample_input_ids = model_inputs[\"input_ids\"][i]\n","            label_input_ids = labels[\"input_ids\"][i]\n","            model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n","                max_length - len(sample_input_ids)\n","            ) + sample_input_ids\n","            model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n","                \"attention_mask\"\n","            ][i]\n","            labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n","            model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n","            model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n","            labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n","        model_inputs[\"labels\"] = labels[\"input_ids\"]\n","        return model_inputs\n","\n","    def test_preprocess_function(examples):\n","        batch_size = len(examples[text_column])\n","        inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n","        model_inputs = tokenizer(inputs)\n","        # print(model_inputs)\n","        for i in range(batch_size):\n","            sample_input_ids = model_inputs[\"input_ids\"][i]\n","            model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n","                max_length - len(sample_input_ids)\n","            ) + sample_input_ids\n","            model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n","                \"attention_mask\"\n","            ][i]\n","            model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n","            model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n","        return model_inputs\n","\n","    with accelerator.main_process_first():\n","        processed_datasets = dataset.map(\n","            preprocess_function,\n","            batched=True,\n","            num_proc=1,\n","            remove_columns=dataset[\"train\"].column_names,\n","            load_from_cache_file=True,\n","            desc=\"Running tokenizer on dataset\",\n","        )\n","    accelerator.wait_for_everyone()\n","\n","    train_dataset = processed_datasets[\"train\"]\n","\n","    with accelerator.main_process_first():\n","        processed_datasets = dataset.map(\n","            test_preprocess_function,\n","            batched=True,\n","            num_proc=1,\n","            remove_columns=dataset[\"train\"].column_names,\n","            load_from_cache_file=False,\n","            desc=\"Running tokenizer on dataset\",\n","        )\n","    eval_dataset = processed_datasets[\"train\"]\n","    test_dataset = processed_datasets[\"test\"]\n","\n","    train_dataloader = DataLoader(\n","        train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n","    )\n","    eval_dataloader = DataLoader(\n","        eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n","    )\n","    test_dataloader = DataLoader(\n","        test_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n","    )\n","\n","    print(next(iter(train_dataloader)))\n","\n","    # creating model\n","    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,trust_remote_code=True)\n","    model = get_peft_model(model, peft_config)\n","    model.print_trainable_parameters()\n","\n","    # optimizer\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","\n","    # lr scheduler\n","    lr_scheduler = get_linear_schedule_with_warmup(\n","        optimizer=optimizer,\n","        num_warmup_steps=0,\n","        num_training_steps=(len(train_dataloader) * num_epochs),\n","    )\n","\n","    model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler = accelerator.prepare(\n","        model, train_dataloader, eval_dataloader, test_dataloader, optimizer, lr_scheduler\n","    )\n","    accelerator.print(model)\n","\n","    is_ds_zero_3 = False\n","    if getattr(accelerator.state, \"deepspeed_plugin\", None):\n","        is_ds_zero_3 = accelerator.state.deepspeed_plugin.zero_stage == 3\n","\n","    for epoch in range(num_epochs):\n","        with TorchTracemalloc() as tracemalloc:\n","            model.train()\n","            total_loss = 0\n","            for step, batch in enumerate(tqdm(train_dataloader)):\n","                outputs = model(**batch)\n","                loss = outputs.loss\n","                total_loss += loss.detach().float()\n","                accelerator.backward(loss)\n","                optimizer.step()\n","                lr_scheduler.step()\n","                optimizer.zero_grad()\n","        # Printing the GPU memory usage details such as allocated memory, peak memory, and total memory usage\n","        accelerator.print(\"GPU Memory before entering the train : {}\".format(b2mb(tracemalloc.begin)))\n","        accelerator.print(\"GPU Memory consumed at the end of the train (end-begin): {}\".format(tracemalloc.used))\n","        accelerator.print(\"GPU Peak Memory consumed during the train (max-begin): {}\".format(tracemalloc.peaked))\n","        accelerator.print(\n","            \"GPU Total Peak Memory consumed during the train (max): {}\".format(\n","                tracemalloc.peaked + b2mb(tracemalloc.begin)\n","            )\n","        )\n","\n","        accelerator.print(\"CPU Memory before entering the train : {}\".format(b2mb(tracemalloc.cpu_begin)))\n","        accelerator.print(\"CPU Memory consumed at the end of the train (end-begin): {}\".format(tracemalloc.cpu_used))\n","        accelerator.print(\"CPU Peak Memory consumed during the train (max-begin): {}\".format(tracemalloc.cpu_peaked))\n","        accelerator.print(\n","            \"CPU Total Peak Memory consumed during the train (max): {}\".format(\n","                tracemalloc.cpu_peaked + b2mb(tracemalloc.cpu_begin)\n","            )\n","        )\n","        train_epoch_loss = total_loss / len(train_dataloader)\n","        train_ppl = torch.exp(train_epoch_loss)\n","        accelerator.print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=}\")\n","\n","        model.eval()\n","        eval_preds = []\n","        with TorchTracemalloc() as tracemalloc:\n","            for _, batch in enumerate(tqdm(eval_dataloader)):\n","                batch = {k: v for k, v in batch.items() if k != \"labels\"}\n","                with torch.no_grad():\n","                    outputs = accelerator.unwrap_model(model).generate(\n","                        **batch, synced_gpus=is_ds_zero_3, max_new_tokens=10\n","                    )  # synced_gpus=True for DS-stage 3\n","                outputs = accelerator.pad_across_processes(outputs, dim=1, pad_index=tokenizer.pad_token_id)\n","                preds = accelerator.gather_for_metrics(outputs)\n","                preds = preds[:, max_length:].detach().cpu().numpy()\n","                eval_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\n","\n","        # Printing the GPU memory usage details such as allocated memory, peak memory, and total memory usage\n","        accelerator.print(\"GPU Memory before entering the eval : {}\".format(b2mb(tracemalloc.begin)))\n","        accelerator.print(\"GPU Memory consumed at the end of the eval (end-begin): {}\".format(tracemalloc.used))\n","        accelerator.print(\"GPU Peak Memory consumed during the eval (max-begin): {}\".format(tracemalloc.peaked))\n","        accelerator.print(\n","            \"GPU Total Peak Memory consumed during the eval (max): {}\".format(\n","                tracemalloc.peaked + b2mb(tracemalloc.begin)\n","            )\n","        )\n","\n","        accelerator.print(\"CPU Memory before entering the eval : {}\".format(b2mb(tracemalloc.cpu_begin)))\n","        accelerator.print(\"CPU Memory consumed at the end of the eval (end-begin): {}\".format(tracemalloc.cpu_used))\n","        accelerator.print(\"CPU Peak Memory consumed during the eval (max-begin): {}\".format(tracemalloc.cpu_peaked))\n","        accelerator.print(\n","            \"CPU Total Peak Memory consumed during the eval (max): {}\".format(\n","                tracemalloc.cpu_peaked + b2mb(tracemalloc.cpu_begin)\n","            )\n","        )\n","\n","        correct = 0\n","        total = 0\n","        assert len(eval_preds) == len(\n","            dataset[\"train\"][label_column]\n","        ), f\"{len(eval_preds)} != {len(dataset['train'][label_column])}\"\n","        for pred, true in zip(eval_preds, dataset[\"train\"][label_column]):\n","            if pred.strip() == true.strip():\n","                correct += 1\n","            total += 1\n","        accuracy = correct / total * 100\n","        accelerator.print(f\"{accuracy=}\")\n","        accelerator.print(f\"{eval_preds[:10]=}\")\n","        accelerator.print(f\"{dataset['train'][label_column][:10]=}\")\n","\n","    if do_test:\n","        model.eval()\n","        test_preds = []\n","        for _, batch in enumerate(tqdm(test_dataloader)):\n","            batch = {k: v for k, v in batch.items() if k != \"labels\"}\n","            with torch.no_grad():\n","                outputs = accelerator.unwrap_model(model).generate(\n","                    **batch, synced_gpus=is_ds_zero_3, max_new_tokens=10\n","                )  # synced_gpus=True for DS-stage 3\n","            outputs = accelerator.pad_across_processes(outputs, dim=1, pad_index=tokenizer.pad_token_id)\n","            preds = accelerator.gather(outputs)\n","            preds = preds[:, max_length:].detach().cpu().numpy()\n","            test_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\n","\n","        test_preds_cleaned = []\n","        for _, pred in enumerate(test_preds):\n","            test_preds_cleaned.append(get_closest_label(pred, classes))\n","\n","        test_df = dataset[\"test\"].to_pandas()\n","        assert len(test_preds_cleaned) == len(test_df), f\"{len(test_preds_cleaned)} != {len(test_df)}\"\n","        test_df[label_column] = test_preds_cleaned\n","        test_df[\"text_labels_orig\"] = test_preds\n","        accelerator.print(test_df[[text_column, label_column]].sample(20))\n","\n","        pred_df = test_df[[\"ID\", label_column]]\n","        pred_df.columns = [\"ID\", \"Label\"]\n","\n","        os.makedirs(f\"data/{dataset_name}\", exist_ok=True)\n","        pred_df.to_csv(f\"data/{dataset_name}/predictions.csv\", index=False)\n","\n","    accelerator.wait_for_everyone()\n","    # Option1: Pushing the model to Hugging Face Hub\n","    # model.push_to_hub(\n","    #     f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\".replace(\"/\", \"_\"),\n","    #     token = \"hf_...\"\n","    # )\n","    # token (`bool` or `str`, *optional*):\n","    #     `token` is to be used for HTTP Bearer authorization when accessing remote files. If `True`, will use the token generated\n","    #     when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n","    #     is not specified.\n","    #     Or you can get your token from https://huggingface.co/settings/token\n","    # Option2: Saving the model locally\n","    peft_model_id = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\".replace(\n","        \"/\", \"_\"\n","    )\n","    model.save_pretrained(peft_model_id)\n","    accelerator.wait_for_everyone()\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"markdown","source":["# Example 2\n","\n","# Training PEFT models with new tokens being added to the embedding layers and tokenizer\n","\n","In this example, we will learn how to train a LoRA model when adding new tokens to the tokenizer and model.\n","This is a common usecase when doing the following:\n","1. Instruction finetuning with new tokens beind added such as `<|user|>`, `<|assistant|>`, `<|system|>`, `</s>`, `<s>` to properly format the conversations\n","2. Finetuning on a specific language wherein language spoecific tokens are added, e.g., korean tokens being added to vocabulary for finetuning LLM on Korean datasets.\n","3. Instruction finetuning to return outputs in certain format to enable agent behaviour new tokens such as `<|FUNCTIONS|>`, `<|BROWSE|>`, `<|TEXT2IMAGE|>`, `<|ASR|>`, `<|TTS|>`, `<|GENERATECODE|>`, `<|RAG|>`.\n","\n","In such cases, you add the Embedding modules to the LORA `target_modules`. PEFT will take care of saving the embedding layers with the new added tokens along with the adapter weights that were trained on the specific initialization of the embeddings weights of the added tokens."],"metadata":{"id":"tAGeZyLpKgWw"}},{"cell_type":"code","source":["import os\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n","os.environ[\"WANDB_PROJECT\"] = \"PeftExamples\"\n","import transformers\n","from peft import (\n","    LoraConfig,\n","    PeftConfig,\n","    PeftModel,\n","    get_peft_model,\n","    prepare_model_for_int8_training,\n",")\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    Trainer,\n","    default_data_collator,\n",")\n","import torch\n","from dataclasses import dataclass, field\n","from typing import Optional\n","from dataclass_csv import DataclassReader\n","from torch.utils.data import Dataset, DataLoader\n","\n","from enum import Enum"],"metadata":{"id":"lGf-CpRVKpxE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SpecialTokens(str, Enum):\n","    begin_target = \"<|begintarget|>\"\n","    end_target = \"<|endtarget|>\"\n","    begin_context = \"<|begincontext|>\"\n","    end_context = \"<|endcontext|>\"\n","    system = \"<|system|>\"\n","    user = \"<|user|>\"\n","    begin_last_user_utterance = \"<|beginlastuserutterance|>\"\n","    end_last_user_utterance = \"<|endlastuserutterance|>\"\n","    begin_dsts = \"<|begindsts|>\"\n","    end_dsts = \"<|enddsts|>\"\n","    begin_dst = \"<|begindst|>\"\n","    end_dst = \"<|enddst|>\"\n","    begin_belief = \"<|beginbelief|>\"\n","    end_belief = \"<|endbelief|>\"\n","    begin_response = \"<|beginresponse|>\"\n","    end_response = \"<|endresponse|>\"\n","    begin_action = \"<|beginaction|>\"\n","    end_action = \"<|endaction|>\"\n","    begin_user_action = \"<|beginuseraction|>\"\n","    end_user_action = \"<|enduseraction|>\"\n","    sys_actions = \"<|sysactions|>\"\n","    begin_intent = \"<|beginintent|>\"\n","    end_intent = \"<|endintent|>\"\n","    begin_requested_slots = \"<|beginrequestedslots|>\"\n","    end_requested_slots = \"<|endrequestedslots|>\"\n","    pad_token = \"<|pad|>\"\n","    bos_token = \"<|startoftext|>\"\n","\n","    @classmethod\n","    def list(cls):\n","        return [c.value for c in cls]"],"metadata":{"id":"oIUnIJYwKqTw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = \"mistralai/Mistral-7B-v0.1\"\n","tokenizer = AutoTokenizer.from_pretrained(\n","    model_name,\n","    pad_token=SpecialTokens.pad_token.value,\n","    bos_token=SpecialTokens.bos_token.value,\n","    eos_token=SpecialTokens.end_target.value,\n","    additional_special_tokens=SpecialTokens.list(),\n",")\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    low_cpu_mem_usage=True\n","    # use_flash_attention_2=True, # leading to an error\n",")\n","model.resize_token_embeddings(len(tokenizer))"],"metadata":{"id":"HcD3R3bNK9CX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config = LoraConfig(\n","    r=64, lora_alpha=128, lora_dropout=0.0, target_modules=[\"embed_tokens\", \"lm_head\", \"q_proj\", \"v_proj\"]\n",")\n","model = get_peft_model(model, config)\n","print(model.print_trainable_parameters())\n","print(model)"],"metadata":{"id":"iR8iD1ZDLBcG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"smangrul/assistant_chatbot_dataset\")\n","dataset = dataset[\"train\"].train_test_split(0.2)\n","\n","text_column = \"context\"\n","label_column = \"target\"\n","max_length = 512\n","\n","\n","def preprocess_function(examples):\n","    batch_size = len(examples[text_column])\n","    targets = [str(x) for x in examples[label_column]]\n","    model_inputs = tokenizer(examples[text_column])\n","    labels = tokenizer(targets, add_special_tokens=False)  # don't add bos token because we concatenate with inputs\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n","        # print(i, sample_input_ids, label_input_ids)\n","        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n","        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n","        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n","    # print(model_inputs)\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        label_input_ids = labels[\"input_ids\"][i]\n","        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n","            max_length - len(sample_input_ids)\n","        ) + sample_input_ids\n","        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n","            \"attention_mask\"\n","        ][i]\n","        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n","        model_inputs[\"input_ids\"][i] = model_inputs[\"input_ids\"][i][:max_length]\n","        model_inputs[\"attention_mask\"][i] = model_inputs[\"attention_mask\"][i][:max_length]\n","        labels[\"input_ids\"][i] = labels[\"input_ids\"][i][:max_length]\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","\n","processed_datasets = dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    num_proc=1,\n","    remove_columns=dataset[\"train\"].column_names,\n","    load_from_cache_file=False,\n","    desc=\"Running tokenizer on dataset\",\n",")\n","\n","train_dataset = processed_datasets[\"train\"]"],"metadata":{"id":"b3yAoZkYLEy_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataloader = DataLoader(\n","    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=8, pin_memory=True\n",")"],"metadata":{"id":"YGUnt-znLISJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["next(iter(train_dataloader))"],"metadata":{"id":"rDSbgCK2LQlO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.decode(train_dataset[0][\"input_ids\"])"],"metadata":{"id":"FuA_QayfLWxz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir=\"mistral_lora_clm_with_added_tokens\",\n","    num_train_epochs=2,\n","    save_total_limit=5,\n","    per_device_train_batch_size=8,\n","    warmup_steps=10,\n","    weight_decay=0.0001,\n","    dataloader_drop_last=True,\n","    bf16=True,\n","    logging_steps=10,\n","    learning_rate=1e-5,\n","    gradient_checkpointing=True,\n","    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n","    remove_unused_columns=False,\n","    hub_model_id=\"smangrul/mistral_lora_clm_with_added_tokens\",\n","    push_to_hub=True,\n","    hub_private_repo=True,\n",")\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    data_collator=default_data_collator,\n",")\n","# model.config.use_cache = False\n","trainer.train()"],"metadata":{"id":"HSKlasoYLVge"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","i = random.randint(0, len(dataset[\"test\"]))\n","context = dataset[\"test\"][i][\"context\"]\n","\n","batch = tokenizer(context, return_tensors=\"pt\")\n","batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n","model.eval()\n","output_tokens = model.generate(\n","    **batch,\n","    max_new_tokens=256,\n","    do_sample=True,\n","    temperature=0.2,\n","    top_p=0.95,\n","    top_k=50,\n","    eos_token_id=tokenizer.eos_token_id,\n","    pad_token_id=tokenizer.pad_token_id,\n",")\n","target_predicted = tokenizer.decode(output_tokens[0], skip_special_tokens=False).split(\"<|endcontext|>\")[1]\n","target = dataset[\"test\"][i][\"target\"]\n","print(f\"{context=} \\n\\n {target_predicted=} \\n\\n {target=}\")"],"metadata":{"id":"J4W5dnrHLcrj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.push_to_hub()\n","trainer.model.push_to_hub(training_args.output_dir)"],"metadata":{"id":"nyRpyzjQLf6W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from peft import PeftModel\n","\n","inference_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    low_cpu_mem_usage=True,\n","    # use_flash_attention_2=True,\n",")\n","inference_model.resize_token_embeddings(len(tokenizer))\n","\n","inference_model = PeftModel.from_pretrained(inference_model, \"smangrul/mistral_lora_clm_with_added_tokens\")\n","inference_model.to(\"cuda\")\n","inference_model.eval()\n","\n","output_tokens = inference_model.generate(\n","    **batch,\n","    max_new_tokens=256,\n","    do_sample=True,\n","    temperature=0.2,\n","    top_p=0.95,\n","    top_k=50,\n","    eos_token_id=tokenizer.eos_token_id,\n","    pad_token_id=tokenizer.pad_token_id,\n",")\n","\n","target_predicted = tokenizer.decode(output_tokens[0], skip_special_tokens=False).split(\"<|endcontext|>\")[1]\n","print(f\"{context=} \\n\\n {target_predicted=} \\n\\n {target=}\")"],"metadata":{"id":"10uT7HwVLi4p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM\n","from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n","import torch\n","from datasets import load_dataset\n","import os\n","from transformers import AutoTokenizer\n","from torch.utils.data import DataLoader\n","from transformers import default_data_collator, get_linear_schedule_with_warmup\n","from tqdm import tqdm\n","from datasets import load_dataset\n","\n","device = \"cuda\"\n","model_name_or_path = \"bigscience/bloomz-560m\"\n","tokenizer_name_or_path = \"bigscience/bloomz-560m\"\n","peft_config = PromptTuningConfig(\n","    task_type=TaskType.CAUSAL_LM,\n","    prompt_tuning_init=PromptTuningInit.TEXT,\n","    num_virtual_tokens=8,\n","    prompt_tuning_init_text=\"Classify if the tweet is a complaint or not:\",\n","    tokenizer_name_or_path=model_name_or_path,\n",")\n","\n","dataset_name = \"twitter_complaints\"\n","checkpoint_name = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}_v1.pt\".replace(\n","    \"/\", \"_\"\n",")\n","text_column = \"Tweet text\"\n","label_column = \"text_label\"\n","max_length = 64\n","lr = 3e-2\n","num_epochs = 50\n","batch_size = 8\n","\n","\n","from datasets import load_dataset\n","\n","dataset = load_dataset(\"ought/raft\", dataset_name)\n","\n","classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n","print(classes)\n","dataset = dataset.map(\n","    lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n","    batched=True,\n","    num_proc=1,\n",")\n","print(dataset)\n","dataset[\"train\"][0]\n","\n","\n","# data preprocessing\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","target_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes])\n","print(target_max_length)\n","\n","\n","def preprocess_function(examples):\n","    batch_size = len(examples[text_column])\n","    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n","    targets = [str(x) for x in examples[label_column]]\n","    model_inputs = tokenizer(inputs)\n","    labels = tokenizer(targets, add_special_tokens=False)  # don't add bos token because we concatenate with inputs\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n","        # print(i, sample_input_ids, label_input_ids)\n","        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n","        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n","        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n","    # print(model_inputs)\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        label_input_ids = labels[\"input_ids\"][i]\n","        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n","            max_length - len(sample_input_ids)\n","        ) + sample_input_ids\n","        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n","            \"attention_mask\"\n","        ][i]\n","        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n","        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n","        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n","        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","\n","processed_datasets = dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    num_proc=1,\n","    remove_columns=dataset[\"train\"].column_names,\n","    load_from_cache_file=False,\n","    desc=\"Running tokenizer on dataset\",\n",")\n","\n","train_dataset = processed_datasets[\"train\"]\n","eval_dataset = processed_datasets[\"train\"]\n","\n","\n","train_dataloader = DataLoader(\n","    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",")\n","eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n","\n","\n","def test_preprocess_function(examples):\n","    batch_size = len(examples[text_column])\n","    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n","    model_inputs = tokenizer(inputs)\n","    # print(model_inputs)\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n","            max_length - len(sample_input_ids)\n","        ) + sample_input_ids\n","        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n","            \"attention_mask\"\n","        ][i]\n","        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n","        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n","    return model_inputs\n","\n","\n","test_dataset = dataset[\"test\"].map(\n","    test_preprocess_function,\n","    batched=True,\n","    num_proc=1,\n","    remove_columns=dataset[\"train\"].column_names,\n","    load_from_cache_file=False,\n","    desc=\"Running tokenizer on dataset\",\n",")\n","\n","test_dataloader = DataLoader(test_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n","next(iter(test_dataloader))\n","\n","\n","\n","# creating model\n","model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n","model = get_peft_model(model, peft_config)\n","model.print_trainable_parameters()\n","\n","\n","# model\n","# optimizer and lr scheduler\n","optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","lr_scheduler = get_linear_schedule_with_warmup(\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=(len(train_dataloader) * num_epochs),\n",")\n","\n","\n","\n","\n","# training and evaluation\n","model = model.to(device)\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for step, batch in enumerate(tqdm(train_dataloader)):\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        #         print(batch)\n","        #         print(batch[\"input_ids\"].shape)\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        total_loss += loss.detach().float()\n","        loss.backward()\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","\n","    model.eval()\n","    eval_loss = 0\n","    eval_preds = []\n","    for step, batch in enumerate(tqdm(eval_dataloader)):\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","        loss = outputs.loss\n","        eval_loss += loss.detach().float()\n","        eval_preds.extend(\n","            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n","        )\n","\n","    eval_epoch_loss = eval_loss / len(eval_dataloader)\n","    eval_ppl = torch.exp(eval_epoch_loss)\n","    train_epoch_loss = total_loss / len(train_dataloader)\n","    train_ppl = torch.exp(train_epoch_loss)\n","    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"],"metadata":{"id":"_-Q36GhBL4IF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","i = 33\n","inputs = tokenizer(f'{text_column} : {dataset[\"test\"][i][\"Tweet text\"]} Label : ', return_tensors=\"pt\")\n","print(dataset[\"test\"][i][\"Tweet text\"])\n","print(inputs)\n","\n","with torch.no_grad():\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","    outputs = model.generate(\n","        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=3\n","    )\n","    print(outputs)\n","    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"],"metadata":{"id":"7yfGPBe2McBg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# saving model\n","peft_model_id = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\".replace(\n","    \"/\", \"_\"\n",")\n","model.save_pretrained(peft_model_id)"],"metadata":{"id":"HoVi_eKyMlkm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ckpt = f\"{peft_model_id}/adapter_model.bin\"\n","!du -h $ckpt"],"metadata":{"id":"EoiNG_ErMsEK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from peft import PeftModel, PeftConfig\n","\n","peft_model_id = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\".replace(\n","    \"/\", \"_\"\n",")\n","\n","config = PeftConfig.from_pretrained(peft_model_id)\n","model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n","model = PeftModel.from_pretrained(model, peft_model_id)"],"metadata":{"id":"_seU-zZQMxSE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.to(device)\n","model.eval()\n","i = 4\n","inputs = tokenizer(f'{text_column} : {dataset[\"test\"][i][\"Tweet text\"]} Label : ', return_tensors=\"pt\")\n","print(dataset[\"test\"][i][\"Tweet text\"])\n","print(inputs)\n","\n","with torch.no_grad():\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","    outputs = model.generate(\n","        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=3\n","    )\n","    print(outputs)\n","    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"],"metadata":{"id":"mBuWDIE9M4Yq"},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOTISbkI27RjzuOtiwgWOP0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install -q -U datasets accelerate tqdm peft huggingface_hub bitsandbytes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7vY8asIeJMgN","executionInfo":{"status":"ok","timestamp":1706631279761,"user_tz":-330,"elapsed":9050,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"3c6e5175-fd50-41af-ec3d-791c6ef01d1b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/507.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/507.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m399.4/507.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install -q -U bitsandbytes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TD6vaZCRK02w","executionInfo":{"status":"ok","timestamp":1706631615364,"user_tz":-330,"elapsed":14616,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"edb7f5c1-c952-41c9-c8a4-dcaf95e31907"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["pip install -q -U transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"woHn8yhSL8KW","executionInfo":{"status":"ok","timestamp":1706631901767,"user_tz":-330,"elapsed":15780,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"70ee823d-4afb-40eb-a375-42a3880fff2b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/8.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/8.4 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/8.4 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["!python /content/sample_data/hemanth.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BI4qtyVxJpkO","executionInfo":{"status":"ok","timestamp":1706632006409,"user_tz":-330,"elapsed":104649,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"3453d86f-f96d-40b5-8386-d7e68a5944cb"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-01-30 16:25:06.038191: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-01-30 16:25:06.038250: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-01-30 16:25:06.039794: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-01-30 16:25:07.405715: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","01/30/2024 16:25:08 - INFO - __main__ - Distributed environment: NO\n","Num processes: 1\n","Process index: 0\n","Local process index: 0\n","Device: cuda\n","\n","Mixed precision type: no\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--stabilityai--stable-code-3b/snapshots/7c4583da0a36c56c7bf040864b737a70d45d1f4b/config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--stabilityai--stable-code-3b/snapshots/7c4583da0a36c56c7bf040864b737a70d45d1f4b/config.json\n","Model config StableLMEpochConfig {\n","  \"_name_or_path\": \"stabilityai/stable-code-3b\",\n","  \"architectures\": [\n","    \"StableLMEpochForCausalLM\"\n","  ],\n","  \"auto_map\": {\n","    \"AutoConfig\": \"stabilityai/stable-code-3b--configuration_stablelm_epoch.StableLMEpochConfig\",\n","    \"AutoModelForCausalLM\": \"stabilityai/stable-code-3b--modeling_stablelm_epoch.StableLMEpochForCausalLM\"\n","  },\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 0,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 2560,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 6912,\n","  \"max_position_embeddings\": 16384,\n","  \"model_type\": \"stablelm_epoch\",\n","  \"norm_eps\": 1e-05,\n","  \"num_attention_heads\": 32,\n","  \"num_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"rope_pct\": 0.25,\n","  \"rope_theta\": 1000000,\n","  \"rotary_scaling_factor\": 1.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.37.2\",\n","  \"use_cache\": true,\n","  \"use_qkv_bias\": false,\n","  \"vocab_size\": 50304\n","}\n","\n","loading file vocab.json from cache at None\n","loading file merges.txt from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--stabilityai--stable-code-3b/snapshots/7c4583da0a36c56c7bf040864b737a70d45d1f4b/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--stabilityai--stable-code-3b/snapshots/7c4583da0a36c56c7bf040864b737a70d45d1f4b/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--stabilityai--stable-code-3b/snapshots/7c4583da0a36c56c7bf040864b737a70d45d1f4b/tokenizer_config.json\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","The repository for stabilityai/stable-code-3b contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/stabilityai/stable-code-3b.\n","You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n","\n","Do you wish to run the custom code? [y/N] y\n","Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n","The device_map was not initialized. Setting device_map to {'':torch.cuda.current_device()}. If you want to use the model for inference, please set device_map ='auto' \n","model.safetensors.index.json: 100% 29.4k/29.4k [00:00<00:00, 81.6MB/s]\n","loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--stabilityai--stable-code-3b/snapshots/7c4583da0a36c56c7bf040864b737a70d45d1f4b/model.safetensors.index.json\n","Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n","model-00001-of-00002.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n","model-00001-of-00002.safetensors:   1% 31.5M/4.98G [00:00<00:17, 289MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   1% 73.4M/4.98G [00:00<00:14, 333MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   2% 115M/4.98G [00:00<00:14, 343MB/s] \u001b[A\n","model-00001-of-00002.safetensors:   3% 157M/4.98G [00:00<00:14, 327MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   4% 199M/4.98G [00:00<00:14, 327MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   5% 241M/4.98G [00:00<00:14, 337MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   6% 283M/4.98G [00:00<00:13, 341MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   7% 325M/4.98G [00:00<00:13, 339MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   7% 367M/4.98G [00:01<00:14, 328MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   8% 409M/4.98G [00:01<00:13, 329MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   9% 451M/4.98G [00:01<00:14, 317MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  10% 493M/4.98G [00:01<00:15, 295MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  11% 524M/4.98G [00:01<00:15, 286MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  11% 556M/4.98G [00:01<00:16, 273MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  12% 587M/4.98G [00:01<00:16, 266MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  12% 619M/4.98G [00:02<00:16, 258MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  13% 650M/4.98G [00:02<00:16, 257MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  14% 682M/4.98G [00:02<00:16, 257MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  14% 713M/4.98G [00:02<00:16, 256MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  15% 744M/4.98G [00:02<00:16, 250MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  16% 776M/4.98G [00:02<00:16, 255MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  16% 807M/4.98G [00:02<00:16, 251MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  17% 839M/4.98G [00:02<00:16, 256MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  17% 870M/4.98G [00:03<00:16, 254MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  18% 902M/4.98G [00:03<00:16, 244MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  19% 933M/4.98G [00:03<00:17, 236MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  19% 965M/4.98G [00:03<00:17, 232MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  20% 996M/4.98G [00:03<00:17, 227MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  21% 1.03G/4.98G [00:03<00:17, 226MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  21% 1.06G/4.98G [00:03<00:17, 226MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  22% 1.09G/4.98G [00:04<00:17, 228MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  23% 1.12G/4.98G [00:04<00:16, 228MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  23% 1.16G/4.98G [00:04<00:14, 259MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  24% 1.20G/4.98G [00:04<00:14, 266MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  25% 1.23G/4.98G [00:04<00:13, 275MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  25% 1.26G/4.98G [00:04<00:17, 208MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  26% 1.29G/4.98G [00:04<00:19, 189MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  27% 1.32G/4.98G [00:05<00:20, 176MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  27% 1.34G/4.98G [00:05<00:20, 179MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  27% 1.36G/4.98G [00:06<01:14, 48.6MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  28% 1.38G/4.98G [00:08<02:34, 23.3MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  28% 1.41G/4.98G [00:09<01:58, 30.2MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  29% 1.43G/4.98G [00:09<01:31, 38.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  29% 1.45G/4.98G [00:09<01:12, 48.7MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  29% 1.47G/4.98G [00:09<00:56, 61.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  30% 1.50G/4.98G [00:09<00:40, 86.6MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  31% 1.53G/4.98G [00:09<00:30, 113MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  31% 1.55G/4.98G [00:09<00:26, 127MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  32% 1.57G/4.98G [00:10<00:25, 131MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  32% 1.60G/4.98G [00:11<01:07, 49.9MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  33% 1.63G/4.98G [00:13<02:08, 26.1MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  33% 1.65G/4.98G [00:13<01:38, 34.0MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  33% 1.67G/4.98G [00:13<01:17, 42.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  34% 1.69G/4.98G [00:13<01:02, 52.9MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  34% 1.71G/4.98G [00:13<00:48, 67.1MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  35% 1.74G/4.98G [00:13<00:35, 92.4MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  36% 1.77G/4.98G [00:14<00:27, 118MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  36% 1.79G/4.98G [00:14<00:25, 125MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  37% 1.84G/4.98G [00:14<00:18, 170MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  37% 1.87G/4.98G [00:14<00:22, 136MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  38% 1.91G/4.98G [00:14<00:17, 180MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  39% 1.94G/4.98G [00:14<00:15, 202MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  40% 1.98G/4.98G [00:14<00:12, 234MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  40% 2.01G/4.98G [00:15<00:12, 247MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  41% 2.04G/4.98G [00:15<00:11, 261MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  42% 2.08G/4.98G [00:15<00:10, 265MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  42% 2.11G/4.98G [00:15<00:11, 246MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  43% 2.14G/4.98G [00:15<00:11, 239MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  44% 2.17G/4.98G [00:15<00:11, 241MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  44% 2.20G/4.98G [00:15<00:11, 243MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  45% 2.23G/4.98G [00:15<00:11, 231MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  45% 2.26G/4.98G [00:16<00:11, 231MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  46% 2.30G/4.98G [00:16<00:11, 239MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  47% 2.33G/4.98G [00:16<00:11, 238MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  47% 2.36G/4.98G [00:16<00:11, 226MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  48% 2.39G/4.98G [00:16<00:10, 238MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  49% 2.42G/4.98G [00:16<00:11, 231MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  49% 2.46G/4.98G [00:16<00:09, 257MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  50% 2.50G/4.98G [00:17<00:10, 229MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  51% 2.53G/4.98G [00:17<00:10, 230MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  52% 2.57G/4.98G [00:17<00:09, 257MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  52% 2.60G/4.98G [00:17<00:09, 250MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  53% 2.63G/4.98G [00:17<00:08, 262MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  53% 2.66G/4.98G [00:17<00:08, 267MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  54% 2.69G/4.98G [00:17<00:08, 261MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  55% 2.73G/4.98G [00:17<00:08, 257MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  55% 2.76G/4.98G [00:18<00:08, 271MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  56% 2.79G/4.98G [00:18<00:24, 90.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  57% 2.82G/4.98G [00:19<00:19, 111MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  57% 2.85G/4.98G [00:19<00:17, 121MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  58% 2.87G/4.98G [00:19<00:17, 123MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  58% 2.89G/4.98G [00:19<00:15, 137MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  59% 2.93G/4.98G [00:19<00:12, 159MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  59% 2.96G/4.98G [00:19<00:11, 179MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  60% 2.99G/4.98G [00:19<00:09, 199MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  61% 3.02G/4.98G [00:20<00:10, 183MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  61% 3.04G/4.98G [00:20<00:25, 76.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  61% 3.06G/4.98G [00:23<01:27, 22.0MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  62% 3.09G/4.98G [00:24<00:58, 32.0MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  63% 3.11G/4.98G [00:24<00:46, 40.0MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  63% 3.14G/4.98G [00:24<00:37, 48.7MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  63% 3.16G/4.98G [00:24<00:30, 60.2MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  64% 3.19G/4.98G [00:24<00:21, 83.9MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  65% 3.22G/4.98G [00:24<00:16, 108MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  65% 3.24G/4.98G [00:24<00:14, 121MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  65% 3.26G/4.98G [00:25<00:14, 122MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  66% 3.29G/4.98G [00:25<00:11, 150MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  67% 3.31G/4.98G [00:28<01:24, 19.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  67% 3.34G/4.98G [00:29<00:56, 28.9MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  68% 3.37G/4.98G [00:29<00:45, 35.5MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  68% 3.39G/4.98G [00:29<00:35, 44.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  69% 3.42G/4.98G [00:29<00:24, 64.0MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  69% 3.45G/4.98G [00:29<00:18, 84.5MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  70% 3.47G/4.98G [00:29<00:15, 97.1MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  70% 3.50G/4.98G [00:29<00:12, 118MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  71% 3.52G/4.98G [00:30<00:11, 128MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  71% 3.55G/4.98G [00:30<00:08, 160MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  72% 3.59G/4.98G [00:34<01:01, 22.6MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  73% 3.63G/4.98G [00:34<00:38, 35.0MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  73% 3.65G/4.98G [00:34<00:32, 40.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  74% 3.67G/4.98G [00:34<00:27, 48.4MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  74% 3.70G/4.98G [00:34<00:19, 66.7MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  75% 3.73G/4.98G [00:34<00:14, 87.4MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  76% 3.76G/4.98G [00:34<00:11, 104MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  76% 3.79G/4.98G [00:35<00:11, 109MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  77% 3.82G/4.98G [00:35<00:08, 137MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  77% 3.84G/4.98G [00:35<00:16, 71.3MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  77% 3.86G/4.98G [00:39<00:55, 20.2MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  78% 3.89G/4.98G [00:39<00:36, 29.9MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  79% 3.92G/4.98G [00:39<00:24, 43.0MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  79% 3.95G/4.98G [00:39<00:17, 58.3MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  80% 3.98G/4.98G [00:39<00:12, 77.5MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  81% 4.02G/4.98G [00:39<00:10, 88.3MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  81% 4.04G/4.98G [00:40<00:09, 99.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  82% 4.07G/4.98G [00:40<00:07, 124MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  82% 4.10G/4.98G [00:40<00:06, 145MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  83% 4.13G/4.98G [00:40<00:05, 167MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  84% 4.16G/4.98G [00:40<00:04, 175MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  84% 4.19G/4.98G [00:40<00:04, 195MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  85% 4.23G/4.98G [00:40<00:03, 205MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  85% 4.26G/4.98G [00:40<00:03, 228MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  86% 4.29G/4.98G [00:41<00:03, 220MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  87% 4.32G/4.98G [00:41<00:02, 237MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  87% 4.35G/4.98G [00:41<00:02, 252MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  88% 4.38G/4.98G [00:41<00:02, 241MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  89% 4.41G/4.98G [00:41<00:02, 228MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  89% 4.45G/4.98G [00:41<00:02, 247MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  90% 4.48G/4.98G [00:43<00:08, 58.4MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  91% 4.51G/4.98G [00:43<00:06, 72.7MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  91% 4.53G/4.98G [00:43<00:05, 78.7MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  92% 4.56G/4.98G [00:43<00:04, 101MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  92% 4.59G/4.98G [00:43<00:03, 124MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  93% 4.62G/4.98G [00:43<00:02, 147MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  93% 4.66G/4.98G [00:44<00:02, 146MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  94% 4.69G/4.98G [00:44<00:01, 165MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  95% 4.72G/4.98G [00:44<00:01, 183MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  95% 4.75G/4.98G [00:44<00:01, 209MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  96% 4.78G/4.98G [00:44<00:00, 231MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  97% 4.81G/4.98G [00:44<00:00, 244MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  97% 4.84G/4.98G [00:44<00:00, 252MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  98% 4.88G/4.98G [00:45<00:00, 259MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  99% 4.91G/4.98G [00:45<00:00, 262MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  99% 4.94G/4.98G [00:45<00:00, 247MB/s]\u001b[A\n","model-00001-of-00002.safetensors: 100% 4.98G/4.98G [00:45<00:00, 110MB/s]\n","Downloading shards:  50% 1/2 [00:46<00:46, 46.02s/it]\n","model-00002-of-00002.safetensors:   0% 0.00/610M [00:00<?, ?B/s]\u001b[A\n","model-00002-of-00002.safetensors:   5% 31.5M/610M [00:00<00:02, 272MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  12% 73.4M/610M [00:00<00:01, 353MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  19% 115M/610M [00:00<00:01, 324MB/s] \u001b[A\n","model-00002-of-00002.safetensors:  26% 157M/610M [00:00<00:01, 275MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  31% 189M/610M [00:00<00:01, 274MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  36% 220M/610M [00:00<00:01, 284MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  41% 252M/610M [00:00<00:01, 289MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  46% 283M/610M [00:00<00:01, 295MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  53% 325M/610M [00:01<00:00, 304MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  58% 357M/610M [00:01<00:00, 303MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  64% 388M/610M [00:01<00:00, 305MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  69% 419M/610M [00:01<00:00, 295MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  76% 461M/610M [00:01<00:00, 268MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  81% 493M/610M [00:01<00:00, 257MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  86% 524M/610M [00:01<00:00, 266MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  91% 556M/610M [00:02<00:00, 243MB/s]\u001b[A\n","model-00002-of-00002.safetensors: 100% 610M/610M [00:02<00:00, 258MB/s]\n","Downloading shards: 100% 2/2 [00:48<00:00, 24.46s/it]\n","Instantiating StableLMEpochForCausalLM model under default dtype torch.float16.\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 0\n","}\n","\n","Detected 4-bit loading: activating 4-bit loading for this model\n","Loading checkpoint shards: 100% 2/2 [00:29<00:00, 14.87s/it]\n","All model checkpoint weights were used when initializing StableLMEpochForCausalLM.\n","\n","All the weights of StableLMEpochForCausalLM were initialized from the model checkpoint at stabilityai/stable-code-3b.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use StableLMEpochForCausalLM for predictions without further training.\n","generation_config.json: 100% 111/111 [00:00<00:00, 573kB/s]\n","loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--stabilityai--stable-code-3b/snapshots/7c4583da0a36c56c7bf040864b737a70d45d1f4b/generation_config.json\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 0\n","}\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 286, in hf_raise_for_status\n","    response.raise_for_status()\n","  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1021, in raise_for_status\n","    raise HTTPError(http_error_msg, response=self)\n","requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/stabilityai/stable-code-3b/resolve/main/loftq_init/adapter_config.json\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/peft/config.py\", line 198, in _get_peft_type\n","    config_file = hf_hub_download(\n","  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1238, in hf_hub_download\n","    metadata = get_hf_file_metadata(\n","  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1631, in get_hf_file_metadata\n","    r = _request_wrapper(\n","  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 385, in _request_wrapper\n","    response = _request_wrapper(\n","  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 409, in _request_wrapper\n","    hf_raise_for_status(response)\n","  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 296, in hf_raise_for_status\n","    raise EntryNotFoundError(message, response) from e\n","huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-65b92343-77f3667a33f0394c5f2857cf;9d2062c3-b3c1-4cc7-9d25-d721e61e02ee)\n","\n","Entry Not Found for url: https://huggingface.co/stabilityai/stable-code-3b/resolve/main/loftq_init/adapter_config.json.\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/content/sample_data/hemanth.py\", line 846, in <module>\n","    main()\n","  File \"/content/sample_data/hemanth.py\", line 470, in main\n","    model = PeftModel.from_pretrained(model, args.model_name_or_path, subfolder=\"loftq_init\", is_trainable=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 326, in from_pretrained\n","    PeftConfig._get_peft_type(\n","  File \"/usr/local/lib/python3.10/dist-packages/peft/config.py\", line 204, in _get_peft_type\n","    raise ValueError(f\"Can't find '{CONFIG_NAME}' at '{model_id}'\")\n","ValueError: Can't find 'adapter_config.json' at 'stabilityai/stable-code-3b'\n"]}]},{"cell_type":"markdown","source":["1. **Data Collection:**\n","   - Identify and collect raw datasets relevant to your AI task, ensuring they cover a diverse range of scenarios.\n","   - Organize datasets into folders based on categories or labels.\n","\n","2. **Data Inspection and Cleaning:**\n","   - Explore the raw data to understand its structure, format, and potential issues.\n","   - Handle missing data by either imputation or removal, depending on the extent and nature of missing values.\n","   - Address outliers or anomalies that might adversely affect model training.\n","\n","3. **Data Formatting:**\n","   - Convert data into a format suitable for your model. This may involve reshaping images, normalizing numerical data, or tokenizing text.\n","   - Ensure consistency in data types and scales across features.\n","\n","4. **Data Splitting:**\n","   - Divide the dataset into training, validation, and test sets. A common split is 70-15-15%, but this can vary based on the size of the dataset.\n","\n","5. **Data Augmentation (for Computer Vision):**\n","   - Apply augmentation techniques to increase the diversity of your training data. This helps the model generalize better to unseen examples.\n","\n","6. **Feature Engineering (if applicable):**\n","   - Extract relevant features from the data or create new features that might enhance the model's performance.\n","\n","7. **Data Preprocessing:**\n","   - Normalize or standardize the data to ensure that all features have a similar scale. This step is crucial for many machine learning models.\n","\n","8. **Data Pipeline Setup:**\n","   - Create a data pipeline that efficiently loads and preprocesses batches of data. This is especially important when dealing with large datasets that can't fit into memory.\n","\n","9. **Model Input Preparation:**\n","   - Depending on the model architecture, format the data into the required input shape. For instance, reshape images or sequence data to match the input specifications of your neural network.\n","\n","10. **Verification:**\n","    - Validate that the input data is correctly processed and compatible with your model. Check a few samples to ensure the transformation has been applied appropriately.\n","----\n","**Data Collection:**\n","\n","1. **Define AI Task:**\n","   - Clearly articulate the specific goal of your AI task, whether it's image classification, natural language processing, or another application.\n","\n","2. **Identify Data Requirements:**\n","   - Determine the types of data needed for your AI task. For example, if working on image recognition, you might need a dataset containing labeled images.\n","\n","3. **Search for Datasets:**\n","   - Explore existing datasets from reputable sources such as academic repositories, government databases, or domain-specific platforms. Consider datasets like ImageNet, COCO, or MNIST for various AI tasks.\n","\n","4. **Data Licensing and Permissions:**\n","   - Verify the licensing agreements and permissions associated with each dataset. Ensure that you have the legal right to use and distribute the data for your intended purpose.\n","\n","5. **Data Diversity:**\n","   - Aim for diversity in your dataset. Include examples from various sources, environments, and conditions relevant to the real-world scenarios your AI system may encounter.\n","\n","6. **Data Size:**\n","   - Consider the size of your dataset. While larger datasets often lead to better models, ensure that the volume of data is manageable and aligns with your computational resources.\n","\n","7. **Data Quality:**\n","   - Assess the quality of potential datasets. Look for datasets with accurate and reliable annotations or labels, as these are crucial for supervised learning tasks.\n","\n","8. **Create a Data Inventory:**\n","   - Document the datasets you've identified, including details such as the number of samples, data format, and any unique characteristics.\n","\n","9. **Download and Extract Data:**\n","   - Download the selected datasets from their respective sources. Extract the files to a designated location on your system.\n","\n","**Organize datasets into folders based on categories or labels:**\n","\n","10. **Create Main Dataset Directory:**\n","    - Establish a main directory to store all your datasets. This directory will serve as the root for organizing your data.\n","\n","11. **Subdirectories for Categories:**\n","    - Within the main directory, create subdirectories for each category or label. For instance, if working on a dog breed classification task, create subdirectories for each dog breed.\n","\n","12. **Distribution of Data:**\n","    - Distribute the data into these subdirectories based on their corresponding categories. Ensure a balanced distribution to prevent biases in model training.\n","\n","13. **Training, Validation, and Test Sets:**\n","    - Further divide the data within each category into training, validation, and test sets. A common split is 70% for training, 15% for validation, and 15% for testing.\n","\n","14. **Randomization:**\n","    - If applicable, randomize the order of samples within each split to prevent any inherent ordering patterns from affecting the model's learning process.\n","\n","15. **Verify Organization:**\n","    - Double-check that each subdirectory contains the correct samples and that the overall organization aligns with your dataset structure plan.\n","\n","----\n","**Data Inspection and Cleaning:**\n","\n","1. **Data Exploration:**\n","   - Begin by loading a small sample of the raw data to understand its structure. This step helps you get a preliminary overview of the data's format, features, and potential challenges.\n","\n","2. **Dataset Overview:**\n","   - Display basic statistics and information about the dataset, such as the number of samples, features, and data types. This provides an initial understanding of the data's characteristics.\n","\n","3. **Visual Inspection:**\n","   - Use visualizations (e.g., histograms, scatter plots) to inspect the distribution of individual features. This helps identify patterns, trends, and potential outliers.\n","\n","4. **Data Format and Encoding:**\n","   - Check for inconsistencies in data format and encoding. Ensure that all features are in the expected format, and if needed, convert them to a consistent data type.\n","\n","5. **Identify Missing Data:**\n","   - Examine the dataset for missing values in each feature. Create visualizations or summary statistics to highlight the extent of missing data.\n","\n","6. **Handle Missing Data:**\n","   - Decide on a strategy for handling missing values based on their nature:\n","      - Imputation: Replace missing values with a suitable estimate (e.g., mean, median, or mode) if the missing data is relatively small and doesn't introduce significant bias.\n","      - Removal: If missing values are extensive or systematic, consider removing the corresponding samples or features.\n","\n","7. **Outlier Detection:**\n","   - Utilize statistical methods or visualization techniques to identify outliers—data points that deviate significantly from the majority. Outliers can adversely affect model training.\n","\n","8. **Outlier Treatment:**\n","   - Decide on a strategy for addressing outliers:\n","      - Transformation: Apply mathematical transformations to features to reduce the impact of outliers.\n","      - Removal: In extreme cases, remove outliers if they are likely to introduce noise or bias to the model.\n","\n","9. **Data Integrity Check:**\n","   - Verify data integrity by checking for duplicates or inconsistencies in labeling. Ensure that each sample is unique and correctly labeled.\n","\n","10. **Documentation:**\n","    - Document the steps taken during data inspection and cleaning, including the rationale behind decisions regarding missing data and outliers. This documentation is crucial for reproducibility and collaboration.\n","\n","11. **Iterative Process:**\n","    - Understand that data inspection and cleaning are iterative processes. After initial cleaning, revisit the dataset exploration to ensure that subsequent steps have not introduced new issues.\n","\n","12. **Version Control:**\n","    - If applicable, implement version control for your dataset. This allows you to track changes and revert to previous states if needed.\n","\n","----\n","\n","**Data Formatting:**\n","\n","1. **Understand Model Input Requirements:**\n","   - Examine the input requirements of your chosen model architecture. Different models, such as convolutional neural networks (CNNs) for images or recurrent neural networks (RNNs) for sequences, have specific input expectations.\n","\n","2. **Reshape Images (if applicable):**\n","   - For computer vision tasks, reshape images to the required dimensions. Ensure consistency in image size across the dataset to facilitate model training.\n","\n","3. **Normalize Numerical Data:**\n","   - Standardize numerical features by normalizing them. Use techniques like Z-score normalization to ensure that numerical values have a similar scale. This step is crucial for models that are sensitive to input scales.\n","\n","4. **Tokenize Text (if applicable):**\n","   - If working with natural language processing (NLP) tasks, tokenize text data into words or subword units. This process converts text into a format suitable for embedding layers in neural networks.\n","\n","5. **Handling Categorical Variables:**\n","   - Encode categorical variables using one-hot encoding or other suitable techniques. This transforms categorical data into a numerical format that can be fed into the model.\n","\n","6. **Ensure Consistent Data Types:**\n","   - Confirm that all features have consistent data types. For example, ensure that numerical features are represented as floats or integers, and categorical features are appropriately encoded.\n","\n","7. **Check for Data Skewness:**\n","   - Examine the distribution of numerical features. If a feature is heavily skewed, consider applying appropriate transformations (e.g., log transformation) to reduce skewness and improve model performance.\n","\n","8. **Handling Time Series Data (if applicable):**\n","   - If your data involves time series, organize it in chronological order. Consider features like lag values or rolling statistics to capture temporal patterns.\n","\n","9. **Handling Missing Values (if not addressed earlier):**\n","   - If missing values were not handled during the data inspection and cleaning phase, address them now by applying suitable imputation methods specific to your data type.\n","\n","10. **Feature Scaling:**\n","    - Apply feature scaling techniques such as Min-Max scaling or Standard scaling to ensure that all features contribute equally to the model's training process.\n","\n","11. **Data Representation for Embeddings (if applicable):**\n","    - If using embeddings for certain types of data (e.g., word embeddings for NLP), ensure that the data is represented in a way compatible with embedding layers in your model.\n","\n","12. **Verification:**\n","    - Verify that the data has been correctly formatted according to the model's requirements. Inspect a few samples to ensure that reshaping, normalization, and encoding have been applied consistently.\n","\n","13. **Documentation:**\n","    - Document the data formatting steps performed, including any transformations or encoding applied. This documentation aids in understanding and reproducing the preprocessing steps during model deployment or further experimentation.\n","\n","-----\n","**Data Splitting:**\n","\n","1. **Understand the Purpose of Splitting:**\n","   - Recognize the need for splitting the dataset into distinct sets for training, validation, and testing. This separation allows for model training, hyperparameter tuning, and evaluation on unseen data.\n","\n","2. **Randomize the Dataset:**\n","   - If not done during the initial organization, randomize the order of samples in the dataset. This helps prevent any inherent order or biases from affecting the model's learning.\n","\n","3. **Determine Split Ratios:**\n","   - Decide on the split ratios for training, validation, and test sets. A common split is 70-15-15%, but these ratios can vary based on factors such as the size of the dataset and specific project requirements.\n","\n","4. **Implement Stratified Sampling (if applicable):**\n","   - If your dataset has imbalanced classes, consider using stratified sampling to ensure that each subset (training, validation, test) maintains the same class distribution as the original dataset.\n","\n","5. **Perform the Split:**\n","   - Divide the dataset into three sets: training, validation, and test. This can be achieved programmatically by selecting samples based on the predetermined ratios.\n","\n","6. **Verification of Split:**\n","   - Verify that each subset (training, validation, test) accurately reflects the intended split ratios. Check that the distribution of classes remains consistent across all sets.\n","\n","7. **Data Shuffle (if needed):**\n","   - If the dataset is large and processed in batches during training, implement a shuffling mechanism for each epoch to ensure that the model encounters diverse samples during training.\n","\n","8. **Separate Features and Labels:**\n","   - Ensure that the features and labels are separated in each subset. This separation is crucial for feeding the input data into the model during training and evaluation.\n","\n","9. **Save Split Datasets:**\n","   - Save the split datasets into separate files or directories. This facilitates easy access and loading during subsequent stages of model development.\n","\n","10. **Documentation:**\n","    - Document the split ratios and any considerations made during the process. This documentation aids in transparency and reproducibility of experiments.\n","\n","11. **Consider Cross-Validation (if needed):**\n","    - Depending on the size and nature of the dataset, you may choose to implement cross-validation. This involves multiple splits and training the model on different combinations of training and validation sets, providing a more robust evaluation.\n","\n","12. **Maintain Data Integrity:**\n","    - Ensure that the integrity of each split is maintained. Avoid data leakage between sets, and be cautious not to use information from the validation or test sets during model development.\n"],"metadata":{"id":"rj003RWh5sC7"}},{"cell_type":"code","source":["!pip install ratelimiter requests_cache backoff"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yqWcPGLK_RuR","executionInfo":{"status":"ok","timestamp":1706628596748,"user_tz":-330,"elapsed":8375,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"82b0d881-c32d-4231-e757-d5217d7eadc7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ratelimiter\n","  Downloading ratelimiter-1.2.0.post0-py3-none-any.whl (6.6 kB)\n","Collecting requests_cache\n","  Downloading requests_cache-1.1.1-py3-none-any.whl (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting backoff\n","  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: attrs>=21.2 in /usr/local/lib/python3.10/dist-packages (from requests_cache) (23.2.0)\n","Collecting cattrs>=22.2 (from requests_cache)\n","  Downloading cattrs-23.2.3-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests_cache) (4.1.0)\n","Requirement already satisfied: requests>=2.22 in /usr/local/lib/python3.10/dist-packages (from requests_cache) (2.31.0)\n","Collecting url-normalize>=1.4 (from requests_cache)\n","  Downloading url_normalize-1.4.3-py2.py3-none-any.whl (6.8 kB)\n","Requirement already satisfied: urllib3>=1.25.5 in /usr/local/lib/python3.10/dist-packages (from requests_cache) (2.0.7)\n","Requirement already satisfied: exceptiongroup>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from cattrs>=22.2->requests_cache) (1.2.0)\n","Requirement already satisfied: typing-extensions!=4.6.3,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from cattrs>=22.2->requests_cache) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22->requests_cache) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22->requests_cache) (3.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22->requests_cache) (2023.11.17)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from url-normalize>=1.4->requests_cache) (1.16.0)\n","Installing collected packages: ratelimiter, url-normalize, cattrs, backoff, requests_cache\n","Successfully installed backoff-2.2.1 cattrs-23.2.3 ratelimiter-1.2.0.post0 requests_cache-1.1.1 url-normalize-1.4.3\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BGUNOKvT5pGE","executionInfo":{"status":"ok","timestamp":1706628892827,"user_tz":-330,"elapsed":406,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"5ad8b9b8-cb32-409c-87cc-8a91bde72232"},"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:__main__:Error processing https://en.wikipedia.org/wiki/Elon_Musk: Session is closed\n","ERROR:asyncio:Task exception was never retrieved\n","future: <Task finished name='Task-1' coro=<main() done, defined at <ipython-input-6-00a458fa6101>:42> exception=TypeError(\"object Lock can't be used in 'await' expression\")>\n","Traceback (most recent call last):\n","  File \"<ipython-input-6-00a458fa6101>\", line 48, in main\n","    await rate_limiter.__aenter__()  # Wait for rate limiter\n","  File \"<string>\", line 6, in __aenter__\n","TypeError: object Lock can't be used in 'await' expression\n"]}],"source":["import aiohttp\n","import asyncio\n","from bs4 import BeautifulSoup\n","from ratelimiter import RateLimiter\n","import requests_cache\n","import logging\n","import backoff\n","\n","# Set up logging\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","# Install and enable requests-cache\n","requests_cache.install_cache('scrape_cache')\n","\n","# Respect robots.txt - This needs to be checked separately for each website\n","# This code does not directly implement robots.txt checking\n","\n","# Define backoff strategy for retries\n","@backoff.on_exception(backoff.expo, aiohttp.ClientError, max_tries=8)\n","async def fetch_html_content(session, url):\n","    async with session.get(url) as response:\n","        response.raise_for_status()  # Will trigger retry on client errors\n","        return await response.text()\n","\n","# Rate limiting decorator\n","rate_limiter = RateLimiter(max_calls=10, period=1)\n","\n","async def process_html_content(html_content):\n","    # Replace this with your actual processing code\n","    soup = BeautifulSoup(html_content, 'html.parser')\n","    return soup.title.text\n","\n","async def scrape_url(session, url):\n","    try:\n","        html_content = await fetch_html_content(session, url)\n","        data = await process_html_content(html_content)\n","        logger.info(f\"Processed data from {url}: {data}\")\n","    except Exception as e:\n","        logger.error(f\"Error processing {url}: {e}\")\n","\n","async def main(urls):\n","    async with aiohttp.ClientSession(headers={'User-Agent': 'Your Custom User Agent'}) as session:\n","        tasks = []\n","        for url in urls:\n","            task = asyncio.create_task(scrape_url(session, url))\n","            tasks.append(task)\n","            await rate_limiter.__aenter__()  # Wait for rate limiter\n","        await asyncio.gather(*tasks)\n","\n","# List of URLs to scrape\n","urls = [\n","  \"https://en.wikipedia.org/wiki/Elon_Musk\",\n","  \"https://en.wikipedia.org/wiki/Elon_Musk\",\n","  \"https://en.wikipedia.org/wiki/Elon_Musk\"\n","]\n","loop = asyncio.get_event_loop()\n","if loop.is_running():\n","    # If the event loop is already running (e.g., under Jupyter notebook), use `create_task`\n","    loop.create_task(main(urls))\n","else:\n","    # Otherwise, use `run_until_complete` to start the event loop\n","    loop.run_until_complete(main(urls))\n","\n","\n","# Note: Implementing multi-threading or multi-processing depends on the specific\n","# requirements of your scraping tasks and is not directly shown in this example.\n","# If needed, you can combine the asyncio event loop with concurrent.futures for threading or multiprocessing."]}]}
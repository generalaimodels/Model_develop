{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyM7pRBErpvWobHBU7QPxebL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["- **Author:** **Kandimalla Hemanth**\n","- **Date of modified:**  **1-13-2024**\n","- **E-mail:** **speechcodehemanth2@gmail.com**\n"],"metadata":{"id":"MOTw3YvnW_Qv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"J_Q9-Lk5LJ8h"},"outputs":[],"source":["!pip install -q -U transformers accelerate evaluate deepspeed tqdm datasets peft"]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM\n","from peft import PeftModel, PeftConfig\n","import torch\n","from datasets import load_dataset\n","import os\n","from transformers import AutoTokenizer\n","from torch.utils.data import DataLoader\n","from transformers import default_data_collator, get_linear_schedule_with_warmup\n","from tqdm import tqdm\n","from datasets import load_dataset\n","\n","device = \"cuda\"\n","model_name_or_path = \"bigscience/bloomz-7b1\"\n","tokenizer_name_or_path = \"bigscience/bloomz-7b1\"\n","dataset_name = \"twitter_complaints\"\n","text_column = \"Tweet text\"\n","label_column = \"text_label\"\n","max_length = 64\n","lr = 1e-3\n","num_epochs = 50\n","batch_size = 8"],"metadata":{"id":"NlG6FrGeLzse"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"ought/raft\", dataset_name)\n","\n","classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n","print(classes)\n","dataset = dataset.map(\n","    lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n","    batched=True,\n","    num_proc=1,\n",")\n","print(dataset)\n","dataset[\"train\"][0]"],"metadata":{"id":"KN-SbF21L-kB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data preprocessing\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","target_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes])\n","print(target_max_length)\n","\n","\n","def preprocess_function(examples):\n","    batch_size = len(examples[text_column])\n","    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n","    targets = [str(x) for x in examples[label_column]]\n","    model_inputs = tokenizer(inputs)\n","    labels = tokenizer(targets, add_special_tokens=False)  # don't add bos token because we concatenate with inputs\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n","        # print(i, sample_input_ids, label_input_ids)\n","        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n","        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n","        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n","    # print(model_inputs)\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        label_input_ids = labels[\"input_ids\"][i]\n","        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n","            max_length - len(sample_input_ids)\n","        ) + sample_input_ids\n","        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n","            \"attention_mask\"\n","        ][i]\n","        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n","        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n","        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n","        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","\n","processed_datasets = dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    num_proc=1,\n","    remove_columns=dataset[\"train\"].column_names,\n","    load_from_cache_file=False,\n","    desc=\"Running tokenizer on dataset\",\n",")\n","\n","train_dataset = processed_datasets[\"train\"]\n","\n","\n","train_dataloader = DataLoader(\n","    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",")"],"metadata":{"id":"RYYCiQ4IMHUK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import argparse\n","import evaluate\n","import torch\n","from datasets import load_dataset\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader, random_split\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, get_linear_schedule_with_warmup, set_seed\n","from accelerate import Accelerator\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","\n","# Set random seed for reproducibility\n","set_seed(42)\n","\n","# Argument parser to easily change dataset and model\n","parser = argparse.ArgumentParser()\n","parser.add_argument(\"--dataset_name\", type=str, default=\"wikitext\", help=\"The name of the dataset to load.\")\n","parser.add_argument(\"--model_name\", type=str, default=\"gpt2\", help=\"The name of the model to use.\")\n","args = parser.parse_args()\n","\n","# Load the dataset\n","dataset = load_dataset(args.dataset_name, 'wikitext-2-raw-v1')\n","\n","# Pre-processing\n","tokenizer = GPT2Tokenizer.from_pretrained(args.model_name)\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n","\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)\n","train_dataset, test_dataset = random_split(tokenized_datasets['train'], [int(0.9 * len(tokenized_datasets['train'])), int(0.1 * len(tokenized_datasets['train']))])\n","\n","# Make DataLoaders\n","train_loader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n","test_loader = DataLoader(test_dataset, batch_size=8)\n","\n","# Load the model\n","model = GPT2LMHeadModel.from_pretrained(args.model_name)\n","\n","# Setup for training\n","accelerator = Accelerator()\n","model, train_loader, test_loader = accelerator.prepare(model, train_loader, test_loader)\n","\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=len(train_loader) * 3)\n","\n","# Training loop\n","model.train()\n","for epoch in range(3):\n","    tqdm_train_loader = tqdm(train_loader, desc=f\"Training Epoch {epoch}\")\n","    for batch in tqdm_train_loader:\n","        outputs = model(batch['input_ids'], labels=batch['input_ids'])\n","        loss = outputs.loss\n","        accelerator.backward(loss)\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","        tqdm_train_loader.set_postfix(loss=loss.item())\n","\n","# Testing loop\n","model.eval()\n","test_loss = 0\n","with torch.no_grad():\n","    for batch in test_loader:\n","        outputs = model(batch['input_ids'], labels=batch['input_ids'])\n","        test_loss += outputs.loss.item()\n","test_loss /= len(test_loader)\n","\n","# Plotting\n","plt.figure(figsize=(10, 4))\n","plt.title(\"Training and Testing Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.plot([1, 2, 3], [train_loss, train_loss, train_loss], label=\"Training Loss\")\n","plt.plot([1, 2, 3], [test_loss, test_loss, test_loss], label=\"Testing Loss\")\n","plt.legend()\n","plt.show()\n","\n","# Evaluation using ROUGE\n","rouge = evaluate.load(\"rouge\")\n","model.eval()\n","predictions = []\n","references = []\n","for batch in test_loader:\n","    generated_tokens = accelerator.unwrap_model(model).generate(batch['input_ids'], max_length=30)\n","    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n","    decoded_labels = tokenizer.batch_decode(batch['input_ids'], skip_special_tokens=True)\n","    predictions.extend(decoded_preds)\n","    references.extend(decoded_labels)\n","\n","result = rouge.compute(predictions=predictions, references=references)\n","print(result)"],"metadata":{"id":"1Y2xqh2gRimy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import gc\n","import os\n","import torch\n","import matplotlib.pyplot as plt\n","from accelerate import Accelerator\n","from datasets import load_dataset, concatenate_datasets\n","from torch.utils.data import DataLoader\n","from tqdm.auto import tqdm\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    DataCollatorForLanguageModeling,\n","    Trainer,\n","    TrainingArguments,\n","    get_linear_schedule_with_warmup\n",")\n","import evaluate\n","\n","# Define hyperparameters\n","model_checkpoint = \"gpt2\"\n","dataset_name = \"wikitext\"\n","dataset_config_name = \"wikitext-103-raw-v1\"\n","batch_size = 4\n","num_train_epochs = 3\n","learning_rate = 5e-5\n","weight_decay = 0.01\n","max_seq_length = 512\n","pad_to_max_length = True\n","\n","# Check GPU availability and select device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load dataset\n","raw_datasets = load_dataset(dataset_name, dataset_config_name)\n","# Pre-process datasets\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","def tokenize(examples):\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)\n","\n","tokenized_datasets = raw_datasets.map(tokenize, batched=True)\n","# Split datasets into train and test\n","train_dataset = tokenized_datasets[\"train\"]\n","test_dataset = tokenized_datasets[\"test\"]\n","\n","# Data collator\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","\n","# Initialize Accelerator\n","accelerator = Accelerator()\n","\n","# Load model\n","model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)\n","\n","# Prepare optimizer and scheduler\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","num_training_steps = num_train_epochs * (len(train_dataset) // batch_size)\n","lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","# Prepare training arguments for Trainer\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n",")\n","\n","# Initialize Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    tokenizer=tokenizer,\n","    optimizers=(optimizer, lr_scheduler)\n",")\n","\n","# Train model\n","train_results = trainer.train()\n","trainer.save_model()\n","\n","# Evaluate model\n","metrics = trainer.evaluate()\n","perplexity = torch.exp(torch.tensor(metrics[\"eval_loss\"]))\n","metrics[\"perplexity\"] = perplexity.item()\n","\n","# Use evaluate library for ROUGE calculation\n","rouge_score = evaluate.load(\"rouge\")\n","\n","# Generate predictions for test set\n","test_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=batch_size)\n","model.eval()\n","predictions = []\n","references = []\n","for batch in test_dataloader:\n","    with torch.no_grad():\n","        generated_tokens = model.generate(batch[\"input_ids\"].to(device), max_length=max_seq_length)\n","        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n","        decoded_labels = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n","        predictions.extend(decoded_preds)\n","        references.extend(decoded_labels)\n","\n","# Calculate ROUGE score\n","rouge_results = rouge_score.compute(predictions=predictions, references=references)\n","\n","# Plot training and testing losses\n","losses = train_results.training_loss_history\n","eval_losses = metrics[\"eval_loss\"]\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(losses, label=\"Training Loss\")\n","plt.plot(eval_losses, label=\"Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Training and Validation Losses\")\n","plt.legend()\n","plt.show()\n","\n","# Print evaluation metrics\n","print(\"Evaluation metrics:\", metrics)\n","print(\"ROUGE metrics:\", rouge_results)\n","\n","# Cleanup to save memory\n","del model\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"id":"fQueWW_dUdSx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gc\n","import matplotlib.pyplot as plt\n","from datasets import load_dataset\n","from accelerate import Accelerator\n","from torch.utils.data import DataLoader\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    DataCollatorForLanguageModeling,\n","    Trainer,\n","    TrainingArguments,\n",")\n","import evaluate\n","from tqdm.auto import tqdm\n","\n","# Define hyperparameters\n","model_checkpoint = \"gpt2\"\n","dataset_name = \"wikitext\"\n","dataset_config_name = \"wikitext-103-raw-v1\"\n","batch_size = 4\n","num_train_epochs = 3\n","learning_rate = 5e-5\n","weight_decay = 0.01\n","max_seq_length = 512\n","pad_to_max_length = True\n","\n","# Load dataset\n","raw_datasets = load_dataset(dataset_name, dataset_config_name)\n","\n","# Pre-process datasets\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","def tokenize(examples):\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)\n","\n","tokenized_datasets = raw_datasets.map(tokenize, batched=True)\n","\n","# Split datasets into train and test\n","train_dataset = tokenized_datasets[\"train\"]\n","test_dataset = tokenized_datasets[\"test\"]\n","\n","# Data collator\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","\n","# Initialize Accelerator\n","accelerator = Accelerator()\n","\n","# Load model\n","model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n","\n","# Prepare training arguments for Trainer\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n",")\n","\n","# Initialize Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    tokenizer=tokenizer,\n","    accelerate=accelerator,\n",")\n","\n","# Train model\n","train_results = trainer.train()\n","trainer.save_model()\n","\n","# Evaluate model\n","metrics = trainer.evaluate()\n","perplexity = torch.exp(torch.tensor(metrics[\"eval_loss\"]))\n","metrics[\"perplexity\"] = perplexity.item()\n","\n","# Use evaluate library for ROUGE calculation\n","rouge_score = evaluate.load(\"rouge\")\n","\n","# Generate predictions for test set\n","test_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=batch_size)\n","model.eval()\n","predictions = []\n","references = []\n","for batch in tqdm(test_dataloader, desc=\"Generating predictions\"):\n","    with torch.no_grad():\n","        generated_tokens = accelerator.unwrap_model(model).generate(\n","            batch[\"input_ids\"].to(accelerator.device), max_length=max_seq_length\n","        )\n","        generated_tokens = accelerator.pad_across_processes(generated_tokens, dim=1, pad_index=tokenizer.pad_token_id)\n","        generated_tokens = accelerator.gather(generated_tokens)\n","\n","        preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n","        labels = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n","        predictions.extend(preds)\n","        references.extend(labels)\n","\n","# Calculate ROUGE score\n","rouge_results = rouge_score.compute(predictions=predictions, references=references)\n","\n","# Plot training and testing losses\n","plt.figure(figsize=(10, 5))\n","plt.plot(train_results.log_history, label=\"Training Loss\")\n","plt.plot(test_results.log_history, label=\"Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Training and Validation Losses\")\n","plt.legend()\n","plt.show()\n","\n","# Print evaluation metrics\n","print(\"Evaluation metrics:\", metrics)\n","print(\"ROUGE metrics:\", rouge_results)\n","\n","# Cleanup to save memory\n","del model\n","accelerator.free_memory()\n","gc.collect()"],"metadata":{"id":"0xX5x5KChcR5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import gc\n","import torch\n","from torch.utils.data import DataLoader\n","from datasets import load_dataset\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_linear_schedule_with_warmup\n","from accelerate import Accelerator\n","from peft import PeftModel, PeftConfig  # Assuming these are part of a custom library\n","import evaluate\n","\n","# Check GPU availability and select device\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    torch.cuda.empty_cache()  # Clear memory cache\n","    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Using CPU\")\n","\n","# Function to track memory usage (using `os` module)\n","def report_memory_usage():\n","    pid = os.getpid()\n","    py = psutil.Process(pid)\n","    memory_use = py.memory_info()[0] / 2. ** 30  # Memory usage in GB\n","    print(f'Memory used: {memory_use:.2f} GB')\n","\n","# Load dataset and tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n","dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n","\n","# Tokenization function\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n","\n","# Tokenize dataset\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)\n","train_dataset, eval_dataset = tokenized_datasets[\"train\"].train_test_split(test_size=0.1).values()\n","\n","# Instantiate custom model (assuming PeftModel is akin to GPT2LMHeadModel)\n","model_config = PeftConfig.from_pretrained(\"gpt2-large\")  # Assuming PeftConfig exists\n","model = PeftModel(model_config)\n","\n","# Move model to appropriate device\n","model.to(device)\n","\n","# Prepare for distributed training using Accelerate\n","accelerator = Accelerator()\n","model = accelerator.prepare(model)\n","\n","# DataLoader\n","train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=1)  # Small batch size for demonstration\n","eval_dataloader = DataLoader(eval_dataset, batch_size=1)\n","\n","# Optimizer and scheduler\n","optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=len(train_dataloader) * 3)\n","\n","# Training loop\n","model.train()\n","for epoch in range(3):\n","    for batch in train_dataloader:\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        accelerator.backward(loss)\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","        report_memory_usage()  # Report memory usage after each batch\n","\n","# Evaluation\n","model.eval()\n","evaluator = evaluate.load(\"rouge\")\n","for batch in eval_dataloader:\n","    with torch.no_grad():\n","        outputs = model.generate(**batch)\n","        # ... (evaluation logic)\n","    report_memory_usage()  # Report memory usage after each evaluation batch\n","\n","# Clean up to save memory\n","del model\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","# Note: Actual evaluation logic, saving/loading models, detailed memory management, and other aspects are omitted due to complexity"],"metadata":{"id":"HGuOxAubUjGA"},"execution_count":null,"outputs":[]}]}
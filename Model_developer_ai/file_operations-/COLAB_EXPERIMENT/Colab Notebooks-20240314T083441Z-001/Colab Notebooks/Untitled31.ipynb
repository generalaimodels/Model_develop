{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNDaSHrBxENiXhYE4NDRykS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -q -U datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ad2e_35N-ly0","executionInfo":{"status":"ok","timestamp":1705386912650,"user_tz":-330,"elapsed":21675,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"bb9b37a0-6e95-4222-f9e8-0f115cbca3c4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vMIkfPcl-kVU","outputId":"050a1e6f-0ccb-409c-9c1d-dcbfada3ffee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Available splits: ['train']\n","Dataset: DatasetDict({\n","    train: Dataset({\n","        features: ['act', 'prompt'],\n","        num_rows: 153\n","    })\n","})\n","First example of the 'train' split:\n","{'input_ids': [5377, 1930, 263, 314, 765, 345, 284, 719, 355, 257, 26777, 13, 314, 481, 2148, 262, 15844, 284, 257, 3496, 290, 345, 481, 2251, 2647, 329, 340, 13, 770, 714, 2291, 1262, 2972, 12834, 393, 4899, 11, 884, 355, 24983, 11341, 393, 6072, 489, 364, 11, 287, 1502, 284, 2251, 47077, 290, 4419, 17300, 326, 2222, 262, 15844, 284, 1204, 13, 2011, 717, 2581, 318, 366, 40, 423, 3194, 257, 21247, 3706, 564, 250, 31306, 282, 316, 37918, 37718, 320, 447, 251, 290, 761, 2647, 284, 467, 351, 340, 526, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"]}],"source":["import os\n","import torch\n","import multiprocessing\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from torch.utils.data import DataLoader\n","from typing import Dict\n","import matplotlib.pyplot as plt\n","\n","# Suppress warnings if necessary (you can remove this if you want to see all warnings).\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Constants\n","DATASET_NAME = 'fka/awesome-chatgpt-prompts'  # Replace with the actual dataset name\n","TOKENIZER_NAME = 'gpt2'  # Replace with the actual tokenizer name\n","MAX_LENGTH = 128\n","BATCH_SIZE = 16\n","NUM_WORKERS = multiprocessing.cpu_count()\n","\n","# Set device based on CUDA availability\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Load dataset\n","dataset = load_dataset(DATASET_NAME)\n","\n","# Print dataset information\n","print('Available splits:', list(dataset.keys()))\n","print('Dataset:', dataset)\n","\n","# Define tokenizer with padding token\n","tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n","tokenizer.pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n","\n","def preprocess_function(examples: Dict[str, list]) -> Dict[str, torch.Tensor]:\n","    concatenated_examples = [' '.join(str(examples[col][i]) for col in examples if isinstance(examples[col][0], str))\n","                             for i in range(len(examples[next(iter(examples))]))]\n","    tokenized_inputs = tokenizer(concatenated_examples, padding='max_length', truncation=True,\n","                                 max_length=MAX_LENGTH, return_tensors='pt')\n","    return {\n","        'input_ids': tokenized_inputs['input_ids'],\n","        'attention_mask': tokenized_inputs['attention_mask']\n","    }\n","\n","# Preprocessing and tokenization\n","for split in dataset.keys():\n","    dataset[split] = dataset[split].map(\n","        preprocess_function,\n","        batched=True,\n","        num_proc=min(NUM_WORKERS, 4),\n","        remove_columns=dataset[split].column_names\n","    )\n","\n","# Set format for PyTorch\n","dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n","\n","# Prepare DataLoader\n","dataloaders = {split: DataLoader(\n","    dataset[split],\n","    batch_size=BATCH_SIZE,\n","    shuffle=(split == 'train'),\n","    collate_fn=lambda batch: {k: torch.stack([b[k] for b in batch]).to(device) for k in batch[0]},\n","    num_workers=NUM_WORKERS\n",") for split in dataset.keys()}\n","\n","# Display the first example of all splits\n","for split in dataset.keys():\n","    print(f\"First example of the '{split}' split:\")\n","    first_example = next(iter(dataloaders[split]))\n","    print({key: val[0].cpu().tolist() for key, val in first_example.items()})\n","\n","# Initialize model\n","model = AutoModelForCausalLM.from_pretrained(TOKENIZER_NAME).to(device)\n","model.config.pad_token_id = tokenizer.pad_token_id\n","\n","# Define loss and optimizer\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n","\n","# Training loop\n","train_loss, valid_loss, test_loss = [], [], []\n","for epoch in range(10):\n","    for split in ['train']:\n","        model.train() if split == 'train' else model.eval()\n","        running_loss = 0.0\n","        for i, data in enumerate(dataloaders[split], 0):\n","            inputs, labels = data['input_ids'], data['attention_mask']\n","            optimizer.zero_grad()\n","            outputs = model(inputs, attention_mask=labels)\n","            loss = criterion(outputs.logits.view(-1, model.config.vocab_size), labels.view(-1))\n","            if split == 'train':\n","                loss.backward()\n","                optimizer.step()\n","            running_loss += loss.item()\n","        print(f'Epoch: {epoch+1}, Split: {split}, Loss: {running_loss/i}')\n","        (train_loss if split == 'train' else valid_loss).append(running_loss/i)\n","\n","# Evaluation on test data\n","model.eval()\n","with torch.no_grad():\n","    for i, data in enumerate(dataloaders['test'], 0):\n","        inputs, labels = data['input_ids'], data['attention_mask']\n","        outputs = model(inputs, attention_mask=labels)\n","        loss = criterion(outputs.logits.view(-1, model.config.vocab_size), labels.view(-1))\n","        test_loss.append(loss.item())\n","\n","# Plot training, validation, and test losses\n","plt.figure(figsize=(12, 6))\n","plt.plot(train_loss, label='Training loss')\n","plt.plot(valid_loss, label='Validation loss')\n","plt.plot(test_loss, label='Test loss')\n","plt.legend()\n","plt.show()\n","\n","# Save the trained model\n","model.save_pretrained('./model_save_directory')\n","tokenizer.save_pretrained('./model_save_directory')"]},{"cell_type":"code","source":["import os\n","import torch\n","import multiprocessing\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n","from torch.utils.data import DataLoader\n","from typing import Dict\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","\n","# Suppress unnecessary warnings (optional)\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Constants\n","DATASET_NAME = 'fka/awesome-chatgpt-prompts'  # Replace with the actual dataset name\n","TOKENIZER_NAME = 'gpt2'  # Replace with the actual tokenizer name\n","MAX_LENGTH = 128  # Maximum sequence length\n","BATCH_SIZE = 16  # Batch size\n","NUM_WORKERS = multiprocessing.cpu_count()  # Number of workers for DataLoader\n","\n","# Set device based on CUDA availability\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Load dataset and tokenizer\n","dataset = load_dataset(DATASET_NAME)\n","tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n","\n","# Ensure that tokenizer has a padding token\n","tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token\n","\n","# Define the collate function\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,  # Masked Language Modeling set to False since we use causal language modeling\n","    return_tensors=\"pt\"\n",")\n","\n","# Tokenization and dataset preparation function\n","def tokenize_function(examples):\n","    concatenated_examples = {\n","        'text': [' '.join(ex[feature] for feature in examples if isinstance(ex[feature], str))\n","                 for ex in zip(*examples.values())]\n","    }\n","    return tokenizer(\n","        concatenated_examples['text'],\n","        padding='max_length',\n","        truncation=True,\n","        max_length=MAX_LENGTH\n","    )\n","\n","# Apply tokenization to all splits\n","tokenized_datasets = dataset.map(\n","    tokenize_function,\n","    batched=True,\n","    remove_columns=dataset['train'].column_names\n",")\n","\n","# DataLoaders\n","dataloaders: Dict[str, DataLoader] = {\n","    split: DataLoader(\n","        tokenized_datasets[split],\n","        shuffle=(split == 'train'),\n","        collate_fn=data_collator,\n","        batch_size=BATCH_SIZE,\n","        num_workers=NUM_WORKERS\n","    )\n","    for split in dataset.keys()\n","}\n","\n","# Model definition\n","model = AutoModelForCausalLM.from_pretrained(TOKENIZER_NAME).to(device)\n","model.resize_token_embeddings(len(tokenizer))\n","\n","# Training setup\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = optim.AdamW(model.parameters(), lr=0.01)\n","\n","# Training loop\n","def train_model(dataloaders, model, optimizer, criterion, epochs=10):\n","    train_loss, valid_loss, test_loss = [], [], []\n","\n","    for epoch in range(epochs):\n","        for split, dataloader in dataloaders.items():\n","            if split == 'train':\n","                model.train()\n","            else:\n","                model.eval()\n","\n","            running_loss = 0.0\n","            for data in dataloader:\n","                inputs, labels = data['input_ids'].to(device), data['labels'].to(device)\n","\n","                optimizer.zero_grad()\n","                outputs = model(inputs, labels=labels)\n","                loss = criterion(outputs.logits.view(-1, model.config.vocab_size), labels.view(-1))\n","                running_loss += loss.item()\n","\n","                if split == 'train':\n","                    loss.backward()\n","                    optimizer.step()\n","\n","            average_loss = running_loss / len(dataloader)\n","            print(f'Epoch:{epoch+1}, Split:{split}, Loss:{average_loss}')\n","            if split == 'train':\n","                train_loss.append(average_loss)\n","            elif split == 'validation':\n","                valid_loss.append(average_loss)\n","            else:\n","                test_loss.append(average_loss)\n","\n","    return train_loss, valid_loss, test_loss\n","\n","# Run the training and validation\n","train_loss, valid_loss, test_loss = train_model(dataloaders, model, optimizer, criterion)\n","\n","# Plot the results\n","plt.figure(figsize=(12, 6))\n","plt.plot(train_loss, label='Training loss')\n","plt.plot(valid_loss, label='Validation loss')\n","plt.plot(test_loss, label='Test loss')\n","plt.legend()\n","plt.show()\n","\n","# Save the model\n","model.save_pretrained('path_to_save_model')\n","tokenizer.save_pretrained('path_to_save_tokenizer')"],"metadata":{"id":"_G9T3axT-zxL"},"execution_count":null,"outputs":[]}]}
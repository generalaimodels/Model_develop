{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-zgFf9J8JeZV"},"outputs":[],"source":["from typing import Set\n","\n","# 1. Create Sets\n","set_A = {1, 2, 3}\n","set_B = {3, 4, 5}\n","\n","# 2. Cardinality\n","cardinality_A = len(set_A)\n","\n","# 3. Union\n","union_set = set_A.union(set_B)\n","\n","# 4. Intersection\n","intersection_set = set_A.intersection(set_B)\n","\n","# 5. Complement (for demonstration, considering universal set)\n","universal_set = {1, 2, 3, 4, 5}\n","complement_A = universal_set - set_A\n","\n","# 6. Subset\n","is_subset = set_A.issubset(set_B)\n","\n","# 7. Proper Subset\n","is_proper_subset = set_A < set_B\n","\n","# 8. Disjoint\n","is_disjoint = set_A.isdisjoint(set_B)\n","\n","# 9. Power Set\n","def power_set(s: Set) -> Set[Set]:\n","    from itertools import chain, combinations\n","    return set(chain.from_iterable(combinations(s, r) for r in range(len(s)+1)))\n","\n","power_set_A = power_set(set_A)\n","\n","# 10. Cartesian Product\n","cartesian_product = {(a, b) for a in set_A for b in set_B}\n","\n","# 11. Symmetric Difference\n","symmetric_difference = set_A.symmetric_difference(set_B)\n","\n","# 12. Set Difference\n","set_difference = set_A - set_B\n","\n","# 13. Empty Set\n","empty_set = set()\n","\n","# 14. Universal Set (defined earlier)\n","\n","# 15. Commutative Property\n","commutative_union = set_A.union(set_B) == set_B.union(set_A)\n","commutative_intersection = set_A.intersection(set_B) == set_B.intersection(set_A)\n","\n","# 16. Associative Property\n","associative_union = (set_A.union(set_B)).union(set_C) == set_A.union(set_B.union(set_C))\n","associative_intersection = (set_A.intersection(set_B)).intersection(set_C) == set_A.intersection(set_B.intersection(set_C))\n","\n","# 17. Distributive Property\n","distributive_1 = set_A.intersection(set_B.union(set_C)) == (set_A.intersection(set_B)).union(set_A.intersection(set_C))\n","distributive_2 = set_A.union(set_B.intersection(set_C)) == (set_A.union(set_B)).intersection(set_A.union(set_C))\n","\n","# 18. Idempotent Property (for demonstration, considering union)\n","idempotent_union = set_A.union(set_A) == set_A\n","\n","# 19. Absorption Property\n","absorption_1 = set_A.intersection(set_A.union(set_B)) == set_A\n","absorption_2 = set_A.union(set_A.intersection(set_B)) == set_A\n","\n","# 20. De Morgan's Laws\n","demorgan_1 = set_A.intersection(set_B).union(set_C) == (set_A.union(set_C)).intersection(set_B.union(set_C))\n","demorgan_2 = set_A.union(set_B).intersection(set_C) == (set_A.intersection(set_C)).union(set_B.intersection(set_C))\n","\n","# 21. Partition\n","# Define partition function\n","\n","# 22. Binary Relation\n","# Define binary relation function\n","\n","# 23. Equivalence Relation\n","# Define equivalence relation function\n","\n","# 24. Equivalence Classes\n","# Define equivalence classes function\n","\n","# 25. Cantor's Theorem (for demonstration)\n","cantor_theorem = len(power_set_A) == 2 ** cardinality_A\n","\n","# Print results for demonstration\n","print(\"Set A:\", set_A)\n","print(\"Set B:\", set_B)\n","print(\"Cardinality of Set A:\", cardinality_A)\n","print(\"Union of Set A and Set B:\", union_set)\n","print(\"Intersection of Set A and Set B:\", intersection_set)\n","print(\"Complement of Set A:\", complement_A)\n","print(\"Is Set A a subset of Set B:\", is_subset)\n","print(\"Is Set A a proper subset of Set B:\", is_proper_subset)\n","print(\"Are Set A and Set B disjoint:\", is_disjoint)\n","print(\"Power set of Set A:\", power_set_A)\n","print(\"Cartesian product of Set A and Set B:\", cartesian_product)\n","print(\"Symmetric difference of Set A and Set B:\", symmetric_difference)\n","print(\"Set difference of Set A and Set B:\", set_difference)\n","print(\"Empty Set:\", empty_set)\n","print(\"Is Union commutative:\", commutative_union)\n","print(\"Is Intersection commutative:\", commutative_intersection)\n","print(\"Is Union associative:\", associative_union)\n","print(\"Is Intersection associative:\", associative_intersection)\n","print(\"Distributive Property 1:\", distributive_1)\n","print(\"Distributive Property 2:\", distributive_2)\n","print(\"Is Union idempotent:\", idempotent_union)\n","print(\"Absorption Property 1:\", absorption_1)\n","print(\"Absorption Property 2:\", absorption_2)\n","print(\"De Morgan's Laws 1:\", demorgan_1)\n","print(\"De Morgan's Laws 2:\", demorgan_2)\n","print(\"Cantor's Theorem:\", cantor_theorem)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CabD5ZsZJeXM"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xw5A5GFHJeUr"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Nwq3mrkJeSM"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"udA-cfPRJeP4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bs1xV73UJeNh"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S6J9zGv6JeLB"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"UqY_26ZAMviz"},"source":["Given that you're still encountering out-of-memory errors even after attempting to mitigate them, we need to consider other potential solutions or identify if there are any deeper issues with your setup. Here are some additional steps to troubleshoot the problem:\n","\n","1. **Reduce Model Size**: If your model is extremely large (which it seems to be, given the `Total no of parameters is : 2506172416`), consider using a smaller model. Large models have high memory requirements, which may not be feasible for your current hardware.\n","\n","2. **Use a Smaller Tokenizer**: If your tokenizer is producing a very large number of tokens due to a high `chars_per_token` setting, consider reducing this value to decrease the number of tokens and, consequently, the memory footprint.\n","\n","3. **Disable Features Not Supported by Tokenizer**: Your output indicates that \"FIM is not supported by tokenizer, disabling FIM\". Make sure that any features or processing that are not compatible with your model or tokenizer are disabled to avoid unnecessary memory usage.\n","\n","4. **Model Parallelism**: If your model is too large for a single GPU, you may need to use model parallelism, where different parts of the model are placed on different GPUs. This is more complex to set up and may require manual modification of your model.\n","\n","5. **Use a Machine with More Memory**: At this point, if you've tried everything and are still running out of memory, you may need to use a machine with more GPU memory or distribute the training across multiple GPUs.\n","\n","6. **Check Model Implementation for Memory Leaks**: Sometimes, custom implementations of models or layers can have memory leaks that cause excessive memory usage. Verify that your model implementation is efficient and does not inadvertently hold onto memory that should be freed.\n","\n","7. **Check Data Pipeline**: Analyze your data pipeline to ensure that it's not unintentionally loading too much data into memory at once. Consider using lazy loading or on-the-fly data processing to minimize memory footprint.\n","\n","Since you are using a large model with over 2.5 billion parameters, you're at the edge of what's feasible with a single GPU, even with one that has nearly 40 GB of memory. If none of the above steps help, you might need▌\n","\n","Given the persistent nature of the CUDA out of memory error despite attempting various strategies to mitigate it, we need to consider some additional, more drastic measures to resolve the issue. It's evident that the model you are trying to train is exceptionally large, with over 2.5 billion trainable parameters. Training such a model requires careful planning around memory management, especially on a single GPU setup.\n","\n","### Re-evaluate the Training Setup\n","\n","1. **Model Size**: If possible, consider using a smaller model. Training models with billions of parameters is challenging without access to an infrastructure that supports model parallelism across multiple GPUs or nodes.\n","\n","2. **Model Parallelism**: For models of this size, data parallelism (splitting data across multiple GPUs) might not be enough. Look into model parallelism libraries like DeepSpeed or FairScale which can split the model itself across multiple GPUs, thereby reducing the memory requirements on any single GPU.\n","\n","3. **Offloading**: DeepSpeed and similar frameworks offer features like offloading parts of the model and/or the optimizer states to the CPU, which can drastically reduce GPU memory usage. This does come at the cost of increased data transfer between CPU and GPU, potentially slowing down your training, but it can make training feasible when it otherwise wouldn't be.\n","\n","4. **Activation Checkpointing**: While you've already enabled gradient checkpointing, ensure you're implementing it as efficiently as possible. Libraries like DeepSpeed offer more advanced checkpointing strategies that significantly reduce memory usage at the cost of additional compute.\n","\n","5. **Use a High-Memory GPU**: If you're limited by the hardware, consider training on a GPU with more memory. For instance, NVIDIA's A100 GPUs come with up to 80 GB of memory, which could accommodate larger models or allow for larger batch sizes.\n","\n","6. **Distributed Training**: If you have access to multiple GPUs across several nodes, consider setting up distributed training. This can be complex to configure but allows you to leverage collective hardware resources more effectively.\n","\n","### Technical Considerations\n","\n","1. **Pre-Training**: For very large models, it might be more feasible to fine-tune from a pre-trained checkpoint rather than training from scratch. If you're not already doing so, consider using a pre-trained version▌"]},{"cell_type":"markdown","metadata":{"id":"nOveyLd7M6l2"},"source":["To fix the error in your code, you will need to address the CUDA out of memory issue. Here's what you can do to mitigate it:\n","\n","1. **Correct the Training Dataset**: Ensure that you are using the correct dataset for training. Replace `eval_datasets` with `train_datasets` in the `Trainer` setup:\n","\n","```python\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_datasets,  # Use the correct training dataset here\n","    eval_dataset=eval_datasets,\n","    data_collator=default_data_collator,\n",")\n","```\n","\n","2. **Reduce Memory Usage**: Since you are already using the smallest batch size, consider other methods to reduce memory usage.\n","\n","   - **Reduce Sequence Length**: If `args.max_seq_length` is too large, consider reducing it to a smaller number that still captures the necessary context for your task.\n","   - **Enable Mixed Precision Training**: If your GPU supports it, enable mixed precision training by setting `fp16=True` in your `TrainingArguments`.\n","   - **Disable Gradient Checkpointing**: Although gradient checkpointing can save memory, it may not be compatible with all models or might not be properly implemented. Try disabling it if you suspect issues there by setting `gradient_checkpointing=False`.\n","\n","3. **Clear CUDA Cache**: Before starting your training, you can clear the CUDA cache to free up any leftover memory from previous operations:\n","\n","```python\n","import torch\n","\n","torch.cuda.empty_cache()\n","```\n","\n","4. **Monitor GPU Usage**: Continuously monitor your GPU memory usage using `nvidia-smi` to ensure that no other processes are using the GPU while you are trying to train your model.\n","\n","After applying these changes, your modified code should look something like this:\n","\n","```python\n","import torch\n","from transformers import Trainer, TrainingArguments\n","from datasets import load_dataset\n","\n","# ... (your previous setup code) ...\n","\n","# Clear CUDA cache\n","torch.cuda.empty_cache()\n","\n","# Update the training arguments to enable mixed precision training\n","training_args = TrainingArguments(\n","    # ... (other arguments)\n","    fp16=True,  # Enable mixed precision training\n","    gradient_checkpointing=False,  # Disable gradient checkpointing if not compatible\n","    # ... (other arguments)\n",")\n","\n","# Initialize Trainer with the correct datasets\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_datasets,  # Corrected to use the training dataset\n","    eval_dataset=eval_datasets,\n","    data_collator=default_data_collator,\n",")\n","\n","# Start training\n","trainer.train()\n","```\n","\n","Try running the training process after making these changes. If you still encounter out-of-memory errors, you may need to further reduce the `max_seq_length` or consider using a machine with a GPU that has more memory."]},{"cell_type":"markdown","metadata":{"id":"tgLeXbTeM-Mq"},"source":["It looks like you are setting up a training and evaluation pipeline using a custom `ConstantLengthDataset` and the `transformers.Trainer` API. However, there's an issue with your `trainer` setup: you are using the same dataset for both training and evaluation (`eval_datasets`). You should use `train_datasets` for training:\n","\n","```python\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_datasets,  # This should be your training dataset\n","    eval_dataset=eval_datasets,\n","    data_collator=default_data_collator,\n",")\n","```\n","\n","Additionally, there are several other potential issues and improvements you can address to prevent the CUDA out of memory error:\n","\n","1. **Reduce Sequence Length**: If `args.max_seq_length` is too high, consider reducing it. Longer sequences require more memory.\n","\n","2. **Gradient Checkpointing**: You've enabled gradient checkpointing with `gradient_checkpointing=True`, which should help reduce memory usage by trading off computation. However, keep in mind that gradient checkpointing can be less effective if your model doesn't have many layers or if it's not implemented properly in the model you are using.\n","\n","3. **Remove Unnecessary Features**: You have `remove_unused_columns=False`. If your dataset contains columns that are not used during training, setting this to `True` might save memory.\n","\n","4. **Mixed Precision Training**: If your hardware supports it, consider enabling automatic mixed precision (AMP) by setting `fp16=True` instead of using `bf16=False`. AMP can significantly reduce memory usage.\n","\n","5. **Training Schedule**: You have `max_steps=1000`. If that's too high for your setup, consider reducing it. Also, `num_train_epochs=50` might be more than you actually need. Try reducing the number of epochs to see if the training can complete without running out of memory.\n","\n","6. **Per Device Batch Size**: You have `per_device_train_batch_size=1`, which is already the smallest batch size. If you're still running out of memory, it's likely not due to the batch size (unless your sequences are extremely long).\n","\n","7. **Warmup Steps**: The `warmup_steps=100` might not be an issue, but ensure this is an appropriate number for your training regime.\n","\n","8. **Learning Rate**: The initial `learning_rate=0.001` is a typical default, but make sure it's suitable for your model and training data.\n","\n","9. **Save Total Limit**: The `save_total_limit=5` should also be fine, but make sure you have enough disk space for the model checkpoints.\n","\n","10. **Push to Hub**: You have `push_to_hub=True`, which means after training, the model will be pushed to the Hugging Face Model Hub. Ensure this is intended, and you have the necessary authentication set up.\n","\n","After you've made these adjustments, try running `trainer.train()` again and monitor your GPU memory usage to ensure it doesn't run out of memory. If you still encounter memory issues, you might need to consider more aggressive changes or using a different setup with more GPU memory."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nmG9EqVDMuW9"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMh9WGGzIEVzljKEHQ8ZBlY","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}

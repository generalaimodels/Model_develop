{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"mount_file_id":"1Q4T7xAK9eN2jPdTBPq47htvoYbL2rFJF","authorship_tag":"ABX9TyOmTl9IBOyST9hoBLXIXlbv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["- **Author:** **Kandimalla Hemanth**\n","- **Date of modified:**  **1-13-2024**\n","- **E-mail:** **speechcodehemanth2@gmail.com**\n"],"metadata":{"id":"HgAg2hXwstc5"}},{"cell_type":"markdown","source":["# Cloud TPU v5p Overview\n","\n","Cloud TPU v5p stands as the fifth-generation offering from Google Cloud in the TPU (Tensor Processing Unit) series, succeeding the v4 TPU. Engineered specifically for large-scale training, it positions itself as a cutting-edge platform for the advancement of foundational Large Language Models (LLMs), diffusion models, and generative Artificial Intelligence (AI).\n","\n","## Performance Enhancements\n","\n","At a macroscopic level, Cloud TPU v5p boasts notable performance improvements over its predecessor, the v4. It achieves up to 2x the performance of v4, elevating its capabilities for intensive computational tasks associated with AI model training.\n","\n","## Enhanced Scalability\n","\n","One of the standout features is the increased scalability by packing 2x more TPUs into a Pod. In comparison to v4, which had a capacity of 3k TPUs in its largest slice, v5p scales up to 6k TPUs, resulting in an impressive 4x performance boost at the Pod level.\n","\n","## Increased Clock Frequency\n","\n","Cloud TPU v5p operates at a higher clock frequency, running at 1.75Ghz as opposed to the 1.05Ghz clock speed of its predecessor. This acceleration contributes to faster processing and improved efficiency in handling complex AI workloads.\n","\n","## SparseCore Integration\n","\n","The introduction of SparseCore is a noteworthy addition, specifically designed for large-scale embeddings. This enhancement addresses the growing demand for handling vast amounts of data efficiently in AI applications, ensuring optimal performance in scenarios where sparse data structures are prevalent.\n","\n","## Expanded High Bandwidth Memory (HBM) Capacity\n","\n","To further augment its capabilities, Cloud TPU v5p triples the High Bandwidth Memory (HBM) capacity compared to the v4. This expansion provides a more extensive and faster memory access, facilitating enhanced data handling and manipulation.\n","\n","-  Cloud TPU v5p represents a significant leap forward in the realm of AI hardware, with its emphasis on performance, scalability, and specialized features for handling the complexities of modern AI model development.\n"],"metadata":{"id":"0QRe0LVuj9zA"}},{"cell_type":"code","source":["!pip install -q -U transformers  datasets"],"metadata":{"id":"CuhAHZtQSzKi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","from transformers import AutoTokenizer\n","import multiprocessing\n","\n","# Load the dataset\n","dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")\n","\n","# Define the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('gpt2')\n","\n","if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","# Extract the split name\n","split_name = list(dataset.keys())[0]\n","\n","# Get the list of columns\n","columns = dataset[split_name].column_names\n","\n","if len(columns)<=1:\n"],"metadata":{"id":"goC9kKkATh8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(split_name,columns)"],"metadata":{"id":"fZvsPv6QTk7e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  split_name = list(examples.keys())[0]\n","    columns = examples[split_name].column_names\n","    batch_size = len(columns)\n","    text_column=columns[0]\n","    label_column=columns[1]"],"metadata":{"id":"yc5HnIVvT3g4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset('csv', data_files='/content/drive/MyDrive/training.csv')"],"metadata":{"id":"In-cCl0hvvn5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","from transformers import AutoTokenizer\n","import multiprocessing\n","import torch\n","\n","# Load the dataset\n","dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")\n","\n","# dataset = load_dataset('csv', data_files='/content/drive/MyDrive/training.csv')\n","\n","# Define the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('gpt2')\n","\n","if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","split_name=str(list(dataset.keys())[0])\n","print(split_name)\n","# Get the list of columns for the first split\n","columns = dataset[str(list(dataset.keys())[0])].column_names\n","print(columns)\n","\n","# Define the maximum sequence length\n","max_length = 512\n","\n","def preprocess_function(examples):\n","    tokenized_outputs = {}\n","    for column in columns:\n","        # Ensure that every input to the tokenizer is a string\n","        column_text = [str(item) for item in examples[column]]\n","        column_tokens = tokenizer(column_text, padding='max_length', truncation=True, max_length=max_length)\n","        tokenized_outputs[column] = column_tokens['input_ids']\n","    # Create a tensor for input_ids by concatenating the tokens from each column\n","    input_ids = []\n","    for i in range(len(tokenized_outputs[columns[0]])): # Number of examples\n","        row_tokens = []\n","        for column in columns:\n","            row_tokens.extend(tokenized_outputs[column][i])\n","        row_tokens = row_tokens[:max_length]  # Truncate to max_length if necessary\n","        row_tokens += [tokenizer.pad_token_id] * (max_length - len(row_tokens))  # Pad if necessary\n","        input_ids.append(row_tokens)\n","\n","    # Create a tensor for attention_mask based on input_ids\n","    attention_mask = [[int(token_id != tokenizer.pad_token_id) for token_id in input_row] for input_row in input_ids]\n","\n","    # Convert input_ids and attention_mask to tensors\n","    model_inputs = {'input_ids': torch.tensor(input_ids), 'attention_mask': torch.tensor(attention_mask)}\n","\n","    return model_inputs\n","\n","processed_datasets = dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    batch_size=16,  # Example of setting batch size to 16\n","    num_proc=multiprocessing.cpu_count(),\n","    remove_columns=columns,  # Remove all original columns except for 'input_ids' and 'attention_mask'\n","    load_from_cache_file=False,\n","    desc=\"Running tokenizer on dataset\",\n",")\n","# Now 'processed_datasets' should be ready to feed to a model for training"],"metadata":{"id":"e77TjuMpdImG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","from transformers import AutoTokenizer\n","import multiprocessing\n","import torch\n","\n","# Load the dataset\n","dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")\n","\n","# dataset = load_dataset('csv', data_files='/content/drive/MyDrive/training.csv')\n","\n","# Define the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('gpt2')\n","\n","if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","split_name=str(list(dataset.keys())[0])\n","print(split_name)\n","# Get the list of columns for the first split\n","columns = dataset[str(list(dataset.keys())[0])].column_names\n","print(columns)\n","\n","# Define the maximum sequence length\n","max_length = 512\n","\n","\n","\n","def preprocess_function(examples):\n","    # print(\"Preprocessing examples...\")\n","    tokenized_outputs = {}\n","    for column in columns:\n","        # Ensure that every input to the tokenizer is a string\n","        column_text = [str(item) for item in examples[column]]\n","        column_tokens = tokenizer(column_text, padding='max_length', truncation=True, max_length=max_length)\n","        tokenized_outputs[column] = column_tokens['input_ids']\n","\n","    # Debugging: print an example of tokenized outputs\n","    # print(\"Tokenized outputs example:\", tokenized_outputs[columns[0]][0])\n","\n","    input_ids = []\n","    for i in range(len(tokenized_outputs[columns[0]])):\n","        row_tokens = []\n","        for column in columns:\n","            row_tokens.extend(tokenized_outputs[column][i])\n","        row_tokens = row_tokens[:max_length]  # Truncate to max_length if necessary\n","        row_tokens += [tokenizer.pad_token_id] * (max_length - len(row_tokens))  # Pad if necessary\n","        input_ids.append(row_tokens)\n","\n","    attention_mask = [[int(token_id != tokenizer.pad_token_id) for token_id in input_row] for input_row in input_ids]\n","\n","    model_inputs = {'input_ids': torch.tensor(input_ids), 'attention_mask': torch.tensor(attention_mask)}\n","\n","    # Debugging: print an example of model inputs\n","    # print(\"Model inputs example:\", model_inputs['input_ids'][0])\n","\n","    return model_inputs\n","\n","\n","processed_datasets = dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    batch_size=16,  # Example of setting batch size to 16\n","    num_proc=multiprocessing.cpu_count(),\n","    remove_columns=columns,  # Remove all original columns except for 'input_ids' and 'attention_mask'\n","    load_from_cache_file=False,\n","    desc=\"Running tokenizer on dataset\",\n",")"],"metadata":{"id":"I-HSV_HFwaJr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(processed_datasets['train']['input_ids']))"],"metadata":{"id":"ieYTRZQ-hiT-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fkUYNhd-ehPj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","from transformers import AutoTokenizer\n","import multiprocessing\n","import torch\n","\n","# Load the dataset\n","dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")\n","\n","# Define the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('gpt2')\n","\n","if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","# Get the list of columns for the first split\n","columns = dataset[str(list(dataset.keys())[0])].column_names\n","\n","# Define the maximum sequence length\n","max_length = 512\n","\n","def preprocess_function(examples):\n","    example=example[str(dataset[list(dataset.keys())[0]])]\n","    # Tokenize each column separately and concatenate their token IDs.\n","    tokenized_outputs = {}\n","    for column in columns:\n","        column_tokens = tokenizer(examples[column], padding='max_length', truncation=True, max_length=max_length)\n","        tokenized_outputs[column] = column_tokens['input_ids']\n","\n","    # Create a tensor for input_ids by concatenating the tokens from each column\n","    input_ids = []\n","    for i in range(len(tokenized_outputs[columns[0]])): # Number of examples\n","        row_tokens = []\n","        for column in columns:\n","            row_tokens.extend(tokenized_outputs[column][i])\n","        row_tokens = row_tokens[:max_length]  # Truncate to max_length if necessary\n","        row_tokens += [tokenizer.pad_token_id] * (max_length - len(row_tokens))  # Pad if necessary\n","        input_ids.append(row_tokens)\n","\n","    # Create a tensor for attention_mask based on input_ids\n","    attention_mask = [[int(token_id != tokenizer.pad_token_id) for token_id in input_row] for input_row in input_ids]\n","\n","    # Convert input_ids and attention_mask to tensors\n","    model_inputs = {'input_ids': torch.tensor(input_ids), 'attention_mask': torch.tensor(attention_mask)}\n","\n","    return model_inputs\n","\n","# Map the preprocessing function across the entire dataset\n","processed_datasets = dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    num_proc=multiprocessing.cpu_count(),\n","    remove_columns=columns,\n","    load_from_cache_file=False,\n","    desc=\"Running tokenizer on dataset\",\n",")\n","\n","# Now 'processed_datasets' should be ready to feed to a model for training"],"metadata":{"id":"PtE5aqNkd-dF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","from transformers import AutoTokenizer\n","import multiprocessing\n","\n","# Load the dataset\n","dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")\n","\n","# Define the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('gpt2')\n","\n","if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","# Extract the split name\n","split_name = list(dataset.keys())[0]\n","\n","# Get the list of columns\n","columns = dataset[split_name].column_names\n","\n","if len(columns)<=1:\n","\n","\n","\n","max_length=512\n","import torch\n","def preprocess_function(examples):\n","    split_name = list(examples.keys())[0]\n","    columns = examples[str(split_name)].column_names\n","    batch_size = len(columns)\n","    text_column=columns[0]\n","    label_column=columns[1]\n","    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n","    targets = [str(x) for x in examples[label_column]]\n","    model_inputs = tokenizer(inputs)\n","    labels = tokenizer(targets, add_special_tokens=False)  # don't add bos token because we concatenate with inputs\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n","        # print(i, sample_input_ids, label_input_ids)\n","        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n","        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n","        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n","    # print(model_inputs)\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        label_input_ids = labels[\"input_ids\"][i]\n","        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n","            max_length - len(sample_input_ids)\n","        ) + sample_input_ids\n","        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n","            \"attention_mask\"\n","        ][i]\n","        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n","        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n","        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n","        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","\n","processed_datasets = dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    num_proc=multiprocessing.cpu_count(),\n","    remove_columns=dataset[\"train\"].column_names,\n","    load_from_cache_file=False,\n","    desc=\"Running tokenizer on dataset\",\n",")"],"metadata":{"id":"UoUoQMjAS-ql"},"execution_count":null,"outputs":[]}]}
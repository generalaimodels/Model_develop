{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyODj5r3Ic6nXxFxh+309aou"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"YCzOt68maM_N"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9igquIvUgBOB"},"outputs":[],"source":["from datasets import load_dataset, DatasetDict, Dataset\n","\n","from transformers import (\n","    AutoTokenizer,\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    DataCollatorWithPadding,\n","    TrainingArguments,\n","    Trainer)\n","\n","from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n","import evaluate\n","import torch\n","import numpy as np"]},{"cell_type":"code","source":["# how dataset was generated\n","\n","# load imdb data\n","imdb_dataset = load_dataset(\"imdb\")\n","\n","# define subsample size\n","N = 1000\n","# generate indexes for random subsample\n","rand_idx = np.random.randint(24999, size=N)\n","\n","# extract train and test data\n","x_train = imdb_dataset['train'][rand_idx]['text']\n","y_train = imdb_dataset['train'][rand_idx]['label']\n","\n","x_test = imdb_dataset['test'][rand_idx]['text']\n","y_test = imdb_dataset['test'][rand_idx]['label']\n","\n","# create new dataset\n","dataset = DatasetDict({'train':Dataset.from_dict({'label':y_train,'text':x_train}),\n","                             'validation':Dataset.from_dict({'label':y_test,'text':x_test})})"],"metadata":{"id":"z-Hrt-8FgjNK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load dataset\n","dataset = load_dataset('shawhin/imdb-truncated')\n","dataset"],"metadata":{"id":"bAvoS-vFgsH7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# display % of training data with label=1\n","np.array(dataset['train']['label']).sum()/len(dataset['train']['label'])"],"metadata":{"id":"2maPvpOrgxsI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_checkpoint = 'distilbert-base-uncased'\n","# model_checkpoint = 'roberta-base' # you can alternatively use roberta-base but this model is bigger thus training will take longer\n","\n","# define label maps\n","id2label = {0: \"Negative\", 1: \"Positive\"}\n","label2id = {\"Negative\":0, \"Positive\":1}\n","\n","# generate classification model from model_checkpoint\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)"],"metadata":{"id":"ah0QIgXjg2Q7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# display architecture\n","model"],"metadata":{"id":"6DBxMySjg7jY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n","\n","# add pad token if none exists\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","    model.resize_token_embeddings(len(tokenizer))"],"metadata":{"id":"Cgp2c4gTg__c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create tokenize function\n","def tokenize_function(examples):\n","    # extract text\n","    text = examples[\"text\"]\n","\n","    #tokenize and truncate text\n","    tokenizer.truncation_side = \"left\"\n","    tokenized_inputs = tokenizer(\n","        text,\n","        return_tensors=\"np\",\n","        truncation=True,\n","        max_length=512\n","    )\n","\n","    return tokenized_inputs"],"metadata":{"id":"4V9A_YFMhE19"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenize training and validation datasets\n","tokenized_dataset = dataset.map(tokenize_function, batched=True)\n","tokenized_dataset"],"metadata":{"id":"0FpsEA3DhIJA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create data collator\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"],"metadata":{"id":"J7s4RUsmhNWV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import accuracy evaluation metric\n","accuracy = evaluate.load(\"accuracy\")"],"metadata":{"id":"_3eGqs7OhQux"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define an evaluation function to pass into trainer later\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=1)\n","\n","    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}"],"metadata":{"id":"1GSR-49fhUHU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define list of examples\n","text_list = [\"It was good.\", \"Not a fan, don't recommed.\", \"Better than the first one.\", \"This is not worth watching even once.\", \"This one is a pass.\"]\n","\n","print(\"Untrained model predictions:\")\n","print(\"----------------------------\")\n","for text in text_list:\n","    # tokenize text\n","    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n","    # compute logits\n","    logits = model(inputs).logits\n","    # convert logits to label\n","    predictions = torch.argmax(logits)\n","\n","    print(text + \" - \" + id2label[predictions.tolist()])"],"metadata":{"id":"2RrSeJt5hXjQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["peft_config = LoraConfig(task_type=\"SEQ_CLS\",\n","                        r=4,\n","                        lora_alpha=32,\n","                        lora_dropout=0.01,\n","                        target_modules = ['q_lin'])"],"metadata":{"id":"ObqOQ0aphcB2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["peft_config"],"metadata":{"id":"qgsV061phfqS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = get_peft_model(model, peft_config)\n","model.print_trainable_parameters()"],"metadata":{"id":"DFuZXvythjiG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hyperparameters\n","lr = 1e-3\n","batch_size = 4\n","num_epochs = 10"],"metadata":{"id":"Y8woTfeVhmY-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define training arguments\n","training_args = TrainingArguments(\n","    output_dir= model_checkpoint + \"-lora-text-classification\",\n","    learning_rate=lr,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=num_epochs,\n","    weight_decay=0.01,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n",")"],"metadata":{"id":"xtQBx9Dfhq1T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creater trainer object\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\n","    compute_metrics=compute_metrics,\n",")\n","\n","# train model\n","trainer.train()"],"metadata":{"id":"rWH2mx78ht69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.to('mps') # moving to mps for Mac (can alternatively do 'cpu')\n","\n","print(\"Trained model predictions:\")\n","print(\"--------------------------\")\n","for text in text_list:\n","    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"mps\") # moving to mps for Mac (can alternatively do 'cpu')\n","\n","    logits = model(inputs).logits\n","    predictions = torch.max(logits,1).indices\n","\n","    print(text + \" - \" + id2label[predictions.tolist()[0]])"],"metadata":{"id":"jQtIDbnth0s1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # option 1: notebook login\n","# from huggingface_hub import notebook_login\n","# notebook_login() # ensure token gives write access\n","\n","# option 2: key login\n","from huggingface_hub import login\n","write_key = \"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\" # paste token here\n","login(write_key)"],"metadata":{"id":"lPE9SHBFh6kh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iJ9kbKlVigiY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hf_name = 'hemanthkandimalla' # your hf username or org name\n","model_id = hf_name + \"/\" + model_checkpoint + \"-lora-text-classification\" # you can name the model whatever you want"],"metadata":{"id":"pFbcFFvEiAeV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","model.push_to_hub(model_id) # save model\n","trainer.push_to_hub(model_id) # save trainer"],"metadata":{"id":"Mh7Mry-9iJiE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# how to load peft model from hub for inference\n","config = PeftConfig.from_pretrained(model_id)\n","inference_model = AutoModelForSequenceClassification.from_pretrained(\n","    config.base_model_name_or_path, num_labels=2, id2label=id2label, label2id=label2id\n",")\n","tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n","model = PeftModel.from_pretrained(inference_model, model_id)"],"metadata":{"id":"wgeIIP8WiRsd"},"execution_count":null,"outputs":[]}]}
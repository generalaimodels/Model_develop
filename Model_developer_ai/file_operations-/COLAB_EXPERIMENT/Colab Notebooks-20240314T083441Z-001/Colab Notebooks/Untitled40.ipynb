{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPOXDtX4SQfBDMuVzBltCxP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Explore the Advanced Ollama-Companion: Streamlit Enhanced\n","\n","## Welcome to Ollama-Companion's Latest Iteration\n","\n","As the developer spearheading the Ollama-Companion project, I am delighted to present the enhanced version of our platform. This iteration integrates Streamlit, offering users a more interactive and intuitive experience. Below are the key innovations that redefine how users interact with and manage language models.\n","\n","## Key Innovations in Ollama-Companion:\n","\n","1. **Quantization of Huggingface Models via UI**\n","   - Effortlessly quantize Huggingface models through a user-friendly interface.\n","   - Convert models into different formats, catering to diverse computational needs.\n","\n","2. **Dynamic Module Integration**\n","   - Seamlessly integrate various modules defined in shared.py.\n","   - Adopt a modular and scalable approach to application development.\n","\n","3. **Streamlit-Powered User Interface**\n","   - Revamped UI using Streamlit for enhanced intuitiveness and responsiveness.\n","   - Easier navigation and interaction with various features.\n","\n","4. **Enhanced Model Interaction Features**\n","   - *Modelfile Manager*: Delve into the details of each model beyond selection.\n","   - *Interactive Modelfile Creator*: Customize model files in real-time for enhanced control.\n","   - *Chat Interface with LLAVA Image Analysis*: Utilize LLAVA for dynamic image recognition and analysis during conversations with language models.\n","\n","5. **Advanced Configuration Tools**\n","   - *Ollama API Configurator*: Manage Ollama API endpoints directly from the UI.\n","   - *LiteLLM Proxy and Public Endpoint*: Set up proxies and public endpoints effortlessly for secure model sharing.\n","\n","6. **Efficient Model Management Systems**\n","   - *Fast Model Downloading*: Download models from Huggingface with improved speed.\n","   - *Quantization Options*: Choose between high or medium precision GGUF formats for model transformation.\n","   - *Secure Model Uploads to Huggingface*: Confidently upload models to Huggingface with enhanced security measures.\n","\n","7. **Security Enhancements**\n","   - *Token Encryption*: Add an extra layer of encryption to protect your Huggingface token, ensuring increased data security."],"metadata":{"id":"s7AmAf-VoipY"}},{"cell_type":"code","source":["# Clone the repository\n","!git clone --branch Colab-installer https://github.com/Luxadevi/Ollama-Companion.git 2> /dev/null 1>&2\n","print(\"Cloning Ollama-Companion from git...\")\n","\n","# Install virtualenv\n","!sudo apt install virtualenv 2> /dev/null 1>&2\n","print(\"Installing some dependencies, please hold on...\")\n","\n","# Convert Windows line endings to Unix line endings in the install.sh script\n","!sed -i 's/\\r//' /content/Ollama-Companion/install.sh 2> /dev/null 1>&2\n","print(\"Converting line endings in install script...\")\n","\n","# Make the script executable and run it\n","!chmod +x /content/Ollama-Companion/install.sh 2> /dev/null 1>&2\n","print(\"Running the installation script and compiling Llama.cpp...\")\n","!/content/Ollama-Companion/install.sh 2> /dev/null 1>&2\n","\n","# Purge pip cache and clean up temporary files\n","!pip cache purge 2> /dev/null 1>&2\n","!find /tmp -type f -atime +1 -delete 2> /dev/null 1>&2\n","!sudo apt-get clean 2> /dev/null 1>&2\n","!sudo apt-get autoremove 2> /dev/null 1>&2\n","print(\"Cleaning up and finalizing setup...\")\n","print(\"Started all apps run the cell below if you want to restart\")\n","# Run the application\n","!python3 /content/Ollama-Companion/run_app.py\n","print(\"Started all apps run the cell below if you want to restart\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ltta5H0TmNFT","executionInfo":{"status":"ok","timestamp":1706506227357,"user_tz":-330,"elapsed":610,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"4afd6cbb-bf10-4a81-b7dd-7822b02e7edc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Exception ignored in: <module 'threading' from '/usr/lib/python3.10/threading.py'>\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/threading.py\", line 1567, in _shutdown\n","  File \"/content/Ollama-Companion/run_app.py\", line 39, in <module>\n","    main()\n","  File \"/content/Ollama-Companion/run_app.py\", line 35, in main\n","    streamlit_thread.join()\n","  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n","    lock.acquire()\n","KeyboardInterrupt: \n","    self._wait_for_tstate_lock()\n","  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n","    if lock.acquire(block, timeout):\n","KeyboardInterrupt\n","\u001b[34m  Stopping...\u001b[0m\n","Exception ignored in: <module 'threading' from '/usr/lib/python3.10/threading.py'>\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/threading.py\", line 1567, in _shutdown\n","    lock.acquire()\n","KeyboardInterrupt: \n","^C\n","Started all apps run the cell below if you want to restart\n"]}]},{"cell_type":"code","source":["!python3 /content/Ollama-Companion/run_app.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LH9DtExZmiLf","outputId":"c870bbbb-5788-4df5-e348-747fb3862445"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Starting Cloudflare Tunnel...\n","Starting Streamlit App...\n","Starting Ollama...\n","Tunnel URL: https://fist-spice-busy-jim.trycloudflare.com\n","\n","Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n","\u001b[0m\n","\u001b[0m\n","\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n","\u001b[0m\n","\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n","\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.87.172.98:8501\u001b[0m\n","\u001b[0m\n","Removed models from config file: ollama/dummyentry\n","Config file updated successfully.\n","\n","\n","01/29 05:37:44 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n","01/29 05:37:44 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n","\n","01/29 05:37:44 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n","\n","01/29 05:37:44 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n","\n","\n","01/29 05:37:44 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n","01/29 05:37:44 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n","\n","\n","01/29 05:37:44 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n","\n","01/29 05:37:44 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n","01/29 05:37:44 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Download complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/.gitattributes\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","bfa3dc|\u001b[1;32mOK\u001b[0m  |    92KiB/s|/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/.gitattributes\n","\n","Status Legend:\n","(OK):download completed.\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Download complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/README.md\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","65a2c1|\u001b[1;32mOK\u001b[0m  |   426KiB/s|/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/README.md\n","\n","Status Legend:\n","(OK):download completed.\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Download complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/config.json\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","c728a6|\u001b[1;32mOK\u001b[0m  |    41KiB/s|/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/config.json\n","\n","Status Legend:\n","(OK):download completed.\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs-us-1.huggingface.co/repos/d1/96/d19680dc3a89c3faf4662824f2065a51142006fc67cd821c163223bfc58fb52d/78074bbe2b8e78e15f3edb237b7d1b5fd744bbb40d60ac7298de8ed0afe1846e?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27consolidated.00.pt%3B+filename%3D%22consolidated.00.pt%22%3B&Expires=1706765865&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjc2NTg2NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2QxLzk2L2QxOTY4MGRjM2E4OWMzZmFmNDY2MjgyNGYyMDY1YTUxMTQyMDA2ZmM2N2NkODIxYzE2MzIyM2JmYzU4ZmI1MmQvNzgwNzRiYmUyYjhlNzhlMTVmM2VkYjIzN2I3ZDFiNWZkNzQ0YmJiNDBkNjBhYzcyOThkZThlZDBhZmUxODQ2ZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=ihTV880cLWYoJbvPwvhyC86KmSVKjHGFi4X%7E9PYD1AnHXnIilkHFSp-d%7ExJqMR4QixcSA3uMqUQaeOfvXbiBSRQ6KFhcyvFC7mSV-OaCH3bHn6eXpZEdW3zKw6vf8QlNntsJOI-JQ2pykQQSou6-pL7G7wO-0XqSeLWME3aPPD8mI9zQsTKjVdQ0M7huNEQatZO7-nFE1A7thvEugiCI20mKFW9zSDh1XRn8Dzn97KpnFrjm4ArTeUs1sajy3ROXabLwELXKNjyCV-pOR4NQVKbG5wePgnbjeLEBAkRL7MEds2e3yAauyHvtOfN6niuUJv8ImDPq6tIvb3KFSZWyMg__&Key-Pair-Id=KCD77M1F0VK2B\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs-us-1.huggingface.co/repos/d1/96/d19680dc3a89c3faf4662824f2065a51142006fc67cd821c163223bfc58fb52d/0ae2c9b84dc1d1585db8672a53c64e9f4644b5b0401326da120d686378e5ff70?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27consolidated.02.pt%3B+filename%3D%22consolidated.02.pt%22%3B&Expires=1706765865&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjc2NTg2NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2QxLzk2L2QxOTY4MGRjM2E4OWMzZmFmNDY2MjgyNGYyMDY1YTUxMTQyMDA2ZmM2N2NkODIxYzE2MzIyM2JmYzU4ZmI1MmQvMGFlMmM5Yjg0ZGMxZDE1ODVkYjg2NzJhNTNjNjRlOWY0NjQ0YjViMDQwMTMyNmRhMTIwZDY4NjM3OGU1ZmY3MD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=A71KQ0JFb9LvkvWI5t-F8suKrdQj6kT6cxQCaRAkOSvJZg1XS57oKPLfAjGqL-xftckolCzpDpOdAAn6HEdjcPZqUqXIZITbPQZGASP6KuDndKmQ4KudpfANOZEUiQeFU-T0ywJ%7E7RUyX4wZEpU5SPhMSAi96p5-xuLbuz9WgZ7Hrb0970aqvmszlUAVGLVkijVZQVUcbHGl7MC7Y1N0KM9GyLqs5w6zbPuA0FrLHl1OpVuknzUDVvxGnpMuSx1DCLNo9FHJYgXYw6USwFPhonY5hBOhfeuAvFF7VMvvv3diPovMfGcgl7OzTV77TmhcGnao06vwTzVX83IioIy01g__&Key-Pair-Id=KCD77M1F0VK2B\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs-us-1.huggingface.co/repos/d1/96/d19680dc3a89c3faf4662824f2065a51142006fc67cd821c163223bfc58fb52d/f5a3d70225a42ade1d5b1d277fada2cdcde6a403609829ab2acd7bff5d12bb83?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27consolidated.01.pt%3B+filename%3D%22consolidated.01.pt%22%3B&Expires=1706765865&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjc2NTg2NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2QxLzk2L2QxOTY4MGRjM2E4OWMzZmFmNDY2MjgyNGYyMDY1YTUxMTQyMDA2ZmM2N2NkODIxYzE2MzIyM2JmYzU4ZmI1MmQvZjVhM2Q3MDIyNWE0MmFkZTFkNWIxZDI3N2ZhZGEyY2RjZGU2YTQwMzYwOTgyOWFiMmFjZDdiZmY1ZDEyYmI4Mz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=cwzP-LGn2mM8wwgVxYt-dlmpTrWLl6I9UVy0VWmlUZ0z2BZPAKSV-B22pbu52Qovp3Ax0hPlsrO2cbP6zrZYxQSHYt3rNEJUm5RFreB6Z5mIYL2P74BIVdHe5gWCvsFWMkGjU9k1cY3EyiiaVdnIESdXi%7EmX7wEbQqWtsCTGMSk6kQYI88pPi-qMfBeLZnlri3GITHU4JYBkUm4lQqLvfjMYLZR8MRG84y1QnSckzd57OpUjhT35BMF1ck8cKgjjBWXHYkH%7EfCHgaByPlbE8LWV-510deDypGEET1Y5r9ADvyGbZLOnxUoAjn1Cpp0y2XchBnBcRlyQzC6R9zrm7fA__&Key-Pair-Id=KCD77M1F0VK2B\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs-us-1.huggingface.co/repos/d1/96/d19680dc3a89c3faf4662824f2065a51142006fc67cd821c163223bfc58fb52d/e10c447d68257b98f2bb70a987899df44baf87c39a20be30002c83aa3a4fbe15?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27consolidated.04.pt%3B+filename%3D%22consolidated.04.pt%22%3B&Expires=1706765865&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjc2NTg2NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2QxLzk2L2QxOTY4MGRjM2E4OWMzZmFmNDY2MjgyNGYyMDY1YTUxMTQyMDA2ZmM2N2NkODIxYzE2MzIyM2JmYzU4ZmI1MmQvZTEwYzQ0N2Q2ODI1N2I5OGYyYmI3MGE5ODc4OTlkZjQ0YmFmODdjMzlhMjBiZTMwMDAyYzgzYWEzYTRmYmUxNT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=T%7E80docSBIGVOjIfkVBeoExX7hunH94TOaE80v89G-OsAfRynjkWjz5%7EWsbHQsCR286qit85yFORatKWFaKLINOsp9roQxczTl2Cu5mPeA27sH%7Ep-53u4l%7EW9wzSFc9BZs2GBn5fgtuwoGD8NkpsLwgWM4n002HznTM7Tgj3EbJmqjBrHqjpvszrIp6XEWOur-dTTAi1oj52uwkjRrV524tGGQcEzipzISrI7G0GBM893jNDyGocyvExpKH8Vkn6IshJNi4AOFxH%7ELZfqSZ-D5MpTUgzN5SI0MYx-AvYFGHvnQFKuYv71OKFBG%7E47jqxvn8QZ8JmEq2uQ2jZojrqiA__&Key-Pair-Id=KCD77M1F0VK2B\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs-us-1.huggingface.co/repos/d1/96/d19680dc3a89c3faf4662824f2065a51142006fc67cd821c163223bfc58fb52d/6f3c6160191f4df0a3b4d4a86696c5e93dc8af7e135146fa6f28d69544fa7f31?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27consolidated.05.pt%3B+filename%3D%22consolidated.05.pt%22%3B&Expires=1706765865&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjc2NTg2NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2QxLzk2L2QxOTY4MGRjM2E4OWMzZmFmNDY2MjgyNGYyMDY1YTUxMTQyMDA2ZmM2N2NkODIxYzE2MzIyM2JmYzU4ZmI1MmQvNmYzYzYxNjAxOTFmNGRmMGEzYjRkNGE4NjY5NmM1ZTkzZGM4YWY3ZTEzNTE0NmZhNmYyOGQ2OTU0NGZhN2YzMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=bWzStE6fXsZ7KAcc49oaqZPAX-DIaIveWGEU0tsD8oB03fLHmNG5yqhRwAHVE4DXzZJdm7dCzhcfVuL3WFNzNVJW0U6RcMr%7E3-263UQNow-w9tXUMTMZ1bTZ1Fs7xZvei2d3ZkF%7ECdfsPY-3g6qVentA3NG7gSfLF7mcuEazZCaeZkmsHPsodPXUhCeqVjzidehrnsINM93kneGyJrurdsrSKTEBxx-BMQQH1wjKNKyYR7ML9PdORRfZ1CgbD-xEZjNP2Y2pVTiC-moKAZxBn8LnutW-K63cP3cZVevSx4ACDq5jru%7Eg1FRLMsPWA7TvKpSMW1yp83ajIlRhkjpN4w__&Key-Pair-Id=KCD77M1F0VK2B\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs-us-1.huggingface.co/repos/d1/96/d19680dc3a89c3faf4662824f2065a51142006fc67cd821c163223bfc58fb52d/878f18604323d63509ca982018e396ad4294da90477b0604f5b27f6dcbc1eae7?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27consolidated.03.pt%3B+filename%3D%22consolidated.03.pt%22%3B&Expires=1706765865&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjc2NTg2NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2QxLzk2L2QxOTY4MGRjM2E4OWMzZmFmNDY2MjgyNGYyMDY1YTUxMTQyMDA2ZmM2N2NkODIxYzE2MzIyM2JmYzU4ZmI1MmQvODc4ZjE4NjA0MzIzZDYzNTA5Y2E5ODIwMThlMzk2YWQ0Mjk0ZGE5MDQ3N2IwNjA0ZjViMjdmNmRjYmMxZWFlNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=gVqM5e8wBvnPpB5mbhAbOMArsP4xWHMeKXDndY-JYYE%7E6P1Fhx3h-vnmU5M-Xdx-QNgKRn3oRnBMUxgaflkgSkLkOrtkZ8YrGQQQc7aAsSmOHqhClgihueCF4AgPwmCq%7E2wU2w7C9KWkSSH--g5eMjv4USHIH7XYxeOfWM4A4nOkW-YL-Ona%7Er%7EHgPyJLKgYqu0rPT%7EA3iqu9P5sypr3Ejd7QShH5HYKIIXzrBV5fsgOw9wgBSm7S0NbPnUxgf4kqHzU10B6bSDAgi-D5ks1dRmuDAyB5HuqQekHOMtJVJ7e%7EUumDeJUowx7b4UNv3CyqLQPQxSd7PVMN7dK--6fiQ__&Key-Pair-Id=KCD77M1F0VK2B\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs-us-1.huggingface.co/repos/d1/96/d19680dc3a89c3faf4662824f2065a51142006fc67cd821c163223bfc58fb52d/29101bd71fb7bc2a0ed166a7cf844c3a5a02600b65ea8e8a8fd691c9ed14a311?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27consolidated.06.pt%3B+filename%3D%22consolidated.06.pt%22%3B&Expires=1706765865&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjc2NTg2NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2QxLzk2L2QxOTY4MGRjM2E4OWMzZmFmNDY2MjgyNGYyMDY1YTUxMTQyMDA2ZmM2N2NkODIxYzE2MzIyM2JmYzU4ZmI1MmQvMjkxMDFiZDcxZmI3YmMyYTBlZDE2NmE3Y2Y4NDRjM2E1YTAyNjAwYjY1ZWE4ZThhOGZkNjkxYzllZDE0YTMxMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=mNjHwu4KFIhJkRBDBChOLfbhyXF9HEQgW9ydlhbDbp1%7EPKoOGwTGxvLrdPgOxX7xQ7Vlq3Qkub61ywke70K6iK71G34%7E-%7EdFoVu6jo7J7zIsmcD%7EAbIfCvfgNbuIN4qVSf%7Evk17lZv4DsOiSi0-6KnVFM-YVa6q4qTC2UZR2PN-B8PZnINNwtG5Ae8x1kNFdJtvSLcm3l09YaYt254O8q3OYD5VIlaJhU6Qap14Le2ILtr66FbKVDA1zHKBZG8T-1BGoKFxyhVkWZesFc8OCU%7ECSneGY6P%7Ef%7EObfi3IpuIrY0B3SOl6iNFJTHR3OSHk88abtRLaEWvMKeXDrnXxhSw__&Key-Pair-Id=KCD77M1F0VK2B\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs-us-1.huggingface.co/repos/d1/96/d19680dc3a89c3faf4662824f2065a51142006fc67cd821c163223bfc58fb52d/21fce3d6d32a761871f6eb0b4ce98ca727ab501c6819bc8e74c4ad00e58c9aa3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27consolidated.07.pt%3B+filename%3D%22consolidated.07.pt%22%3B&Expires=1706765865&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjc2NTg2NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2QxLzk2L2QxOTY4MGRjM2E4OWMzZmFmNDY2MjgyNGYyMDY1YTUxMTQyMDA2ZmM2N2NkODIxYzE2MzIyM2JmYzU4ZmI1MmQvMjFmY2UzZDZkMzJhNzYxODcxZjZlYjBiNGNlOThjYTcyN2FiNTAxYzY4MTliYzhlNzRjNGFkMDBlNThjOWFhMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=EOcGZLscTsNxG23b0sl%7ES2M6eZIkTIYTQIMcWGH2k-T0BN1I9zTc0EAerbwmS8JyCrmoyQT9ZDrvwhplPVd-XvL-tWb41ki6by-KXBPJx30YuGVYEyYoQmgZPqPCgaO0Y95rorFh8NO0WUQHSXu-wxygh%7EFL42fawaVGXxwB66W9EWHUVpbOZ2K7iPgKJBMmgOW1-%7EMxlKTYgurKwfKSl2XPNeQ4tiVR3MDJDfkpnc-To9ja9nOJ-Pero93S73fmJnpsaDtFO0RsnqvSoyEWvvH6PnzFen1EHiEV9YpKZ88S5AuOatmKAqkaBDCPad7UmVwstjen0uTElKRgOn9CRA__&Key-Pair-Id=KCD77M1F0VK2B\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs-us-1.huggingface.co/repos/d1/96/d19680dc3a89c3faf4662824f2065a51142006fc67cd821c163223bfc58fb52d/b43400ce8695edf089a8c276245f670ef0f39e798688e7cf9256636c8f5da053?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model-00001-of-00019.safetensors%3B+filename%3D%22model-00001-of-00019.safetensors%22%3B&Expires=1706765756&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjc2NTc1Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2QxLzk2L2QxOTY4MGRjM2E4OWMzZmFmNDY2MjgyNGYyMDY1YTUxMTQyMDA2ZmM2N2NkODIxYzE2MzIyM2JmYzU4ZmI1MmQvYjQzNDAwY2U4Njk1ZWRmMDg5YThjMjc2MjQ1ZjY3MGVmMGYzOWU3OTg2ODhlN2NmOTI1NjYzNmM4ZjVkYTA1Mz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=j2838ViI82xU1OhhgEh4nnOkynJjcr-28HOZjxCLDrMpxIcfcLBzOuk1ZPBvwys23Zo3FvU4xFY-C7g7NoOrgUmq3IrUsbs2NA5Y7qD%7EWKY0nTEtMbSNK8Vy8utvAPwuHMKf4GPD9k8%7ETn5cmmNDZllb%7ENH4XinrqFtqGV9SPrD9sMuLCP%7ER1DzqByVKU3P2O8VNJFpwadViHFicjY0W5O7nrlWsL4ESBEwgg6DFioYRdAF03891dVXKLSY3rA2LyulxqFtW21NMvIpF2xrtABFQaXPDsCgORVRhzDUdgVCoYX%7EIBu9cE0T1Lnv5DZ2zUiGVxLUqKIZB%7ENfgzOAD4g__&Key-Pair-Id=KCD77M1F0VK2B\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Download complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/generation_config.json\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","571e7b|\u001b[1;32mOK\u001b[0m  |   9.4KiB/s|/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/generation_config.json\n","\n","Status Legend:\n","(OK):download completed.\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n","\n","01/29 05:37:45 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Exception caught while allocating file space.\n","Exception: [AbstractDiskWriter.cc:557] errNum=28 errorCode=9 fallocate failed. cause: No space left on device\n","\n","01/29 05:37:45 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Download not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.00.pt\n","\n","01/29 05:37:45 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Exception caught while allocating file space.\n","Exception: [AbstractDiskWriter.cc:557] errNum=28 errorCode=9 fallocate failed. cause: No space left on device\n","\n","01/29 05:37:45 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Download not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.06.pt\n","\n","\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Allocating disk space. Use --file-allocation=none to disable it. See --file-allocation option in man page for more details.\n","\n","01/29 05:37:45 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Exception caught while allocating file space.\n","Exception: [AbstractDiskWriter.cc:557] errNum=28 errorCode=9 fallocate failed. cause: No space left on device\n","\n","01/29 05:37:45 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Download not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.01.pt\n","01/29 05:37:45 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Exception caught while allocating file space.\n","Exception: [AbstractDiskWriter.cc:557] errNum=28 errorCode=9 fallocate failed. cause: No space left on device\n","\n","01/29 05:37:45 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Download not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.04.pt\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Download GID#4930e8307bbf4077 not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.00.pt\n","\n","01/29 05:37:45 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Exception caught while allocating file space.\n","Exception: [AbstractDiskWriter.cc:453] errNum=28 errorCode=9 Failed to write into the file /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.02.pt, cause: No space left on device\n","\n","01/29 05:37:45 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Download not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.02.pt\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Download GID#5358fb4abd049ede not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.06.pt\n","01/29 05:37:45 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Exception caught while allocating file space.\n","Exception: [AbstractDiskWriter.cc:557] errNum=28 errorCode=9 fallocate failed. cause: No space left on device\n","\n","01/29 05:37:45 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Download not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.03.pt\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Download GID#831f8b15fffad0c3 not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.02.pt\n","\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Download GID#717eb907eb325800 not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.01.pt\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Download GID#d80d1a1617ca42a2 not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.04.pt\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Download GID#f168a645cc7611fc not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.03.pt\n","\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","Download Results:\n","4930e8|\u001b[1;31mERR\u001b[0m |       0B/s|/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.00.pt\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","5358fb|\u001b[1;31mERR\u001b[0m |       0B/s|/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.06.pt\n","\n","\n","Status Legend:\n","(ERR):error occurred.\n","Status Legend:\n","\n","(ERR):error occurred.\n","\n","aria2 will resume download if the transfer is restarted.\n","If there are any errors, then see the log file. See '-l' option in help/man page for details.\n","aria2 will resume download if the transfer is restarted.\n","If there are any errors, then see the log file. See '-l' option in help/man page for details.\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","831f8b|\u001b[1;31mERR\u001b[0m |       0B/s|/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.02.pt\n","\n","Status Legend:\n","(ERR):error occurred.\n","\n","aria2 will resume download if the transfer is restarted.\n","If there are any errors, then see the log file. See '-l' option in help/man page for details.\n","Error downloading consolidated.00.pt: Command '['aria2c', 'https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/consolidated.00.pt', '--max-connection-per-server=16', '--split=8', '--min-split-size=25M', '--allow-overwrite=true', '-d', '/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1', '-o', 'consolidated.00.pt', '--continue=true']' returned non-zero exit status 9.\n","Run time of job \"download_file_task (trigger: date[2024-01-29 05:37:44 UTC], next run at: 2024-01-29 05:37:44 UTC)\" was missed by 0:00:01.025458\n","Run time of job \"download_file_task (trigger: date[2024-01-29 05:37:44 UTC], next run at: 2024-01-29 05:37:44 UTC)\" was missed by 0:00:01.025678\n","Run time of job \"download_file_task (trigger: date[2024-01-29 05:37:44 UTC], next run at: 2024-01-29 05:37:44 UTC)\" was missed by 0:00:01.025739\n","Run time of job \"download_file_task (trigger: date[2024-01-29 05:37:44 UTC], next run at: 2024-01-29 05:37:44 UTC)\" was missed by 0:00:01.025749\n","Run time of job \"download_file_task (trigger: date[2024-01-29 05:37:44 UTC], next run at: 2024-01-29 05:37:44 UTC)\" was missed by 0:00:01.025724\n","Run time of job \"download_file_task (trigger: date[2024-01-29 05:37:44 UTC], next run at: 2024-01-29 05:37:44 UTC)\" was missed by 0:00:01.025709\n","Run time of job \"download_file_task (trigger: date[2024-01-29 05:37:44 UTC], next run at: 2024-01-29 05:37:44 UTC)\" was missed by 0:00:01.025692\n","\n","\n","Run time of job \"download_file_task (trigger: date[2024-01-29 05:37:44 UTC], next run at: 2024-01-29 05:37:44 UTC)\" was missed by 0:00:01.027508\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","Download Results:\n","======+====+===========+=======================================================\n","gid   |stat|avg speed  |path/URI\n","717eb9|\u001b[1;31mERR\u001b[0m |       0B/s|/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.01.pt\n","======+====+===========+=======================================================\n","d80d1a|\u001b[1;31mERR\u001b[0m |       0B/s|/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.04.pt\n","\n","Status Legend:\n","(ERR):error occurred.\n","\n","aria2 will resume download if the transfer is restarted.\n","\n","If there are any errors, then see the log file. See '-l' option in help/man page for details.\n","Status Legend:\n","(ERR):error occurred.\n","\n","aria2 will resume download if the transfer is restarted.\n","If there are any errors, then see the log file. See '-l' option in help/man page for details.\n","Run time of job \"download_file_task (trigger: date[2024-01-29 05:37:44 UTC], next run at: 2024-01-29 05:37:44 UTC)\" was missed by 0:00:01.030036\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","f168a6|\u001b[1;31mERR\u001b[0m |       0B/s|/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.03.pt\n","\n","Status Legend:\n","(ERR):error occurred.\n","\n","aria2 will resume download if the transfer is restarted.\n","If there are any errors, then see the log file. See '-l' option in help/man page for details.\n","Error downloading consolidated.02.pt: Command '['aria2c', 'https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/consolidated.02.pt', '--max-connection-per-server=16', '--split=8', '--min-split-size=25M', '--allow-overwrite=true', '-d', '/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1', '-o', 'consolidated.02.pt', '--continue=true']' returned non-zero exit status 9.\n","Error downloading consolidated.03.pt: Command '['aria2c', 'https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/consolidated.03.pt', '--max-connection-per-server=16', '--split=8', '--min-split-size=25M', '--allow-overwrite=true', '-d', '/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1', '-o', 'consolidated.03.pt', '--continue=true']' returned non-zero exit status 9.\n","Error downloading consolidated.04.pt: Command '['aria2c', 'https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/consolidated.04.pt', '--max-connection-per-server=16', '--split=8', '--min-split-size=25M', '--allow-overwrite=true', '-d', '/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1', '-o', 'consolidated.04.pt', '--continue=true']' returned non-zero exit status 9.\n","Error downloading consolidated.06.pt: Command '['aria2c', 'https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/consolidated.06.pt', '--max-connection-per-server=16', '--split=8', '--min-split-size=25M', '--allow-overwrite=true', '-d', '/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1', '-o', 'consolidated.06.pt', '--continue=true']' returned non-zero exit status 9.\n","Error downloading consolidated.01.pt: Command '['aria2c', 'https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/consolidated.01.pt', '--max-connection-per-server=16', '--split=8', '--min-split-size=25M', '--allow-overwrite=true', '-d', '/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1', '-o', 'consolidated.01.pt', '--continue=true']' returned non-zero exit status 9.\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n","\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n","01/29 05:37:45 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Exception caught while allocating file space.\n","Exception: [AbstractDiskWriter.cc:557] errNum=28 errorCode=9 fallocate failed. cause: No space left on device\n","\n","01/29 05:37:45 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Download not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.05.pt\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Download GID#76a73ed09c0ba40e not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.05.pt\n","\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","76a73e|\u001b[1;31mERR\u001b[0m |       0B/s|/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.05.pt\n","\n","Status Legend:\n","(ERR):error occurred.\n","\n","aria2 will resume download if the transfer is restarted.\n","If there are any errors, then see the log file. See '-l' option in help/man page for details.\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n","Error downloading consolidated.05.pt: Command '['aria2c', 'https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/consolidated.05.pt', '--max-connection-per-server=16', '--split=8', '--min-split-size=25M', '--allow-overwrite=true', '-d', '/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1', '-o', 'consolidated.05.pt', '--continue=true']' returned non-zero exit status 9.\n","Run time of job \"download_file_task (trigger: date[2024-01-29 05:37:44 UTC], next run at: 2024-01-29 05:37:44 UTC)\" was missed by 0:00:01.046633\n","Run time of job \"download_file_task (trigger: date[2024-01-29 05:37:44 UTC], next run at: 2024-01-29 05:37:44 UTC)\" was missed by 0:00:01.046764\n","Run time of job \"download_file_task (trigger: date[2024-01-29 05:37:44 UTC], next run at: 2024-01-29 05:37:44 UTC)\" was missed by 0:00:01.046749\n","Run time of job \"download_file_task (trigger: date[2024-01-29 05:37:44 UTC], next run at: 2024-01-29 05:37:44 UTC)\" was missed by 0:00:01.046696\n","Run time of job \"download_file_task (trigger: date[2024-01-29 05:37:44 UTC], next run at: 2024-01-29 05:37:44 UTC)\" was missed by 0:00:01.046680\n","Run time of job \"download_file_task (trigger: date[2024-01-29 05:37:44 UTC], next run at: 2024-01-29 05:37:44 UTC)\" was missed by 0:00:01.046658\n","Run time of job \"download_file_task (trigger: date[2024-01-29 05:37:44 UTC], next run at: 2024-01-29 05:37:44 UTC)\" was missed by 0:00:01.046627\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Allocating disk space. Use --file-allocation=none to disable it. See --file-allocation option in man page for more details.\n","\n","01/29 05:37:45 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Exception caught while allocating file space.\n","Exception: [AbstractDiskWriter.cc:453] errNum=28 errorCode=9 Failed to write into the file /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.07.pt, cause: No space left on device\n","\n","01/29 05:37:45 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Download not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.07.pt\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Download GID#f10fb93a31a0efea not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.07.pt\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Allocating disk space. Use --file-allocation=none to disable it. See --file-allocation option in man page for more details.\n","\n","01/29 05:37:45 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Exception caught while allocating file space.\n","Exception: [AbstractDiskWriter.cc:453] errNum=28 errorCode=9 Failed to write into the file /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00001-of-00019.safetensors, cause: No space left on device\n","\n","01/29 05:37:45 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Download not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00001-of-00019.safetensors\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","f10fb9|\u001b[1;31mERR\u001b[0m |       0B/s|/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/consolidated.07.pt\n","\n","Status Legend:\n","(ERR):error occurred.\n","\n","aria2 will resume download if the transfer is restarted.\n","If there are any errors, then see the log file. See '-l' option in help/man page for details.\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] Download GID#1b7d0ca0546262d3 not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00001-of-00019.safetensors\n","Error downloading consolidated.07.pt: Command '['aria2c', 'https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/consolidated.07.pt', '--max-connection-per-server=16', '--split=8', '--min-split-size=25M', '--allow-overwrite=true', '-d', '/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1', '-o', 'consolidated.07.pt', '--continue=true']' returned non-zero exit status 9.\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","1b7d0c|\u001b[1;31mERR\u001b[0m |       0B/s|/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00001-of-00019.safetensors\n","\n","Status Legend:\n","(ERR):error occurred.\n","\n","aria2 will resume download if the transfer is restarted.\n","If there are any errors, then see the log file. See '-l' option in help/man page for details.\n","Error downloading model-00001-of-00019.safetensors: Command '['aria2c', 'https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/model-00001-of-00019.safetensors', '--max-connection-per-server=16', '--split=8', '--min-split-size=25M', '--allow-overwrite=true', '-d', '/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1', '-o', 'model-00001-of-00019.safetensors', '--continue=true']' returned non-zero exit status 9.\n","\n","01/29 05:37:45 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs-us-1.huggingface.co/repos/d1/96/d19680dc3a89c3faf4662824f2065a51142006fc67cd821c163223bfc58fb52d/ae019aa5319535c0f14629cc009da551e86ec385315bf4bb871f23547ad4fa59?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model-00002-of-00019.safetensors%3B+filename%3D%22model-00002-of-00019.safetensors%22%3B&Expires=1706765865&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjc2NTg2NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2QxLzk2L2QxOTY4MGRjM2E4OWMzZmFmNDY2MjgyNGYyMDY1YTUxMTQyMDA2ZmM2N2NkODIxYzE2MzIyM2JmYzU4ZmI1MmQvYWUwMTlhYTUzMTk1MzVjMGYxNDYyOWNjMDA5ZGE1NTFlODZlYzM4NTMxNWJmNGJiODcxZjIzNTQ3YWQ0ZmE1OT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=c22W9ZDjz1W7QSbIodXwWno7GKgNObt1Q7QYa8O111w-QpOapP5QgSvU%7EL9sMIxZPXXrMITXuEW2N6%7EWunKf4tXwVr6Qxg2szEGuJ0HXiDnNdt0bcPriy%7ECDMl4fp6zHFHWz2ub099aHxIbnrM4WRQdtXxi4E6XeiGW95QiaDGi4s-E9D2gZ25uHQZbb5MVNF0cBNoDotZTlCt6Ayz2CFma2HfuPpaXcLI%7ENe-3T9UpxL8gIKvQvhMgWeoKiOdxpPf6fwdg-xCavGfpJpwc-NiTLl4sr9QpH8mnReAe%7EW57%7Ez1BjXoXky%7EZP6bril7EvYQ6kgNGNEEeTpPEfEinM1A__&Key-Pair-Id=KCD77M1F0VK2B\n","\n","01/29 05:37:46 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs-us-1.huggingface.co/repos/d1/96/d19680dc3a89c3faf4662824f2065a51142006fc67cd821c163223bfc58fb52d/8b976a0fb2a154361a06b4dbabce532772a06ca3b06949277395881a0884758d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model-00012-of-00019.safetensors%3B+filename%3D%22model-00012-of-00019.safetensors%22%3B&Expires=1706765866&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjc2NTg2Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2QxLzk2L2QxOTY4MGRjM2E4OWMzZmFmNDY2MjgyNGYyMDY1YTUxMTQyMDA2ZmM2N2NkODIxYzE2MzIyM2JmYzU4ZmI1MmQvOGI5NzZhMGZiMmExNTQzNjFhMDZiNGRiYWJjZTUzMjc3MmEwNmNhM2IwNjk0OTI3NzM5NTg4MWEwODg0NzU4ZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=U9%7ERdXr3EQCJ583HiMpU20U1TtQB%7EKqGnwoccyoaTnUsLsV-isX-ahnrVySpW9cq1nEKwnVOwXYTdR0XAj%7E38zaqf8UyCrYdpyEIbqJ4HRYOyR211UEWXyGCPUVnNRy0cJZnJc4iWtkloKWhH91%7EjS43YoAHeXrszlaBH3VsCm72b3NDfQe6LyuBYq2rdDB0hLworS6jbU%7EmSMDbyO7rYpyPZW2s3CUEpzp4U%7EnL-Bx-BMN7DiVAT2Z79d7prQ-r9iqsEEn5GXuvc8dg7ohmxlCZMiNnTXo-lmOY-3HK50KYRnGjARynvrV5ugjixfw8tcbzwpyyfq7TNWouTlvIoQ__&Key-Pair-Id=KCD77M1F0VK2B\n","\n","01/29 05:37:46 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs-us-1.huggingface.co/repos/d1/96/d19680dc3a89c3faf4662824f2065a51142006fc67cd821c163223bfc58fb52d/bd2004f5328c123169325a0079e8bc5fc63262eeb4aadf81558762ce4b3b5e26?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model-00013-of-00019.safetensors%3B+filename%3D%22model-00013-of-00019.safetensors%22%3B&Expires=1706765866&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjc2NTg2Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2QxLzk2L2QxOTY4MGRjM2E4OWMzZmFmNDY2MjgyNGYyMDY1YTUxMTQyMDA2ZmM2N2NkODIxYzE2MzIyM2JmYzU4ZmI1MmQvYmQyMDA0ZjUzMjhjMTIzMTY5MzI1YTAwNzllOGJjNWZjNjMyNjJlZWI0YWFkZjgxNTU4NzYyY2U0YjNiNWUyNj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=S5TqcoP--XZRPYUS0%7EeNFoE0NRAy0%7En-ztyvdvF-OOZgz5mNNtl9m1J%7EpkwYeZNArlvPMelHRqY%7ESOlqX0-9PG2dBoTPOU0jd8kLHa5TiJNtyJTmX5aJY4BX7h10O7eAnb4yX5Q7cnCC4AuWP0isHBYjLTv5gKU0ZDRHDoIpHbL6HsUZRcgh6UW1pcvWtpfBTUmBoLC8pIKI8sCq6xLS7GcKELEPq25wKx2FsNBLdK-UinKLZfo-LXvRu8cGxIyL7cclep30UbLX0h19%7EU8ihf7wEt0VU%7E1r%7E0S0at5hXMldUwfaxTCFE6pDZRusDBLCgFblcrTsKLC7ygd0VWuwIg__&Key-Pair-Id=KCD77M1F0VK2B\n","\n","01/29 05:37:46 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs-us-1.huggingface.co/repos/d1/96/d19680dc3a89c3faf4662824f2065a51142006fc67cd821c163223bfc58fb52d/f6e8d6e53aa3ed88b0863807f141ceb01cf7fd4c701d7fc90f7f08d02f706497?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model-00015-of-00019.safetensors%3B+filename%3D%22model-00015-of-00019.safetensors%22%3B&Expires=1706765866&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjc2NTg2Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2QxLzk2L2QxOTY4MGRjM2E4OWMzZmFmNDY2MjgyNGYyMDY1YTUxMTQyMDA2ZmM2N2NkODIxYzE2MzIyM2JmYzU4ZmI1MmQvZjZlOGQ2ZTUzYWEzZWQ4OGIwODYzODA3ZjE0MWNlYjAxY2Y3ZmQ0YzcwMWQ3ZmM5MGY3ZjA4ZDAyZjcwNjQ5Nz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=nSFlwzHDoS4z6jBaNZ14pu3zeMGAbjpJgyGkamrL2QXBBLqz1Q%7EjuFcYhBPRq1tuWs63LArAl9DXCD%7EjOsi-QPOpoiR9Eko2TwHFHPD1Jb9dAyCkfC5bYaQeX3MyTJUrLOTeKACd5mZGvymoiqDGeINMt22fnOKf6pf4yjY2cO6zKh1tHmicH8RJOFWS-uSPUIWhTTJeWl7xlD5nkoQ0OlgjBG8%7EwVwCbA1vGPabgpWCgYg4WZ9WeRJfRiOgeP9XVXACh3mV5g664JyQsLwEBLcYD57I%7E6KWgLBpAbIeByLKw4MkRMnTB05LXhBaf3WADcIEu5AmDICQItYeACN26A__&Key-Pair-Id=KCD77M1F0VK2B\n","\n","01/29 05:37:46 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs-us-1.huggingface.co/repos/d1/96/d19680dc3a89c3faf4662824f2065a51142006fc67cd821c163223bfc58fb52d/91f6634461c880890b79bcee1f0eeab88e437dd5b0832a25d8a0261c667132ad?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model-00014-of-00019.safetensors%3B+filename%3D%22model-00014-of-00019.safetensors%22%3B&Expires=1706765866&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjc2NTg2Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2QxLzk2L2QxOTY4MGRjM2E4OWMzZmFmNDY2MjgyNGYyMDY1YTUxMTQyMDA2ZmM2N2NkODIxYzE2MzIyM2JmYzU4ZmI1MmQvOTFmNjYzNDQ2MWM4ODA4OTBiNzliY2VlMWYwZWVhYjg4ZTQzN2RkNWIwODMyYTI1ZDhhMDI2MWM2NjcxMzJhZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=jqoKAbE5VRIo%7E-G%7Ez%7EgEied3we9z-8DlMEogcc7Gov%7EmE4dTuIWF8a2Ht9BhCFLWxOj5ZbFHAMLQg8rudVkQa7UiB4fZ4JMdR-7R2DHSZi9Bm73phLkyr5gUgpL%7ElqSBVfqA1pnMxNvoxaMfnIhF21XHKNn%7E9tIspXqGrpcv%7EGbJe561k8II-UAUFQ2ONY4yNCx6-PmSTDsLuYhllrv1zUgqgS4nq9W7ztfzC-B40XhjM%7EwHZQDw3OcNoLUStj3dBmAD6a9QUmy93JYzaqX7Kusqx5%7Ecz-RcHMkYNZvJ%7ECvAe7OV4HrfZ1gdX9URA8d2qMozp9RFAhg7xzcVUVb27g__&Key-Pair-Id=KCD77M1F0VK2B\n","\n","01/29 05:37:46 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs-us-1.huggingface.co/repos/d1/96/d19680dc3a89c3faf4662824f2065a51142006fc67cd821c163223bfc58fb52d/f47fd7d1ac5473df061032b776e8d022652530ab3176ad1e7681b7e7e50b2323?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model-00017-of-00019.safetensors%3B+filename%3D%22model-00017-of-00019.safetensors%22%3B&Expires=1706765866&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjc2NTg2Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2QxLzk2L2QxOTY4MGRjM2E4OWMzZmFmNDY2MjgyNGYyMDY1YTUxMTQyMDA2ZmM2N2NkODIxYzE2MzIyM2JmYzU4ZmI1MmQvZjQ3ZmQ3ZDFhYzU0NzNkZjA2MTAzMmI3NzZlOGQwMjI2NTI1MzBhYjMxNzZhZDFlNzY4MWI3ZTdlNTBiMjMyMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=M8DtJCGNQuyfU1s4Iu%7ErmWm-1jxS2FXpTki4oK5qGaon2KYRaDrI0G5yoHy4ZMFg4E3xfVulQbCwDRDswhRL59e-ZZkIeWmXSbVEBvFZrml0a3CZ8YZVX4so34w7xngSj1QWcPvWj0Yf41E20nVPOHFEnIbWcPvSCdYK1h88oCjwG8M7erTBOuqnRIbVNEPUO8n9mLt7XsXz4EllEzNSpYzk5H3f8FlS1JZv2c6%7EJ-WDnZHJ5TRH4eBgNOMsWuUU9qBgnPUOIKRGJjK67wjzqDZvH-78GE18%7EKVHMH54H2BjQnH60xtUp96RWS-WppQoqDDnjuHsM-QNzROB6opKiw__&Key-Pair-Id=KCD77M1F0VK2B\n","\n","01/29 05:37:46 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs-us-1.huggingface.co/repos/d1/96/d19680dc3a89c3faf4662824f2065a51142006fc67cd821c163223bfc58fb52d/fc98831a2558443f6da2f841488d934d10c1d20699607548c70f2212926c15b5?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model-00016-of-00019.safetensors%3B+filename%3D%22model-00016-of-00019.safetensors%22%3B&Expires=1706765866&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjc2NTg2Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2QxLzk2L2QxOTY4MGRjM2E4OWMzZmFmNDY2MjgyNGYyMDY1YTUxMTQyMDA2ZmM2N2NkODIxYzE2MzIyM2JmYzU4ZmI1MmQvZmM5ODgzMWEyNTU4NDQzZjZkYTJmODQxNDg4ZDkzNGQxMGMxZDIwNjk5NjA3NTQ4YzcwZjIyMTI5MjZjMTViNT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=bV%7EhlKHlPzCHbaE2GC9u1D4M5L-g87FGaX05c-ZblMC6PHCWz%7Es3iiB%7Eh4R-vrxE12ku2OVKxbHQ4WNxqC65X0BSViIxcOFPXb78HLNouFqf3gfpuGmU5ohq4DEUf0XfSD0KC-0fKODZPCUfoTbV0iVuSfd5MZfxFHsJKre46U2VEEkoqL9W12JeuVtfGHMd3Nz%7Em5dFwzDGN9Uf4Ybdy%7ElBxdeCVTV3FDGnMp4DkNTybB4b09BiFymwlfwkpEGgNWh-T9MUfBSPikWuRw6h6c1RdywOMjZ9kKFcZj5HO2OXcTTHZMdGba-iRRxeYGP462q0zKSsLjJgGJE7V0tlrw__&Key-Pair-Id=KCD77M1F0VK2B\n","\n","01/29 05:37:46 [\u001b[1;32mNOTICE\u001b[0m] Allocating disk space. Use --file-allocation=none to disable it. See --file-allocation option in man page for more details.\n","\n","01/29 05:37:46 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Exception caught while allocating file space.\n","Exception: [AbstractDiskWriter.cc:453] errNum=28 errorCode=9 Failed to write into the file /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00002-of-00019.safetensors, cause: No space left on device\n","\n","01/29 05:37:46 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Download not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00002-of-00019.safetensors\n","\n","01/29 05:37:46 [\u001b[1;32mNOTICE\u001b[0m] Download GID#c9d91a6e589c33b0 not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00002-of-00019.safetensors\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","c9d91a|\u001b[1;31mERR\u001b[0m |       0B/s|/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00002-of-00019.safetensors\n","\n","Status Legend:\n","(ERR):error occurred.\n","\n","aria2 will resume download if the transfer is restarted.\n","If there are any errors, then see the log file. See '-l' option in help/man page for details.\n","Error downloading model-00002-of-00019.safetensors: Command '['aria2c', 'https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/model-00002-of-00019.safetensors', '--max-connection-per-server=16', '--split=8', '--min-split-size=25M', '--allow-overwrite=true', '-d', '/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1', '-o', 'model-00002-of-00019.safetensors', '--continue=true']' returned non-zero exit status 9.\n","\n","01/29 05:37:46 [\u001b[1;32mNOTICE\u001b[0m] Allocating disk space. Use --file-allocation=none to disable it. See --file-allocation option in man page for more details.\n","\n","01/29 05:37:46 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Exception caught while allocating file space.\n","Exception: [AbstractDiskWriter.cc:453] errNum=28 errorCode=9 Failed to write into the file /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00012-of-00019.safetensors, cause: No space left on device\n","\n","01/29 05:37:46 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Download not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00012-of-00019.safetensors\n","\n","01/29 05:37:46 [\u001b[1;32mNOTICE\u001b[0m] Download GID#580f8970e255d756 not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00012-of-00019.safetensors\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","580f89|\u001b[1;31mERR\u001b[0m |       0B/s|/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00012-of-00019.safetensors\n","\n","Status Legend:\n","(ERR):error occurred.\n","\n","aria2 will resume download if the transfer is restarted.\n","If there are any errors, then see the log file. See '-l' option in help/man page for details.\n","Error downloading model-00012-of-00019.safetensors: Command '['aria2c', 'https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/model-00012-of-00019.safetensors', '--max-connection-per-server=16', '--split=8', '--min-split-size=25M', '--allow-overwrite=true', '-d', '/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1', '-o', 'model-00012-of-00019.safetensors', '--continue=true']' returned non-zero exit status 9.\n","\n","01/29 05:37:46 [\u001b[1;32mNOTICE\u001b[0m] Allocating disk space. Use --file-allocation=none to disable it. See --file-allocation option in man page for more details.\n","\n","01/29 05:37:46 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Exception caught while allocating file space.\n","Exception: [AbstractDiskWriter.cc:453] errNum=28 errorCode=9 Failed to write into the file /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00013-of-00019.safetensors, cause: No space left on device\n","\n","01/29 05:37:46 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Download not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00013-of-00019.safetensors\n","\n","01/29 05:37:46 [\u001b[1;32mNOTICE\u001b[0m] Allocating disk space. Use --file-allocation=none to disable it. See --file-allocation option in man page for more details.\n","\n","01/29 05:37:46 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Exception caught while allocating file space.\n","Exception: [AbstractDiskWriter.cc:453] errNum=28 errorCode=9 Failed to write into the file /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00015-of-00019.safetensors, cause: No space left on device\n","\n","01/29 05:37:46 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Download not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00015-of-00019.safetensors\n","\n","01/29 05:37:46 [\u001b[1;32mNOTICE\u001b[0m] Download GID#673621db6dc9392d not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00013-of-00019.safetensors\n","\n","01/29 05:37:46 [\u001b[1;32mNOTICE\u001b[0m] Download GID#aac953665aef4c20 not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00015-of-00019.safetensors\n","\n","01/29 05:37:46 [\u001b[1;32mNOTICE\u001b[0m] Allocating disk space. Use --file-allocation=none to disable it. See --file-allocation option in man page for more details.\n","\n","01/29 05:37:46 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Exception caught while allocating file space.\n","Exception: [AbstractDiskWriter.cc:453] errNum=28 errorCode=9 Failed to write into the file /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00014-of-00019.safetensors, cause: No space left on device\n","\n","01/29 05:37:46 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Download not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00014-of-00019.safetensors\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","673621|\u001b[1;31mERR\u001b[0m |       0B/s|/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00013-of-00019.safetensors\n","\n","Status Legend:\n","(ERR):error occurred.\n","\n","aria2 will resume download if the transfer is restarted.\n","If there are any errors, then see the log file. See '-l' option in help/man page for details.\n","\n","01/29 05:37:46 [\u001b[1;32mNOTICE\u001b[0m] Download GID#be3c8f1f3f902487 not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00014-of-00019.safetensors\n","Error downloading model-00013-of-00019.safetensors: Command '['aria2c', 'https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/model-00013-of-00019.safetensors', '--max-connection-per-server=16', '--split=8', '--min-split-size=25M', '--allow-overwrite=true', '-d', '/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1', '-o', 'model-00013-of-00019.safetensors', '--continue=true']' returned non-zero exit status 9.\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","aac953|\u001b[1;31mERR\u001b[0m |       0B/s|/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00015-of-00019.safetensors\n","\n","Status Legend:\n","(ERR):error occurred.\n","\n","aria2 will resume download if the transfer is restarted.\n","If there are any errors, then see the log file. See '-l' option in help/man page for details.\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","be3c8f|\u001b[1;31mERR\u001b[0m |       0B/s|/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00014-of-00019.safetensors\n","\n","Status Legend:\n","(ERR):error occurred.\n","\n","aria2 will resume download if the transfer is restarted.\n","If there are any errors, then see the log file. See '-l' option in help/man page for details.\n","Error downloading model-00014-of-00019.safetensors: Command '['aria2c', 'https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/model-00014-of-00019.safetensors', '--max-connection-per-server=16', '--split=8', '--min-split-size=25M', '--allow-overwrite=true', '-d', '/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1', '-o', 'model-00014-of-00019.safetensors', '--continue=true']' returned non-zero exit status 9.\n","Error downloading model-00015-of-00019.safetensors: Command '['aria2c', 'https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/model-00015-of-00019.safetensors', '--max-connection-per-server=16', '--split=8', '--min-split-size=25M', '--allow-overwrite=true', '-d', '/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1', '-o', 'model-00015-of-00019.safetensors', '--continue=true']' returned non-zero exit status 9.\n","\n","01/29 05:37:46 [\u001b[1;32mNOTICE\u001b[0m] Allocating disk space. Use --file-allocation=none to disable it. See --file-allocation option in man page for more details.\n","\n","01/29 05:37:46 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Exception caught while allocating file space.\n","Exception: [AbstractDiskWriter.cc:453] errNum=28 errorCode=9 Failed to write into the file /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00016-of-00019.safetensors, cause: No space left on device\n","\n","01/29 05:37:46 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Download not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00016-of-00019.safetensors\n","\n","01/29 05:37:46 [\u001b[1;32mNOTICE\u001b[0m] Download GID#1ff356a65077da6f not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00016-of-00019.safetensors\n","\n","01/29 05:37:46 [\u001b[1;32mNOTICE\u001b[0m] Allocating disk space. Use --file-allocation=none to disable it. See --file-allocation option in man page for more details.\n","\n","01/29 05:37:46 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Exception caught while allocating file space.\n","Exception: [AbstractDiskWriter.cc:453] errNum=28 errorCode=9 Failed to write into the file /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00017-of-00019.safetensors, cause: No space left on device\n","\n","01/29 05:37:46 [\u001b[1;31mERROR\u001b[0m] CUID#8 - Download not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00017-of-00019.safetensors\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","1ff356|\u001b[1;31mERR\u001b[0m |       0B/s|/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00016-of-00019.safetensors\n","\n","Status Legend:\n","(ERR):error occurred.\n","\n","aria2 will resume download if the transfer is restarted.\n","If there are any errors, then see the log file. See '-l' option in help/man page for details.\n","Error downloading model-00016-of-00019.safetensors: Command '['aria2c', 'https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/model-00016-of-00019.safetensors', '--max-connection-per-server=16', '--split=8', '--min-split-size=25M', '--allow-overwrite=true', '-d', '/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1', '-o', 'model-00016-of-00019.safetensors', '--continue=true']' returned non-zero exit status 9.\n","\n","01/29 05:37:46 [\u001b[1;32mNOTICE\u001b[0m] Download GID#d010343ee27cca83 not complete: /content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00017-of-00019.safetensors\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","d01034|\u001b[1;31mERR\u001b[0m |       0B/s|/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1/model-00017-of-00019.safetensors\n","\n","Status Legend:\n","(ERR):error occurred.\n","\n","aria2 will resume download if the transfer is restarted.\n","If there are any errors, then see the log file. See '-l' option in help/man page for details.\n","Error downloading model-00017-of-00019.safetensors: Command '['aria2c', 'https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/model-00017-of-00019.safetensors', '--max-connection-per-server=16', '--split=8', '--min-split-size=25M', '--allow-overwrite=true', '-d', '/content/Ollama-Companion/llama.cpp/models/Mixtral-8x7B-v0.1', '-o', 'model-00017-of-00019.safetensors', '--continue=true']' returned non-zero exit status 9.\n"]}]},{"cell_type":"code","source":["# Download and install ollama to the system\n","!curl https://ollama.ai/install.sh | sh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DXYRy7m_v6ls","executionInfo":{"status":"ok","timestamp":1706507095379,"user_tz":-330,"elapsed":4217,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"f0b7167a-e7eb-480a-b432-44928b38a953"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0>>> Downloading ollama...\n","100  8422    0  8422    0     0  27169      0 --:--:-- --:--:-- --:--:-- 27255\n","############################################################################################# 100.0%\n",">>> Installing ollama to /usr/local/bin...\n",">>> Creating ollama user...\n",">>> Adding current user to ollama group...\n",">>> Creating ollama systemd service...\n","WARNING: Unable to detect NVIDIA GPU. Install lspci or lshw to automatically detect and install NVIDIA CUDA drivers.\n",">>> The Ollama API is now available at 0.0.0.0:11434.\n",">>> Install complete. Run \"ollama\" from the command line.\n"]}]},{"cell_type":"code","source":["!pip install aiohttp pyngrok\n","\n","import os\n","import asyncio\n","\n","# Set LD_LIBRARY_PATH so the system NVIDIA library\n","os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n","\n","async def run_process(cmd):\n","  print('>>> starting', *cmd)\n","  p = await asyncio.subprocess.create_subprocess_exec(\n","      *cmd,\n","      stdout=asyncio.subprocess.PIPE,\n","      stderr=asyncio.subprocess.PIPE,\n","  )\n","\n","  async def pipe(lines):\n","    async for line in lines:\n","      print(line.strip().decode('utf-8'))\n","\n","  await asyncio.gather(\n","      pipe(p.stdout),\n","      pipe(p.stderr),\n","  )\n","\n","#register an account at ngrok.com and create an authtoken and place it here\n","await asyncio.gather(\n","    run_process(['ngrok', 'config', 'add-authtoken','your-auth-token'])\n",")\n","\n","await asyncio.gather(\n","    run_process(['ollama', 'serve']),\n","    run_process(['ngrok', 'http', '--log', 'stderr', '11434']),\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"GFpA_B1wwGFt","outputId":"b6f02bce-019b-4bc2-8127-c529f64664f4","executionInfo":{"status":"error","timestamp":1706512895967,"user_tz":-330,"elapsed":1301471,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}}},"execution_count":2,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.9.1)\n","Collecting pyngrok\n","  Downloading pyngrok-7.0.5-py3-none-any.whl (21 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp) (3.6)\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-7.0.5\n",">>> starting ngrok config add-authtoken your-auth-token\n","Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",">>> starting ollama serve\n",">>> starting ngrok http --log stderr 11434\n","Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.\n","Your new public key is:\n","\n","ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIHkPpoNUO7naQAEUGl2MebcDcKDA+FwR3CNL0QQ1sC/\n","\n","2024/01/29 05:45:44 images.go:857: INFO total blobs: 0\n","2024/01/29 05:45:44 images.go:864: INFO total unused blobs removed: 0\n","2024/01/29 05:45:44 routes.go:950: INFO Listening on 127.0.0.1:11434 (version 0.1.22)\n","2024/01/29 05:45:44 payload_common.go:106: INFO Extracting dynamic libraries...\n","t=2024-01-29T05:45:44+0000 lvl=info msg=\"no configuration paths supplied\"\n","t=2024-01-29T05:45:44+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n","t=2024-01-29T05:45:44+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n","t=2024-01-29T05:45:44+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n","t=2024-01-29T05:45:44+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session obj=csess id=4428717ff766 err=\"authentication failed: The authtoken you specified does not look like a proper ngrok tunnel authtoken.\\nYour authtoken: your-auth-token\\nInstructions to install your authtoken are on your ngrok dashboard:\\nhttps://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_105\\r\\n\"\n","t=2024-01-29T05:45:44+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: The authtoken you specified does not look like a proper ngrok tunnel authtoken.\\nYour authtoken: your-auth-token\\nInstructions to install your authtoken are on your ngrok dashboard:\\nhttps://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_105\\r\\n\"\n","t=2024-01-29T05:45:44+0000 lvl=info msg=\"received stop request\" obj=app stopReq=\"{err:{Remote:true Inner:{Inner:0xc0003d0ee0}} restart:false}\"\n","t=2024-01-29T05:45:44+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: The authtoken you specified does not look like a proper ngrok tunnel authtoken.\\nYour authtoken: your-auth-token\\nInstructions to install your authtoken are on your ngrok dashboard:\\nhttps://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_105\\r\\n\"\n","t=2024-01-29T05:45:44+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: The authtoken you specified does not look like a proper ngrok tunnel authtoken.\\nYour authtoken: your-auth-token\\nInstructions to install your authtoken are on your ngrok dashboard:\\nhttps://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_105\\r\\n\"\n","\n","ERROR:  authentication failed: The authtoken you specified does not look like a proper ngrok tunnel authtoken.\n","ERROR:  Your authtoken: your-auth-token\n","ERROR:  Instructions to install your authtoken are on your ngrok dashboard:\n","ERROR:  https://dashboard.ngrok.com/get-started/your-authtoken\n","ERROR:\n","ERROR:  ERR_NGROK_105\n","ERROR:\n","NAME:\n","http - start an HTTP tunnel\n","\n","USAGE:\n","ngrok http [address:port | port] [flags]\n","\n","DESCRIPTION:\n","Starts a tunnel listening for HTTP/HTTPS traffic with a specific hostname.\n","The HTTP Host header on incoming public requests is inspected to\n","determine which tunnel it matches.\n","\n","HTTPS endpoints terminate TLS traffic at the ngrok server using the\n","appropriate certificates. The decrypted, HTTP traffic is then forwarded\n","through the secure tunnel and then to your local server. If you don't want\n","your TLS traffic to terminate at the ngrok server, use a TLS or TCP tunnel.\n","\n","TERMS OF SERVICE: https://ngrok.com/tos\n","\n","EXAMPLES:\n","ngrok http 8080                             # forward ngrok subdomain to port 80\n","ngrok http example.com:9000                 # forward traffic to example.com:9000\n","ngrok http --domain=bar.ngrok.dev 80        # request subdomain name: 'bar.ngrok.dev'\n","ngrok http --domain=example.com 1234        # request tunnel 'example.com' (DNS CNAME)\n","ngrok http --basic-auth='falken:joshua' 80  # enforce basic auth on tunnel endpoint\n","ngrok http --host-header=example.com 80     # rewrite the Host header to 'example.com'\n","ngrok http file:///var/log                  # serve local files in /var/log\n","ngrok http https://localhost:8443           # forward to a local https server\n","\n","OPTIONS:\n","--app-protocol string              Set application protocol: 'http2'\n","--authtoken string                 ngrok.com authtoken identifying a user\n","--basic-auth strings               enforce basic auth on tunnel endpoint, 'user:password'\n","--cidr-allow strings               reject connections that do not match the given CIDRs\n","--cidr-deny strings                reject connections that match the given CIDRs\n","--circuit-breaker float            reject requests when 5XX responses exceed this ratio\n","--compression                      gzip compress http responses from your web service\n","--config strings                   path to config files; they are merged if multiple\n","--domain string                    host tunnel on a custom subdomain or hostname (requires DNS CNAME)\n","-h, --help                             help for http\n","--host-header string               set Host header; if 'rewrite' use local address hostname\n","--inspect                          enable/disable http introspection (default true) (default <nil>)\n","--log string                       path to log file, 'stdout', 'stderr' or 'false' (default \"false\")\n","--log-format string                log record format: 'term', 'logfmt', 'json' (default \"term\")\n","--log-level string                 logging level: 'debug', 'info', 'warn', 'error', 'crit' (default \"info\")\n","--mutual-tls-cas string            path to TLS certificate authority to verify client certs in mutual tls\n","--oauth string                     enforce authentication oauth provider on tunnel endpoint, e.g. 'google'\n","--oauth-allow-domain strings       allow only oauth users with these email domains\n","--oauth-allow-email strings        allow only oauth users with these emails\n","--oauth-client-id string           oauth app client id, optional\n","--oauth-client-secret string       oauth app client secret, optional\n","--oauth-scope strings              request these oauth scopes when users authenticate\n","--oidc string                      oidc issuer url, e.g. https://accounts.google.com\n","--oidc-client-id string            oidc app client id\n","--oidc-client-secret string        oidc app client secret\n","--oidc-scope strings               request these oidc scopes when users authenticate\n","--proxy-proto string               version of proxy proto to use with this tunnel, empty if not using\n","--request-header-add strings       header key:value to add to request\n","--request-header-remove strings    header field to remove from request if present\n","--response-header-add strings      header key:value to add to response\n","--response-header-remove strings   header field to remove from response if present\n","--scheme strings                   which schemes to listen on (default [https])\n","--ua-filter-allow strings          a list of regular expressions for user-agents to allow\n","--ua-filter-deny strings           a list of regular expressions for user-agents to deny\n","--upstream-tls-verify              enables TLS verification of server TLS certificates\n","--upstream-tls-verify-cas string   path to CA cert to use to verify server certs\n","--verify-webhook string            validate webhooks are signed by this provider, e.g. 'slack'\n","--verify-webhook-secret string     secret used by provider to sign webhooks, if any\n","--websocket-tcp-converter          convert ingress websocket connections to TCP upstream\n","2024/01/29 05:45:52 payload_common.go:145: INFO Dynamic LLM libraries [cpu rocm_v6 cpu_avx2 rocm_v5 cpu_avx cuda_v11]\n","2024/01/29 05:45:52 gpu.go:94: INFO Detecting GPU type\n","2024/01/29 05:45:52 gpu.go:236: INFO Searching for GPU management library libnvidia-ml.so\n","2024/01/29 05:45:52 gpu.go:282: INFO Discovered GPU libraries: [/usr/lib64-nvidia/libnvidia-ml.so.535.104.05]\n","2024/01/29 05:45:52 gpu.go:99: INFO Nvidia GPU detected\n","2024/01/29 05:45:52 gpu.go:140: INFO CUDA Compute Capability detected: 7.5\n"]},{"output_type":"error","ename":"CancelledError","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-5b3d8918a261>\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(lines)\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/asyncio/streams.py\u001b[0m in \u001b[0;36m__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    722\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__anext__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/asyncio/streams.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaduntil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncompleteReadError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/asyncio/streams.py\u001b[0m in \u001b[0;36mreaduntil\u001b[0;34m(self, separator)\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0;31m# _wait_for_data() will resume reading if stream was paused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'readuntil'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/asyncio/streams.py\u001b[0m in \u001b[0;36m_wait_for_data\u001b[0;34m(self, func_name)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_waiter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mCancelledError\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mCancelledError\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-5b3d8918a261>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m await asyncio.gather(\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mrun_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ollama'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'serve'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mrun_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ngrok'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--log'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stderr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'11434'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mCancelledError\u001b[0m: "]}]},{"cell_type":"code","source":["!ngrok authtoken <your-authtoken>\n"],"metadata":{"id":"YZuXg6UYyF7S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import copy\n","import warnings\n","import torch\n","from typing import Union, List, Dict\n","from datasets import load_dataset\n","from transformers import AutoTokenizer\n","\n","def create_tokenizer(model_name_or_path: str = 'gpt2') -> AutoTokenizer:\n","    \"\"\"\n","    Creates a tokenizer object for a pre-trained Transformers model.\n","\n","    Args:\n","        model_name_or_path (str): The name or path of the pre-trained model.\n","\n","    Returns:\n","        AutoTokenizer: A tokenizer object for the pre-trained model.\n","    \"\"\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","\n","    # Set default values for special tokens if they haven't already been set\n","    if tokenizer.pad_token_id is None:\n","        tokenizer.pad_token_id = tokenizer.eos_token_id\n","    if tokenizer.bos_token_id is None:\n","        tokenizer.bos_token_id = tokenizer.pad_token_id\n","    if tokenizer.eos_token_id is None:\n","        tokenizer.eos_token_id = tokenizer.pad_token_id\n","    if tokenizer.unk_token_id is None:\n","        tokenizer.unk_token_id = tokenizer.pad_token_id\n","    if tokenizer.sep_token_id is None:\n","        tokenizer.sep_token_id = tokenizer.pad_token_id\n","    if tokenizer.cls_token_id is None:\n","        tokenizer.cls_token_id = tokenizer.pad_token_id\n","    if tokenizer.mask_token_id is None:\n","        tokenizer.mask_token_id = tokenizer.pad_token_id\n","\n","    return tokenizer\n","\n","def advanced_data_loader(input: Union[str, dict], format: str = None) -> Dict[str, List[Dict]]:\n","    \"\"\"\n","    Loads a dataset from a file, directory, or Hugging Face dataset.\n","\n","    Args:\n","        input (Union[str, dict]): A string specifying the file, directory, or dataset name, or a dictionary\n","                                 specifying the paths to one or more CSV or JSON files.\n","        format (str, optional): The format of the input data. Supported formats are 'csv', 'json', and 'text'.\n","                                If input is a string, this argument is required. Defaults to None.\n","\n","    Returns:\n","        Dict[str, List[Dict]]: A dictionary containing the loaded dataset, with keys for each split (e.g., 'train', 'test')\n","                              and values for the corresponding lists of dictionaries.\n","    \"\"\"\n","    if isinstance(input, dict):\n","        if format in ['csv', 'json']:\n","            try:\n","                return load_dataset(format, data_files=input)\n","            except FileNotFoundError:\n","                warnings.warn(\"File not found. Please check your file path.\")\n","                return None\n","        else:\n","            warnings.warn(\"Invalid format. Please choose 'csv' or 'json'.\")\n","            return None\n","    elif isinstance(input, str):\n","        if format == 'text':\n","            if os.path.isdir(input):\n","                try:\n","                    return load_dataset(format, data_dir=input)\n","                except FileNotFoundError:\n","                    warnings.warn(\"Directory not found. Please check your folder path.\")\n","                    return None\n","            else:\n","                warnings.warn(\"Invalid directory. Please check your folder path.\")\n","                return None\n","        elif format is None:\n","            try:\n","                return load_dataset(input)\n","            except FileNotFoundError:\n","                warnings.warn(\"Dataset not found. Please check your dataset name.\")\n","                return None\n","        else:\n","            warnings.warn(\"Invalid input. Please enter a valid dataset name or folder path.\")\n","            return None\n","    else:\n","        warnings.warn(\"Invalid input type. Please enter a string (for dataset name or folder) or a dictionary (for CSV or JSON files).\")\n","        return None\n","\n","def preprocess_function_train(examples, tokenizer: AutoTokenizer, source_column: str, target_column: str, max_source_length: int, max_target_length: int, padding: str, task_prompt: str = \"\\nAnswer the above question. First think step by step and then answer the final number.\\n\") -> Dict[str, torch.Tensor]:\n","    \"\"\"\n","    Preprocesses the training data for a Transformers model.\n","\n","    Args:\n","        examples (List[Dict]): A list of dictionaries containing the training data.\n","        tokenizer (AutoTokenizer): A tokenizer object for the pre-trained model.\n","        source_column (str): The name of the column containing the input text.\n","        target_column (str): The name of the column containing the output text.\n","        max_source_length (int): The maximum length of the input sequence.\n","        max_target_length (int): The maximum length of the output sequence.\n","        padding (str): The padding strategy to use. Supported values are 'max_length' and False.\n","        task_prompt (str, optional): The prompt to use for generating the input sequence. Defaults to \"\\nAnswer the above question. First think step by step and then answer the final number.\\n\".\n","\n","    Returns:\n","        Dict[str, torch.Tensor]: A dictionary containing the preprocessed data, with keys for 'input_ids', 'attention_mask', and 'labels'.\n","    \"\"\"\n","    sources = [examples[i][source_column] for i in range(len(examples))]\n","    targets = [examples[i][target_column] for i in range(len(examples))]\n","\n","    inputs = [task_prompt + sources[i] + \"\\n\" + targets[i] for i in range(len(examples))]\n","\n","    model_inputs = tokenizer(\n","        inputs,\n","        max_length=max_source_length + max_target_length,\n","        padding=padding,\n","        truncation=True,\n","        return_tensors=\"pt\",\n","    )\n","\n","    labels = copy.deepcopy(model_inputs)\n","\n","    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n","    # padding in the loss.\n","    if padding == \"max_length\" and"],"metadata":{"id":"jHRu35vnC8jm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import warnings\n","from typing import Union, Optional, Dict, Any\n","from datasets import load_dataset\n","from transformers import AutoTokenizer\n","import torch\n","\n","class DataArguments:\n","    \"\"\"\n","    Data arguments needed for data loading and preprocessing.\n","    \"\"\"\n","    def __init__(\n","        self,\n","        model_name_or_path: str = 'gpt2',\n","        max_source_length: int = 1024,\n","        max_target_length: int = 128,\n","        pad_to_max_length: bool = True,\n","        ignore_pad_token_for_loss: bool = True\n","    ):\n","        self.model_name_or_path = model_name_or_path\n","        self.max_source_length = max_source_length\n","        self.max_target_length = max_target_length\n","        self.pad_to_max_length = pad_to_max_length\n","        self.ignore_pad_token_for_loss = ignore_pad_token_for_loss\n","\n","\n","def create_tokenizer(model_name_or_path: str = 'gpt2') -> AutoTokenizer:\n","    \"\"\"\n","    Creates a tokenizer for a given model, setting special tokens if they are not already set.\n","\n","    :param model_name_or_path: The name or path of the model to load the tokenizer for.\n","    :return: An instance of AutoTokenizer with special tokens set.\n","    \"\"\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","    special_tokens_dict = {'pad_token': '<pad>'}\n","    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n","\n","    # Update the model embeddings with the new number of tokens\n","    if num_added_toks > 0:\n","        tokenizer.model_max_length += num_added_toks\n","\n","    return tokenizer\n","\n","\n","def advanced_data_loader(input: Union[str, Dict[str, str]], format: Optional[str] = None) -> Optional[Any]:\n","    \"\"\"\n","    Loads a dataset from a given input path or dictionary specifying file paths.\n","\n","    :param input: A string representing the dataset name or directory, or a dictionary containing file paths.\n","    :param format: The format of the dataset if loading from a file (e.g., 'csv' or 'json').\n","    :return: A loaded dataset or None in case of failure.\n","    \"\"\"\n","    try:\n","        if isinstance(input, dict) and format in ['csv', 'json']:\n","            dataset = load_dataset(format, data_files=input)\n","        elif isinstance(input, str) and format == 'text':\n","            dataset = load_dataset(format, data_dir=input)\n","        elif isinstance(input, str) and format is None:\n","            dataset = load_dataset(input)\n","        else:\n","            warnings.warn(\"Invalid input or format. Please provide a valid dataset name, directory, or file paths.\")\n","            return None\n","    except FileNotFoundError as e:\n","        warnings.warn(str(e))\n","        return None\n","\n","    print(\"Splits: \", dataset.keys())\n","    print(\"Columns: \", dataset.column_names)\n","    return dataset\n","\n","\n","def preprocess_function(examples: Dict[str, list], tokenizer: AutoTokenizer, args: DataArguments, mode: str = 'train') -> Dict[str, torch.Tensor]:\n","    \"\"\"\n","    Preprocesses the data by tokenizing and applying the prompt process for training or testing.\n","\n","    :param examples: A dictionary of examples with source and target texts.\n","    :param tokenizer: The tokenizer to use for tokenization.\n","    :param args: DataArguments containing preprocessing configurations.\n","    :param mode: The mode of preprocessing, can be 'train' or 'test'.\n","    :return: A dictionary of model inputs with tokenized data.\n","    \"\"\"\n","    task_prompt = \"\\nAnswer the above question. First think step by step and then answer the final number.\\n\"\n","\n","    def prompt_process(sent_1: str, sent_2: str, prompt_1: str = \"\", prompt_2: str = \"\", prompt_3: str = \"\") -> str:\n","        sent_2 = sent_2.replace(\"####\", \"The final answer is\")\n","        return prompt_1 + sent_1 + prompt_2 + sent_2 + prompt_3\n","\n","    source_column, target_column = \"question\", \"answer\"\n","    padding = \"max_length\" if args.pad_to_max_length else False\n","\n","    if mode == 'train':\n","        inputs = [prompt_process(source, target, prompt_2=task_prompt) for source, target in zip(examples[source_column], examples[target_column])]\n","    else:  # mode == 'test'\n","        inputs = [source + task_prompt for source in examples[source_column]]\n","\n","    model_inputs = tokenizer(inputs, max_length=args.max_source_length, padding=padding, truncation=True, return_tensors=\"pt\")\n","\n","    if mode == 'train':\n","        # Prepare labels for loss calculation\n","        labels = copy.deepcopy(model_inputs[\"input_ids\"])\n","        if args.pad_to_max_length and args.ignore_pad_token_for_loss:\n","            labels = torch.tensor(labels)\n","            labels[labels == tokenizer.pad_token_id] = -100\n","        model_inputs[\"labels\"] = labels\n","\n","    return model_inputs"],"metadata":{"id":"BDdUY3ZEDwcH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import warnings\n","from typing import Union, Optional, Dict, Any\n","from datasets import load_dataset, DatasetDict\n","from transformers import AutoTokenizer\n","import torch\n","from torch.utils.data import DataLoader\n","\n","class DataArguments:\n","    # ... (same as before)\n","\n","def create_tokenizer(model_name_or_path: str = 'gpt2') -> AutoTokenizer:\n","    # ... (same as before)\n","\n","def advanced_data_loader(input: Union[str, Dict[str, str]], format: Optional[str] = None, split_ratios: Optional[Dict[str, float]] = None) -> Optional[DatasetDict]:\n","    \"\"\"\n","    Loads a dataset from a given input path or dictionary specifying file paths and splits it.\n","\n","    :param input: A string representing the dataset name or directory, or a dictionary containing file paths.\n","    :param format: The format of the dataset if loading from a file (e.g., 'csv' or 'json').\n","    :param split_ratios: A dictionary with keys 'train', 'test', and 'eval' containing split ratios.\n","    :return: A loaded and split dataset or None in case of failure.\n","    \"\"\"\n","    if split_ratios is None:\n","        split_ratios = {'train': 0.8, 'test': 0.1, 'eval': 0.1}\n","\n","    try:\n","        # Load the dataset\n","        if isinstance(input, dict) and format in ['csv', 'json']:\n","            dataset = load_dataset(format, data_files=input)\n","        elif isinstance(input, str) and format == 'text':\n","            dataset = load_dataset(format, data_dir=input)\n","        elif isinstance(input, str) and format is None:\n","            dataset = load_dataset(input)\n","        else:\n","            warnings.warn(\"Invalid input or format. Please provide a valid dataset name, directory, or file paths.\")\n","            return None\n","    except FileNotFoundError as e:\n","        warnings.warn(str(e))\n","        return None\n","\n","    # Split the dataset\n","    if dataset:\n","        split_dataset = dataset['train'].train_test_split(test_size=split_ratios['test'] + split_ratios['eval'])\n","        test_eval_dataset = split_dataset['test'].train_test_split(test_size=split_ratios['eval'] / (split_ratios['test'] + split_ratios['eval']))\n","        dataset = DatasetDict({\n","            'train': split_dataset['train'],\n","            'test': test_eval_dataset['train'],\n","            'eval': test_eval_dataset['test']\n","        })\n","\n","    print(\"Splits: \", dataset.keys())\n","    print(\"Columns: \", {split: dataset[split].column_names for split in dataset.keys()})\n","    return dataset\n","\n","def preprocess_function(examples: Dict[str, list], tokenizer: AutoTokenizer, args: DataArguments, mode: str = 'train') -> Dict[str, torch.Tensor]:\n","    # ... (same as before)\n","\n","# Example of how to use the updated functions\n","args = DataArguments()\n","tokenizer = create_tokenizer(args.model_name_or_path)\n","\n","dataset = advanced_data_loader(\"path_or_name_of_your_dataset\", split_ratios={'train': 0.8, 'test': 0.1, 'eval': 0.1})\n","if dataset is not None:\n","    processed_datasets = {split: preprocess_function(dataset[split], tokenizer, args, mode=split) for split in dataset.keys()}\n","\n","    # Create PyTorch DataLoader for each split\n","    data_loaders = {split: DataLoader(processed_datasets[split], batch_size=8) for split in processed_datasets}"],"metadata":{"id":"7kHQRCCoFBre"},"execution_count":null,"outputs":[]}]}
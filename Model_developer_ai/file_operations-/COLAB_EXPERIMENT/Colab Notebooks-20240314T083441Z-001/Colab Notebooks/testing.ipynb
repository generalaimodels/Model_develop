{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN1PWaBk8AfVoWJ+5AUT99B"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install torch torchvision foolbox\n","!pip install gym\n","!pip install stable-baselines3\n","!pip install matplotlib\n","!pip install shimmy\n"],"metadata":{"id":"BGFgBLWE2BTH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nkNW69Pe1jQk"},"outputs":[],"source":["import torch\n","# import torchvision.models as models\n","# import torchvision.datasets as datasets\n","# import torchvision.transforms as transforms\n","# from foolbox import PyTorchModel, accuracy\n","# from foolbox.attacks import LinfFGSM, LinfPGD\n","# import gym\n","# from gym import spaces\n","# from stable_baselines3 import PPO\n","# import matplotlib.pyplot as plt\n","\n","# # Custom Environment for Adversarial Attack and Defense\n","# class AdversarialEnv(gym.Env):\n","#     def __init__(self, model, attacks, defenses, data_loader):\n","#         super().__init__()\n","\n","#         self.model = model\n","#         self.attacks = attacks\n","#         self.defenses = defenses\n","#         self.data_loader = iter(data_loader)\n","#         self.action_space = spaces.Discrete(len(attacks) * len(defenses))\n","#         self.observation_space = spaces.Box(low=0, high=1, shape=(3, 32, 32), dtype=float)\n","\n","#         self.state, self.current_label = next(self.data_loader)\n","\n","#     def step(self, action):\n","#         attack_idx = action // len(self.defenses)\n","#         defense_idx = action % len(self.defenses)\n","\n","#         # Apply the selected attack and defense\n","#         adversarial_example = self.attacks[attack_idx](self.model, self.state)\n","#         defended_example = self.defenses[defense_idx](adversarial_example)\n","\n","#         # Evaluate the model's performance\n","#         prediction = self.model(defended_example)\n","#         reward = -accuracy(self.model, prediction, self.current_label)\n","\n","#         self.state = defended_example\n","#         done = False\n","\n","#         return self.state, reward, done, {}\n","\n","#     def reset(self):\n","#         self.state, self.current_label = next(self.data_loader)\n","#         return self.state\n","\n","# # Load Pretrained Model\n","# model = models.resnet18(pretrained=True)\n","\n","# # Load CIFAR-10 Dataset\n","# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","# trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","# trainloader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=True)\n","\n","# # Define Attacks\n","# attacks = [LinfFGSM(), LinfPGD()]\n","\n","# # Define Defenses\n","# defenses = [lambda x: x] # Sample defense; real defenses should be implemented\n","\n","# # Create Environment\n","# env = AdversarialEnv(model, attacks, defenses, trainloader)\n","\n","# # Train the Agent using PPO\n","# agent = PPO(\"MlpPolicy\", env, verbose=1)\n","# agent.learn(total_timesteps=10000)\n","\n","# # Visualization\n","# rewards = [] # Collect rewards during training\n","# plt.plot(rewards)\n","# plt.xlabel('Iterations')\n","# plt.ylabel('Reward')\n","# plt.show()\n"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","import gym\n","from gym import spaces\n","from stable_baselines3 import PPO\n","import matplotlib.pyplot as plt\n","\n","# Model Definition\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 16 * 5 * 5)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","# Attack Functions\n","def fgsm_attack(image, epsilon, data_grad):\n","    sign_data_grad = data_grad.sign()\n","    perturbed_image = image + epsilon * sign_data_grad\n","    return torch.clamp(perturbed_image, 0, 1)\n","\n","def noise_attack(image, epsilon):\n","    noise = torch.rand_like(image) * epsilon\n","    perturbed_image = image + noise\n","    return torch.clamp(perturbed_image, 0, 1)\n","\n","def gaussian_attack(image, epsilon):\n","    noise = torch.randn_like(image) * epsilon\n","    perturbed_image = image + noise\n","    return torch.clamp(perturbed_image, 0, 1)\n","\n","# ... Add more attacks ...\n","\n","# Defense Functions\n","def gaussian_smoothing(image, std_dev):\n","    gaussian_filter = torch.tensor([[1, 2, 1], [2, 4, 2], [1, 2, 1]]) / 16.0\n","    gaussian_filter = gaussian_filter.view(1, 1, 3, 3).repeat(3, 1, 1, 1).to(image.device)\n","    smoothed_image = nn.functional.conv2d(image, gaussian_filter, padding=1, groups=3)\n","    return smoothed_image\n","\n","def clip_values(image, clip_value):\n","    return torch.clamp(image, clip_value, 1 - clip_value)\n","\n","def reduce_quality(image, factor):\n","    reduced_image = nn.functional.interpolate(image, scale_factor=factor, mode='bilinear')\n","    return nn.functional.interpolate(reduced_image, scale_factor=1/factor, mode='bilinear')\n","\n","# ... Add more defenses ...\n","\n","# Custom Gym Environment\n","class AdversarialEnv(gym.Env):\n","    def __init__(self, model, attacks, defenses, data_loader):\n","        super().__init__()\n","\n","        self.model = model\n","        self.attacks = attacks\n","        self.defenses = defenses\n","        self.data_loader = iter(data_loader)\n","        self.action_space = spaces.Discrete(len(attacks) * len(defenses))\n","        self.observation_space = spaces.Box(low=0, high=1, shape=(3, 32, 32), dtype=float)\n","\n","        self.state, self.current_label = next(self.data_loader)\n","\n","    def step(self, action):\n","        attack_idx = action // len(self.defenses)\n","        defense_idx = action % len(self.defenses)\n","\n","        # Apply the selected attack and defense\n","        adversarial_example = self.attacks[attack_idx](self.model, self.state)\n","        defended_example = self.defenses[defense_idx](adversarial_example)\n","\n","        # Evaluate the model's performance\n","        prediction = self.model(defended_example)\n","        reward = -accuracy(self.model, prediction, self.current_label)\n","\n","        self.state = defended_example\n","        done = False\n","\n","        return self.state, reward, done, {}\n","\n","    def reset(self):\n","        self.state, self.current_label = next(self.data_loader)\n","        return self.state\n","\n","# Training and Evaluation\n","# ... Include training and evaluation similar to previous code ...\n","\n","# Visualization\n","# ... Include visualization similar to previous code ...\n"],"metadata":{"id":"jllUBACZ3xsE","executionInfo":{"status":"ok","timestamp":1691648308765,"user_tz":-330,"elapsed":10,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!pip install shimmy"],"metadata":{"id":"ibGZ6FKc4U_n","executionInfo":{"status":"aborted","timestamp":1691648395924,"user_tz":-330,"elapsed":7,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","import torch.nn.functional as F\n","\n","# Loss and Optimizer\n","model = SimpleCNN()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","# CIFAR-10 Data Loading\n","transform = transforms.Compose([transforms.ToTensor()])\n","trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n","testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)\n","\n","# Training the Model\n","for epoch in range(2):\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        if i % 2000 == 1999:\n","            print(f'Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {running_loss / 2000}')\n","            running_loss = 0.0\n","\n","print('Finished Training')\n","\n","# Defining the Attacks and Defenses\n","attacks = [\n","    fgsm_attack,\n","    noise_attack,\n","    gaussian_attack,\n","    # ... add more ...\n","]\n","\n","defenses = [\n","    gaussian_smoothing,\n","    clip_values,\n","    reduce_quality,\n","    # ... add more ...\n","]\n","\n","# Creating the Environment\n","env = AdversarialEnv(model, attacks, defenses, testloader)\n","\n","# Train the Agent using PPO\n","agent = PPO(\"MlpPolicy\", env, verbose=1)\n","agent.learn(total_timesteps=10000)\n","\n","# Visualization\n","rewards = [] # Collect rewards during training (modify the environment or RL training to log these)\n","plt.plot(rewards)\n","plt.xlabel('Iterations')\n","plt.ylabel('Reward')\n","plt.show()\n"],"metadata":{"id":"NoJdIufr37Ud"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import gym\n","from gym import spaces\n","from stable_baselines3 import PPO\n","import matplotlib.pyplot as plt\n","\n","# Defense Parameters\n","defense_parameters = {\n","    \"gaussian_smoothing\": {\"std_dev\": 1.0},\n","    \"clip_values\": {\"clip_value\": 0.05},\n","    \"reduce_quality\": {\"factor\": 0.5}\n","}\n","\n","\n","    # ... rest of code ...\n","\n","# Model Definition\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 16 * 5 * 5)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","# Attack Functions\n","def fgsm_attack(model, image, epsilon, label):\n","    image.requires_grad = True\n","    output = model(image)\n","    loss = nn.CrossEntropyLoss()(output, label)\n","    model.zero_grad()\n","    loss.backward()\n","    data_grad = image.grad.data\n","    perturbed_image = image + epsilon * data_grad.sign()\n","    return torch.clamp(perturbed_image, 0, 1)\n","\n","def noise_attack(model, image, epsilon, label):\n","    noise = torch.rand_like(image) * epsilon\n","    perturbed_image = image + noise\n","    return torch.clamp(perturbed_image, 0, 1)\n","\n","def gaussian_attack(model, image, epsilon, label):\n","    noise = torch.randn_like(image) * epsilon\n","    perturbed_image = image + noise\n","    return torch.clamp(perturbed_image, 0, 1)\n","\n","# Defense Functions\n","def gaussian_smoothing(image, std_dev):\n","    gaussian_filter = torch.tensor([[1, 2, 1], [2, 4, 2], [1, 2, 1]]) / 16.0\n","    gaussian_filter = gaussian_filter.view(1, 1, 3, 3).repeat(3, 1, 1, 1).to(image.device)\n","    smoothed_image = nn.functional.conv2d(image, gaussian_filter, padding=1, groups=3)\n","    return smoothed_image\n","\n","def clip_values(image, clip_value):\n","    return torch.clamp(image, clip_value, 1 - clip_value)\n","\n","def reduce_quality(image, factor):\n","    reduced_image = nn.functional.interpolate(image, scale_factor=factor, mode='bilinear')\n","    return nn.functional.interpolate(reduced_image, scale_factor=1/factor, mode='bilinear')\n","\n","# Custom Gym Environment\n","class AdversarialEnv(gym.Env):\n","    def __init__(self, model, attacks, defenses, data_loader, epsilon):\n","        super().__init__()\n","\n","        self.model = model\n","        self.attacks = attacks\n","        self.defenses = defenses\n","        self.data_loader = iter(data_loader)\n","        self.action_space = spaces.Discrete(len(attacks) * len(defenses))\n","        self.observation_space = spaces.Box(low=0, high=1, shape=(3, 32, 32), dtype=float)\n","        self.epsilon = epsilon\n","\n","        self.state, self.current_label = next(self.data_loader)\n","        self.state = self.state.float().unsqueeze(0)\n","\n","    def step(self, action):\n","        attack_idx = action // len(self.defenses)\n","        defense_idx = action % len(self.defenses)\n","\n","        # Apply the selected attack\n","        adversarial_example = self.attacks[attack_idx](self.model, self.state, self.epsilon, self.current_label)\n","\n","        # Retrieve the correct defense function and its parameters\n","        defense_func = self.defenses[defense_idx]\n","        defense_name = defense_func.__name__\n","        defense_params = self.defense_parameters[defense_name]\n","\n","        # Apply the selected defense with the correct parameters\n","        defended_example = defense_func(adversarial_example, **defense_params)\n","\n","\n","        # Evaluate the model's performance\n","        prediction = self.model(defended_example)\n","        reward = -F.cross_entropy(prediction, self.current_label).item()\n","\n","        self.state, self.current_label = next(self.data_loader)\n","        self.state = self.state.float().unsqueeze(0)\n","        done = False\n","\n","        return self.state, reward, done, {}\n","\n","    def reset(self):\n","        self.state, self.current_label = next(self.data_loader)\n","        self.state = self.state.float().unsqueeze(0)\n","        return self.state\n","\n","# Loss and Optimizer\n","model = SimpleCNN()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","# CIFAR-10 Data Loading\n","transform = transforms.Compose([transforms.ToTensor()])\n","trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n","testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G-3Ritz76Ami","executionInfo":{"status":"ok","timestamp":1691647602778,"user_tz":-330,"elapsed":2711,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"32082294-65c2-4ac9-b32e-c240bbcb257e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["# ... continuing the code\n","\n","testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)\n","\n","# Training the Model\n","for epoch in range(2):\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        if i % 2000 == 1999:\n","            print(f'Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {running_loss / 2000}')\n","            running_loss = 0.0\n","\n","print('Finished Training')\n","\n","# Defining the Attacks and Defenses\n","attacks = [\n","    fgsm_attack,\n","    noise_attack,\n","    gaussian_attack,\n","    # ... add more ...\n","]\n","\n","defenses = [\n","    gaussian_smoothing,\n","    clip_values,\n","    reduce_quality,\n","    # ... add more ...\n","]\n","\n","# Creating the Environment\n","epsilon = 0.1\n","env = AdversarialEnv(model, attacks, defenses, testloader, epsilon)\n","\n","# Train the Agent using PPO\n","agent = PPO(\"MlpPolicy\", env, verbose=1)\n","agent.learn(total_timesteps=10000)\n","\n","# Visualization (modify the environment or RL training to log the rewards for visualization)\n","rewards = []  # Collect rewards during training\n","plt.plot(rewards)\n","plt.xlabel('Iterations')\n","plt.ylabel('Reward')\n","plt.show()\n","\n","print('Experiment Complete!')\n"],"metadata":{"id":"PLri5pCf6JD5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = AdversarialEnv(model, attacks, defenses, defense_parameters, testloader, epsilon)\n"],"metadata":{"id":"Ja4ra5TI8BMP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","- Line 1: Define the class name and the parent class.\n","- Line 2: Define the constructor method that takes an argument epsilon, which is a parameter for the adversarial attack.\n","- Line 3: Call the constructor method of the parent class to initialize the environment.\n","- Line 5: Create an iterator that loads the CIFAR10 dataset, which is a collection of 60,000 color images of size 32x32 pixels, belonging to 10 classes. The iterator returns one image and its label at a time, in random order. The images are transformed to tensors, which are multidimensional arrays of numbers.\n","- Line 6: Define the action space as a discrete space with two possible actions: 0 or 1. The action represents whether to apply a defense mechanism or not.\n","- Line 7: Define the observation space as a box space with values between 0 and 1, and shape (3, 32, 32). The observation represents the image that is fed to the model.\n","- Line 8: Assign the epsilon argument to an instance variable with the same name.\n","- Line 10: Get the first image and label from the iterator and assign them to instance variables state and label. The state variable will be used as the initial observation of the environment.\n","- Line 11: Convert the state variable to a float tensor, which is a tensor with floating-point numbers.\n","- Line 13: Define the step method that takes an argument action, which is an integer representing the action chosen by the agent. The step method returns four values: observation, reward, done, and info. The observation is the next state of the environment after applying the action. The reward is a scalar value that measures how well the agent performed. The done is a boolean value that indicates whether the episode has ended. The info is a dictionary that contains additional information about the environment.\n","- Line 14: Apply a fast gradient sign method (FGSM) attack to the state variable, using the epsilon instance variable as a parameter. The FGSM attack is a method of generating adversarial examples, which are slightly modified inputs that can fool a machine learning model. The FGSM attack adds a small perturbation to each pixel of the input image, in the direction of increasing the loss function of the model. The result is assigned to a local variable named adversarial_example.\n","- Line 15: Apply a defense mechanism to the adversarial_example variable, depending on the action argument. If the action is 0, apply a clipping function that limits each pixel value to be within [0.05, 0.95]. This can reduce the effect of the perturbation added by the FGSM attack. If the action is 1, do nothing and keep the adversarial_example as it is. The result is assigned to a local variable named defended_example.\n","- Lines 17-28: Visualize and display the original image, the adversarial example, and the defended example using matplotlib.pyplot library. This library provides functions for creating and manipulating plots and figures. The squeeze method removes any dimensions of size one from the tensors. The permute method changes the order of dimensions of the tensors. The detach method detaches the tensors from any computation graph they belong to. The numpy method converts the tensors to numpy arrays, which are compatible with matplotlib.pyplot functions.\n","- Line 30: Feed the defended_example variable to a model variable, which is assumed to be a pretrained neural network that can classify images. The result is assigned to a local variable named prediction, which is a tensor of probabilities for each class.\n","- Line 31: Compute and assign the negative log-likelihood loss between the prediction variable and the label variable to a local variable named reward. This loss function measures how well the prediction matches the true label. The negative sign makes sure that higher probabilities lead to higher rewards. The item method converts the tensor to a scalar value.\n","- Line 33: Get the next image and label from the iterator and assign them to instance variables state and label. These will be used as the next observation of the environment.\n","- Line 34: Convert the state variable to a float tensor.\n","- Line 35: Assign False to a local variable named done, indicating that there is no termination condition for this environment.\n","- Line 37: Return four values: state, reward, done, and an empty dictionary as info.\n","- Line 39: Define the reset method that returns an initial observation for a new episode.\n","- Line 40: Get an image and label from the iterator and assign them to instance variables state and label.\n","- Line 41: Convert the state variable to a float tensor.\n","- Line 42: Return state as an initial observation.\n","\n"],"metadata":{"id":"0ux3BjBb5epH"}},{"cell_type":"code","source":["import torch.optim as optim\n","import torch.nn.functional as F\n","\n","# Loss and Optimizer\n","model = SimpleCNN()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","# CIFAR-10 Data Loading\n","transform = transforms.Compose([transforms.ToTensor()])\n","trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n","testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)\n"],"metadata":{"id":"VPrw_j4tG1Fc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from stable_baselines3 import PPO\n","import matplotlib.pyplot as plt\n","import gym\n","from gym import spaces\n","import numpy as np\n","\n","# Predefined ResNet Model\n","model = models.resnet18(pretrained=True)\n","model.eval()\n","\n","# Attack Functions\n","def fgsm_attack(image, epsilon):\n","    image.requires_grad = True\n","    output = model(image)\n","    loss = F.nll_loss(output, torch.argmax(output, dim=1))\n","    model.zero_grad()\n","    loss.backward()\n","    data_grad = image.grad.data\n","    perturbed_image = image + epsilon * data_grad.sign()\n","    return perturbed_image\n","\n","# Defense Functions\n","def clip_values(image, clip_value):\n","    return torch.clamp(image, clip_value, 1 - clip_value)\n","\n","# Custom Gym Environment\n","class AdversarialEnv(gym.Env):\n","    def __init__(self, epsilon):\n","        super().__init__()\n","\n","        self.data_loader = iter(torch.utils.data.DataLoader(datasets.CIFAR10(root='./data', train=False, transform=transforms.ToTensor()), batch_size=1, shuffle=False))\n","        self.action_space = spaces.Discrete(2) # 1 attack * 1 defense\n","        self.observation_space = spaces.Box(low=0, high=1, shape=(3, 32, 32), dtype=np.float32)\n","        self.epsilon = epsilon\n","\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.float()\n","\n","        self.original_image = self.state.clone() # Store the original image to keep it the same all the time\n","\n","    def step(self, action):\n","        adversarial_example = fgsm_attack(self.original_image, self.epsilon)\n","        defended_example = clip_values(adversarial_example, 0.05) if action == 0 else adversarial_example\n","\n","        # Visualization\n","        plt.imshow(self.original_image.squeeze().permute(1, 2, 0).detach().numpy())\n","        plt.title(\"Original Image\")\n","        plt.show()\n","\n","        plt.imshow(adversarial_example.squeeze().detach().permute(1, 2, 0).numpy())\n","        plt.title(\"Adversarial Example\")\n","        plt.show()\n","\n","        plt.imshow(defended_example.squeeze().detach().permute(1, 2, 0).numpy())\n","        plt.title(\"Defended Example\")\n","        plt.show()\n","\n","        prediction = model(defended_example)\n","        reward = -F.nll_loss(prediction, torch.argmax(prediction, dim=1)).item()\n","\n","        done = False\n","\n","        return self.original_image, reward, done, {}\n","\n","    def reset(self):\n","        return self.original_image\n","\n","# Check the environment\n","epsilon = 0.1\n","env = AdversarialEnv(epsilon)\n","\n","# Train the Agent using PPO\n","agent = PPO(\"MlpPolicy\", env, verbose=1)\n","agent.learn(total_timesteps=5000)\n"],"metadata":{"id":"UmCfht9dIJ_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import torch\n","import torch.nn.functional as F\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.env_checker import check_env\n","import matplotlib.pyplot as plt\n","import gym\n","from gym import spaces\n","import numpy as np\n","def fgsm_attack(image, epsilon, data_grad):\n","    sign_data_grad = data_grad.sign()\n","    perturbed_image = image + epsilon * sign_data_grad\n","    return torch.clamp(perturbed_image, 0, 1)\n","\n","def noise_attack(image, epsilon):\n","    noise = torch.rand_like(image) * epsilon\n","    perturbed_image = image + noise\n","    return torch.clamp(perturbed_image, 0, 1)\n","\n","def gaussian_attack(image, epsilon):\n","    noise = torch.randn_like(image) * epsilon\n","    perturbed_image = image + noise\n","    return torch.clamp(perturbed_image, 0, 1)\n","\n","# ... Add more attacks ...\n","\n","# Defense Functions\n","def gaussian_smoothing(image, std_dev):\n","    gaussian_filter = torch.tensor([[1, 2, 1], [2, 4, 2], [1, 2, 1]]) / 16.0\n","    gaussian_filter = gaussian_filter.view(1, 1, 3, 3).repeat(3, 1, 1, 1).to(image.device)\n","    smoothed_image = nn.functional.conv2d(image, gaussian_filter, padding=1, groups=3)\n","    return smoothed_image\n","\n","def clip_values(image, clip_value):\n","    return torch.clamp(image, clip_value, 1 - clip_value)\n","\n","def reduce_quality(image, factor):\n","    reduced_image = nn.functional.interpolate(image, scale_factor=factor, mode='bilinear')\n","    return nn.functional.interpolate(reduced_image, scale_factor=1/factor, mode='bilinear')\n","class AdversarialEnv(gym.Env):\n","    def __init__(self, epsilon):\n","        super().__init__()\n","\n","        self.data_loader = iter(torch.utils.data.DataLoader(datasets.CIFAR10(root='./data', train=False, transform=transforms.ToTensor()), batch_size=1, shuffle=False))\n","        self.action_space = spaces.Discrete(2) # 1 attack * 1 defense\n","        self.observation_space = spaces.Box(low=0, high=1, shape=(3, 32, 32), dtype=np.float32)\n","        self.epsilon = epsilon\n","\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.float()\n","\n","    def step(self, action):\n","        adversarial_example = fgsm_attack(self.state, self.epsilon)\n","        defended_example = clip_values(adversarial_example, 0.05) if action == 0 else adversarial_example\n","        # Visualization\n","        plt.imshow(self.state.squeeze().permute(1, 2, 0).detach().numpy())\n","        plt.title(\"Original Image\")\n","        plt.show()\n","\n","        plt.imshow(adversarial_example.squeeze().detach().permute(1, 2, 0).numpy())\n","        plt.title(\"Adversarial Example\")\n","        plt.show()\n","\n","        plt.imshow(defended_example.squeeze().detach().permute(1, 2, 0).numpy())\n","        plt.title(\"Defended Example\")\n","        plt.show()\n","\n","\n","        # # Visualization\n","        # plt.imshow(self.state.squeeze().permute(1, 2, 0))\n","        # plt.title(\"Original Image\")\n","        # plt.show()\n","\n","        # plt.imshow(adversarial_example.squeeze().detach().permute(1, 2, 0))\n","        # plt.title(\"Adversarial Example\")\n","        # plt.show()\n","\n","        # plt.imshow(defended_example.squeeze().detach().permute(1, 2, 0))\n","        # plt.title(\"Defended Example\")\n","        # plt.show()\n","\n","        prediction = model(defended_example)\n","\n","        reward = -F.nll_loss(prediction, torch.argmax(prediction, dim=1)).item()\n","        print(\"Reward value :\",reward )\n","\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.float()\n","        done = False\n","\n","        return self.state, reward, done, {}\n","\n","    def reset(self):\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.float()\n","        return self.state"],"metadata":{"id":"NYigD5eXMFKy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"ntU07ftnx1j0"}},{"cell_type":"code","source":["\n","import torch\n","import torch.nn.functional as F\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.env_checker import check_env\n","import matplotlib.pyplot as plt\n","import gym\n","from gym import spaces\n","import numpy as np\n","\n","# Predefined ResNet Model\n","model = models.resnet18(pretrained=True)\n","model.eval()\n","\n","# Attack Functions\n","def fgsm_attack(image, epsilon):\n","    image.requires_grad = True\n","    output = model(image)\n","    loss = F.nll_loss(output, torch.argmax(output, dim=1))\n","    model.zero_grad()\n","    loss.backward()\n","    data_grad = image.grad.data\n","    perturbed_image = image + epsilon * data_grad.sign()\n","    return perturbed_image\n","\n","# Defense Functions\n","def clip_values(image, clip_value):\n","    return torch.clamp(image, clip_value, 1 - clip_value)\n","\n","# Custom Gym Environment\n","class AdversarialEnv(gym.Env):\n","    def __init__(self, epsilon):\n","        super().__init__()\n","\n","        self.data_loader = iter(torch.utils.data.DataLoader(datasets.CIFAR10(root='./data', train=False, transform=transforms.ToTensor()), batch_size=1, shuffle=False))\n","        self.action_space = spaces.Discrete(2) # 1 attack * 1 defense\n","        self.observation_space = spaces.Box(low=0, high=1, shape=(3, 32, 32), dtype=np.float32)\n","        self.epsilon = epsilon\n","\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.float()\n","\n","    def step(self, action):\n","        adversarial_example = fgsm_attack(self.state, self.epsilon)\n","        defended_example = clip_values(adversarial_example, 0.05) if action == 0 else adversarial_example\n","        # Visualization\n","        plt.imshow(self.state.squeeze().permute(1, 2, 0).detach().numpy())\n","        plt.title(\"Original Image\")\n","        plt.show()\n","\n","        plt.imshow(adversarial_example.squeeze().detach().permute(1, 2, 0).numpy())\n","        plt.title(\"Adversarial Example\")\n","        plt.show()\n","\n","        plt.imshow(defended_example.squeeze().detach().permute(1, 2, 0).numpy())\n","        plt.title(\"Defended Example\")\n","        plt.show()\n","\n","\n","        # # Visualization\n","        # plt.imshow(self.state.squeeze().permute(1, 2, 0))\n","        # plt.title(\"Original Image\")\n","        # plt.show()\n","\n","        # plt.imshow(adversarial_example.squeeze().detach().permute(1, 2, 0))\n","        # plt.title(\"Adversarial Example\")\n","        # plt.show()\n","\n","        # plt.imshow(defended_example.squeeze().detach().permute(1, 2, 0))\n","        # plt.title(\"Defended Example\")\n","        # plt.show()\n","\n","        prediction = model(defended_example)\n","\n","        reward = -F.nll_loss(prediction, torch.argmax(prediction, dim=1)).item()\n","        print(\"Reward value :\",reward )\n","\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.float()\n","        done = False\n","\n","        return self.state, reward, done, {}\n","\n","    def reset(self):\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.float()\n","        return self.state\n","\n","# Check the environment\n","epsilon = 0.1\n","env = AdversarialEnv(epsilon)\n","# check_env(env)\n","\n","# Train the Agent using PPO\n","agent = PPO(\"MlpPolicy\", env, verbose=1)\n","agent.learn(total_timesteps=5000)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1IgGM1zDaddAnT9iLQmqz1xiRb_cxRK8K"},"id":"MIfOhCrsKo5q","outputId":"f77f6502-431b-433b-d6e7-8efcbf9dba74"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["# working the code\n","import torch\n","import torch.nn.functional as F\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.env_checker import check_env\n","import matplotlib.pyplot as plt\n","import gym\n","from gym import spaces\n","import numpy as np\n","\n","# Predefined ResNet Model\n","model = models.resnet18(pretrained=True)\n","model.eval()\n","\n","# Attack Functions\n","def fgsm_attack(image, epsilon):\n","    image.requires_grad = True\n","    output = model(image)\n","    loss = F.nll_loss(output, torch.argmax(output, dim=1))\n","    model.zero_grad()\n","    loss.backward()\n","    data_grad = image.grad.data\n","    perturbed_image = image + epsilon * data_grad.sign()\n","    return perturbed_image\n","\n","# Defense Functions\n","def clip_values(image, clip_value):\n","    return torch.clamp(image, clip_value, 1 - clip_value)\n","\n","# Custom Gym Environment\n","class AdversarialEnv(gym.Env):\n","    def __init__(self, epsilon):\n","        super().__init__()\n","\n","        self.data_loader = iter(torch.utils.data.DataLoader(datasets.CIFAR10(root='./data', train=False, transform=transforms.ToTensor()), batch_size=1, shuffle=False))\n","        self.action_space = spaces.Discrete(2) # 1 attack * 1 defense\n","        self.observation_space = spaces.Box(low=0, high=1, shape=(3, 32, 32), dtype=np.float32)\n","        self.epsilon = epsilon\n","\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.float()\n","\n","    def step(self, action):\n","        adversarial_example = fgsm_attack(self.state, self.epsilon)\n","        defended_example = clip_values(adversarial_example, 0.05) if action == 0 else adversarial_example\n","\n","        prediction = model(defended_example)\n","        reward = -F.nll_loss(prediction, torch.argmax(prediction, dim=1)).item()\n","\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.float()\n","        done = False\n","\n","        return self.state, reward, done, {}\n","\n","    def reset(self):\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.float()\n","        return self.state\n","\n","# Check the environment\n","epsilon = 0.1\n","env = AdversarialEnv(epsilon)\n","# check_env(env)\n","\n","# Train the Agent using PPO\n","agent = PPO(\"MlpPolicy\", env, verbose=1)\n","print(agent.learn(total_timesteps=5000))\n","\n","# Visualization\n","rewards = [] # Modify the environment or RL training to log the rewards for visualization\n","plt.plot(rewards)\n","plt.xlabel('Iterations')\n","plt.ylabel('Reward')\n","plt.show()\n","\n","print('Experiment Complete!')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"wFG7g33D9A6e","executionInfo":{"status":"ok","timestamp":1691598844800,"user_tz":-330,"elapsed":265983,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"cf6ca298-3f74-471d-c811-b32573ff331d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Using cuda device\n","Wrapping the env with a `Monitor` wrapper\n","Wrapping the env in a DummyVecEnv.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["-----------------------------\n","| time/              |      |\n","|    fps             | 23   |\n","|    iterations      | 1    |\n","|    time_elapsed    | 87   |\n","|    total_timesteps | 2048 |\n","-----------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 23          |\n","|    iterations           | 2           |\n","|    time_elapsed         | 176         |\n","|    total_timesteps      | 4096        |\n","| train/                  |             |\n","|    approx_kl            | 0.010453736 |\n","|    clip_fraction        | 0.0255      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.685      |\n","|    explained_variance   | -0.00278    |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.93e+03    |\n","|    n_updates            | 10          |\n","|    policy_gradient_loss | -0.00357    |\n","|    value_loss           | 4.23e+03    |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 23          |\n","|    iterations           | 3           |\n","|    time_elapsed         | 263         |\n","|    total_timesteps      | 6144        |\n","| train/                  |             |\n","|    approx_kl            | 0.002352026 |\n","|    clip_fraction        | 0.000635    |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.671      |\n","|    explained_variance   | -2.62e-06   |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.11e+03    |\n","|    n_updates            | 20          |\n","|    policy_gradient_loss | -0.00148    |\n","|    value_loss           | 4.65e+03    |\n","-----------------------------------------\n","<stable_baselines3.ppo.ppo.PPO object at 0x7af9a5d82ce0>\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAksAAAGwCAYAAAC5ACFFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAq80lEQVR4nO3de3RU5aH38d8kIQGBmXDNGEgMlFuACBpKDB6FY7KMlx6goNIcQEg5cKjgDaSCUqitNd5A8ErbcziIgnAAiy0HsTR4hRggoBJuWuUeJoCYCRdJQvK8f/gydSQ8JukMk8HvZ629JHuenXmevSLzXTt7BocxxggAAAA1igj1BAAAABoyYgkAAMCCWAIAALAglgAAACyIJQAAAAtiCQAAwIJYAgAAsIgK9QQuBdXV1SouLlbz5s3lcDhCPR0AAFALxhidOHFC8fHxioi48PUjYikAiouLlZCQEOppAACAejhw4IDat29/wceJpQBo3ry5pG9OttPpDPFsAABAbZSVlSkhIcH3On4hxFIAnPvVm9PpJJYAAAgz33cLDTd4AwAAWBBLAAAAFsQSAACABbEEAABgQSwBAABYEEsAAAAWxBIAAIAFsQQAAGBBLAEAAFgQSwAAABbEEgAAgAWxBAAAYEEsAQAAWBBLAAAAFsQSAACABbEEAABgQSwBAABYEEsAAAAWxBIAAIAFsQQAAGBBLAEAAFgQSwAAABbEEgAAgAWxBAAAYEEsAQAAWBBLAAAAFsQSAACABbEEAABgQSwBAABYEEsAAAAWxBIAAIAFsQQAAGBBLAEAAFgQSwAAABbEEgAAgAWxBAAAYEEsAQAAWBBLAAAAFsQSAACABbEEAABgQSwBAABYEEsAAAAWxBIAAIAFsQQAAGARdrH0wgsvKCkpSY0bN1ZaWpo2btxoHb9s2TJ169ZNjRs3VkpKilavXn3BsePHj5fD4dCcOXMCPGsAABCuwiqWli5dqkmTJmnmzJnasmWLevXqpaysLB05cqTG8Rs2bFB2drbGjBmjrVu3avDgwRo8eLCKiorOG/unP/1JH374oeLj44O9DAAAEEbCKpZmz56tsWPHKicnR927d9e8efN02WWXaf78+TWOnzt3rm666SZNmTJFycnJ+u1vf6urr75azz//vN+4Q4cO6e6779aiRYvUqFGji7EUAAAQJsImlioqKlRYWKjMzEzfvoiICGVmZio/P7/GY/Lz8/3GS1JWVpbf+Orqao0cOVJTpkxRjx49ajWX8vJylZWV+W0AAODSFDaxdOzYMVVVVSkuLs5vf1xcnDweT43HeDye7x3/xBNPKCoqSvfcc0+t55KbmyuXy+XbEhIS6rASAAAQTsImloKhsLBQc+fO1YIFC+RwOGp93LRp0+T1en3bgQMHgjhLAAAQSmETS61bt1ZkZKRKSkr89peUlMjtdtd4jNvtto5///33deTIESUmJioqKkpRUVHat2+fJk+erKSkpAvOJSYmRk6n028DAACXprCJpejoaKWmpiovL8+3r7q6Wnl5eUpPT6/xmPT0dL/xkrR27Vrf+JEjR+qTTz7RRx995Nvi4+M1ZcoUvfXWW8FbDAAACBtRoZ5AXUyaNEmjRo1Snz591LdvX82ZM0enTp1STk6OJOnOO+9Uu3btlJubK0m699571b9/f82aNUu33nqrlixZos2bN+sPf/iDJKlVq1Zq1aqV33M0atRIbrdbXbt2vbiLAwAADVJYxdKwYcN09OhRzZgxQx6PR71799aaNWt8N3Hv379fERH/uFjWr18/LV68WNOnT9dDDz2kzp07a+XKlerZs2eolgAAAMKMwxhjQj2JcFdWViaXyyWv18v9SwAAhInavn6HzT1LAAAAoUAsAQAAWBBLAAAAFsQSAACABbEEAABgQSwBAABYEEsAAAAWxBIAAIAFsQQAAGBBLAEAAFgQSwAAABbEEgAAgAWxBAAAYEEsAQAAWBBLAAAAFsQSAACABbEEAABgQSwBAABYEEsAAAAWxBIAAIAFsQQAAGBBLAEAAFgQSwAAABbEEgAAgAWxBAAAYEEsAQAAWBBLAAAAFsQSAACABbEEAABgQSwBAABYEEsAAAAWxBIAAIAFsQQAAGBBLAEAAFgQSwAAABbEEgAAgAWxBAAAYEEsAQAAWBBLAAAAFsQSAACABbEEAABgQSwBAABYEEsAAAAWxBIAAIAFsQQAAGBBLAEAAFgQSwAAABbEEgAAgAWxBAAAYEEsAQAAWBBLAAAAFsQSAACABbEEAABgQSwBAABYEEsAAAAWxBIAAIBF2MXSCy+8oKSkJDVu3FhpaWnauHGjdfyyZcvUrVs3NW7cWCkpKVq9erXvscrKSj344INKSUlR06ZNFR8frzvvvFPFxcXBXgYAAAgTYRVLS5cu1aRJkzRz5kxt2bJFvXr1UlZWlo4cOVLj+A0bNig7O1tjxozR1q1bNXjwYA0ePFhFRUWSpNOnT2vLli361a9+pS1btuj111/X7t27NXDgwIu5LAAA0IA5jDEm1JOorbS0NP34xz/W888/L0mqrq5WQkKC7r77bk2dOvW88cOGDdOpU6e0atUq375rrrlGvXv31rx582p8jk2bNqlv377at2+fEhMTazWvsrIyuVwueb1eOZ3OeqwMAABcbLV9/Q6bK0sVFRUqLCxUZmamb19ERIQyMzOVn59f4zH5+fl+4yUpKyvrguMlyev1yuFwKDY29oJjysvLVVZW5rcBAIBLU9jE0rFjx1RVVaW4uDi//XFxcfJ4PDUe4/F46jT+zJkzevDBB5WdnW0tzNzcXLlcLt+WkJBQx9UAAIBwETaxFGyVlZW64447ZIzRSy+9ZB07bdo0eb1e33bgwIGLNEsAAHCxRYV6ArXVunVrRUZGqqSkxG9/SUmJ3G53jce43e5ajT8XSvv27dO6deu+976jmJgYxcTE1GMVAAAg3ITNlaXo6GilpqYqLy/Pt6+6ulp5eXlKT0+v8Zj09HS/8ZK0du1av/HnQumzzz7T3/72N7Vq1So4CwAAAGEpbK4sSdKkSZM0atQo9enTR3379tWcOXN06tQp5eTkSJLuvPNOtWvXTrm5uZKke++9V/3799esWbN06623asmSJdq8ebP+8Ic/SPomlG677TZt2bJFq1atUlVVle9+ppYtWyo6Ojo0CwUAAA1GWMXSsGHDdPToUc2YMUMej0e9e/fWmjVrfDdx79+/XxER/7hY1q9fPy1evFjTp0/XQw89pM6dO2vlypXq2bOnJOnQoUP685//LEnq3bu333O9/fbbGjBgwEVZFwAAaLjC6nOWGio+ZwkAgPBzyX3OEgAAQCgQSwAAABbEEgAAgAWxBAAAYEEsAQAAWBBLAAAAFsQSAACABbEEAABgQSwBAABYEEsAAAAWxBIAAIAFsQQAAGBBLAEAAFgQSwAAABbEEgAAgAWxBAAAYEEsAQAAWBBLAAAAFsQSAACABbEEAABgQSwBAABYEEsAAAAWxBIAAIAFsQQAAGBBLAEAAFgQSwAAABbEEgAAgAWxBAAAYEEsAQAAWBBLAAAAFsQSAACABbEEAABgQSwBAABYEEsAAAAWxBIAAIAFsQQAAGBBLAEAAFgQSwAAABbEEgAAgEVUbQdOmjSp1t909uzZ9ZoMAABAQ1PrWNq6davf11u2bNHZs2fVtWtXSdKnn36qyMhIpaamBnaGAAAAIVTrWHr77bd9f549e7aaN2+ul19+WS1atJAkffXVV8rJydF1110X+FkCAACEiMMYY+p6ULt27fTXv/5VPXr08NtfVFSkG2+8UcXFxQGbYDgoKyuTy+WS1+uV0+kM9XQAAEAt1Pb1u143eJeVleno0aPn7T969KhOnDhRn28JAADQINUrln76058qJydHr7/+ug4ePKiDBw9qxYoVGjNmjIYMGRLoOQIAAIRMre9Z+rZ58+bpgQce0L//+7+rsrLym28UFaUxY8boqaeeCugEAQAAQqnO9yxVVVVp/fr1SklJUXR0tD7//HNJ0o9+9CM1bdo0KJNs6LhnCQCA8FPb1+86X1mKjIzUjTfeqJ07d6pDhw668sor/6mJAgAANGT1umepZ8+e+uKLLwI9FwAAgAanXrH06KOP6oEHHtCqVat0+PBhlZWV+W0AAACXinp9zlJExD8ay+Fw+P5sjJHD4VBVVVVgZhcmuGcJAIDwE7R7liT/T/MGAAC4lNUrlvr37x/oeQAAADRI9Yqlc06fPq39+/eroqLCbz/vkAMAAJeKesXS0aNHlZOTozfffLPGx39o9ywBAIBLV73eDXffffeptLRUBQUFatKkidasWaOXX35ZnTt31p///OdAzxEAACBk6hVL69at0+zZs9WnTx9FREToiiuu0IgRI/Tkk08qNzc30HP088ILLygpKUmNGzdWWlqaNm7caB2/bNkydevWTY0bN1ZKSopWr17t97gxRjNmzNDll1+uJk2aKDMzU5999lkwlwAAAMJIvWLp1KlTatu2rSSpRYsWOnr0qCQpJSVFW7ZsCdzsvmPp0qWaNGmSZs6cqS1btqhXr17KysrSkSNHahy/YcMGZWdna8yYMdq6dasGDx6swYMHq6ioyDfmySef1LPPPqt58+apoKBATZs2VVZWls6cORO0dQAAgPBRr1jq2rWrdu/eLUnq1auXfv/73+vQoUOaN2+eLr/88oBO8Ntmz56tsWPHKicnR927d9e8efN02WWXaf78+TWOnzt3rm666SZNmTJFycnJ+u1vf6urr75azz//vKRvrirNmTNH06dP16BBg3TllVdq4cKFKi4u1sqVK4O2DgAAED7qFUv33nuvDh8+LEmaOXOm3nzzTSUmJurZZ5/VY489FtAJnlNRUaHCwkJlZmb69kVERCgzM1P5+fk1HpOfn+83XpKysrJ84/fs2SOPx+M3xuVyKS0t7YLfU5LKy8v51HIAAH4g6vVuuBEjRvj+nJqaqn379mnXrl1KTExU69atAza5bzt27JiqqqoUFxfntz8uLk67du2q8RiPx1PjeI/H43v83L4LjalJbm6uHnnkkTqvAQAAhJ96XVn67j+ie9lll+nqq68OWig1NNOmTZPX6/VtBw4cCPWUAABAkNTrylKnTp3Uvn179e/fXwMGDFD//v3VqVOnQM/NT+vWrRUZGamSkhK//SUlJXK73TUe43a7rePP/bekpMTvXquSkhL17t37gnOJiYlRTExMfZYBAADCTL2uLB04cEC5ublq0qSJnnzySXXp0kXt27fX8OHD9V//9V+BnqMkKTo6WqmpqcrLy/Ptq66uVl5entLT02s8Jj093W+8JK1du9Y3vkOHDnK73X5jysrKVFBQcMHvCQAAfmBMAHz66adm1KhRJioqykRERATiW9ZoyZIlJiYmxixYsMDs2LHDjBs3zsTGxhqPx2OMMWbkyJFm6tSpvvHr1683UVFR5umnnzY7d+40M2fONI0aNTLbtm3zjXn88cdNbGyseeONN8wnn3xiBg0aZDp06GC+/vrrWs/L6/UaScbr9QZusQAAIKhq+/pdr1/DnT59Wh988IHeeecdvfPOO9q6dau6deumiRMnasCAAQGNuW8bNmyYjh49qhkzZsjj8ah3795as2aN7wbt/fv3KyLiHxfL+vXrp8WLF2v69Ol66KGH1LlzZ61cuVI9e/b0jfnlL3+pU6dOady4cSotLdW//Mu/aM2aNWrcuHHQ1gEAAMKHwxhj6npQdHS0WrRooeHDh2vAgAG67rrr1KJFi2DMLyyUlZXJ5XLJ6/XK6XSGejoAAKAWavv6Xa8rS7fccos++OADLVmyRB6PRx6PRwMGDFCXLl3qPWEAAICGqF43eK9cuVLHjh3TmjVrlJ6err/+9a+67rrr1K5dOw0fPjzQcwQAAAiZel1ZOiclJUVnz55VRUWFzpw5o7feektLly7VokWLAjU/AACAkKrXlaXZs2dr4MCBatWqldLS0vTaa6+pS5cuWrFihe8f1QUAALgU1OvK0muvvab+/ftr3Lhxuu666+RyuQI9LwAAgAahXrG0adOmQM8DAACgQarXr+Ek6f3339eIESOUnp6uQ4cOSZJeeeUVffDBBwGbHAAAQKjVK5ZWrFihrKwsNWnSRFu3blV5ebkkyev16rHHHgvoBAEAAEKpXrH06KOPat68efrjH/+oRo0a+fZfe+212rJlS8AmBwAAEGr1iqXdu3fr+uuvP2+/y+VSaWnpPzsnAACABqNeseR2u/X3v//9vP0ffPCBOnbs+E9PCgAAoKGoVyyNHTtW9957rwoKCuRwOFRcXKxFixZp8uTJ+sUvfhHoOQIAAIRMvT46YOrUqaqurlZGRoZOnz6t66+/XjExMZoyZYr+4z/+I9BzBAAACJl6XVlyOBx6+OGHdfz4cRUVFenDDz/U0aNH5XK51KFDh0DPEQAAIGTqFEvl5eWaNm2a+vTpo2uvvVarV69W9+7dtX37dnXt2lVz587V/fffH6y5AgAAXHR1+jXcjBkz9Pvf/16ZmZnasGGDbr/9duXk5OjDDz/UrFmzdPvttysyMjJYcwUAALjo6hRLy5Yt08KFCzVw4EAVFRXpyiuv1NmzZ/Xxxx/L4XAEa44AAAAhU6dfwx08eFCpqamSpJ49eyomJkb3338/oQQAAC5ZdYqlqqoqRUdH+76OiopSs2bNAj4pAACAhqJOv4Yzxmj06NGKiYmRJJ05c0bjx49X06ZN/ca9/vrrgZshAABACNUplkaNGuX39YgRIwI6GQAAgIamTrH0P//zP8GaBwAAQINUrw+lBAAA+KEglgAAACyIJQAAAAtiCQAAwIJYAgAAsCCWAAAALIglAAAAC2IJAADAglgCAACwIJYAAAAsiCUAAAALYgkAAMCCWAIAALAglgAAACyIJQAAAAtiCQAAwIJYAgAAsCCWAAAALIglAAAAC2IJAADAglgCAACwIJYAAAAsiCUAAAALYgkAAMCCWAIAALAglgAAACyIJQAAAAtiCQAAwIJYAgAAsCCWAAAALIglAAAAC2IJAADAglgCAACwIJYAAAAsiCUAAACLsIml48ePa/jw4XI6nYqNjdWYMWN08uRJ6zFnzpzRhAkT1KpVKzVr1kxDhw5VSUmJ7/GPP/5Y2dnZSkhIUJMmTZScnKy5c+cGeykAACCMhE0sDR8+XNu3b9fatWu1atUqvffeexo3bpz1mPvvv19/+ctftGzZMr377rsqLi7WkCFDfI8XFhaqbdu2evXVV7V9+3Y9/PDDmjZtmp5//vlgLwcAAIQJhzHGhHoS32fnzp3q3r27Nm3apD59+kiS1qxZo1tuuUUHDx5UfHz8ecd4vV61adNGixcv1m233SZJ2rVrl5KTk5Wfn69rrrmmxueaMGGCdu7cqXXr1l1wPuXl5SovL/d9XVZWpoSEBHm9Xjmdzn9mqQAA4CIpKyuTy+X63tfvsLiylJ+fr9jYWF8oSVJmZqYiIiJUUFBQ4zGFhYWqrKxUZmamb1+3bt2UmJio/Pz8Cz6X1+tVy5YtrfPJzc2Vy+XybQkJCXVcEQAACBdhEUsej0dt27b12xcVFaWWLVvK4/Fc8Jjo6GjFxsb67Y+Li7vgMRs2bNDSpUu/99d706ZNk9fr9W0HDhyo/WIAAEBYCWksTZ06VQ6Hw7rt2rXrosylqKhIgwYN0syZM3XjjTdax8bExMjpdPptAADg0hQVyiefPHmyRo8ebR3TsWNHud1uHTlyxG//2bNndfz4cbnd7hqPc7vdqqioUGlpqd/VpZKSkvOO2bFjhzIyMjRu3DhNnz69XmsBAACXppDGUps2bdSmTZvvHZeenq7S0lIVFhYqNTVVkrRu3TpVV1crLS2txmNSU1PVqFEj5eXlaejQoZKk3bt3a//+/UpPT/eN2759u2644QaNGjVKv/vd7wKwKgAAcCkJi3fDSdLNN9+skpISzZs3T5WVlcrJyVGfPn20ePFiSdKhQ4eUkZGhhQsXqm/fvpKkX/ziF1q9erUWLFggp9Opu+++W9I39yZJ3/zq7YYbblBWVpaeeuop33NFRkbWKuLOqe3d9AAAoOGo7et3SK8s1cWiRYs0ceJEZWRkKCIiQkOHDtWzzz7re7yyslK7d+/W6dOnffueeeYZ39jy8nJlZWXpxRdf9D2+fPlyHT16VK+++qpeffVV3/4rrrhCe/fuvSjrAgAADVvYXFlqyLiyBABA+LmkPmcJAAAgVIglAAAAC2IJAADAglgCAACwIJYAAAAsiCUAAAALYgkAAMCCWAIAALAglgAAACyIJQAAAAtiCQAAwIJYAgAAsCCWAAAALIglAAAAC2IJAADAglgCAACwIJYAAAAsiCUAAAALYgkAAMCCWAIAALAglgAAACyIJQAAAAtiCQAAwIJYAgAAsCCWAAAALIglAAAAC2IJAADAglgCAACwIJYAAAAsiCUAAAALYgkAAMCCWAIAALAglgAAACyIJQAAAAtiCQAAwIJYAgAAsCCWAAAALIglAAAAC2IJAADAglgCAACwIJYAAAAsiCUAAAALYgkAAMCCWAIAALAglgAAACyIJQAAAAtiCQAAwIJYAgAAsCCWAAAALIglAAAAC2IJAADAglgCAACwIJYAAAAsiCUAAAALYgkAAMCCWAIAALAIm1g6fvy4hg8fLqfTqdjYWI0ZM0YnT560HnPmzBlNmDBBrVq1UrNmzTR06FCVlJTUOPbLL79U+/bt5XA4VFpaGoQVAACAcBQ2sTR8+HBt375da9eu1apVq/Tee+9p3Lhx1mPuv/9+/eUvf9GyZcv07rvvqri4WEOGDKlx7JgxY3TllVcGY+oAACCMOYwxJtST+D47d+5U9+7dtWnTJvXp00eStGbNGt1yyy06ePCg4uPjzzvG6/WqTZs2Wrx4sW677TZJ0q5du5ScnKz8/Hxdc801vrEvvfSSli5dqhkzZigjI0NfffWVYmNjLzif8vJylZeX+74uKytTQkKCvF6vnE5ngFYNAACCqaysTC6X63tfv8PiylJ+fr5iY2N9oSRJmZmZioiIUEFBQY3HFBYWqrKyUpmZmb593bp1U2JiovLz8337duzYod/85jdauHChIiJqdzpyc3Plcrl8W0JCQj1XBgAAGrqwiCWPx6O2bdv67YuKilLLli3l8XgueEx0dPR5V4ji4uJ8x5SXlys7O1tPPfWUEhMTaz2fadOmyev1+rYDBw7UbUEAACBshDSWpk6dKofDYd127doVtOefNm2akpOTNWLEiDodFxMTI6fT6bcBAIBLU1Qon3zy5MkaPXq0dUzHjh3ldrt15MgRv/1nz57V8ePH5Xa7azzO7XaroqJCpaWlfleXSkpKfMesW7dO27Zt0/LlyyVJ527fat26tR5++GE98sgj9VwZAAC4VIQ0ltq0aaM2bdp877j09HSVlpaqsLBQqampkr4JnerqaqWlpdV4TGpqqho1aqS8vDwNHTpUkrR7927t379f6enpkqQVK1bo66+/9h2zadMm/fznP9f777+vH/3oR//s8gAAwCUgpLFUW8nJybrppps0duxYzZs3T5WVlZo4caJ+9rOf+d4Jd+jQIWVkZGjhwoXq27evXC6XxowZo0mTJqlly5ZyOp26++67lZ6e7nsn3HeD6NixY77ns70bDgAA/HCERSxJ0qJFizRx4kRlZGQoIiJCQ4cO1bPPPut7vLKyUrt379bp06d9+5555hnf2PLycmVlZenFF18MxfQBAECYCovPWWroavs5DQAAoOG4pD5nCQAAIFSIJQAAAAtiCQAAwIJYAgAAsCCWAAAALIglAAAAC2IJAADAglgCAACwIJYAAAAsiCUAAAALYgkAAMCCWAIAALAglgAAACyIJQAAAAtiCQAAwIJYAgAAsCCWAAAALIglAAAAC2IJAADAglgCAACwIJYAAAAsiCUAAAALYgkAAMCCWAIAALAglgAAACyIJQAAAAtiCQAAwIJYAgAAsCCWAAAALIglAAAAC2IJAADAglgCAACwIJYAAAAsiCUAAAALYgkAAMCCWAIAALAglgAAACyIJQAAAAtiCQAAwIJYAgAAsCCWAAAALIglAAAAi6hQT+BSYIyRJJWVlYV4JgAAoLbOvW6fex2/EGIpAE6cOCFJSkhICPFMAABAXZ04cUIul+uCjzvM9+UUvld1dbWKi4vVvHlzORyOUE8npMrKypSQkKADBw7I6XSGejqXLM7zxcO5vjg4zxcH59mfMUYnTpxQfHy8IiIufGcSV5YCICIiQu3btw/1NBoUp9PJ/4gXAef54uFcXxyc54uD8/wPtitK53CDNwAAgAWxBAAAYEEsIaBiYmI0c+ZMxcTEhHoqlzTO88XDub44OM8XB+e5frjBGwAAwIIrSwAAABbEEgAAgAWxBAAAYEEsAQAAWBBLqLPjx49r+PDhcjqdio2N1ZgxY3Ty5EnrMWfOnNGECRPUqlUrNWvWTEOHDlVJSUmNY7/88ku1b99eDodDpaWlQVhBeAjGef7444+VnZ2thIQENWnSRMnJyZo7d26wl9KgvPDCC0pKSlLjxo2VlpamjRs3WscvW7ZM3bp1U+PGjZWSkqLVq1f7PW6M0YwZM3T55ZerSZMmyszM1GeffRbMJYSFQJ7nyspKPfjgg0pJSVHTpk0VHx+vO++8U8XFxcFeRoMX6J/nbxs/frwcDofmzJkT4FmHIQPU0U033WR69eplPvzwQ/P++++bTp06mezsbOsx48ePNwkJCSYvL89s3rzZXHPNNaZfv341jh00aJC5+eabjSTz1VdfBWEF4SEY5/m///u/zT333GPeeecd8/nnn5tXXnnFNGnSxDz33HPBXk6DsGTJEhMdHW3mz59vtm/fbsaOHWtiY2NNSUlJjePXr19vIiMjzZNPPml27Nhhpk+fbho1amS2bdvmG/P4448bl8tlVq5caT7++GMzcOBA06FDB/P1119frGU1OIE+z6WlpSYzM9MsXbrU7Nq1y+Tn55u+ffua1NTUi7msBicYP8/nvP7666ZXr14mPj7ePPPMM0FeScNHLKFOduzYYSSZTZs2+fa9+eabxuFwmEOHDtV4TGlpqWnUqJFZtmyZb9/OnTuNJJOfn+839sUXXzT9+/c3eXl5P+hYCvZ5/ra77rrL/Ou//mvgJt+A9e3b10yYMMH3dVVVlYmPjze5ubk1jr/jjjvMrbfe6rcvLS3N/Od//qcxxpjq6mrjdrvNU0895Xu8tLTUxMTEmNdeey0IKwgPgT7PNdm4caORZPbt2xeYSYehYJ3ngwcPmnbt2pmioiJzxRVXEEvGGH4NhzrJz89XbGys+vTp49uXmZmpiIgIFRQU1HhMYWGhKisrlZmZ6dvXrVs3JSYmKj8/37dvx44d+s1vfqOFCxda/0HDH4Jgnufv8nq9atmyZeAm30BVVFSosLDQ7/xEREQoMzPzgucnPz/fb7wkZWVl+cbv2bNHHo/Hb4zL5VJaWpr1nF/KgnGea+L1euVwOBQbGxuQeYebYJ3n6upqjRw5UlOmTFGPHj2CM/kw9MN+RUKdeTwetW3b1m9fVFSUWrZsKY/Hc8FjoqOjz/tLLS4uzndMeXm5srOz9dRTTykxMTEocw8nwTrP37VhwwYtXbpU48aNC8i8G7Jjx46pqqpKcXFxfvtt58fj8VjHn/tvXb7npS4Y5/m7zpw5owcffFDZ2dk/2H8MNljn+YknnlBUVJTuueeewE86jBFLkCRNnTpVDofDuu3atStozz9t2jQlJydrxIgRQXuOhiDU5/nbioqKNGjQIM2cOVM33njjRXlO4J9VWVmpO+64Q8YYvfTSS6GeziWlsLBQc+fO1YIFC+RwOEI9nQYlKtQTQMMwefJkjR492jqmY8eOcrvdOnLkiN/+s2fP6vjx43K73TUe53a7VVFRodLSUr+rHiUlJb5j1q1bp23btmn58uWSvnmHkSS1bt1aDz/8sB555JF6rqxhCfV5PmfHjh3KyMjQuHHjNH369HqtJdy0bt1akZGR570Ls6bzc47b7baOP/ffkpISXX755X5jevfuHcDZh49gnOdzzoXSvn37tG7duh/sVSUpOOf5/fff15EjR/yu7ldVVWny5MmaM2eO9u7dG9hFhJNQ3zSF8HLuxuPNmzf79r311lu1uvF4+fLlvn27du3yu/H473//u9m2bZtvmz9/vpFkNmzYcMF3dlzKgnWejTGmqKjItG3b1kyZMiV4C2ig+vbtayZOnOj7uqqqyrRr1856Q+xPfvITv33p6enn3eD99NNP+x73er3c4B3g82yMMRUVFWbw4MGmR48e5siRI8GZeJgJ9Hk+duyY39/D27ZtM/Hx8ebBBx80u3btCt5CwgCxhDq76aabzFVXXWUKCgrMBx98YDp37uz3lvaDBw+arl27moKCAt++8ePHm8TERLNu3TqzefNmk56ebtLT0y/4HG+//fYP+t1wxgTnPG/bts20adPGjBgxwhw+fNi3/VBefJYsWWJiYmLMggULzI4dO8y4ceNMbGys8Xg8xhhjRo4caaZOneobv379ehMVFWWefvpps3PnTjNz5swaPzogNjbWvPHGG+aTTz4xgwYN4qMDAnyeKyoqzMCBA0379u3NRx995PezW15eHpI1NgTB+Hn+Lt4N9w1iCXX25ZdfmuzsbNOsWTPjdDpNTk6OOXHihO/xPXv2GEnm7bff9u37+uuvzV133WVatGhhLrvsMvPTn/7UHD58+ILPQSwF5zzPnDnTSDpvu+KKKy7iykLrueeeM4mJiSY6Otr07dvXfPjhh77H+vfvb0aNGuU3/n//939Nly5dTHR0tOnRo4f5v//7P7/Hq6urza9+9SsTFxdnYmJiTEZGhtm9e/fFWEqDFsjzfO5nvabt2z//P0SB/nn+LmLpGw5j/v/NIQAAADgP74YDAACwIJYAAAAsiCUAAAALYgkAAMCCWAIAALAglgAAACyIJQAAAAtiCQAAwIJYAoB6SEpK0pw5c0I9DQAXAbEEoMEbPXq0Bg8eLEkaMGCA7rvvvov23AsWLFBsbOx5+zdt2qRx48ZdtHkACJ2oUE8AAEKhoqJC0dHR9T6+TZs2AZwNgIaMK0sAwsbo0aP17rvvau7cuXI4HHI4HNq7d68kqaioSDfffLOaNWumuLg4jRw5UseOHfMdO2DAAE2cOFH33XefWrduraysLEnS7NmzlZKSoqZNmyohIUF33XWXTp48KUl65513lJOTI6/X63u+X//615LO/zXc/v37NWjQIDVr1kxOp1N33HGHSkpKfI//+te/Vu/evfXKK68oKSlJLpdLP/vZz3TixAnfmOXLlyslJUVNmjRRq1atlJmZqVOnTgXpbAKoLWIJQNiYO3eu0tPTNXbsWB0+fFiHDx9WQkKCSktLdcMNN+iqq67S5s2btWbNGpWUlOiOO+7wO/7ll19WdHS01q9fr3nz5kmSIiIi9Oyzz2r79u16+eWXtW7dOv3yl7+UJPXr109z5syR0+n0Pd8DDzxw3ryqq6s1aNAgHT9+XO+++67Wrl2rL774QsOGDfMb9/nnn2vlypVatWqVVq1apXfffVePP/64JOnw4cPKzs7Wz3/+c+3cuVPvvPOOhgwZIv6tcyD0+DUcgLDhcrkUHR2tyy67TG6327f/+eef11VXXaXHHnvMt2/+/PlKSEjQp59+qi5dukiSOnfurCeffNLve377/qekpCQ9+uijGj9+vF588UVFR0fL5XLJ4XD4Pd935eXladu2bdqzZ48SEhIkSQsXLlSPHj20adMm/fjHP5b0TVQtWLBAzZs3lySNHDlSeXl5+t3vfqfDhw/r7NmzGjJkiK644gpJUkpKyj9xtgAECleWAIS9jz/+WG+//baaNWvm27p16ybpm6s556Smpp537N/+9jdlZGSoXbt2at68uUaOHKkvv/xSp0+frvXz79y5UwkJCb5QkqTu3bsrNjZWO3fu9O1LSkryhZIkXX755Tpy5IgkqVevXsrIyFBKSopuv/12/fGPf9RXX31V+5MAIGiIJQBh7+TJk/q3f/s3ffTRR37bZ599puuvv943rmnTpn7H7d27Vz/5yU905ZVXasWKFSosLNQLL7wg6ZsbwAOtUaNGfl87HA5VV1dLkiIjI7V27Vq9+eab6t69u5577jl17dpVe/bsCfg8ANQNsQQgrERHR6uqqspv39VXX63t27crKSlJnTp18tu+G0jfVlhYqOrqas2aNUvXXHONunTpouLi4u99vu9KTk7WgQMHdODAAd++HTt2qLS0VN27d6/12hwOh6699lo98sgj2rp1q6Kjo/WnP/2p1scDCA5iCUBYSUpKUkFBgfbu3atjx46purpaEyZM0PHjx5Wdna1Nmzbp888/11tvvaWcnBxr6HTq1EmVlZV67rnn9MUXX+iVV17x3fj97ec7efKk8vLydOzYsRp/PZeZmamUlBQNHz5cW7Zs0caNG3XnnXeqf//+6tOnT63WVVBQoMcee0ybN2/W/v379frrr+vo0aNKTk6u2wkCEHDEEoCw8sADDygyMlLdu3dXmzZttH//fsXHx2v9+vWqqqrSjTfeqJSUFN13332KjY1VRMSF/5rr1auXZs+erSeeeEI9e/bUokWLlJub6zemX79+Gj9+vIYNG6Y2bdqcd4O49M0VoTfeeEMtWrTQ9ddfr8zMTHXs2FFLly6t9bqcTqfee+893XLLLerSpYumT5+uWbNm6eabb679yQEQFA7D+1IBAAAuiCtLAAAAFsQSAACABbEEAABgQSwBAABYEEsAAAAWxBIAAIAFsQQAAGBBLAEAAFgQSwAAABbEEgAAgAWxBAAAYPH/AIdlS+1d2NlKAAAAAElFTkSuQmCC\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Experiment Complete!\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from stable_baselines3 import PPO\n","import matplotlib.pyplot as plt\n","import gym\n","from gym import spaces\n","import numpy as np\n","\n","# Predefined ResNet Model\n","model = models.resnet18(pretrained=True)\n","model.eval()\n","\n","# Attack Functions\n","def fgsm_attack(image, epsilon):\n","    image.requires_grad = True\n","    output = model(image)\n","    loss = F.nll_loss(output, torch.argmax(output, dim=1))\n","    model.zero_grad()\n","    loss.backward()\n","    data_grad = image.grad.data\n","    perturbed_image = image + epsilon * data_grad.sign()\n","    return perturbed_image\n","\n","def gaussian_noise_attack(image, epsilon):\n","    noise = torch.randn_like(image) * epsilon\n","    perturbed_image = image + noise\n","    return torch.clamp(perturbed_image, 0, 1)\n","\n","# Defense Functions\n","def clip_values(image, clip_value):\n","    return torch.clamp(image, clip_value, 1 - clip_value)\n","\n","def gaussian_smoothing(image, std_dev):\n","    return F.gaussian_blur(image, kernel_size=5, sigma=std_dev)\n","\n","# Custom Gym Environment\n","class AdversarialEnv(gym.Env):\n","    def __init__(self, epsilon):\n","        super().__init__()\n","\n","        self.data_loader = iter(torch.utils.data.DataLoader(datasets.CIFAR10(root='./data', train=False, transform=transforms.ToTensor()), batch_size=1, shuffle=False))\n","        self.action_space = spaces.Discrete(4) # 2 attacks * 2 defenses\n","        self.observation_space = spaces.Box(low=0, high=1, shape=(3, 32, 32), dtype=np.float32)\n","        self.epsilon = epsilon\n","\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.float()\n","\n","    def step(self, action):\n","        attack = [fgsm_attack, gaussian_noise_attack][action // 2]\n","        defense = [clip_values, gaussian_smoothing][action % 2]\n","\n","        adversarial_example = attack(self.state, self.epsilon)\n","        defended_example = defense(adversarial_example, 0.05 if action % 2 == 0 else 1.0)\n","\n","        prediction = model(defended_example)\n","        reward = -F.nll_loss(prediction, torch.argmax(prediction, dim=1)).item()\n","\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.float()\n","        done = False\n","\n","        return self.state, reward, done, {}\n","\n","    def reset(self):\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.float()\n","        return self.state\n","\n","# Check the environment\n","epsilon = 0.1\n","env = AdversarialEnv(epsilon)\n","\n","# Train the Agent using PPO\n","agent = PPO(\"MlpPolicy\", env, verbose=1)\n","agent.learn(total_timesteps=10000)\n","\n","# Visualization\n","# Modify the environment or RL training to log the rewards for visualization\n","plt.plot([]) # You'll need to log the rewards and pass them here\n","plt.xlabel('Iterations')\n","plt.ylabel('Reward')\n","plt.show()\n","\n","print('Experiment Complete!')\n"],"metadata":{"id":"BqqsOs-uASI0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.env_checker import check_env\n","from stable_baselines3.common.monitor import Monitor\n","from stable_baselines3.common.vec_env import DummyVecEnv\n","import matplotlib.pyplot as plt\n","import gym\n","from gym import spaces\n","import numpy as np\n","from scipy.ndimage import gaussian_filter\n","\n","# Set device to CUDA if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Predefined ResNet Model\n","model = models.resnet18(pretrained=True)\n","model = model.to(device)\n","model.eval()\n","\n","# Attack Functions\n","def fgsm_attack(image, epsilon):\n","    image.requires_grad = True\n","    output = model(image)\n","    loss = F.nll_loss(output, torch.argmax(output, dim=1))\n","    model.zero_grad()\n","    loss.backward()\n","    data_grad = image.grad.data\n","    perturbed_image = image + epsilon * data_grad.sign()\n","    return perturbed_image\n","\n","def gaussian_smoothing(image, std_dev):\n","    smoothed_image = gaussian_filter(image.cpu().numpy(), sigma=std_dev)\n","    return torch.tensor(smoothed_image).to(device)\n","\n","# Defense Functions\n","def clip_values(image, clip_value):\n","    return torch.clamp(image, clip_value, 1 - clip_value)\n","\n","# Custom Gym Environment\n","class AdversarialEnv(gym.Env):\n","    def __init__(self, epsilon):\n","        super().__init__()\n","\n","        self.data_loader = iter(torch.utils.data.DataLoader(datasets.CIFAR10(root='./data', train=False, transform=transforms.ToTensor()), batch_size=1, shuffle=False))\n","        self.action_space = spaces.Discrete(4) # 2 attacks * 2 defenses\n","        self.observation_space = spaces.Box(low=0, high=1, shape=(3, 32, 32), dtype=np.float32)\n","        self.epsilon = epsilon\n","        self.reward_log = []\n","\n","    def step(self, action):\n","        attack_idx, defense_idx = divmod(action, 2)\n","        adversarial_example = fgsm_attack(self.state, self.epsilon) if attack_idx == 0 else gaussian_smoothing(self.state, 0.1)\n","        defended_example = clip_values(adversarial_example, 0.05) if defense_idx == 0 else adversarial_example\n","\n","        prediction = model(defended_example)\n","        reward = -F.nll_loss(prediction, torch.argmax(prediction, dim=1)).item()\n","        self.reward_log.append(reward)\n","\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.to(device).float()\n","        done = False\n","\n","        return self.state, reward, done, {}\n","\n","    def reset(self):\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.to(device).float()\n","        return self.state\n","\n","    def get_reward_log(self):\n","        return self.reward_log\n","\n","epsilon = 0.1\n","env = AdversarialEnv(epsilon)\n","env = Monitor(env) # Wrapping the environment with a Monitor wrapper\n","env = DummyVecEnv([lambda: env]) # Wrapping the environment with DummyVecEnv\n","# env = DummyVecEnv([lambda: env]) # Wrapping the environment with DummyVecEnv\n","# env = Monitor(env) # Wrapping the environment with a Monitor wrapper\n","\n","# Train the Agent using PPO\n","agent = PPO(\"MlpPolicy\", env, verbose=1)\n","agent.learn(total_timesteps=10000)\n","\n","# Visualization\n","rewards = env.envs[0].get_reward_log() # Retrieve the reward log\n","plt.plot(rewards)\n","plt.xlabel('Iterations')\n","plt.ylabel('Reward')\n","plt.show()\n","\n","print('Experiment Complete!')\n"],"metadata":{"id":"Qx6XucbUBEls"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","- **Fast Gradient Sign Method (FGSM)**: This method adds a small perturbation to the input image in the direction of the sign of the gradient of the loss function with respect to the input. This can fool the model into misclassifying the image with high confidence¹⁴.\n","- **Projected Gradient Descent (PGD)**: This method iteratively applies FGSM with a small step size and projects the perturbed image back to an epsilon-ball around the original image. This can generate more robust adversarial examples than FGSM¹.\n","- **Carlini and Wagner (C&W) Attack**: This method optimizes a different objective function than FGSM or PGD, which incorporates the confidence of the model and a distance metric between the original and perturbed images. This can produce adversarial examples that are hard to detect and have minimal distortion¹.\n","\n","Some of the best defenses against adversarial attacks on image models are:\n","\n","- **Adversarial Training**: This method trains the model on a mix of clean and adversarial examples, so that it can learn to resist the attacks. This can improve the robustness of the model, but it may also reduce its accuracy on clean examples¹³.\n","- **Input Transformation**: This method applies some transformation to the input image before feeding it to the model, such as resizing, cropping, JPEG compression, or adding noise. This can reduce or remove the effect of the adversarial perturbation, but it may also degrade the quality of the image¹.\n","- **Adversarial Example Detection**: This method tries to distinguish between clean and adversarial examples, either by using a separate classifier or by measuring some statistics of the input. This can help to reject or correct the adversarial examples, but it may also have a high false positive or false negative rate¹.\n","\n","-  (Adversarial Attacks and Defenses in Images, Graphs and Text ... - Springer.) [https://link.springer.com/article/10.1007/s11633-019-1211-x.]\n","- ( Adversarial Attacks and Defenses in Images, Graphs and ....)[ https://arxiv.org/abs/1909.08072.]\n","- [Defending against adversarial image attacks with Keras ... - PyImageSearch.] (https://pyimagesearch.com/2021/03/08/defending-against-adversarial-image-attacks-with-keras-and-tensorflow/)\n","- ( Adversarial Attacks and Defenses in Image Classification: A Practical ....)[ https://ieeexplore.ieee.org/document/9886997/.]\n","- ( undefined.)[ https://ieeexplore.ieee.org/servlet/opac?punumber=9886001.]"],"metadata":{"id":"fAx5zcE2OLtT"}},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.env_checker import check_env\n","import matplotlib.pyplot as plt\n","import gym\n","from gym import spaces\n","import numpy as np\n","\n","# Predefined ResNet Model\n","model = models.resnet18(pretrained=True)\n","model.eval()\n","\n","# Attack Functions\n","def fgsm_attack(image, epsilon, data_grad):\n","    sign_data_grad = data_grad.sign()\n","    perturbed_image = image + epsilon * sign_data_grad\n","    return torch.clamp(perturbed_image, 0, 1)\n","\n","def noise_attack(image, epsilon):\n","    noise = torch.rand_like(image) * epsilon\n","    perturbed_image = image + noise\n","    return torch.clamp(perturbed_image, 0, 1)\n","\n","# ... Other attack methods ...\n","\n","# Defense Functions\n","def clip_values(image, clip_value):\n","    return torch.clamp(image, clip_value, 1 - clip_value)\n","\n","# ... Other defense methods ...\n","\n","class AdversarialEnv(gym.Env):\n","    def __init__(self, epsilon):\n","        super().__init__()\n","\n","        self.data_loader = iter(torch.utils.data.DataLoader(datasets.CIFAR10(root='./data', train=False, transform=transforms.ToTensor()), batch_size=1, shuffle=False))\n","        self.action_space = spaces.Discrete(2) # 1 attack * 1 defense\n","        self.observation_space = spaces.Box(low=0, high=1, shape=(3, 32, 32), dtype=np.float32)\n","        self.epsilon = epsilon\n","\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.float()\n","\n","    def step(self, action):\n","        # Worst Attack: Noise Attack\n","        adversarial_example = noise_attack(self.state, self.epsilon)\n","\n","        # Best Defense: Clip Values\n","        defended_example = clip_values(adversarial_example, 0.05) if action == 0 else adversarial_example\n","\n","        # Visualization\n","        plt.imshow(self.state.squeeze().permute(1, 2, 0).detach().numpy())\n","        plt.title(\"Original Image\")\n","        plt.show()\n","\n","        plt.imshow(adversarial_example.squeeze().detach().permute(1, 2, 0).numpy())\n","        plt.title(\"Adversarial Example (Worst Attack: Noise Attack)\")\n","        plt.show()\n","\n","        plt.imshow(defended_example.squeeze().detach().permute(1, 2, 0).numpy())\n","        plt.title(\"Defended Example (Best Defense: Clip Values)\")\n","        plt.show()\n","\n","        prediction = model(defended_example)\n","        reward = -F.nll_loss(prediction, torch.argmax(prediction, dim=1)).item()\n","        print(\"Reward value :\", reward)\n","\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.float()\n","        done = False\n","\n","        return self.state, reward, done, {}\n","\n","    def reset(self):\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.float()\n","        return self.state\n"],"metadata":{"id":"jpsjs14mNxeo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import torch\n","import torch.nn.functional as F\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.env_checker import check_env\n","import matplotlib.pyplot as plt\n","import gym\n","from gym import spaces\n","import numpy as np\n","def fgsm_attack(image, epsilon, data_grad):\n","    sign_data_grad = data_grad.sign()\n","    perturbed_image = image + epsilon * sign_data_grad\n","    return torch.clamp(perturbed_image, 0, 1)\n","\n","def noise_attack(image, epsilon):\n","    noise = torch.rand_like(image) * epsilon\n","    perturbed_image = image + noise\n","    return torch.clamp(perturbed_image, 0, 1)\n","\n","def gaussian_attack(image, epsilon):\n","    noise = torch.randn_like(image) * epsilon\n","    perturbed_image = image + noise\n","    return torch.clamp(perturbed_image, 0, 1)\n","\n","# ... Add more attacks ...\n","\n","# Defense Functions\n","def gaussian_smoothing(image, std_dev):\n","    gaussian_filter = torch.tensor([[1, 2, 1], [2, 4, 2], [1, 2, 1]]) / 16.0\n","    gaussian_filter = gaussian_filter.view(1, 1, 3, 3).repeat(3, 1, 1, 1).to(image.device)\n","    smoothed_image = nn.functional.conv2d(image, gaussian_filter, padding=1, groups=3)\n","    return smoothed_image\n","\n","def clip_values(image, clip_value):\n","    return torch.clamp(image, clip_value, 1 - clip_value)\n","\n","def reduce_quality(image, factor):\n","    reduced_image = nn.functional.interpolate(image, scale_factor=factor, mode='bilinear')\n","    return nn.functional.interpolate(reduced_image, scale_factor=1/factor, mode='bilinear')\n","class AdversarialEnv(gym.Env):\n","    def __init__(self, epsilon):\n","        super().__init__()\n","\n","        self.data_loader = iter(torch.utils.data.DataLoader(datasets.CIFAR10(root='./data', train=False, transform=transforms.ToTensor()), batch_size=1, shuffle=False))\n","        self.action_space = spaces.Discrete(2) # 1 attack * 1 defense\n","        self.observation_space = spaces.Box(low=0, high=1, shape=(3, 32, 32), dtype=np.float32)\n","        self.epsilon = epsilon\n","\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.float()\n","\n","    def step(self, action):\n","        adversarial_example = fgsm_attack(self.state, self.epsilon)\n","        defended_example = clip_values(adversarial_example, 0.05) if action == 0 else adversarial_example\n","        # Visualization\n","        plt.imshow(self.state.squeeze().permute(1, 2, 0).detach().numpy())\n","        plt.title(\"Original Image\")\n","        plt.show()\n","\n","        plt.imshow(adversarial_example.squeeze().detach().permute(1, 2, 0).numpy())\n","        plt.title(\"Adversarial Example\")\n","        plt.show()\n","\n","        plt.imshow(defended_example.squeeze().detach().permute(1, 2, 0).numpy())\n","        plt.title(\"Defended Example\")\n","        plt.show()\n","\n","\n","        # # Visualization\n","        # plt.imshow(self.state.squeeze().permute(1, 2, 0))\n","        # plt.title(\"Original Image\")\n","        # plt.show()\n","\n","        # plt.imshow(adversarial_example.squeeze().detach().permute(1, 2, 0))\n","        # plt.title(\"Adversarial Example\")\n","        # plt.show()\n","\n","        # plt.imshow(defended_example.squeeze().detach().permute(1, 2, 0))\n","        # plt.title(\"Defended Example\")\n","        # plt.show()\n","\n","        prediction = model(defended_example)\n","\n","        reward = -F.nll_loss(prediction, torch.argmax(prediction, dim=1)).item()\n","        print(\"Reward value :\",reward )\n","\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.float()\n","        done = False\n","\n","        return self.state, reward, done, {}\n","\n","    def reset(self):\n","        self.state, _ = next(self.data_loader)\n","        self.state = self.state.float()\n","        return self.state"],"metadata":{"id":"vXBsMsmYMoGW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from stable_baselines3 import PPO\n","import matplotlib.pyplot as plt\n","import gym\n","from gym import spaces\n","import numpy as np\n","\n","# Define the attack\n","def fgsm_attack(image, epsilon, data_grad):\n","    perturbed_image = image + epsilon * data_grad.sign()\n","    return perturbed_image\n","\n","# Define the defense\n","def clip_values(image, clip_value):\n","    return torch.clamp(image, clip_value, 1 - clip_value)\n","\n","# Custom environment\n","class AdversarialEnv(gym.Env):\n","    def __init__(self, model, epsilon):\n","        super().__init__()\n","        self.action_space = spaces.Discrete(4) # 2 attacks * 2 defenses\n","        self.observation_space = spaces.Box(low=0, high=1, shape=(3, 32, 32), dtype=np.float32)\n","        self.model = model\n","        self.epsilon = epsilon\n","        self.data_loader = iter(torch.utils.data.DataLoader(datasets.CIFAR10(root='./data', train=False, transform=transforms.ToTensor()), batch_size=1, shuffle=False))\n","        self.state, self.current_label = next(self.data_loader)\n","        self.state = self.state.float()\n","\n","    def step(self, action):\n","        attack_idx = action // 2\n","        defense_idx = action % 2\n","\n","        # Compute the gradient for attack\n","        self.state.requires_grad = True\n","        output = self.model(self.state)\n","        loss = F.nll_loss(output, self.current_label)\n","        self.model.zero_grad()\n","        loss.backward()\n","        data_grad = self.state.grad.data\n","\n","        # Apply the selected attack and defense\n","        if attack_idx == 0:\n","            adversarial_example = fgsm_attack(self.state, self.epsilon, data_grad)\n","        else:\n","            adversarial_example = self.state # Other attack logic here\n","\n","        if defense_idx == 0:\n","            defended_example = clip_values(adversarial_example, 0.05)\n","        else:\n","            defended_example = adversarial_example # Other defense logic here\n","\n","        # Evaluate the model's performance\n","        prediction = self.model(defended_example)\n","        reward = -F.nll_loss(prediction, self.current_label).item()\n","\n","        self.state, self.current_label = next(self.data_loader)\n","        self.state = self.state.float()\n","        done = False\n","\n","        return self.state, reward, done, {}\n","\n","    def reset(self):\n","        self.state, self.current_label = next(self.data_loader)\n","        self.state = self.state.float()\n","        return self.state\n","\n","# Define model\n","model = models.resnet18(pretrained=True)\n","model.eval()\n","\n","# Define environment\n","epsilon = 0.1\n","env = AdversarialEnv(model, epsilon)\n","\n","# Train the Agent using PPO\n","agent = PPO(\"MlpPolicy\", env, verbose=1)\n","agent.learn(total_timesteps=10000)\n","\n","# Visualization code here\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"8nFLhC0QDJdo","executionInfo":{"status":"error","timestamp":1691597620858,"user_tz":-330,"elapsed":432881,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"810a5691-01bf-4bec-a7c3-3b32cb7757c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n","Wrapping the env with a `Monitor` wrapper\n","Wrapping the env in a DummyVecEnv.\n","-----------------------------\n","| time/              |      |\n","|    fps             | 23   |\n","|    iterations      | 1    |\n","|    time_elapsed    | 86   |\n","|    total_timesteps | 2048 |\n","-----------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 23          |\n","|    iterations           | 2           |\n","|    time_elapsed         | 175         |\n","|    total_timesteps      | 4096        |\n","| train/                  |             |\n","|    approx_kl            | 0.020032529 |\n","|    clip_fraction        | 0.286       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.37       |\n","|    explained_variance   | 0.00889     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 12.9        |\n","|    n_updates            | 10          |\n","|    policy_gradient_loss | -0.0274     |\n","|    value_loss           | 23.9        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 23          |\n","|    iterations           | 3           |\n","|    time_elapsed         | 263         |\n","|    total_timesteps      | 6144        |\n","| train/                  |             |\n","|    approx_kl            | 0.014714781 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.33       |\n","|    explained_variance   | 0.00855     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 11          |\n","|    n_updates            | 20          |\n","|    policy_gradient_loss | -0.0179     |\n","|    value_loss           | 27          |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 23          |\n","|    iterations           | 4           |\n","|    time_elapsed         | 352         |\n","|    total_timesteps      | 8192        |\n","| train/                  |             |\n","|    approx_kl            | 0.012213715 |\n","|    clip_fraction        | 0.183       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.26       |\n","|    explained_variance   | 0.000299    |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 9.79        |\n","|    n_updates            | 30          |\n","|    policy_gradient_loss | -0.0165     |\n","|    value_loss           | 22.3        |\n","-----------------------------------------\n"]},{"output_type":"error","ename":"StopIteration","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-efee9eac3f9d>\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# Train the Agent using PPO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MlpPolicy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# Visualization code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 308\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \"\"\"\n\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Avoid circular imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to step environment that needs reset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shimmy/openai_gym_compatibility.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \"\"\"\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgym_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_mode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-efee9eac3f9d>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mStopIteration\u001b[0m: "]}]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","import matplotlib.pyplot as plt\n","import gym\n","from gym import spaces\n","import numpy as np\n","from scipy.ndimage import gaussian_filter\n","\n","# Attacks\n","def fgsm_attack(image, epsilon, data_grad):\n","    return image + epsilon * data_grad.sign()\n","\n","def invert_attack(image):\n","    return 1 - image\n","\n","# Defenses\n","def clip_values(image, clip_value):\n","    return torch.clamp(image, clip_value, 1 - clip_value)\n","\n","def gaussian_smoothing(image, std_dev):\n","    return torch.tensor(gaussian_filter(image, std_dev))\n","\n","class AdversarialEnv(gym.Env):\n","    def __init__(self, model, epsilon):\n","        super().__init__()\n","        self.action_space = spaces.Discrete(4) # 2 attacks * 2 defenses\n","        self.observation_space = spaces.Box(low=0, high=1, shape=(3, 32, 32), dtype=np.float32)\n","        self.model = model\n","        self.epsilon = epsilon\n","        self.data_loader = iter(torch.utils.data.DataLoader(datasets.CIFAR10(root='./data', train=False, transform=transforms.ToTensor()), batch_size=1, shuffle=False))\n","        self.state, self.current_label = next(self.data_loader)\n","        self.state = self.state.float()\n","\n","    def step(self, action):\n","        attack_idx = action // 2\n","        defense_idx = action % 2\n","\n","        # Compute the gradient for attack\n","        self.state.requires_grad = True\n","        output = self.model(self.state)\n","        loss = F.nll_loss(output, torch.max(self.current_label, 1)[1])\n","        self.model.zero_grad()\n","        loss.backward()\n","        data_grad = self.state.grad.data\n","\n","        # Apply the selected attack and defense\n","        if attack_idx == 0:\n","            adversarial_example = fgsm_attack(self.state, self.epsilon, data_grad)\n","        else:\n","            adversarial_example = invert_attack(self.state)\n","\n","        if defense_idx == 0:\n","            defended_example = clip_values(adversarial_example, 0.05)\n","        else:\n","            defended_example = gaussian_smoothing(adversarial_example, 1.0)\n","\n","        # Evaluate the model's performance\n","        prediction = self.model(defended_example)\n","        reward = -F.nll_loss(prediction, torch.max(self.current_label, 1)[1]).item()\n","\n","        self.state, self.current_label = next(self.data_loader)\n","        self.state = self.state.float()\n","        done = False\n","\n","        return self.state, reward, done, {}\n","    def reset(self):\n","        self.state, self.current_label = next(self.data_loader)\n","        self.state = self.state.float()\n","        return self.state\n","# Define model\n","model = models.resnet18(pretrained=True)\n","model.eval()\n","\n","# Define environment\n","epsilon = 0.1\n","env = AdversarialEnv(model, epsilon)\n","\n","# Train the Agent using PPO\n","agent = PPO(\"MlpPolicy\", env, verbose=1)\n","rewards_log = []\n","for i in range(10000):\n","    reward = agent.learn(total_timesteps=1)\n","    rewards_log.append(reward)\n","\n","# Visualization\n","plt.plot(rewards_log)\n","plt.xlabel('Iterations')\n","plt.ylabel('Reward')\n","plt.show()\n"],"metadata":{"id":"XQ5G1RG9E91q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" some of the best attacks, defenses, and models for adversarial robustness on image classification:\n","\n","| Attack | Defense | Model |\n","| --- | --- | --- |\n","| FGSM¹ | Adversarial Training¹³ | ResNet-50-AT³ |\n","| PGD¹ | Input Transformation¹ | ResNet-50-IT³ |\n","| C&W¹ | Adversarial Example Detection¹ | ResNet-50-AED³ |\n","| DeepFool² | Randomized Smoothing² | ResNet-50-RS² |\n","| JSMA⁴ | Feature Squeezing⁴ | ResNet-50-FS⁴ |\n","| BIM | Distillation | ResNet-50-DIST |\n","| MI-FGSM | Ensemble Adversarial Training | ResNet-50-EAT |\n","| One-Pixel Attack | Spatial Smoothing | ResNet-50-SS |\n","| Universal Adversarial Perturbations | Defense-GAN | ResNet-50-DGAN |\n","| Patch Attack | AdvPatch Detector | ResNet-50-APD|\n","\n","\n","\n","Source: Conversation with Bing, 8/10/2023\n","(1) Defending against adversarial image attacks with Keras ... - PyImageSearch. https://pyimagesearch.com/2021/03/08/defending-against-adversarial-image-attacks-with-keras-and-tensorflow/.\n","(2) Adversarial Attacks and Defences for Convolutional Neural Networks. https://medium.com/onfido-tech/adversarial-attacks-and-defences-for-convolutional-neural-networks-66915ece52e7.\n","(3) Adversarial Attacks and Defenses in Images, Graphs and Text ... - Springer. https://link.springer.com/article/10.1007/s11633-019-1211-x.\n","(4) Adversarial examples: attacks and defences on medical deep ... - Springer. https://link.springer.com/article/10.1007/s11042-023-14702-9."],"metadata":{"id":"bSnpbMDsQiFD"}},{"cell_type":"code","source":[],"metadata":{"id":"bgdn9xqvQuMa"},"execution_count":null,"outputs":[]}]}
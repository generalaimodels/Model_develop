{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOl2WfWfb6mwxUxuLjY4BEQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MJ-tj7OlhEy4"},"outputs":[],"source":["!pip install -q -U transformers"]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import os\n","\n","#13B model\n","model_name = \"TheBloke/vicuna-13B-1.1-HF\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","output_path = \"./local_vicuna13B\"\n","os.makedirs(output_path, exist_ok=True)\n","tokenizer.save_pretrained(output_path)\n","model.save_pretrained(output_path,max_shard_size=\"100GB\")\n","\n","#7B model\n","model_name = \"TheBloke/vicuna-7B-1.1-HF\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","output_path = \"./local_vicuna7B\"\n","os.makedirs(output_path, exist_ok=True)\n","tokenizer.save_pretrained(output_path)\n","model.save_pretrained(output_path,max_shard_size=\"100GB\")"],"metadata":{"id":"t-1uskw1hN2l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import logging\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# Set up logging\n","logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n","\n","# Define model names and paths\n","models_dir = \"path/to/models\"  # Path to directory containing the pre-trained models\n","models = [\n","    {\"name\": \"TheBloke/vicuna-13B-1.1-HF\", \"size\": \"13B\"},\n","    {\"name\": \"TheBloke/vicuna-7B-1.1-HF\", \"size\": \"7B\"},\n","]\n","\n","# Load and save the pre-trained models\n","for model in models:\n","    try:\n","        model_name = model[\"name\"]\n","        output_path = os.path.join(models_dir, f\"local_vicuna_{model['size']}B\")\n","\n","        logging.info(f\"Loading and saving {model['size']}B model...\")\n","        tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","        # Save the tokenizer and model to the output path\n","        os.makedirs(output_path, exist_ok=True)\n","        tokenizer.save_pretrained(output_path)\n","        model.save_pretrained(output_path, max_shard_size=\"100GB\")\n","\n","        logging.info(f\"Saved {model['size']}B model to {output_path}.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading and saving {model['size']}B model: {e}\")"],"metadata":{"id":"Jn64ysoDim7P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this Python script, we use the powerful HuggingFace transformers library to preprocess a text dataset for further natural language processing tasks. This script is useful, especially when working with transformer models, that require their input text to be tokenized in a very specific way. Here is the detailed explanation of the code:\n","\n","# `Primary libraries used`:\n","\n","- transformers to access pre-trained models and tokenizers.\n","- argparse to manage command-line arguments.\n","- pandas and numpy for data processing and computations.\n","- tqdm for a progress display during loop execution.\n","- os for environment variables and path manipulations.\n","\n","### Command-Line Arguments:\n","\n","The script starts by defining command-line arguments using the argparse library. These arguments include a model name (\"-m\"/\"--model_name\"), a directory containing the dataset (\"-d\"/\"--dataset\"), and a pad token id (\"--pad_token_id\"). If these parameters aren't provided, default values are used.\n","\n","Access Token:\n","An access token is obtained from the environment variable \"HF_TOKEN\". This is particularly useful when you need to authenticate your script with the Hugging Face Model Hub.\n","\n","## `Tokenizer`:\n","We then get the tokenizer corresponding to the model name provided as an argument. This tokenizer is fetched from the pre-trained models available in the transformers library. The 'use_auth_token' flag is set to True to use the Hugging Face access token for authentication.\n","\n","# `Preparing Dataset`:\n","We read the dataset files named as \"train.csv\" and \"validation.csv\" from the provided directory using pandas. These dataframes are then concatenated to form a combined dataframe.\n","\n","# `Tokenization & Calculation`:\n","We iterate over each row of the 'text' column in the combined dataframe. For each text input, we use our tokenizer to tokenize the text and get the input IDs. We also calculate the length (or size) of the tokenized input. If a pad token id is provided, the script checks that it does not appear in the tokenized input.\n","\n","# `Token size statistics`:\n","Finally, we find the maximum, minimum, mean, and median size of all tokenized inputs in the dataset and print these statistics. This analysis is important to understand the distribution of lengths of the tokenized sequences in your dataset.\n","\n","The script ends by printing these calculated statistics. These statistics give insights into the sizes of the tokenized texts and can be useful when defining the maximum sequence length for your transformer model.\n","\n"],"metadata":{"id":"3_kMZVxjjy1n"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, BatchEncoding\n","import argparse\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import os\n","import logging\n","from multiprocessing import Pool\n","\n","# Command-line arguments setup\n","parser = argparse.ArgumentParser()\n","parser.add_argument(\"-m\",\"--model_name\", type=str, required=True)\n","parser.add_argument(\"-d\",\"--data_dir\", type=str, required=True)\n","parser.add_argument(\"-t\",\"--train_file\", type=str, default=\"train.csv\")\n","parser.add_argument(\"-v\",\"--valid_file\", type=str, default=\"validation.csv\")\n","parser.add_argument(\"--pad_token_id\", type=int, default=None)\n","parser.add_argument(\"--max_length\", type=int, default=128)\n","args = parser.parse_args()\n","\n","# Logging setup\n","logging.basicConfig(level=logging.INFO)\n","\n","# Fetch access token\n","access_token = os.getenv(\"HF_TOKEN\", \"\")\n","\n","# Initialize tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_auth_token=True, use_fast=True)\n","\n","# Load datasets\n","dataset_dir = os.path.realpath(args.data_dir)\n","train_file = os.path.join(dataset_dir, args.train_file)\n","valid_file = os.path.join(dataset_dir, args.valid_file)\n","\n","try:\n","    train_df = pd.read_csv(train_file)\n","    valid_df = pd.read_csv(valid_file)\n","    combined_df = pd.concat([train_df, valid_df])\n","except FileNotFoundError:\n","    logging.error(\"One of the data files does not exist.\")\n","    raise\n","\n","if 'text' not in combined_df:\n","    logging.error(\"Column named 'text' does not exist in the dataset.\")\n","    raise ValueError(\"Column named 'text' does not exist in the dataset.\")\n","\n","# Clean text data, removing unnecessary characters, handling special cases, etc.\n","# Add your text cleaning logic here\n","\n","# Tokenize data in batches for efficiency\n","texts = combined_df['text'].tolist()\n","encoded_inputs: BatchEncoding = tokenizer(texts, truncation=True, padding='max_length', max_length=args.max_length, return_tensors='np')\n","\n","assert args.pad_token_id is None or args.pad_token_id not in encoded_inputs['input_ids'], \"Pad token is in tokenized text.\"\n","\n","# Calculate token sizes\n","all_sizes = [len(input_ids) for input_ids in encoded_inputs['input_ids'].tolist()]\n","\n","logging.info(\"Max size: %s\", max(all_sizes))\n","logging.info(\"Min size: %s\", min(all_sizes))\n","logging.info(\"Mean size: %s\", np.mean(all_sizes))\n","logging.info(\"Median size: %s\", np.median(all_sizes))"],"metadata":{"id":"h3dqs2ZEmh8B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","import re\n","\n","# Constants\n","EMPTY_STRING = ''\n","END_OF_TEXT_PATTERN = re.compile(r'<\\|endoftext\\|>')\n","TRAIN_FILENAME = 'train.csv'\n","VALIDATION_FILENAME = 'validation.csv'\n","OUTPUT_DIR = './OPT/'\n","\n","def read_csv_with_error_handling(file_path):\n","    try:\n","        if not os.path.isfile(file_path):\n","            raise FileNotFoundError(f\"File not found: {file_path}\")\n","\n","        df = pd.read_csv(file_path)\n","\n","        # Check if 'text' column exists\n","        if 'text' not in df.columns:\n","            raise KeyError(f\"'text' column is missing from the input file {file_path}\")\n","\n","        # Check if 'text' column has the correct data type\n","        if not pd.api.types.is_string_dtype(df['text']):\n","            raise TypeError(f\"'text' column must be of string type in the file {file_path}\")\n","\n","        return df\n","    except (FileNotFoundError, KeyError, TypeError) as e:\n","        print(e)\n","        return pd.DataFrame()\n","\n","def replace_empty_strings(df, column_name='text'):\n","    if column_name in df.columns:\n","        df[column_name] = df[column_name].str.replace(END_OF_TEXT_PATTERN, EMPTY_STRING, regex=True)\n","\n","def save_to_csv(df, file_path):\n","    df.to_csv(file_path, index=False)\n","\n","def process_and_save_files(input_dir, output_dir):\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # Process training data\n","    train_file_path = os.path.join(input_dir, TRAIN_FILENAME)\n","    train_df = read_csv_with_error_handling(train_file_path)\n","    replace_empty_strings(train_df)\n","    save_to_csv(train_df, os.path.join(output_dir, TRAIN_FILENAME))\n","\n","    # Process validation data\n","    validation_file_path = os.path.join(input_dir, VALIDATION_FILENAME)\n","    valid_df = read_csv_with_error_handling(validation_file_path)\n","    replace_empty_strings(valid_df)\n","    save_to_csv(valid_df, os.path.join(output_dir, VALIDATION_FILENAME))\n","\n","if __name__ == '__main__':\n","    input_dir = os.path.join('.', 'GPT')\n","    output_dir = os.path.realpath(OUTPUT_DIR)\n","    process_and_save_files(input_dir, output_dir)"],"metadata":{"id":"CWkAFQOsrnYK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# inference"],"metadata":{"id":"QjBMUMfltD8n"}},{"cell_type":"code","source":["! pip install -q -U deepspeed"],"metadata":{"id":"LB-M755ctNXP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip install -q -U transformers"],"metadata":{"id":"gChChqfPtafN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import deepspeed\n","import torch\n","from transformers import pipeline\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# Set the model name directly since we won't be using command-line arguments\n","model_name = 'EleutherAI/gpt-j-6B'\n","\n","# Assuming LOCAL_RANK and WORLD_SIZE are set outside of this cell\n","local_rank = int(os.getenv('LOCAL_RANK', '0'))\n","world_size = int(os.getenv('WORLD_SIZE', '1'))\n","\n","model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=local_rank, torch_dtype=torch.float16)\n","\n","# Initialize DeepSpeed Inference\n","generator.model = deepspeed.init_inference(generator.model,\n","                                           mp_size=world_size,\n","                                           dtype=torch.half,\n","                                           replace_method='auto',\n","                                           max_tokens=2048,\n","                                           replace_with_kernel_inject=True)\n","torch.cuda.synchronize()\n","\n","# Generate some text\n","input_text = \"DeepSpeed is\"\n","generated_text = generator(input_text, do_sample=True, max_length=2047, min_length=2047, top_k=50, top_p=0.95, temperature=0.9)\n","\n","# Printing the output\n","if not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0:\n","    print(generated_text)"],"metadata":{"id":"moPapn-6t4II"},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNschI6CebpwWlfCYXkRjIb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -q -U sentencepiece"],"metadata":{"id":"4xhnVj6PhQFu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import the required modules\n","from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n","from peft import LoraConfig, get_peft_model\n","\n","# Load the model and tokenizer\n","model_name_or_path = \"gpt2\"\n","model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","\n","# Define the LoRA configuration\n","lora_config = LoraConfig(\n","    r=8, # rank of the low-rank matrix\n","    lora_alpha=32, # scaling factor of the low-rank matrix\n","    lora_dropout=0.1, # dropout rate of the low-rank matrix\n",")\n","\n","# Wrap the model with LoRA\n","model = get_peft_model(model, lora_config)\n","\n","# Load the dataset\n","dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n","\n","# Define the training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"lora-gpt2\", # output directory\n","    num_train_epochs=1, # number of training epochs\n","    per_device_train_batch_size=16, # batch size per device during training\n","    per_device_eval_batch_size=16, # batch size for evaluation\n","    logging_steps=500, # number of steps between logging\n","    save_steps=500, # number of steps between model saves\n","    evaluation_strategy=\"steps\", # evaluate every logging_steps\n","    eval_steps=500, # number of steps between evaluations\n","    learning_rate=5e-5, # learning rate\n","    weight_decay=0.01, # weight decay\n",")\n","\n","# Define the trainer\n","trainer = Trainer(\n","    model=model, # the model to train\n","    args=training_args, # the training arguments\n","    train_dataset=dataset[\"train\"], # the training dataset\n","    eval_dataset=dataset[\"validation\"], # the evaluation dataset\n",")\n","\n","# Start the training\n","trainer.train()\n"],"metadata":{"id":"TNIWB6SN-rvT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jcXy6SzEhMHK"},"outputs":[],"source":["from pathlib import Path\n","from sentencepiece import SentencePieceProcessor\n","from typing import List\n","\n","\n","class Tokenizer:\n","    def __init__(self, model_path: str):\n","        assert Path(model_path).exists(), model_path\n","        self._model = SentencePieceProcessor(model_file=model_path)\n","        assert self._model.vocab_size() == self._model.get_piece_size()\n","\n","    @property\n","    def n_words(self) -> int:\n","        return self._model.vocab_size()\n","\n","    @property\n","    def bos_id(self) -> int:\n","        return self._model.bos_id()\n","\n","    @property\n","    def eos_id(self) -> int:\n","        return self._model.eos_id()\n","\n","    @property\n","    def pad_id(self) -> int:\n","        return self._model.pad_id()\n","\n","    def encode(self, s: str, bos: bool = True) -> List[int]:\n","        assert isinstance(s, str)\n","        t = self._model.encode(s)\n","        if bos:\n","            t = [self.bos_id, *t]\n","        return t\n","\n","    def decode(self, t: List[int]) -> str:\n","        return self._model.decode(t)"]},{"cell_type":"code","source":["import torch\n","from typing import Tuple\n","\n","# The code you provided\n","def precompute_freqs_cis(dim: int, end: int, theta: float) -> torch.Tensor:\n","    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n","    t = torch.arange(end, device=freqs.device)  # type: ignore\n","    freqs = torch.outer(t, freqs).float()  # type: ignore\n","    return torch.polar(torch.ones_like(freqs), freqs)  # complex64\n","\n","\n","def apply_rotary_emb(\n","    xq: torch.Tensor,\n","    xk: torch.Tensor,\n","    freqs_cis: torch.Tensor,\n",") -> Tuple[torch.Tensor, torch.Tensor]:\n","    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n","    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n","    freqs_cis = freqs_cis[:, None, :]\n","    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(2)\n","    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(2)\n","    return xq_out.type_as(xq), xk_out.type_as(xk)\n","\n","# Some dummy input tensors for xq and xk\n","# Assume the batch size is 2, the sequence length is 4, and the hidden size is 8\n","xq = torch.randn(2, 4, 8)\n","xk = torch.randn(2, 4, 8)\n","\n","# Some values for dim, end, and theta\n","# Assume the dim is 8, the end is 4, and the theta is 1.0\n","dim = 2\n","end = 2\n","theta = 1.0\n","\n","# Compute the freqs_cis tensor\n","freqs_cis = precompute_freqs_cis(dim, end, theta)\n","print(freqs_cis)\n","# Apply the rotary embedding to xq and xk\n","xq_out, xk_out = apply_rotary_emb(xq, xk, freqs_cis)\n","\n","# Print the output tensors\n","print(xq_out)\n","print(xk_out)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pDini2QvjWdG","executionInfo":{"status":"ok","timestamp":1705665437067,"user_tz":-330,"elapsed":411,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"7032dbd1-3748-4657-c89f-ba972da1bc89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.0000+0.0000j],\n","        [0.5403+0.8415j]])\n","tensor([[[ 0.3074,  0.9158,  0.2200, -1.6933,  0.7677,  0.8516,  0.8772,\n","           1.0275],\n","         [ 0.0847,  1.1582, -0.5125, -0.3631,  0.3798, -0.0918, -2.0022,\n","           0.3902],\n","         [ 0.0321, -0.2443, -1.1355, -0.5867,  0.4276,  0.5811,  0.3322,\n","           0.9117],\n","         [ 1.6778, -1.9925, -0.2339,  0.2268,  0.0569,  0.3529,  0.2872,\n","           0.0841]],\n","\n","        [[ 0.1932, -0.5233,  0.8993,  1.0861, -1.2082,  0.3150, -0.1533,\n","          -0.0888],\n","         [-0.1965,  1.2208,  0.8005,  0.3558,  0.8528, -0.5554,  0.6259,\n","           0.2706],\n","         [ 0.8407, -1.0696, -2.5355,  0.3427,  0.7675, -0.2888,  2.1063,\n","          -1.0799],\n","         [ 0.4715,  0.2453,  2.0120, -0.7407, -0.9866, -0.2389, -0.8688,\n","          -0.3356]]])\n","tensor([[[-0.4202, -0.3106,  1.1324,  1.6704, -0.2345, -1.6656,  0.9083,\n","           1.3251],\n","         [ 0.9377,  0.5710,  0.0924, -0.4687, -0.1797, -0.9308, -1.5040,\n","          -0.3121],\n","         [ 0.1929,  0.0932,  0.5034, -0.6395,  0.6626, -0.2427, -0.0573,\n","          -0.3728],\n","         [ 0.2739,  1.6264,  0.2323, -0.1189, -0.2902,  0.4193, -0.5884,\n","          -1.2575]],\n","\n","        [[-1.0236, -0.1688, -1.1739,  0.4890, -0.3501, -0.1445, -0.9061,\n","           0.5570],\n","         [-1.6292, -0.1695, -0.0451,  0.2100,  1.1456, -0.8479, -2.5276,\n","           0.0241],\n","         [ 1.1284,  0.3936, -0.1941,  0.4949, -1.2408, -1.5955, -0.3137,\n","           0.5271],\n","         [-0.7404,  0.3504,  0.6148, -0.7850,  0.2815,  0.1392,  0.7335,\n","           0.1294]]])\n"]}]},{"cell_type":"code","source":["!pip install -q -U simple_parsing"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wzfs3Q2WmeQZ","executionInfo":{"status":"ok","timestamp":1705665779286,"user_tz":-330,"elapsed":6781,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"237caffa-e898-44e9-ccb4-faa8f6479dd9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/113.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/113.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import dataclasses\n","from typing import List\n","\n","import torch\n","import torch.nn.functional as F\n","from simple_parsing.helpers import Serializable\n","from torch import nn\n","\n","# The code you provided\n","@dataclasses.dataclass\n","class MoeArgs(Serializable):\n","    num_experts: int\n","    num_experts_per_tok: int\n","\n","\n","class MoeLayer(nn.Module):\n","    def __init__(self, experts: List[nn.Module], gate: nn.Module, moe_args: MoeArgs):\n","        super().__init__()\n","        assert len(experts) > 0\n","        self.experts = nn.ModuleList(experts)\n","        self.gate = gate\n","        self.args = moe_args\n","\n","    def forward(self, inputs: torch.Tensor):\n","        gate_logits = self.gate(inputs)\n","        weights, selected_experts = torch.topk(gate_logits, self.args.num_experts_per_tok)\n","        weights = F.softmax(weights, dim=1, dtype=torch.float).to(inputs.dtype)\n","        results = torch.zeros_like(inputs)\n","        for i, expert in enumerate(self.experts):\n","\n","            batch_idx, nth_expert = torch.where(selected_experts == i)\n","            results[batch_idx] += weights[batch_idx, nth_expert, None] * expert(\n","                inputs[batch_idx]\n","            )\n","        return results\n","\n","# # Some dummy input tensors for inputs\n","# # Assume the batch size is 2, the sequence length is 4, and the hidden size is 8\n","# inputs = torch.randn(2, 4, 8)\n","\n","# # Some values for num_experts and num_experts_per_tok\n","# # Assume the num_experts is 4 and the num_experts_per_tok is 2\n","# num_experts = 4\n","# num_experts_per_tok = 2\n","\n","# # Define the experts and the gate modules\n","# # Assume the experts are simple linear layers and the gate is a linear layer followed by a softmax\n","# experts = [nn.Linear(8, 8) for _ in range(num_experts)]\n","# gate = nn.Sequential(nn.Linear(8, num_experts), nn.Softmax(dim=-1))\n","\n","# # Create an instance of the MoE layer\n","# moe_layer = MoeLayer(experts, gate, MoeArgs(num_experts, num_experts_per_tok))\n","\n","# # The original code\n","# outputs1 = moe_layer(inputs)\n","\n","# # The modified code\n","# outputs2 = moe_layer(inputs)\n","\n","# # Compare the outputs\n","# print(torch.equal(outputs1, outputs2)) # This should print True\n"],"metadata":{"id":"52qJeMdimbZ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The projections in the architecture are part of the Mistral 7B model, which is a large language model that uses grouped-query attention and sliding window attention to handle long sequences efficiently. The projections are linear transformations that map the input features to different dimensions for different purposes. Here is a brief explanation of each projection:\n","\n","- `(q_proj)`: This projection maps the input features to the query features, which are used to compute the attention scores with the key features. The output dimension is the same as the input dimension (4096) to preserve the information.\n","- `(k_proj)`: This projection maps the input features to the key features, which are used to compute the attention scores with the query features. The output dimension is smaller than the input dimension (1024) to reduce the computation cost and memory footprint of the attention matrix.\n","- `(v_proj)`: This projection maps the input features to the value features, which are used to compute the weighted sum of the attention outputs. The output dimension is the same as the key dimension (1024) to match the attention matrix size.\n","- `(o_proj)`: This projection maps the value features to the output features, which are the final result of the attention layer. The output dimension is the same as the input dimension (4096) to restore the information and match the next layer's input size.\n","- `(rotary_emb)`: This is a special embedding layer that applies a rotation matrix to the input features based on their positions. This helps the model to capture the relative positions of the tokens without using positional embeddings.\n","- `(mlp)`: This is a multilayer perceptron that consists of three linear projections and a SiLU activation function. It is used to apply a non-linear transformation to the output features of the attention layer. The projections are:\n","    - `(gate_proj)`: This projection maps the output features to a larger dimension (14336) and applies a sigmoid function to create a gate vector.\n","    - `(up_proj)`: This projection maps the output features to the same larger dimension (14336) and applies a SiLU function to create an activation vector.\n","    - `(down_proj)`: This projection maps the element-wise product of the gate vector and the activation vector to the original dimension (4096) and adds a residual connection to the output features.\n","\n","The mathematical equations for the projections are:\n","\n","$$\n","\\begin{aligned}\n","Q &= q\\_proj(X) \\\\\n","K &= k\\_proj(X) \\\\\n","V &= v\\_proj(X) \\\\\n","A &= \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) \\\\\n","O &= o\\_proj(AV) \\\\\n","R &= \\text{rotary\\_emb}(O) \\\\\n","G &= \\text{sigmoid}(gate\\_proj(R)) \\\\\n","U &= \\text{SiLU}(up\\_proj(R)) \\\\\n","D &= down\\_proj(G \\odot U) \\\\\n","Y &= D + R\n","\\end{aligned}\n","$$\n","\n","where $X$ is the input features, $Q$ is the query features, $K$ is the key features, $V$ is the value features, $A$ is the attention matrix, $O$ is the output features, $R$ is the rotated features, $G$ is the gate vector, $U$ is the activation vector, $D$ is the down-projected features, $Y$ is the final output features, $d_k$ is the key dimension, and $\\odot$ is the element-wise product."],"metadata":{"id":"754my76KDKYf"}},{"cell_type":"markdown","source":[],"metadata":{"id":"_3ByB-h0GMsk"}},{"cell_type":"code","source":[],"metadata":{"id":"MW3rB5U6GNTa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3Ft6aD5jDOS1"},"execution_count":null,"outputs":[]}]}
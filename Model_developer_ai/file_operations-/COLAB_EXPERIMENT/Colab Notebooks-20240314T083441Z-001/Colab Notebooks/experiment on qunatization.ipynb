{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"cell_execution_strategy":"setup","gpuType":"T4","authorship_tag":"ABX9TyPAavBGmxuPj7bCipdXWTdS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install -q -U bitsandbytes>=0.39.0\n","!pip install -q -U git+https://github.com/huggingface/accelerate.git\n","!pip install -q -U git+https://github.com/huggingface/transformers.git"],"metadata":{"id":"fX5CAK4XD20T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","torch.manual_seed(0)\n","\n","# Set device to CPU for now\n","device = 'cuda'\n","\n","# Load model and tokenizer\n","model_id = 'gpt2'\n","model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","# Print model size\n","print(f\"Model size: {model.get_memory_footprint():,} bytes\")"],"metadata":{"id":"doNq4ZurgvY_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","def uniform_quantize(X, num_bits):\n","    # Calculate the scale factor\n","    scale = 2 ** (num_bits - 1) - 1\n","    min_val = torch.min(X)\n","    max_val = torch.max(X)\n","    if min_val == max_val:\n","        min_val -= 1\n","        max_val += 1\n","    scale /= max(max_val - min_val, 1e-7)\n","\n","    # Quantize the values\n","    X_quant = torch.round((X - min_val) * scale)\n","    X_quant = torch.clamp(X_quant, 0, 2**num_bits - 1)\n","\n","    # Dequantize the values\n","    X_dequant = X_quant / scale + min_val\n","\n","    return X_quant.to(torch.int), X_dequant\n","\n","# # Example usage\n","# X = torch.randn(100)\n","# for num_bits in [2, 4, 8, 16, 32]:\n","#     X_quant, X_dequant = uniform_quantize(X, num_bits)\n","#     print(f\"{num_bits}-bit quantization:\")\n","#     print(f\"Quantized values: {X_quant}\")\n","#     print(f\"Dequantized values: {X_dequant}\")\n","#     print(f\"Max error: {torch.max(torch.abs(X - X_dequant))}\")"],"metadata":{"id":"2bGd-hyJRGYK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","def kl_div_quantize(X, num_bits, eps=1e-7):\n","    # Calculate the probability distribution of the values\n","    probs = torch.histc(X, bins=2**num_bits, min=X.min(), max=X.max())\n","    probs = probs.float() / probs.sum()\n","\n","    # Calculate the cumulative distribution function (CDF)\n","    cdf = torch.cumsum(probs, dim=0)\n","\n","    # Calculate the quantization levels\n","    levels = torch.zeros(2**num_bits + 1)\n","    for i in range(1, 2**num_bits + 1):\n","        levels[i] = (cdf[i-1] + cdf[min(i, 2**num_bits - 1)]) / 2\n","    levels = levels[1:]\n","\n","    # Quantize the values\n","    X_quant = torch.floor((X - levels[0]) / (levels[1] - levels[0]) * 2**num_bits)\n","    X_quant = torch.clamp(X_quant, 0, 2**num_bits - 1)\n","\n","    # Dequantize the values\n","    X_dequant = (X_quant + 0.5) / 2**num_bits * (levels[1] - levels[0]) + levels[0]\n","\n","    return X_quant.to(torch.int), X_dequant\n","\n","# # Example usage\n","# X = torch.randn(100)\n","# for num_bits in [2, 4, 8, 16, 32]:\n","    # X_quant, X_dequant = kl_div_quantize(X, num_bits)\n","    # print(f\"{num_bits}-bit non-uniform quantization:\")\n","    # print(f\"Quantized values: {X_quant}\")\n","    # print(f\"Dequantized values: {X_dequant}\")\n","    # print(f\"Max error: {torch.max(torch.abs(X - X_dequant))}\")"],"metadata":{"id":"qdUfeOtnRq63"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","import datetime\n","from copy import deepcopy\n","\n","# Assuming 'model', 'uniform_quantize', and 'kl_div_quantize' are pre-defined\n","\n","def flatten_and_concatenate_weights(weights_list):\n","    return np.concatenate([weight.flatten() for weight in weights_list])\n","\n","# Function to quantize and store model weights\n","def quantize_and_store(model, quantize_func, bit_width, weights_list):\n","    for param in model.parameters():\n","        _, dequantized = quantize_func(param.data, bit_width)\n","        weights_list.append(dequantized.cpu().numpy().flatten())\n","\n","# Store original weights and flatten them\n","original_weights = flatten_and_concatenate_weights([param.data.clone().cpu() for param in model.parameters()])\n","\n","# Quantize models and store weights (optimized to avoid keeping multiple copies of the model)\n","bit_widths = [2 ]\n","weights_uniform = {bw: [] for bw in bit_widths}\n","weights_non_uniform = {bw: [] for bw in bit_widths}\n","\n","for bw in bit_widths:\n","    model_uniform = deepcopy(model)\n","    quantize_and_store(model_uniform, uniform_quantize, bw, weights_uniform[bw])\n","    del model_uniform  # Free up memory immediately after use\n","\n","    model_non_uniform = deepcopy(model)\n","    quantize_and_store(model_non_uniform, kl_div_quantize, bw, weights_non_uniform[bw])\n","    del model_non_uniform  # Free up memory immediately after use\n","\n","# Flatten and concatenate weights\n","weights_uniform = {bw: flatten_and_concatenate_weights(weights_uniform[bw]) for bw in bit_widths}\n","weights_non_uniform = {bw: flatten_and_concatenate_weights(weights_non_uniform[bw]) for bw in bit_widths}\n","\n","# Function to plot the CDF\n","def plot_cdf(data, ax, label, color):\n","    sorted_data = np.sort(data)\n","    cdf = np.arange(1, len(sorted_data) + 1) / float(len(sorted_data))\n","    ax.plot(sorted_data, cdf, label=label, color=color)\n","\n","# Set background style and plot CDFs\n","plt.style.use('ggplot')\n","fig, axs = plt.subplots(2, figsize=(10,10), dpi=300, sharex=True)\n","\n","# Plot the CDFs for uniform and non-uniform weights\n","colors = ['red', 'green', 'black', 'grey']\n","for i, bw in enumerate(bit_widths):\n","    plot_cdf(weights_uniform[bw], axs[0], f'Uniform weights ({bw}-bit)', colors[i])\n","    plot_cdf(weights_non_uniform[bw], axs[1], f'Non-uniform weights ({bw}-bit)', colors[i])\n","\n","# Plot the CDF for original weights\n","plot_cdf(original_weights, axs[0], 'Original weights', 'blue')\n","plot_cdf(original_weights, axs[1], 'Original weights', 'blue')\n","\n","# Customize the plots\n","for ax in axs:\n","    ax.grid(True, linestyle='--', alpha=0.6)\n","    ax.legend(loc='lower right')\n","    ax.set_xlabel('Weights', fontsize=14)\n","    ax.set_ylabel('Cumulative Distribution Function', fontsize=14)\n","    ax.set_title('CDF of Original and Quantized Weights', fontsize=16)\n","\n","# Improve font and layout\n","plt.rc('font', size=12)\n","plt.tight_layout()\n","\n","# Save plot with a unique name\n","current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","plt.savefig(f\"our_results_{current_time}.png\")\n","plt.show()"],"metadata":{"id":"Vu4L5N_iQuEM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# simple explains\n","# Original Floats\n","original_floats = [-0.9, -0.2, 0.0, 0.4, 0.8]\n","\n","# Scaling Factor\n","s = 7.78 # (based on 7 / max(abs(data)))\n","\n","# Quantized Integers\n","quantized_integers = [round(x * s) for x in original_floats]\n","\n","# Print the results\n","print(\"Original Floats:\", original_floats)\n","print(\"Scaling Factor:\", s)\n","print(\"Quantized Integers:\", quantized_integers)\n","\n","\n","import numpy as np\n","\n","# Generate 100 random float32 numbers\n","original_floats = np.random.rand(100).astype(np.float32)\n","\n","# Scaling Factor\n","s = 7.78 # (based on 7 / max(abs(data)))\n","\n","# Quantize the float32 numbers\n","quantized_integers = np.round(original_floats * s).astype(np.int8)\n","\n","# Calculate memory usage\n","import sys\n","mem_before = sys.getsizeof(original_floats)\n","mem_after = sys.getsizeof(quantized_integers)\n","\n","# Print the results\n","print(\"Memory usage before quantization:\", mem_before)\n","print(\"Memory usage after quantization:\", mem_after)"],"metadata":{"id":"TzcVr3HGGZgw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","\n","# Extract weights of the first layer\n","weights = model.transformer.h[0].attn.c_attn.weight.data\n","print(\"Original weights:\")\n","print(weights)\n","\n","# Quantize layer using uniform quantization with different bit widths\n","weights_uniform_quant_2, _ = uniform_quantize(weights, 2)\n","weights_uniform_quant_4, _ = uniform_quantize(weights, 4)\n","weights_uniform_quant_8, _ = uniform_quantize(weights, 8)\n","weights_uniform_quant_16, _ = uniform_quantize(weights, 16)\n","\n","print(\"\\nUniform quantized weights:\")\n","print(f\"\\n2-bit quantized weights:\\n{weights_uniform_quant_2.int()}\")\n","print(f\"\\n4-bit quantized weights:\\n{weights_uniform_quant_4.int()}\")\n","print(f\"\\n8-bit quantized weights:\\n{weights_uniform_quant_8.int()}\")\n","print(f\"\\n16-bit quantized weights:\\n{weights_uniform_quant_16.int()}\")\n","\n","# Quantize layer using non-uniform quantization with different bit widths\n","weights_non_uniform_quant_2, _ = kl_div_quantize(weights, 2)\n","weights_non_uniform_quant_4, _ = kl_div_quantize(weights, 4)\n","weights_non_uniform_quant_8, _ = kl_div_quantize(weights, 8)\n","weights_non_uniform_quant_16, _ = kl_div_quantize(weights, 16)\n","\n","print(\"\\nNon-uniform quantized weights:\")\n","print(f\"\\n2-bit quantized weights:\\n{weights_non_uniform_quant_2.int()}\")\n","print(f\"\\n4-bit quantized weights:\\n{weights_non_uniform_quant_4.int()}\")\n","print(f\"\\n8-bit quantized weights:\\n{weights_non_uniform_quant_8.int()}\")\n","print(f\"\\n16-bit quantized weights:\\n{weights_non_uniform_quant_16.int()}\")"],"metadata":{"id":"ekad-h4lV-57"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from copy import deepcopy\n","\n","\n","\n","# Store original weights\n","original_weights = [param.data.clone().cpu().numpy().flatten() for param in model.parameters()]\n","\n","# Create lists to hold the flattened quantized weights\n","weights_uniform_2 = []\n","weights_uniform_4 = []\n","weights_uniform_8 = []\n","weights_uniform_16 = []\n","\n","weights_non_uniform_2 = []\n","weights_non_uniform_4 = []\n","weights_non_uniform_8 = []\n","weights_non_uniform_16 = []\n","\n","# Function to quantize and store model weights\n","def quantize_and_store(model, quantize_func, bit_width, weights_list):\n","    for param in model.parameters():\n","        _, dequantized = quantize_func(param.data, bit_width)\n","        weights_list.extend(dequantized.cpu().numpy().flatten())\n","\n","# Quantize models using uniform quantization with different bit widths\n","model_uniform_2 = deepcopy(model)\n","model_uniform_4 = deepcopy(model)\n","model_uniform_8 = deepcopy(model)\n","model_uniform_16 = deepcopy(model)\n","\n","quantize_and_store(model_uniform_2, uniform_quantize, 2, weights_uniform_2)\n","quantize_and_store(model_uniform_4, uniform_quantize, 4, weights_uniform_4)\n","quantize_and_store(model_uniform_8, uniform_quantize, 8, weights_uniform_8)\n","quantize_and_store(model_uniform_16, uniform_quantize, 16, weights_uniform_16)\n","\n","# Quantize models using non-uniform (KL divergence) quantization\n","model_non_uniform_2 = deepcopy(model)\n","model_non_uniform_4 = deepcopy(model)\n","model_non_uniform_8 = deepcopy(model)\n","model_non_uniform_16 = deepcopy(model)\n","\n","quantize_and_store(model_non_uniform_2, kl_div_quantize, 2, weights_non_uniform_2)\n","quantize_and_store(model_non_uniform_4, kl_div_quantize, 4, weights_non_uniform_4)\n","quantize_and_store(model_non_uniform_8, kl_div_quantize, 8, weights_non_uniform_8)\n","quantize_and_store(model_non_uniform_16, kl_div_quantize, 16, weights_non_uniform_16)\n","\n"],"metadata":{"id":"4ODYEjzdg57S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","\n","\n","weights = np.concatenate([t.cpu().numpy().flatten() for t in original_weights])\n","weights_uniform_2 = np.concatenate([t.cpu().numpy().flatten() for t in weights_uniform_2])\n","weights_uniform_4 = np.concatenate([t.cpu().numpy().flatten() for t in weights_uniform_4])\n","weights_uniform_8 = np.concatenate([t.cpu().numpy().flatten() for t in weights_uniform_8])\n","weights_uniform_16 = np.concatenate([t.cpu().numpy().flatten() for t in weights_uniform_16])\n","\n","\n","weights = np.concatenate([t.cpu().numpy().flatten() for t in original_weights])\n","weights_non_uniform_2 = np.concatenate([t.cpu().numpy().flatten() for t in weights_non_uniform_2])\n","weights_non_uniform_4 = np.concatenate([t.cpu().numpy().flatten() for t in weights_non_uniform_4])\n","weights_non_uniform_8 = np.concatenate([t.cpu().numpy().flatten() for t in weights_non_uniform_8])\n","weights_non_uniform_16 = np.concatenate([t.cpu().numpy().flatten() for t in weights_non_uniform_16])\n","\n","# Set background style\n","plt.style.use('ggplot')\n","# Create figure and axes\n","fig, axs = plt.subplots(2, figsize=(10,10), dpi=300, sharex=True)\n","# Plot the histograms for original and zero-point weights\n","axs[0].hist(weights, bins=150, alpha=0.5, label='Original weights', color='blue', range=(-2, 2))\n","axs[0].hist(weights_uniform_2, bins=150, alpha=0.5, label='Uniform weights (2-bit)', color='red', range=(-2, 2))\n","axs[0].hist(weights_uniform_4, bins=150, alpha=0.5, label='Uniform weights (4-bit)', color='green', range=(-2, 2))\n","axs[0].hist(weights_uniform_8, bins=150, alpha=0.5, label='Uniform weights (8-bit)', color='black', range=(-2, 2))\n","axs[0].hist(weights_uniform_16, bins=150, alpha=0.5, label='Uniform weights (16-bit)', color='grey', range=(-2, 2))\n","\n","# Plot the histograms for original and absmax weights\n","axs[1].hist(weights, bins=150, alpha=0.5, label='Original weights', color='blue', range=(-2, 2))\n","axs[1].hist(weights_non_uniform_2, bins=150, alpha=0.5, label='Non-uniform weights (2-bit)', color='red', range=(-2, 2))\n","axs[1].hist(weights_non_uniform_4, bins=150, alpha=0.5, label='Non-uniform weights (4-bit)', color='green', range=(-2, 2))\n","axs[1].hist(weights_non_uniform_8, bins=150, alpha=0.5, label='Non-uniform weights (8-bit)', color='black', range=(-2, 2))\n","axs[1].hist(weights_non_uniform_16, bins=150, alpha=0.5, label='Non-uniform weights (16-bit)', color='grey', range=(-2, 2))\n","\n","# Add grid\n","for ax in axs:\n","    ax.grid(True, linestyle='--', alpha=0.6)\n","\n","# Add legend\n","axs[0].legend()\n","axs[1].legend()\n","\n","# Add title and labels\n","axs[0].set_title('Comparison of Original and Uniform Quantized Weights', fontsize=16)\n","axs[1].set_title('Comparison of Original and Non-uniform Quantized Weights', fontsize=16)\n","\n","for ax in axs:\n","    ax.set_xlabel('Weights', fontsize=14)\n","    ax.set_ylabel('Count', fontsize=14)\n","    ax.yaxis.set_major_formatter(ticker.EngFormatter()) # Make y-ticks more human readable\n","\n","# Improve font\n","plt.rc('font', size=12)\n","\n","# Save plot with a unique name\n","import datetime\n","current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","plt.savefig(f\"our_results_{current_time}.png\")\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"iNtjLGi8Ymm9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Extract weights of the first layer\n","weights = model.transformer.h[0].attn.c_attn.weight.data\n","print(\"Original weights:\")\n","print(weights)\n","\n","# Quantize layer using uniform quantization with different bit widths\n","weights_uniform_quant_2, _ = uniform_quantize(weights, 2)\n","weights_uniform_quant_4, _ = uniform_quantize(weights, 4)\n","weights_uniform_quant_8, _ = uniform_quantize(weights, 8)\n","weights_uniform_quant_16, _ = uniform_quantize(weights, 16)\n","\n","# Flatten weight tensors\n","weights = np.concatenate([t.cpu().numpy().flatten() for t in weights])\n","weights_uniform_2 = np.concatenate([t.cpu().numpy().flatten() for t in weights_uniform_2])\n","weights_uniform_4 = np.concatenate([t.cpu().numpy().flatten() for t in weights_uniform_4])\n","weights_uniform_8 = np.concatenate([t.cpu().numpy().flatten() for t in weights_uniform_8])\n","weights_uniform_16 = np.concatenate([t.cpu().numpy().flatten() for t in weights_uniform_16])\n","\n","\n","# Set the seaborn style\n","sns.set(style=\"whitegrid\")\n","\n","# Create figure\n","plt.figure(figsize=(10, 5), dpi=300)\n","\n","# Plot KDEs\n","sns.kdeplot(weights, shade=True, label='Original weights', color='blue')\n","sns.kdeplot(weights_uniform_2, shade=True, label='kde_non_uniform_weights (2-bit)', color='red')\n","sns.kdeplot(weights_uniform_4, shade=True, label='kde_non_uniform_weights (4-bit)', color='green')\n","sns.kdeplot(weights_uniform_8, shade=True, label='kde_non_uniform_weights (4-bit)', color='black')\n","sns.kdeplot(weights_uniform_16, shade=True, label='kde_non_uniform_ weights (4-bit)', color='grey')\n","\n","# Continue for all your weight sets...\n","\n","# Add labels and title\n","plt.xlabel('Weights')\n","plt.ylabel('Density')\n","plt.title('Uniform weights of Weight Distributions')\n","plt.legend()\n","\n","# Save the plot\n","current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","plt.savefig(f\"weight_distributions_{current_time}.png\")\n","\n","# Show the plot\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"djXndmzChF-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Extract weights of the first layer\n","weights = model.transformer.h[0].attn.c_attn.weight.data\n","print(\"Original weights:\")\n","print(weights)\n","\n","# Quantize layer using non-uniform quantization with different bit widths\n","weights_non_uniform_quant_2, _ = kl_div_quantize(weights, 2)\n","weights_non_uniform_quant_4, _ = kl_div_quantize(weights, 4)\n","weights_non_uniform_quant_8, _ = kl_div_quantize(weights, 8)\n","weights_non_uniform_quant_16, _ = kl_div_quantize(weights, 16)\n","weights_non_uniform_2 = np.concatenate([t.cpu().numpy().flatten() for t in weights_non_uniform_2])\n","weights_non_uniform_4 = np.concatenate([t.cpu().numpy().flatten() for t in weights_non_uniform_4])\n","weights_non_uniform_8 = np.concatenate([t.cpu().numpy().flatten() for t in weights_non_uniform_8])\n","weights_non_uniform_16 = np.concatenate([t.cpu().numpy().flatten() for t in weights_non_uniform_16])\n","\n","# Set the seaborn style\n","sns.set(style=\"whitegrid\")\n","\n","# Create figure\n","plt.figure(figsize=(10, 5), dpi=300)\n","\n","# Plot KDEs\n","sns.kdeplot(weights, shade=True, label='Original weights', color='blue')\n","sns.kdeplot(weights_non_uniform_2, shade=True, label='Uniform weights (2-bit)', color='red')\n","sns.kdeplot(weights_non_uniform_4, shade=True, label='Uniform weights (4-bit)', color='green')\n","sns.kdeplot(weights_non_uniform_8, shade=True, label='Uniform weights (4-bit)', color='black')\n","sns.kdeplot(weights_non_uniform_16, shade=True, label='Uniform weights (4-bit)', color='grey')\n","\n","# Continue for all your weight sets...\n","\n","# Add labels and title\n","plt.xlabel('Weights')\n","plt.ylabel('Density')\n","plt.title('Kernel Density Estimation of Weight Distributions')\n","plt.legend()\n","\n","# Save the plot\n","current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","plt.savefig(f\"weight_distributions_kde_{current_time}.png\")\n","\n","# Show the plot\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"127mLtydcEfh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_text(model, input_text, max_length=50):\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n","    output = model.generate(inputs=input_ids,\n","                            max_length=max_length,\n","                            do_sample=True,\n","                            top_k=30,\n","                            pad_token_id=tokenizer.eos_token_id,\n","                            attention_mask=input_ids.new_ones(input_ids.shape))\n","    return tokenizer.decode(output[0], skip_special_tokens=True)\n","prompt=\"\"\"\n","small language model\n","\"\"\"\n","# Generate text with original and quantized models\n","\n","original_text = generate_text(model, prompt)\n","absmax_text   = generate_text(model_abs, prompt)\n","zp_text       = generate_text(model_zp, prompt)\n","\n","print(f\"Original model:\\n{original_text}\")\n","print(\"-\" * 100)\n","print(f\"Absmax model:\\n{absmax_text}\")\n","print(\"-\" * 100)\n","print(f\"Zeropoint model:\\n{zp_text}\")"],"metadata":{"id":"MIQQZfhhhZit"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_perplexity(model, text):\n","    # Encode the text\n","    encodings = tokenizer(text, return_tensors='pt').to(device)\n","\n","    # Define input_ids and target_ids\n","    input_ids = encodings.input_ids\n","    target_ids = input_ids.clone()\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids, labels=target_ids)\n","\n","    # Loss calculation\n","    neg_log_likelihood = outputs.loss\n","\n","    # Perplexity calculation\n","    ppl = torch.exp(neg_log_likelihood)\n","\n","    return ppl\n","\n","ppl     = calculate_perplexity(model, original_text)\n","ppl_abs = calculate_perplexity(model_abs, absmax_text)\n","ppl_zp  = calculate_perplexity(model_zp, absmax_text)\n","\n","print(f\"Original perplexity: {ppl.item():.2f}\")\n","print(f\"Absmax perplexity:   {ppl_abs.item():.2f}\")\n","print(f\"Zeropoint perplexity: {ppl_zp.item():.2f}\")"],"metadata":{"id":"erVI-VXHhj6v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model_int8 = AutoModelForCausalLM.from_pretrained(model_id,\n","                                             device_map='auto',\n","                                             load_in_8bit=True,\n","                                             )\n","print(f\"Model size: {model_int8.get_memory_footprint():,} bytes\")"],"metadata":{"id":"d6Vr3Z7shmbC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","# Flatten weight tensors\n","weights_int8 = [param.data.clone() for param in model_int8.parameters()]\n","weights_int8 = np.concatenate([t.cpu().numpy().flatten() for t in weights_int8])\n","\n","# Set background style\n","plt.style.use('ggplot')\n","\n","# Create figure and axis\n","fig, ax = plt.subplots(figsize=(10,5), dpi=300)\n","\n","# Plot the histograms\n","ax.hist(weights, bins=150, alpha=0.5, label='Original weights',\n","        color='blue', range=(-2, 2))\n","ax.hist(weights_int8, bins=150, alpha=0.5, label='LLM.int8() weights',\n","        color='red', range=(-2, 2))\n","\n","# Add grid\n","ax.grid(True, linestyle='--', alpha=0.6)\n","\n","# Add legend\n","ax.legend()\n","\n","# Add title and labels\n","ax.set_title('Comparison of Original and Dequantized Weights', fontsize=16)\n","ax.set_xlabel('Weights', fontsize=14)\n","ax.set_ylabel('Count', fontsize=14)\n","plt.gca().yaxis.set_major_formatter(ticker.EngFormatter())\n","\n","# Improve font\n","plt.rc('font', size=12)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","\n","print(f\"Perplexity (original):   {ppl.item():.2f}\")\n","\n","ppl = calculate_perplexity(model_int8, text_int8)\n","print(f\"Perplexity (LLM.int8()): {ppl.item():.2f}\")"],"metadata":{"id":"h_VnvK9qhqGR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# This open sources community"],"metadata":{"id":"kzm1vVeqipX7"}},{"cell_type":"code","source":["# Install ExLLamaV2\n","!git clone https://github.com/turboderp/exllamav2\n","!pip install -e exllamav2"],"metadata":{"id":"NQ7aeDGoiBD9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MODEL_NAME = \"zephyr-7b-beta\"\n","BPW = 5.0\n","\n","# Download model\n","!git lfs install\n","!git clone https://huggingface.co/HuggingFaceH4/{MODEL_NAME}\n","!mv {MODEL_NAME} base_model\n","!rm base_mode/*.bin\n","\n","# Download dataset\n","!wget https://huggingface.co/datasets/wikitext/resolve/9a9e482b5987f9d25b3a9b2883fc6cc9fd8071b3/wikitext-103-v1/wikitext-test.parquet"],"metadata":{"id":"Wy4ufoTOiEtF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Quantize model\n","!mkdir quant\n","!python exllamav2/convert.py \\\n","    -i base_model \\\n","    -o quant \\\n","    -c wikitext-test.parquet \\\n","    -b {BPW}"],"metadata":{"id":"uj2aegj_iKpQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Copy files\n","!rm -rf quant/out_tensor\n","!rsync -av --exclude='*.safetensors' --exclude='.*' ./base_model/ ./quant/"],"metadata":{"id":"oxlIg8ghiNWi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run model\n","!python exllamav2/test_inference.py -m quant/ -p \"I have a dream\""],"metadata":{"id":"JgpqSLiWiQi0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q huggingface_hub\n","!git config --global credential.helper store\n","\n","from huggingface_hub import notebook_login\n","from huggingface_hub import HfApi\n","import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\"\n","\n","notebook_login()\n","api = HfApi()"],"metadata":{"id":"baJf3Wj-iUXs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["username=\" \"\n","api.create_repo(\n","    repo_id=f\"{username}/{MODEL_NAME}-{BPW:.1f}bpw-exl2\",\n","    repo_type=\"model\"\n",")\n","api.upload_folder(\n","    repo_id=f\"{username}/{MODEL_NAME}-{BPW:.1f}bpw-exl2\",\n","    folder_path=\"quant\",\n",")"],"metadata":{"id":"55_TDjBxiXvZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# one interfere with different model"],"metadata":{"id":"TJNXFZDTjCKJ"}},{"cell_type":"code","source":["!BUILD_CUDA_EXT=0 pip install -q auto-gptq transformers\n","import random\n","\n","from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n","from datasets import load_dataset\n","import torch\n","from transformers import AutoTokenizer\n","\n","\n","# Define base model and output directory\n","model_id = \"gpt2\"\n","out_dir = model_id + \"-GPTQ\"\n","\n","# Load quantize config, model and tokenizer\n","quantize_config = BaseQuantizeConfig(\n","    bits=4,\n","    group_size=128,\n","    damp_percent=0.01,\n","    desc_act=False,\n",")\n","model = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config)\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","# Load data and tokenize examples\n","n_samples = 1024\n","data = load_dataset(\"allenai/c4\", data_files=\"en/c4-train.00001-of-01024.json.gz\", split=f\"train[:{n_samples*5}]\")\n","tokenized_data = tokenizer(\"\\n\\n\".join(data['text']), return_tensors='pt')\n","\n","# Format tokenized examples\n","examples_ids = []\n","for _ in range(n_samples):\n","    i = random.randint(0, tokenized_data.input_ids.shape[1] - tokenizer.model_max_length - 1)\n","    j = i + tokenizer.model_max_length\n","    input_ids = tokenized_data.input_ids[:, i:j]\n","    attention_mask = torch.ones_like(input_ids)\n","    examples_ids.append({'input_ids': input_ids, 'attention_mask': attention_mask})\n","\n"],"metadata":{"id":"h3nHCC4YjBil"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","# Quantize with GPTQ\n","model.quantize(\n","    examples_ids,\n","    batch_size=1,\n","    use_triton=True,\n",")\n","\n","# Save model and tokenizer\n","model.save_quantized(out_dir, use_safetensors=True)\n","tokenizer.save_pretrained(out_dir)\n","\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Reload model and tokenizer\n","model = AutoGPTQForCausalLM.from_quantized(\n","    out_dir,\n","    device=device,\n","    use_triton=True,\n","    use_safetensors=True,\n",")\n","tokenizer = AutoTokenizer.from_pretrained(out_dir)\n","\n","from transformers import pipeline\n","\n","generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n","result = generator(\"I have a dream\", do_sample=True, max_length=50)[0]['generated_text']\n","print(result)"],"metadata":{"id":"xu_FjhSqjcC7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# imp work"],"metadata":{"id":"eEJiGXlJjoo4"}},{"cell_type":"code","source":["# Variables\n","MODEL_ID = \"mlabonne/EvolCodeLlama-7b\"\n","QUANTIZATION_METHODS = [\"q4_k_m\", \"q5_k_m\"]\n","\n","# Constants\n","MODEL_NAME = MODEL_ID.split('/')[-1]\n","\n","# Install llama.cpp\n","!git clone https://github.com/ggerganov/llama.cpp\n","!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n","!pip install -r llama.cpp/requirements.txt\n","\n","# Download model\n","!git lfs install\n","!git clone https://huggingface.co/{MODEL_ID}\n","\n","# Convert to fp16\n","fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n","!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}\n","\n","# Quantize the model for each method in the QUANTIZATION_METHODS list\n","for method in QUANTIZATION_METHODS:\n","    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n","    !./llama.cpp/quantize {fp16} {qtype} {method}"],"metadata":{"id":"_78jS9BvjrjE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","model_list = [file for file in os.listdir(MODEL_NAME) if \"gguf\" in file]\n","\n","prompt = input(\"Enter your prompt: \")\n","chosen_method = input(\"Name of the model (options: \" + \", \".join(model_list) + \"): \")\n","\n","# Verify the chosen method is in the list\n","if chosen_method not in model_list:\n","    print(\"Invalid name\")\n","else:\n","    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n","    !./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p \"{prompt}\""],"metadata":{"id":"dmEyJYypjurM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q huggingface_hub\n","from huggingface_hub import create_repo, HfApi\n","from google.colab import userdata\n","\n","username = \"mlabonne\"\n","\n","# Defined in the secrets tab in Google Colab\n","api = HfApi(token=userdata.get(\"HF_TOKEN\"))\n","\n","# Create empty repo\n","create_repo(\n","    repo_id = f\"{username}/{MODEL_NAME}-GGUF\",\n","    repo_type=\"model\",\n","    exist_ok=True,\n",")\n","\n","# Upload gguf files\n","api.upload_folder(\n","    folder_path=MODEL_NAME,\n","    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n","    allow_patterns=f\"*.gguf\",\n",")"],"metadata":{"id":"tqiaGt1mjxfx"},"execution_count":null,"outputs":[]}]}
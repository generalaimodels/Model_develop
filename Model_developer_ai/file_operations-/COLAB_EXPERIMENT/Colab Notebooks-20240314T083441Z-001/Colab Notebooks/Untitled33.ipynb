{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPntP2Be2mvVQIFj1ZY8ErF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["The load_dataset function is used to load a dataset from the Hugging Face Hub or from local files. It has many arguments that can be used to customize the loading process. Here is a table that explains each argument in detail:\n","\n","| Argument | Description |\n","| --- | --- |\n","| path | The path or name of the dataset to load. It can be a local path, a URL, or a dataset identifier on the Hub. |\n","| name | The name of the configuration to use when loading a dataset with multiple configurations. |\n","| data_dir | The directory where the data files are stored. It can be used to load a dataset from a specific subdirectory of the path. |\n","| data_files | The data files to load. It can be a single file, a list of files, or a dictionary that maps splits to files. |\n","| split | The split or splits to load. It can be a single split, a list of splits, or a slice expression. |\n","| cache_dir | The directory where the cached data are stored. It can be used to specify a custom location for the cache files. |\n","| features | The features of the dataset. It can be used to specify the schema of the data, such as the column names and types. |\n","| download_config | The configuration for the download process. It can be used to specify parameters such as the download URL, the expected checksum, or the extraction method. |\n","| download_mode | The download mode to use. It can be one of FORCE_REDOWNLOAD, REUSE_CACHE_IF_EXISTS, or REUSE_DATASET_IF_EXISTS. |\n","| verification_mode | The verification mode to use. It can be one of AUTO, FORCE_VERIFICATION, or SKIP_VERIFICATION. |\n","| ignore_verifications | A deprecated argument that is equivalent to setting verification_mode to SKIP_VERIFICATION. |\n","| keep_in_memory | Whether to load the dataset in memory or not. It can be used to improve performance by avoiding disk I/O. |\n","| save_infos | Whether to save the dataset information or not. It can be used to store metadata such as the citation or the license of the dataset. |\n","| revision | The version of the dataset to load. It can be a git commit hash, a branch name, or a tag name. |\n","| token | The token to use for authentication when loading a private dataset from the Hub. It can be a string or a boolean. |\n","| use_auth_token | A deprecated argument that is equivalent to setting token to True. |\n","| task | A deprecated argument that is not used anymore. |\n","| streaming | Whether to load the dataset in streaming mode or not. It can be used to load large datasets without downloading or caching them. |\n","| num_proc | The number of processes to use for parallel data processing. It can be used to speed up the loading and preprocessing of the dataset. |\n","| storage_options | The options to pass to the backend file system when loading data from remote sources. It can be used to specify credentials or other parameters. |\n","| **config_kwargs | Additional keyword arguments to pass to the builder configuration of the dataset. It can be used to customize the dataset loading script. |\n","\n","For more information and examples,\n","- (1) Load - Hugging Face. https://huggingface.co/docs/datasets/loading.\n","- (2) seaborn.load_dataset — seaborn 0.13.1 documentation. https://seaborn.pydata.org/generated/seaborn.load_dataset.html.\n","- (3) Prepare data for fine tuning Hugging Face models - Databricks. https://docs.databricks.com/en/machine-learning/train-model/huggingface/load-data.html.\n","- (4) Python:Seaborn | Built-in Functions | load_dataset() | Codecademy. https://www.codecademy.com/resources/docs/seaborn/built-in-functions/load-dataset.\n","(5) undefined. https://github.com/mwaskom/seaborn-data."],"metadata":{"id":"7nmQPR3MGSX3"}},{"cell_type":"markdown","source":["\n","\n","- To load a dataset from the Hugging Face Hub with a specific configuration and split:\n","\n","```python\n","from datasets import load_dataset\n","dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")\n","```\n","\n","- To load a dataset from a local directory with multiple data files for different splits:\n","\n","```python\n","from datasets import load_dataset\n","data_files = {\"train\": \"train.csv\", \"test\": \"test.csv\"}\n","dataset = load_dataset(\"path/to/my_dataset\", data_files=data_files)\n","```\n","\n","- To load a dataset from a URL with custom features and download options:\n","\n","```python\n","from datasets import load_dataset, Features, Value, DownloadConfig\n","features = Features({\"text\": Value(\"string\"), \"label\": Value(\"int32\")})\n","download_config = DownloadConfig(extract_compressed_file=True)\n","dataset = load_dataset(\"https://example.com/my_dataset.zip\", features=features, download_config=download_config)\n","```\n","\n","\n","\n","```python\n","from datasets import load_dataset\n","dataset = load_dataset(\"namespace/private_dataset\", streaming=True, revision=\"main\", token=\"my_token\")\n","```\n","\n","- To load a dataset with parallel processing and additional configuration parameters:\n","\n","```python\n","from datasets import load_dataset\n","dataset = load_dataset(\"squad\", num_proc=4, lang=\"en\")\n","```\n","Here are some more code snippets that show how to use the load_dataset function for different formats of files:\n","\n","- To load a dataset from a JSON file with a specific field as the column name:\n","\n","```python\n","from datasets import load_dataset\n","dataset = load_dataset(\"json\", data_files=\"my_data.json\", field=\"data\")\n","```\n","\n","- To load a dataset from a text file with one example per line:\n","\n","```python\n","from datasets import load_dataset\n","dataset = load_dataset(\"text\", data_files=\"my_data.txt\")\n","```\n","\n","- To load a dataset from a pandas dataframe:\n","\n","```python\n","from datasets import load_dataset\n","import pandas as pd\n","df = pd.read_csv(\"my_data.csv\")\n","dataset = load_dataset(\"pandas\", data_files=df)\n","```\n","\n","- To load a dataset from a CSV file with a header row and a delimiter:\n","\n","```python\n","from datasets import load_dataset\n","dataset = load_dataset(\"csv\", data_files=\"my_data.csv\", delimiter=\",\", header=True)\n","```\n","\n","- To load a dataset from multiple JSON files in a folder:\n","\n","```python\n","from datasets import load_dataset\n","dataset = load_dataset(\"json\", data_files=\"path/to/my_folder/*.json\", field=\"data\")\n","```\n","\n","- (1) Load - Hugging Face. https://huggingface.co/docs/datasets/loading.\n","- (2) seaborn.load_dataset — seaborn 0.13.1 documentation. https://seaborn.pydata.org/generated/seaborn.load_dataset.html.\n","- (3) Quickstart - Hugging Face. https://bing.com/search?q=load_dataset+function+examples.\n","- (4) Create a dataset loading script - Hugging Face. https://huggingface.co/docs/datasets/dataset_script.\n","- (5) mlflow.data — MLflow 2.9.2 documentation. https://mlflow.org/docs/latest/python_api/mlflow.data.html.\n","- (6) undefined. https://github.com/mwaskom/seaborn-data."],"metadata":{"id":"Fmh2zO2rHGgV"}},{"cell_type":"code","source":["!pip install -q -U datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oK8uYK0RMJht","executionInfo":{"status":"ok","timestamp":1705936987724,"user_tz":-330,"elapsed":13314,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"d45e9685-2097-461c-9e42-aade47ed7ce8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","\n","dataset=load_dataset(\"fka/awesome-chatgpt-prompts\")\n","\n","split = list(dataset.keys())\n","columns = dataset[str(split[0])].column_names\n","\n"],"metadata":{"id":"QzYOU4gP1uqU","executionInfo":{"status":"ok","timestamp":1705938389938,"user_tz":-330,"elapsed":3328,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","import os\n","\n","# Get a list of all csv files in the directory\n","folder_path = \"/content/sample_data/\"\n","csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n","\n","# Create a dictionary with file labels as keys (train, test, etc.) and file paths as values\n","data_files = {os.path.splitext(file)[0]: os.path.join(folder_path, file) for file in csv_files}\n","\n","# Load the dataset\n","dataset = load_dataset('csv', data_files=data_files)\n"],"metadata":{"id":"LYymllPgw7po"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datasets\n","\n","# List of datasets to load\n","datasets_to_load = [\n","    \"ag_news\",\n","    \"civil_comments\",\n","    \"dbpedia_14\",\n","    \"emotion\",\n","    \"gss\",\n","    \"hate_speech_offensive\",\n","    \"imdb\",\n","    \"quora_duplicate_questions\",\n","    \"sms_spam\",\n","    \"trec\",\n","]\n","\n","# Load the datasets\n","datasets_loaded = [datasets.load_dataset(dataset_name) for dataset_name in datasets_to_load]"],"metadata":{"id":"UE4sCeET0PSO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a dictionary of data type to loader function mappings\n","loaders = {\n","    \"remote\": load_dataset,\n","    \"json\": lambda source, **kwargs: load_dataset(\"json\", data_files=source, **kwargs),\n","    \"csv\": lambda source, **kwargs: load_dataset(\"csv\", data_files=source, **kwargs),\n","    \"parquet\": lambda source, **kwargs: load_dataset(\"parquet\", data_files=source, **kwargs),\n","    \"text\": lambda source, **kwargs: load_dataset(\"text\", data_files=source, **kwargs),\n","    \"pandas\": lambda source, **kwargs: load_dataset(\"pandas\", data_files={'train': source}, **kwargs)\n","}"],"metadata":{"id":"MzX_gkX1MgMv","executionInfo":{"status":"ok","timestamp":1705937079581,"user_tz":-330,"elapsed":801,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sgJ1TEgONXVQ","executionInfo":{"status":"ok","timestamp":1705927800706,"user_tz":-330,"elapsed":22502,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"edb8c57a-1cc9-4889-8768-0a4b6a9d351a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SQ32NEZ4GRs4"},"outputs":[],"source":["from typing import Union, Dict, Any\n","from datasets import load_dataset, Features, Value, DownloadConfig\n","import pandas as pd\n","\n","def load_dataset_from_source(\n","    source: Union[str, Dict[str, str]],\n","    data_type: str = None,  # 'json', 'csv', 'parquet', 'text', etc.\n","    split: str = \"train\",\n","    **kwargs: Any\n","):\n","    \"\"\"\n","    Loads a dataset from a specified source.\n","\n","    :param source: A path or URL to the dataset, or a mapping of split names to paths or URLs.\n","    :param data_type: The format of the dataset (e.g., 'json', 'csv', 'parquet', 'text').\n","                      If not provided, it will be inferred from the file extension or source content.\n","    :param split: The dataset split to load. Defaults to 'train' if not specified.\n","    :param kwargs: Additional keyword arguments for specific data types or custom configurations.\n","    :return: A `Dataset` or `DatasetDict` object.\n","    \"\"\"\n","\n","    # Infer the data type from the source if not provided\n","    if not data_type:\n","        if isinstance(source, str):\n","            if source.endswith(\".json\"):\n","                data_type = \"json\"\n","            elif source.endswith(\".csv\"):\n","                data_type = \"csv\"\n","            elif source.endswith(\".parquet\"):\n","                data_type = \"parquet\"\n","            elif source.endswith(\".txt\"):\n","                data_type = \"text\"\n","            elif source.startswith(\"http\"):\n","                data_type = \"remote\"\n","            else:\n","                raise ValueError(\"Unable to infer the data type from the source. Please specify the data_type.\")\n","        elif isinstance(source, dict):\n","            # Assume the first file in the dictionary to infer the data type\n","            first_key = next(iter(source))\n","            return load_dataset_from_source(source[first_key], data_type, split, **kwargs)\n","        else:\n","            raise ValueError(\"Source must be a string or a dictionary mapping split names to paths or URLs.\")\n","\n","    # Load the dataset based on the inferred or provided data type\n","    if data_type == \"remote\":\n","        dataset = load_dataset(source, split=split, **kwargs)\n","    elif data_type==\"\":\n","        dataset=load_dataset(source)\n","    else:\n","        dataset = load_dataset(data_type, data_files=source, split=split, **kwargs)\n","\n","    return dataset"]},{"cell_type":"code","source":["load_dataset(\"https://huggingface.co/datasets/fka/awesome-chatgpt-prompts/blob/main/prompt\",data_type='remote')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"XszipmfUSSzW","executionInfo":{"status":"error","timestamp":1705929252176,"user_tz":-330,"elapsed":691,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"91e0761e-3d72-412f-bb97-b0a238c96db0"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"Couldn't find a dataset script at /content/https:/huggingface.co/datasets/fka/awesome-chatgpt-prompts/blob/main/prompt/prompt.py or any data file in the same directory.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-51052fe40d9a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://huggingface.co/datasets/fka/awesome-chatgpt-prompts/blob/main/prompt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'remote'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2522\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2523\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2524\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2525\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   2193\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   2196\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1849\u001b[0m             \u001b[0;34mf\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m         )\n","\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a dataset script at /content/https:/huggingface.co/datasets/fka/awesome-chatgpt-prompts/blob/main/prompt/prompt.py or any data file in the same directory."]}]},{"cell_type":"code","source":["\n","\n","# Test 2: Load dataset from Hugging Face with dataset identifier.\n","dataset2 = load_dataset_from_source(\n","    \"fka/awesome-chatgpt-prompts\",data_type=\"\"\n",")\n","print(dataset2)\n","\n","\n","# Test 3: Load dataset from a local CSV file.\n","dataset3 = load_dataset_from_source(\n","    \"/content/sample_data/california_housing_test.csv\",\n","    data_type=\"csv\"\n",")\n","print(dataset3)\n","\n","# Test 4: Load dataset from a local JSON file.\n","dataset4 = load_dataset_from_source(\n","    \"/content/sample_data/anscombe.json\",\n","    data_type=\"json\"\n",")\n","print(dataset4)"],"metadata":{"id":"QHFgGyXSN9f_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset3.column_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vU9Ex1f9PKhT","executionInfo":{"status":"ok","timestamp":1705928274687,"user_tz":-330,"elapsed":3,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"7a409a1d-668e-4771-b884-26cd3ff434f1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['longitude',\n"," 'latitude',\n"," 'housing_median_age',\n"," 'total_rooms',\n"," 'total_bedrooms',\n"," 'population',\n"," 'households',\n"," 'median_income',\n"," 'median_house_value']"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["dataset4.column_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"73DXN9C7P-Kc","executionInfo":{"status":"ok","timestamp":1705928302662,"user_tz":-330,"elapsed":2,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"17e9cb17-b3fb-436d-de1b-3450c283ebf5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Series', 'Y', 'X']"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["!cat /content/drive/MyDrive/Github repo adversarical attack and defence's .gdoc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-AZA1W7OVKPT","executionInfo":{"status":"ok","timestamp":1705929665847,"user_tz":-330,"elapsed":465,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"c899446a-7c72-491b-c722-34e081d82795"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: -c: line 1: unexpected EOF while looking for matching `''\n","/bin/bash: -c: line 2: syntax error: unexpected end of file\n"]}]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPBoCMCFsSMVcDvZLkyoVjH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["require_version(\"transformers>=4.31.0,<4.35.0\", \"To fix: pip install \\\"transformers>=4.31.0,<4.35.0\\\"\")\n","require_version(\"datasets>=2.14.0\", \"To fix: pip install datasets>=2.14.0\")\n","require_version(\"accelerate>=0.21.0\", \"To fix: pip install accelerate>=0.21.0\")\n","require_version(\"peft>=0.6.0\", \"To fix: pip install peft>=0.6.0\")\n","require_version(\"trl>=0.7.4\", \"To fix: pip install trl>=0.7.4\")\n"],"metadata":{"id":"bWth2Pbl9mD7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z37RfmTM9fkb"},"outputs":[],"source":["!pip install transformers datasets accelerate>=0.21.0  peft>=0.6.0  trl>=0.7.4"]},{"cell_type":"code","source":["# Expert-level Python code to handle a Causal Language Model with a Value Head\n","import torch\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    PretrainedConfig,\n","    PreTrainedModel,\n","    PreTrainedTokenizerBase\n",")\n","from trl import AutoModelForCausalLMWithValueHead\n","\n","class CausalValueLanguageModel:\n","    def __init__(self, model_name_or_path: str):\n","        # Load configuration, tokenizer, base model, and model with value head\n","        self.config = AutoConfig.from_pretrained(model_name_or_path)\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","        self.model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n","        self.value_head_model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name_or_path)\n","\n","    def tokenize_input(self, text: str) -> torch.Tensor:\n","        \"\"\"Tokenize input text to tensor.\"\"\"\n","        return self.tokenizer.encode_plus(text, return_tensors='pt')\n","\n","    def generate_text(self, input_text: str, **generation_kwargs) -> str:\n","        \"\"\"Generate text using the causal language model.\"\"\"\n","        tokens_tensor = self.tokenize_input(input_text)\n","        output_sequences = self.model.generate(**tokens_tensor, **generation_kwargs)\n","        return self.tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n","\n","    def get_value_prediction(self, input_text: str) -> torch.Tensor:\n","        \"\"\"Get the value prediction from the value head model.\"\"\"\n","        tokens_tensor = self.tokenize_input(input_text)\n","        value_prediction = self.value_head_model(**tokens_tensor)\n","        return value_prediction\n","\n","    # Further methods for handling additional functionality can be added here\n","\n","# Usage\n","model_name = \"gpt2\"  # Replace with your model of choice\n","cvm = CausalValueLanguageModel(model_name)\n","\n","# Generate text example\n","generated_text = cvm.generate_text(\"The AI said,\")\n","print(generated_text)\n","\n","# Get value prediction example\n","value_prediction = cvm.get_value_prediction(\"The value of this model is\")\n","print(value_prediction)"],"metadata":{"id":"gQu7TsMz98T1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import logging\n","from argparse import ArgumentParser\n","from datasets import load_dataset\n","import torch\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    Trainer,\n","    TrainingArguments\n",")\n","from transformers.utils import logging as hf_logging\n","\n","# Setup logging\n","logger = logging.getLogger(__name__)\n","logging.basicConfig(level=logging.INFO)\n","hf_logging.set_verbosity_info()  # Set transformers logging to info\n","\n","# Define constants for defaults\n","DEFAULT_MODEL_NAME = \"gpt2\"\n","DEFAULT_DATASET_NAME = \"wikitext\"\n","DEFAULT_OUTPUT_DIR = \"./model_output\"\n","\n","# Argument parser for CLI interaction\n","parser = ArgumentParser(description=\"Fine-tune a causal language model.\")\n","parser.add_argument(\"--model_name_or_path\", type=str, default=DEFAULT_MODEL_NAME, help=\"Path to pretrained model or model identifier.\")\n","parser.add_argument(\"--dataset_name\", type=str, default=DEFAULT_DATASET_NAME, help=\"The name of the dataset to use.\")\n","parser.add_argument(\"--output_dir\", type=str, default=DEFAULT_OUTPUT_DIR, help=\"Where to store the fine-tuned model.\")\n","args = parser.parse_args()\n","\n","# Tokenize function for the dataset\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n","\n","# Initialize tokenizer and model\n","try:\n","    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n","    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)\n","except Exception as e:\n","    logger.error(f\"Model or tokenizer loading failed: {e}\")\n","    exit(1)\n","\n","# Load and preprocess the dataset\n","try:\n","    datasets = load_dataset(args.dataset_name)\n","    tokenized_datasets = datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n","except Exception as e:\n","    logger.error(f\"Dataset loading or tokenization failed: {e}\")\n","    exit(1)\n","\n","# Setup training arguments\n","training_args = TrainingArguments(\n","    output_dir=args.output_dir,\n","    overwrite_output_dir=True,\n","    per_device_train_batch_size=4,\n","    gradient_accumulation_steps=8,\n","    learning_rate=2e-5,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=10,\n","    save_steps=100,\n","    fp16=torch.cuda.is_available(),  # Mixed precision training if CUDA is available\n",")\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    tokenizer=tokenizer,\n",")\n","\n","# Launch fine-tuning\n","trainer.train()\n","\n","# Save the model after fine-tuning\n","model.save_pretrained(args.output_dir)\n","\n","# Define function to generate responses using the fine-tuned model\n","def generate_response(prompt_text):\n","    # Prepare the input for generation\n","    input_ids = tokenizer.encode(prompt_text, return_tensors=\"pt\")\n","    input_ids = input_ids.to(model.device)  # Ensure tensor is on the correct device\n","\n","    # Generate a response and decode\n","    with torch.no_grad():\n","        output_ids = model.generate(input_ids, max_length=256, pad_token_id=tokenizer.eos_token_id)\n","    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","    return response\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    logger.info(\"Starting interactive mode with the fine-tuned model.\")\n","    history = []\n","\n","    while True:\n","        user_input = input(\"User: \")\n","        if user_input.lower() == \"exit\":\n","            break\n","        history.append(user_input)\n","        model_input = \" \".join(history[-3:])  # Use the last 3 messages for context\n","        response = generate_response(model_input)\n","        print(f\"Assistant: {response}\")"],"metadata":{"id":"pbxVmNP1_c2p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import sys\n","sys.argv = sys.argv[:1]\n"],"metadata":{"id":"fCPCmSxDBU2a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import logging\n","from argparse import ArgumentParser\n","from datasets import load_dataset\n","import torch\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    Trainer,\n","    TrainingArguments\n",")\n","\n","def main():\n","    # Parse arguments from the command line\n","    parser = ArgumentParser(description=\"Fine-tune a causal language model on Wikipedia dataset.\")\n","    parser.add_argument(\"--model_name_or_path\", type=str, default=\"gpt2\", help=\"Path to pretrained model or model identifier.\")\n","    parser.add_argument(\"--output_dir\", type=str, default=\"./model_output\", help=\"Where to store the fine-tuned model.\")\n","    args = parser.parse_args()\n","\n","    # Setup logging\n","    logging.basicConfig(level=logging.INFO)\n","    logger = logging.getLogger(__name__)\n","\n","    # Load tokenizer and model\n","    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n","    config = AutoConfig.from_pretrained(args.model_name_or_path)\n","    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, config=config)\n","\n","    # Load dataset\n","    dataset = load_dataset(\"wikipedia\", \"20220301.en\")\n","\n","    # Tokenize dataset\n","    def tokenize_function(examples):\n","        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n","\n","    tokenized_datasets = dataset.map(\n","        tokenize_function, batched=True, remove_columns=[\"text\"]\n","    )\n","\n","    # Define training arguments\n","    training_args = TrainingArguments(\n","        output_dir=args.output_dir,\n","        per_device_train_batch_size=2,  # Adjust based on GPU memory\n","        num_train_epochs=1,  # Change to desired number of epochs\n","        weight_decay=0.01,\n","        logging_dir='./logs',\n","        logging_steps=1000,\n","        save_steps=5000,\n","        fp16=torch.cuda.is_available(),\n","    )\n","\n","    # Initialize Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_datasets[\"train\"],\n","        tokenizer=tokenizer\n","    )\n","\n","    # Train and save the model\n","    trainer.train()\n","    model.save_pretrained(args.output_dir)\n","\n","    logger.info(\"Training complete. Model saved.\")\n","\n","if __name__ == \"__main__\":\n","    main()\n","    !python colab_kernel_launcher.py --model_name_or_path gpt2 --output_dir ./model_output\n","\n",""],"metadata":{"id":"pjs6COwOAPnr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NjaRkw9dAo6y"},"execution_count":null,"outputs":[]}]}
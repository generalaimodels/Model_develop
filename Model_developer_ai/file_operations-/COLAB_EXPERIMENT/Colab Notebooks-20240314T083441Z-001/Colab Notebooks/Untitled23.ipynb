{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMwk7Qz6Eg6blXWlAvc0M+y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hl5KaptR-aeD"},"outputs":[],"source":["!pip install -q -U transformers"]},{"cell_type":"code","source":["from typing import Tuple\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    PreTrainedModel,\n","    PreTrainedTokenizer\n",")\n","\n","def load_model_and_tokenizer(model_name_or_path: str) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n","    try:\n","        # Load pre-trained model and tokenizer\n","        model = AutoModelForCausalLM.from_pretrained(model_name_or_path,trust_remote_code=True)\n","        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,trust_remote_code=True)\n","\n","        # Ensure the tokenizer has a padding token\n","        if tokenizer.pad_token is None:\n","            tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n","\n","        return model, tokenizer\n","    except Exception as e:\n","        raise RuntimeError(f\"Failed to load model and tokenizer: {e}\")\n","\n","def generate_text(\n","    model: PreTrainedModel,\n","    tokenizer: PreTrainedTokenizer,\n","    prompt: str,\n","    max_length: int = 100,\n","    temperature: float = 0.9\n",") -> str:\n","    try:\n","        # Encode the prompt\n","        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","\n","        # Generate tokens\n","        gen_tokens = model.generate(\n","            input_ids,\n","            do_sample=True,\n","            temperature=temperature,\n","            max_length=max_length\n","        )\n","\n","        # Decode the generated tokens to text\n","        gen_text = tokenizer.batch_decode(gen_tokens)[0]\n","\n","        return gen_text\n","    except Exception as e:\n","        raise RuntimeError(f\"Failed to generate text: {e}\")\n","\n","# Example usage:\n","# Load the model and tokenizer once\n","model_name_or_path = \"gpt2\"\n","model, tokenizer = load_model_and_tokenizer(model_name_or_path)\n","\n","# Generate text using the loaded model and tokenizer\n","prompt = \"Once upon a time\"\n","generated_text = generate_text(model, tokenizer, prompt)\n","print(generated_text)"],"metadata":{"id":"YksWtbXPWKiF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","model_directory = 'gpt2'\n","csv_file = 'questions_answers5.csv'\n","\n","model = AutoModelForCausalLM.from_pretrained(model_directory)\n","tokenizer = AutoTokenizer.from_pretrained(model_directory)\n","\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token':tokenizer.eos_token})\n","\n","\n","question_instuction=\"\"\"\n","\n","Generate adversarical attacks\n","\n","\n","\"\"\"\n","# Generate a question\n","def generate_question(model, tokenizer, max_length=20):\n","    prompt = f\"Generate a question: {question_instuction}\"\n","    generated = model.generate(tokenizer.encode(prompt, return_tensors=\"pt\"), max_length=max_length)\n","    decoded_output = tokenizer.decode(generated[0], skip_special_tokens=True)\n","    return decoded_output\n","\n","question_g = generate_question(model, tokenizer)\n","\n","\n","\n","\n","\n","answer_prompt=f\"\"\"\n","answer of  {question_g}?\n","\"\"\"\n","\n","\n","\n","\n","# Generate an answer to the question\n","def generate_answer(model, tokenizer, question, max_length=20):\n","    generated = model.generate(tokenizer.encode(question, return_tensors=\"pt\"), max_length=max_length)\n","    decoded_output = tokenizer.decode(generated[0], skip_special_tokens=True)\n","    return decoded_output\n","\n","answer_g = generate_answer(model, tokenizer, question=answer_prompt)\n","# print(\"\\nGenerated answer:\", answer)\n","\n","# Append the question and answer to the CSV file\n","def append_to_csv(csv_file, question, answer):\n","    with open(csv_file, mode='a', newline='', encoding='utf-8') as f:\n","      question_answer = f'{question},{answer}\\n'\n","      f.write(question_answer)\n","\n","\n","\n","for i in range(10):\n","  question_g = generate_question(model, tokenizer)\n","\n","\n","  answer_g = generate_answer(model, tokenizer, question=answer_prompt)\n","  append_to_csv(csv_file, question_g, answer_g)\n","\n","print(\"\\nQuestion and answer appended to CSV file.\")"],"metadata":{"id":"xg5v8YQc-uNe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","device='cpu'\n","def inference(prompt, temperature, max_length):\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","    input_ids = input_ids.to(device)\n","    gen_tokens = model.generate(\n","        input_ids,\n","        do_sample=True,\n","        temperature=temperature,\n","        max_length=max_length,\n","    )\n","    gen_text = tokenizer.batch_decode(gen_tokens)[0]\n","    return gen_text"],"metadata":{"id":"YeaV2FRTFidp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gpt_j(prompt, max_length=100, temp=0.9):\n","    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\",trust_remote_code=True)\n","    model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\",trust_remote_code=True)\n","\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","\n","    gen_tokens = model.generate(\n","        input_ids,\n","        do_sample=True,\n","        temperature=temp,\n","        max_length=max_length)\n","    gen_text = tokenizer.batch_decode(gen_tokens)[0]\n","\n","    return gen_text"],"metadata":{"id":"D3fFWZHcFmvK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip install einops"],"metadata":{"id":"dwDA97Z6HBRb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["k=gpt_j(prompt=\"tell a beautiful joke\")\n","print(k)"],"metadata":{"id":"2GhzULzaF8yO"},"execution_count":null,"outputs":[]}]}
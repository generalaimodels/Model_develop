{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","mount_file_id":"1e5IMQ1XHUrCmVa9k_KK4fjiemaY2Oo5k","authorship_tag":"ABX9TyPgsREqaanz0OXhKX2k98PC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qvIdmKVFwifn"},"outputs":[],"source":["!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"]},{"cell_type":"code","source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer"],"metadata":{"id":"GFhxgt6lwj3H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from datasets import Dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    TrainingArguments,\n","    Trainer,\n",")\n","from huggingface_hub import HfApi, create_repo\n","\n","# Define the token and username for Hugging Face Hub\n","HF_TOKEN = \"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\"\n","username = \"hemanthkandimalla\"\n","api = HfApi(token=HF_TOKEN)\n","\n","# Define the model and dataset parameters\n","model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n","dataset_name = \"mlabonne/guanaco-llama2-1k\"\n","new_model_name = \"/content/drive/MyDrive/HEMANTH_llms\"\n","csv_file_path = \"/content/drive/MyDrive/training.csv\"\n","text_column = \"input\"\n","id_column = \"id\"\n","question_column = \"output\"\n","max_seq_length = 512\n","\n","# Ensure that the CSV file path is provided\n","if not os.path.exists(csv_file_path):\n","    raise ValueError(\"A valid CSV file path must be provided.\")\n","\n","# Load the CSV file as a pandas DataFrame and then into a Hugging Face Dataset\n","df = pd.read_csv(csv_file_path)\n","dataset = Dataset.from_pandas(df)\n","\n","# Load the tokenizer for the model\n","tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Define the preprocessing function\n","def preprocess_function(examples):\n","    inputs = [ex + tokenizer.eos_token + q for ex, q in zip(examples[text_column], examples[question_column])]\n","    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=max_seq_length)\n","    return model_inputs\n","\n","# Apply the preprocessing function to the dataset\n","tokenized_dataset = dataset.map(preprocess_function, batched=True)\n","\n","# Load the model\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    num_train_epochs=1,\n","    per_device_train_batch_size=4,\n","    gradient_accumulation_steps=1,\n","    learning_rate=2e-4,\n","    weight_decay=0.001,\n","    max_grad_norm=0.3,\n","    lr_scheduler_type=\"cosine\",\n","    warmup_ratio=0.03,\n","    logging_dir='./logs',\n","    logging_steps=25,\n","    save_strategy=\"no\",\n","    report_to=\"none\"\n",")\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset,\n","    tokenizer=tokenizer\n",")\n","\n","# Train the model\n","trainer.train()\n","\n","# Save the fine-tuned model\n","trainer.save_model(new_model_name)\n","\n","# Create a repository on the Hugging Face Hub\n","create_repo(\n","    repo_id=f\"{username}/{new_model_name}-GGUF\",\n","    token=HF_TOKEN,\n","    repo_type=\"model\",\n","    exist_ok=True\n",")\n","\n","# Upload the fine-tuned model to the Hugging Face Hub\n","api.upload_folder(\n","    folder_path=new_model_name,\n","    repo_id=f\"{username}/{new_model_name}-GGUF\",\n","    token=HF_TOKEN\n",")\n","\n","print(f\"Training complete. Model saved to: {new_model_name}\")\n"],"metadata":{"id":"ktzPbain4X4p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import argparse\n","import os\n","import torch\n","from datasets import load_dataset, Dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","import pandas as pd\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer\n","from huggingface_hub import create_repo, HfApi\n","import torch\n","HF_TOKEN = \"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\"\n","username = \"hemanthkandimalla\"\n","\n","# Defined in the secrets tab in Google Colab\n","api = HfApi(token=HF_TOKEN)\n","\n","def main():\n","    import torch\n","    parser = argparse.ArgumentParser(description=\"Fine-tune a model from Hugging Face hub using specific parameters.\")\n","\n","    # Model parameters\n","    parser.add_argument(\"--model-name\", type=str, default=\"NousResearch/Llama-2-7b-chat-hf\", help=\"The model from the Hugging Face hub to train.\")\n","    parser.add_argument(\"--dataset-name\", type=str, default=\"mlabonne/guanaco-llama2-1k\", help=\"The instruction dataset to use.\")\n","    parser.add_argument(\"--new-model\", type=str, default=\"Hemanth_LLMs\", help=\"Fine-tuned model name.\")\n","    parser.add_argument(\"--csv-file-path\", type=str, default=\"/content/drive/MyDrive/training.csv\", help=\"Path to the local CSV file.\")\n","    parser.add_argument(\"--text-column\", type=str, default=\"input\", help=\"The column name for the input text in the CSV file.\")\n","    parser.add_argument(\"--id-column\", type=str, default=\"id\", help=\"The column name for the ids in the CSV file.\")\n","    parser.add_argument(\"--question-column\", type=str, default=\"question\", help=\"The column name for the questions in the CSV file.\")\n","\n","    # QLoRA parameters\n","    parser.add_argument(\"--lora-r\", type=int, default=64, help=\"LoRA attention dimension.\")\n","    parser.add_argument(\"--lora-alpha\", type=int, default=16, help=\"Alpha parameter for LoRA scaling.\")\n","    parser.add_argument(\"--lora-dropout\", type=float, default=0.1, help=\"Dropout probability for LoRA layers.\")\n","\n","    # bitsandbytes parameters\n","    parser.add_argument(\"--use-4bit\", action=\"store_true\", help=\"Activate 4-bit precision base model loading.\")\n","    parser.add_argument(\"--bnb-4bit-compute-dtype\", type=str, default=\"float16\", help=\"Compute dtype for 4-bit base models.\")\n","    parser.add_argument(\"--bnb-4bit-quant-type\", type=str, default=\"nf4\", help=\"Quantization type (fp4 or nf4).\")\n","    parser.add_argument(\"--use-nested-quant\", action=\"store_true\", help=\"Activate nested quantization for 4-bit base models (double quantization).\")\n","\n","    # TrainingArguments parameters\n","    parser.add_argument(\"--output-dir\", type=str, default=\"./results\", help=\"Output directory for model predictions and checkpoints.\")\n","    parser.add_argument(\"--num-train-epochs\", type=int, default=1, help=\"Number of training epochs.\")\n","    parser.add_argument(\"--fp16\", action=\"store_true\", help=\"Enable fp16 training.\")\n","    parser.add_argument(\"--bf16\", action=\"store_true\", help=\"Enable bf16 training.\")\n","    parser.add_argument(\"--per-device-train-batch-size\", type=int, default=4, help=\"Batch size per GPU for training.\")\n","    parser.add_argument(\"--per-device-eval-batch-size\", type=int, default=4, help=\"Batch size per GPU for evaluation.\")\n","    parser.add_argument(\"--gradient-accumulation-steps\", type=int, default=1, help=\"Number of update steps to accumulate gradients for.\")\n","    parser.add_argument(\"--gradient-checkpointing\", action=\"store_true\", help=\"Enable gradient checkpointing.\")\n","    parser.add_argument(\"--max-grad-norm\", type=float, default=0.3, help=\"Maximum gradient norm (gradient clipping).\")\n","    parser.add_argument(\"--learning-rate\", type=float, default=2e-4, help=\"Initial learning rate (AdamW optimizer).\")\n","    parser.add_argument(\"--weight-decay\", type=float, default=0.001, help=\"Weight decay to apply to all layers except bias/LayerNorm weights.\")\n","    parser.add_argument(\"--optim\", type=str, default=\"paged_adamw_32bit\", help=\"Optimizer to use.\")\n","    parser.add_argument(\"--lr-scheduler-type\", type=str, default=\"cosine\", help=\"Learning rate schedule.\")\n","    parser.add_argument(\"--max-steps\", type=int, default=-1, help=\"Number of training steps (overrides num_train_epochs).\")\n","    parser.add_argument(\"--warmup-ratio\", type=float, default=0.03, help=\"Ratio of steps for a linear warmup (from 0 to learning rate).\")\n","    parser.add_argument(\"--group-by-length\", action=\"store_true\", help=\"Group sequences into batches with same length.\")\n","    parser.add_argument(\"--save-steps\", type=int, default=0, help=\"Save checkpoint every X updates steps.\")\n","    parser.add_argument(\"--logging-steps\", type=int, default=25, help=\"Log every X updates steps.\")\n","\n","    # SFT parameters\n","    parser.add_argument(\"--max-seq-length\", type=int, default=None, help=\"Maximum sequence length to use.\")\n","    parser.add_argument(\"--packing\", action=\"store_true\", help=\"Pack multiple short examples in the same input sequence to increase efficiency.\")\n","    parser.add_argument(\"--device-map\", type=str, default='{\"\": 0}', help=\"Load the entire model on the GPU 0.\")\n","\n","    args = parser.parse_args()\n","\n","    max_seq_length = 512\n","\n","        # Ensure that the CSV file path is provided\n","    if not args.csv_file_path or not os.path.exists(args.csv_file_path):\n","        raise ValueError(\"A valid CSV file path must be provided.\")\n","        # Create a Hugging Face Dataset from the DataFrame\n","        # Load your local CSV file\n","    df = pd.read_csv(args.csv_file_path)\n","    dataset = Dataset.from_pandas(df)\n","\n","    # Load LLaMA tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(args.model_name, trust_remote_code=True)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n","\n","    def preprocess_function(examples):\n","      inputs = [ex + tokenizer.eos_token + q for ex, q in zip(examples[args.text_column], examples[args.question_column])]\n","      model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=max_seq_length, return_tensors=\"pt\")\n","      return {k: v.squeeze(0) for k, v in model_inputs.items()}\n","\n","\n","    tokenized_dataset = dataset.map(preprocess_function, batched=True)\n","\n","    # # Parse the device_map if provided\n","    # if args.device_map:\n","    #     args.device_map = eval(args.device_map)  # Convert string representation of dictionary to actual dictionary\n","\n","    # # Add code here to use the parsed arguments to train your model\n","    # # For example:\n","    # # model = train_model(args.model_name, args.dataset_name, ...)\n","    # # ...\n","\n","    # print(\"Arguments parsed and model training would start with the following configuration:\")\n","    # for arg in vars(args):\n","    #     print(f\"{arg}: {getattr(args, arg)}\")\n","\n","\n","\n","    # Load tokenizer and model with QLoRA configuration\n","    # compute_dtype = getattr(torch, args.bnb_4bit_compute_dtype)\n","    # bnb_config = BitsAndBytesConfig(\n","    #     load_in_4bit=args.use_4bit\n","    # )\n","\n","    # # Check GPU compatibility with bfloat16\n","    # if compute_dtype == torch.float16 and args.use_4bit:\n","    #     major, _ = torch.cuda.get_device_capability()\n","    #     if major >= 8:\n","    #         print(\"=\" * 80)\n","    #         print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","    #         print(\"=\" * 80)\n","\n","    # Load base model\n","    model = AutoModelForCausalLM.from_pretrained(\n","        args.model_name\n","    )\n","    model.config.use_cache = False\n","    # Load LoRA configuration\n","    peft_config = LoraConfig(\n","        lora_alpha=args.lora_alpha,\n","        lora_dropout=args.lora_dropout,\n","        r=args.lora_r,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","    )\n","\n","    # Set training parameters\n","    training_arguments = TrainingArguments(\n","        output_dir=args.output_dir,\n","        num_train_epochs=args.num_train_epochs,\n","        per_device_train_batch_size=args.per_device_train_batch_size,\n","        gradient_accumulation_steps=args.gradient_accumulation_steps,\n","        optim=args.optim,\n","        save_steps=args.save_steps,\n","        logging_steps=args.logging_steps,\n","        learning_rate=args.learning_rate,\n","        weight_decay=args.weight_decay,\n","        fp16=args.fp16,\n","        bf16=args.bf16,\n","        max_grad_norm=args.max_grad_norm,\n","        max_steps=args.max_steps,\n","        warmup_ratio=args.warmup_ratio,\n","        group_by_length=args.group_by_length,\n","        lr_scheduler_type=args.lr_scheduler_type,\n","        report_to=\"tensorboard\"\n","    )\n","    trainer = SFTTrainer(\n","        model=model,\n","        train_dataset=tokenized_dataset,\n","        peft_config=peft_config,\n","        dataset_text_field=\"input_ids\",  # Changed to input_ids since we tokenized the text\n","        tokenizer=tokenizer,\n","        args=training_arguments,\n","        packing=args.packing,\n","    )\n","\n","    # Train model\n","    trainer.train()\n","\n","    print(f'Fine-tunings of completed  ')\n","    # Save trained model\n","    trainer.model.save_pretrained(args.new_model)\n","\n","    print(\"Training complete. Model saved to:\", args.new_model)\n","\n","\n","    # Create empty repo\n","    create_repo(\n","        repo_id = f\"{username}/{args.new_model}-GGUF\",\n","        repo_type=\"model\",\n","        exist_ok=True,\n","    )\n","\n","    # Upload gguf files\n","    api.upload_folder(\n","        folder_path=args.new_model,\n","        repo_id=f\"{username}/{args.new_model}-GGUF\",\n","        allow_patterns=f\"*.gguf\",\n","    )\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"LsHa0wPM3wZe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The model that you want to train from the Hugging Face hub\n","model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n","# The instruction dataset to use\n","dataset_name = \"mlabonne/guanaco-llama2-1k\"\n","# Fine-tuned model name\n","new_model = \"llama-2-7b-miniguanaco\"\n","################################################################################\n","# QLoRA parameters\n","################################################################################\n","# LoRA attention dimension\n","lora_r = 64\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","\n","################################################################################\n","# TrainingArguments parameters\n","################################################################################\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"./results\"\n","# Number of training epochs\n","num_train_epochs = 1\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = False\n","bf16 = False\n","# Batch size per GPU for training\n","per_device_train_batch_size = 4\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 4\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-4\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.001\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","# Learning rate schedule\n","lr_scheduler_type = \"cosine\"\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = -1\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = True\n","# Save checkpoint every X updates steps\n","save_steps = 0\n","# Log every X updates steps\n","logging_steps = 25\n","################################################################################\n","# SFT parameters\n","################################################################################\n","# Maximum sequence length to use\n","max_seq_length = None\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = False\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0}"],"metadata":{"id":"PFus0BMowr3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load dataset (you can process it here)\n","dataset = load_dataset(dataset_name, split=\"train\")\n","# Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","\n","# Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"tensorboard\"\n",")\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")\n","# Train model\n","trainer.train()\n","# Save trained model\n","trainer.model.save_pretrained(new_model)"],"metadata":{"id":"ucA2r8DHxMED"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ignore warnings\n","logging.set_verbosity(logging.CRITICAL)\n","\n","# Run text generation pipeline with our next model\n","prompt = \"What is a large language model?\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"],"metadata":{"id":"3_5emSXyxgY3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Empty VRAM\n","del model\n","del pipe\n","del trainer\n","import gc\n","gc.collect()\n","gc.collect()"],"metadata":{"id":"CwABDjZxxlLG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reload model in FP16 and merge it with LoRA weights\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map=device_map,\n",")\n","model = PeftModel.from_pretrained(base_model, new_model)\n","model = model.merge_and_unload()\n","\n","# Reload tokenizer to save it\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""],"metadata":{"id":"E7KEIWhjxqld"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!huggingface-cli login\n","\n","model.push_to_hub(new_model, use_temp_dir=False)\n","tokenizer.push_to_hub(new_model, use_temp_dir=False)"],"metadata":{"id":"FikI5O7wxtl_"},"execution_count":null,"outputs":[]}]}
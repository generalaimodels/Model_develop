{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"mount_file_id":"1ytcRZr0ZqWSOtz5ORdJ66xwZ2_G9voGl","authorship_tag":"ABX9TyOLY4CvMt6UBRsPYMzDBmYh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 PyPDF2 datasets"],"metadata":{"id":"3zMiHFD-otNq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZWBKEGNoSYs"},"outputs":[],"source":["import os\n","import torch\n","from datasets import load_dataset\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer\n","\n","import argparse\n","from argparse import ArgumentTypeError\n","import json\n","import csv\n","import os\n","import pandas as pd\n","import random\n","import string\n","import PyPDF2\n","import io\n","import sys\n","import traceback\n","\n","import logging\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","def extract_text_from_pdf(filepath, length_of_chunk):\n","    try:\n","        # Open the PDF file in read-binary mode\n","        with open(filepath, 'rb') as file:\n","            # Initialize a PDF file reader object\n","            reader = PyPDF2.PdfReader(file)\n","\n","            # Initialize an empty string to hold the extracted text\n","            text = ''\n","\n","            # Loop through each page in the PDF\n","            for page in reader.pages:\n","                # Extract the text from the page and add it to the text string\n","                text += page.extract_text()\n","\n","        # Split the text into chunks of the specified length\n","        chunks = [text[i:i + length_of_chunk] for i in range(0, len(text), length_of_chunk)]\n","\n","        # Return the chunks\n","        return chunks\n","\n","    except FileNotFoundError:\n","        print(f\"The file {filepath} does not exist.\")\n","        return None\n","    except:\n","        print(\"An unexpected error occurred.\")\n","        return None\n","\n","def get_module_structure(directory_path):\n","  \"\"\" Generates a list of all files and their module structure within a given directory.\n","\n","  Args:\n","    directory_path (str): Path to the directory you want to explore.\n","\n","  Returns:\n","    A dictionary representing the module structure, with keys as module names and values as lists of file names.\n","  \"\"\"\n","\n","  module_structure = {}\n","\n","  for root, dirs, files in os.walk(directory_path):\n","     # Normalize the root path to ensure consistent separators\n","    normalized_root = os.path.normpath(root).replace('\\\\', '/')\n","        # Get the module name from the directory path\n","    module_name = os.path.basename(normalized_root)\n","    # Add the module name to the dictionary if it doesn't exist\n","    if module_name not in module_structure:\n","      module_structure[module_name] = []\n","\n","    # Add the file names to the list for the module\n","    module_structure[module_name].extend([os.path.join(normalized_root, file).replace('\\\\', '/') for file in files])\n","\n","  return module_structure\n","\n","def get_module_csv(directory_path):\n","    \"\"\" Generates a list of all .csv files and their module structure within a given directory.\n","\n","    Args:\n","      directory_path (str): Path to the directory you want to explore.\n","\n","    Returns:\n","      A dictionary representing the module structure, with keys as module names and values as lists of .csv file names.\n","    \"\"\"\n","\n","    module_csv = {}\n","\n","    for root, dirs, files in os.walk(directory_path):\n","        # Normalize the root path to ensure consistent separators\n","        normalized_root = os.path.normpath(root).replace('\\\\', '/')\n","        # Get the module name from the directory path\n","        module_name = os.path.basename(normalized_root)\n","\n","        # Add the module name to the dictionary if it doesn't exist\n","        if module_name not in module_csv:\n","            module_csv[module_name] = []\n","\n","        # Add only .csv file names to the list for the module\n","        csv_files = [file for file in files if file.lower().endswith('.csv')]\n","        module_csv[module_name].extend([os.path.join(normalized_root, file).replace('\\\\', '/') for file in csv_files])\n","\n","    return module_csv\n","\n","\n","def get_module_json(directory_path):\n","    \"\"\" Generates a list of all .json files and their module structure within a given directory.\n","\n","    Args:\n","      directory_path (str): Path to the directory you want to explore.\n","\n","    Returns:\n","      A dictionary representing the module structure, with keys as module names and values as lists of .json file names.\n","    \"\"\"\n","    module_json = {}\n","\n","    for root, dirs, files in os.walk(directory_path):\n","        # Normalize the root path to ensure consistent separators\n","        normalized_root = os.path.normpath(root).replace('\\\\', '/')\n","\n","        # Get the module name from the directory path\n","        module_name = os.path.basename(normalized_root)\n","\n","        # Add the module name to the dictionary if it doesn't exist\n","        if module_name not in module_json:\n","            module_json[module_name] = []\n","\n","        # Add only .json file names to the list for the module\n","        json_files = [file for file in files if file.lower().endswith('.json')]\n","        module_json[module_name].extend([os.path.join(normalized_root, file).replace('\\\\', '/') for file in json_files])\n","\n","    return module_json\n","\n","def normalize_json_data(data):\n","    \"\"\"\n","    Normalizes JSON data to a list of dictionaries if possible.\n","    \"\"\"\n","    # If it's a dictionary, wrap it in a list\n","    if isinstance(data, dict):\n","        return [data]\n","    # If it's a list of lists or primitives, convert it to a list of dictionaries\n","    elif isinstance(data, list) and all(not isinstance(item, dict) for item in data):\n","        # Assume each item in the list can be a row in the CSV\n","        return [{str(index): value for index, value in enumerate(item)} if isinstance(item, list) else {'value': item} for item in data]\n","    # If it's already a list of dictionaries, no need to modify\n","    elif isinstance(data, list) and all(isinstance(item, dict) for item in data):\n","        return data\n","    else:\n","        raise ValueError('JSON data structure is not supported.')\n","def json_to_csv(json_file_path, csv_file_path):\n","    \"\"\"\n","    Converts a JSON file into a CSV file.\n","    \"\"\"\n","    try:\n","        with open(json_file_path, 'r', encoding='utf-8') as json_file:\n","            data = json.load(json_file)\n","        normalized_data = normalize_json_data(data)\n","        headers = set().union(*(d.keys() for d in normalized_data))\n","        with open(csv_file_path, 'w', encoding='utf-8', newline='') as csv_file:\n","            writer = csv.DictWriter(csv_file, fieldnames=headers)\n","            writer.writeheader()\n","            writer.writerows(normalized_data)\n","        print(f\"Converted JSON to CSV: {csv_file_path}\")\n","    except Exception as e:\n","        print(f\"Error converting JSON to CSV: {e}\")\n","        traceback.print_exc()\n","\n","# def json_to_csv(json_file_path, csv_file_path):\n","#     \"\"\"\n","#     Converts a JSON file into a CSV file.\n","\n","#     Args:\n","#       json_file_path (str): Path to the input JSON file.\n","#       csv_file_path (str): Path to the output CSV file.\n","#     \"\"\"\n","#     try:\n","#         # Check if JSON file exists\n","#         if not os.path.isfile(json_file_path):\n","#             raise FileNotFoundError(f\"JSON file does not exist: {json_file_path}\")\n","\n","#         # Read JSON data\n","#         with open(json_file_path, 'r', encoding='utf-8') as json_file:\n","#             data = json.load(json_file)\n","\n","#         # Normalize the JSON data to a list of dictionaries\n","#         normalized_data = normalize_json_data(data)\n","\n","#         # Aggregate headers from all data entries\n","#         headers = {key for item in normalized_data for key in item.keys()}\n","\n","#         # Write CSV data\n","#         with open(csv_file_path, 'w', encoding='utf-8', newline='') as csv_file:\n","#             csv_writer = csv.DictWriter(csv_file, fieldnames=headers)\n","#             csv_writer.writeheader()  # Write the headers to the CSV file\n","#             csv_writer.writerows(normalized_data)  # Write data rows\n","\n","#         print(f\"Successfully converted {json_file_path} to {csv_file_path}.\")\n","\n","#     except json.JSONDecodeError:\n","#         print(f\"Error: Invalid JSON data in file {json_file_path}.\")\n","#     except FileNotFoundError as e:\n","#         print(str(e))\n","#     except ValueError as e:\n","#         print(str(e))\n","#     except Exception as e:\n","#         print(f\"An unexpected error occurred: {e}\")\n","\n","\n","\n","def print_random_row(dataframe):\n","    \"\"\"\n","    Prints a random row from a Pandas DataFrame.\n","\n","    Args:\n","      dataframe (pd.DataFrame): A Pandas DataFrame object.\n","\n","    Returns:\n","      str: A formatted string of the random row.\n","    \"\"\"\n","    if dataframe.empty:\n","        return \"The DataFrame is empty.\"\n","\n","    random_index = random.randint(0, len(dataframe) - 1)\n","    random_row = dataframe.iloc[random_index]\n","\n","    task = \"\\n\".join(f\"{column}: {value}\" for column, value in random_row.items())\n","    prompt = f\"Following the instructions:\\n{task}\"\n","    return prompt\n","def chunkify(text, length_of_chunk):\n","    \"\"\"\n","    Splits text into chunks of a given length.\n","\n","    Args:\n","        text (str): The text to be split.\n","        length_of_chunk (int): The maximum length of each chunk.\n","\n","    Returns:\n","        list of str: A list of text chunks.\n","    \"\"\"\n","    return [text[i:i+length_of_chunk] for i in range(0, len(text), length_of_chunk)]\n","def get_all_rows(dataframe, length_of_chunk=2048):\n","    \"\"\"\n","    Returns all rows from a Pandas DataFrame in specified chunk lengths.\n","\n","    Args:\n","        dataframe (pd.DataFrame): A Pandas DataFrame object.\n","        length_of_chunk (int): The maximum length of each chunk.\n","\n","    Returns:\n","        List[str]: A list of formatted strings of all the rows, chunked by length.\n","    \"\"\"\n","    if dataframe.empty:\n","        return [\"The DataFrame is empty.\"]\n","\n","    all_chunks = []\n","    for index, row in dataframe.iterrows():\n","        # Format the row into a string\n","        row_str = \"\\n\".join(f\"{column}: {value}\" for column, value in row.items())\n","        prompt = f\"Row {index}:\\n{row_str}\\n\"\n","        # Split the formatted row string into chunks\n","        row_chunks = chunkify(prompt, length_of_chunk)\n","        all_chunks.extend(row_chunks)\n","\n","    return all_chunks\n","\n","def process_json_file(json_file_path, length_of_chunk=2048):\n","    # Convert the JSON file to CSV format\n","    csv_file_path = json_file_path.replace('.json', '.csv')\n","    json_to_csv(json_file_path, csv_file_path)\n","\n","    # Load the CSV into a pandas DataFrame\n","    dataframe = pd.read_csv(csv_file_path)\n","\n","    # Get all rows from the DataFrame, formatted and chunked\n","    return get_all_rows(dataframe, length_of_chunk)\n","\n","def preprocess_text(text):\n","    \"\"\"\n","    Preprocesses the text.\n","\n","    Args:\n","      text (str): The text to preprocess.\n","\n","    Returns:\n","      str: The preprocessed text.\n","    \"\"\"\n","    # Example: simple preprocessing that removes punctuation and converts to lowercase.\n","    return text.translate(str.maketrans('', '', string.punctuation)).lower()\n","\n","def process_files_text(file_path, length_of_chunk=2048):\n","    \"\"\"\n","    Processes a file and returns a list of tuples containing the starting byte and the preprocessed chunk of text.\n","\n","    Args:\n","      file_path (str): The path to the file to process.\n","      length_of_chunk (int): The size of each chunk to be read and processed.\n","\n","    Returns:\n","      List[Tuple[int, str]]: A list of tuples containing the starting byte and the preprocessed chunk of text.\n","    \"\"\"\n","    # Check if file exists and is a file\n","    if not os.path.isfile(file_path):\n","        raise FileNotFoundError(f\"The file at path {file_path} does not exist.\")\n","\n","    processed_chunks = []\n","\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        while True:\n","            start_byte = file.tell()  # Get the current position in the file\n","            chunk = file.read(length_of_chunk)\n","            if not chunk:\n","                break  # End of file reached\n","            preprocessed_chunk = preprocess_text(chunk)\n","            processed_chunks.append((start_byte, preprocessed_chunk))\n","            if len(chunk) < length_of_chunk:\n","                break  # Last chunk, smaller than max size, processed\n","\n","    return processed_chunks\n","# def process_files(file_path, length_of_chunk=200):\n","#     try:\n","#         file_extension = get_file_extension(file_path)\n","#         if not file_extension:\n","#             # Optionally handle files with no extension differently here\n","#             raise ValueError(f'Error processing file {file_path}: File has no extension')\n","#         if file_extension == '.pdf':\n","#             return extract_text_from_pdf(file_path, length_of_chunk)\n","#         elif file_extension in ['.json', '.csv']:\n","#             return process_json_file(file_path, length_of_chunk)\n","#         elif file_extension in ['.txt', '.md', '.py', '.ipynb']:\n","#             return process_files_text(file_path, length_of_chunk)\n","#         else:\n","#             raise NotImplementedError(f'File type {file_extension} is not supported.')\n","#     except (ValueError, NotImplementedError) as e:\n","#         # Log error and continue with the next file\n","#         print(f\"Skipping file {file_path} due to error: {e}\")\n","#         return None\n","\n","def get_file_extension(file_path):\n","    _, file_extension = os.path.splitext(file_path)\n","    return file_extension.lower()\n","\n","def process_files(file_path, length_of_chunk=2048):\n","    \"\"\"\n","    Processes a file and returns a list of processed chunks of text.\n","    \"\"\"\n","    try:\n","        file_extension = get_file_extension(file_path)\n","        if not file_extension:\n","            raise ValueError(f'File {file_path} has no extension.')\n","\n","        if file_extension == '.pdf':\n","            return extract_text_from_pdf(file_path, length_of_chunk)\n","        elif file_extension == '.json':\n","            return process_json_file(file_path, length_of_chunk)\n","        elif file_extension == '.csv':\n","            dataframe = pd.read_csv(file_path)\n","            return get_all_rows(dataframe, length_of_chunk)\n","        elif file_extension in ['.txt', '.md', '.py']:\n","            return process_files_text(file_path, length_of_chunk)\n","        else:\n","            raise NotImplementedError(f'File type {file_extension} is not supported.')\n","\n","    except FileNotFoundError:\n","        print(f'File not found: {file_path}')\n","    except ValueError as ve:\n","        print(f'ValueError: {ve}')\n","    except NotImplementedError as nie:\n","        print(f'NotImplementedError: {nie}')\n","    except Exception as e:\n","        print(f'An unexpected error occurred while processing {file_path}: {e}')\n","        traceback.print_exc()\n","\n","# directory_path = \"/content/drive/MyDrive\"\n","# try:\n","#     module_structure = get_module_structure(directory_path)\n","#     for module, files in module_structure.items():\n","#         print(f'Module: {module}')\n","#         for file in files:\n","#             try:\n","#                 result = process_files(file, length_of_chunk=1000)\n","#                 # Only print the result if it's not None\n","#                 if result is not None:\n","#                     print(\"\\n\", result[:1][0])\n","#             except Exception as e:\n","#                 # Catch any other unexpected exceptions and log them\n","#                 print(f\"An error occurred while processing {file}: {e}\")\n","#         print(\"\\n\")\n","# except Exception as e:\n","#     print(f\"An error occurred while getting the module structure: {e}\")\n","\n","\n","class CustomTextDataset(Dataset):\n","    def __init__(self, tokenizer, file_paths, length_of_chunk):\n","        self.tokenizer = tokenizer\n","        self.texts = []\n","        self.length_of_chunk = length_of_chunk\n","        self.file_paths = file_paths\n","        for file_path in file_paths:\n","            self.texts.extend(self._load_and_process_file(file_path))\n","\n","    def _load_and_process_file(self, file_path):\n","        processed_data = process_files(file_path, self.length_of_chunk)\n","        if processed_data is None:\n","            return []\n","        return processed_data\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        return self.tokenizer(self.texts[idx], return_tensors='pt', truncation=True, padding='max_length', max_length=512)\n","    # Get file paths from the module structure\n","directory_path = \"C:/Users/heman/Desktop/Hemanth/LLM-Finetuning-Hub/\"\n","file_paths = []\n","try:\n","    module_structure = get_module_structure(directory_path)\n","    for module, files in module_structure.items():\n","        file_paths.extend([os.path.join(module, f) for f in files])\n","except Exception as e:\n","    print(f\"An error occurred while getting the module structure: {e}\")\n","\n","def positive_int(value):\n","    # Helper function to ensure the argument is a positive integer\n","    ivalue = int(value)\n","    if ivalue <= 0:\n","        raise ArgumentTypeError(f\"{value} is an invalid positive int value\")\n","    return ivalue\n","\n","def non_negative_int(value):\n","    # Helper function to ensure the argument is a non-negative integer\n","    ivalue = int(value)\n","    if ivalue < 0:\n","        raise ArgumentTypeError(f\"{value} is an invalid non-negative int value\")\n","    return ivalue\n","\n","def str_to_bool(value):\n","    # Helper function to convert string to boolean\n","    if value.lower() in ('yes', 'true', 't', 'y', '1'):\n","        return True\n","    elif value.lower() in ('no', 'false', 'f', 'n', '0'):\n","        return False\n","    else:\n","        raise ArgumentTypeError(f\"{value} is not a valid boolean value\")\n","\n","def create_parser():\n","    # Initialize the argument parser\n","    parser = argparse.ArgumentParser(description=\"Fine-tune a transformer model with customizable parameters.\")\n","\n","    # Add arguments to the parser\n","    parser.add_argument(\"--model_name\", type=str, default=\"gpt2\" ,help=\"The model that you want to train from the Hugging Face hub.\")\n","    parser.add_argument(\"--dataset_name\", type=str,  default='mlabonne/guanaco-llama2-1k',help=\"The instruction dataset to use.\")\n","    parser.add_argument(\"--Dataset_path\", type=str, default='/content/drive/MyDrive/Applied-Deep-Learning/06 - Speech & Music', help=\"Directory where the training data will be stored\")\n","    parser.add_argument(\"--new_model\", type=str, default=\"hemanthkandimalla_models\", help=\"Fine-tuned model name.\")\n","\n","    # QLoRA parameters\n","    parser.add_argument(\"--lora_r\", type=positive_int, default=64, help=\"LoRA attention dimension.\")\n","    parser.add_argument(\"--lora_alpha\", type=positive_int, default=16, help=\"Alpha parameter for LoRA scaling.\")\n","    parser.add_argument(\"--lora_dropout\", type=float, default=0.1, help=\"Dropout probability for LoRA layers.\")\n","\n","    # bitsandbytes parameters\n","    parser.add_argument(\"--use_4bit\", type=str_to_bool, default=True, help=\"Activate 4-bit precision base model loading.\")\n","    parser.add_argument(\"--bnb_4bit_compute_dtype\", type=str, default=\"float16\", choices=[\"float16\", \"float32\"], help=\"Compute dtype for 4-bit base models.\")\n","    parser.add_argument(\"--bnb_4bit_quant_type\", type=str, default=\"nf4\", choices=[\"fp4\", \"nf4\"], help=\"Quantization type (fp4 or nf4).\")\n","    parser.add_argument(\"--use_nested_quant\", type=str_to_bool, default=False, help=\"Activate nested quantization for 4-bit base models (double quantization).\")\n","\n","    # TrainingArguments parameters\n","    parser.add_argument(\"--output_dir\", type=str, default=\"./results\", help=\"Output directory for model predictions and checkpoints.\")\n","    parser.add_argument(\"--num_train_epochs\", type=positive_int, default=1, help=\"Number of training epochs.\")\n","    parser.add_argument(\"--fp16\", type=str_to_bool, default=False, help=\"Enable fp16 training.\")\n","    parser.add_argument(\"--bf16\", type=str_to_bool, default=False, help=\"Enable bf16 training (set to True with an A100).\")\n","    parser.add_argument(\"--per_device_train_batch_size\", type=positive_int, default=4, help=\"Batch size per GPU for training.\")\n","    parser.add_argument(\"--per_device_eval_batch_size\", type=positive_int, default=4, help=\"Batch size per GPU for evaluation.\")\n","    parser.add_argument(\"--gradient_accumulation_steps\", type=positive_int, default=1, help=\"Number of update steps to accumulate gradients for.\")\n","    parser.add_argument(\"--gradient_checkpointing\", type=str_to_bool, default=True, help=\"Enable gradient checkpointing.\")\n","    parser.add_argument(\"--max_grad_norm\", type=float, default=0.3, help=\"Maximum gradient norm (gradient clipping).\")\n","    parser.add_argument(\"--learning_rate\", type=float, default=2e-4, help=\"Initial learning rate (AdamW optimizer).\")\n","    parser.add_argument(\"--weight_decay\", type=float, default=0.001, help=\"Weight decay for all layers except bias/LayerNorm weights.\")\n","    parser.add_argument(\"--optim\", type=str, default=\"paged_adamw_32bit\", choices=[\"adamw\", \"paged_adamw_32bit\"], help=\"Optimizer to use.\")\n","    parser.add_argument(\"--lr_scheduler_type\", type=str, default=\"cosine\", choices=[\"cosine\", \"linear\"], help=\"Learning rate schedule.\")\n","    parser.add_argument(\"--max_steps\", type=non_negative_int, default=-1, help=\"Number of training steps (overrides num_train_epochs if positive).\")\n","    parser.add_argument(\"--warmup_ratio\", type=float, default=0.03, help=\"Ratio of steps for a linear warmup.\")\n","    parser.add_argument(\"--group_by_length\", type=str_to_bool, default=True, help=\"Group sequences into batches with same length.\")\n","    parser.add_argument(\"--save_steps\", type=non_negative_int, default=0, help=\"Save checkpoint every X update steps.\")\n","    parser.add_argument(\"--logging_steps\", type=positive_int, default=25, help=\"Log every X update steps.\")\n","\n","    # SFT parameters\n","    parser.add_argument(\"--max_seq_length\", type=positive_int, default=None, help=\"Maximum sequence length to use. If None, use model's default.\")\n","    parser.add_argument(\"--packing\", type=str_to_bool, default=False, help=\"Pack multiple short examples in the same input sequence.\")\n","    parser.add_argument(\"--device_map\", type=str, default='{\"\": 0}', help=\"Load the entire model on the specified GPU.\")\n","\n","    return parser\n","\n","\n","\n","def main():\n","    parser = create_parser()\n","    args = parser.parse_args()\n","\n","    # Set up logging\n","    logging.basicConfig(level=logging.INFO)\n","    logger = logging.getLogger(__name__)\n","\n","    try:\n","        # Load dataset\n","        if args.Dataset_path is not None:\n","            directory_path = args.Dataset_path\n","            file_paths = []\n","            try:\n","                module_structure = get_module_structure(directory_path)\n","                for module, files in module_structure.items():\n","                  file_paths.extend([os.path.join(module, f) for f in files])\n","                tokenizer = AutoTokenizer.from_pretrained(args.model_name, trust_remote_code=True)\n","                dataset = CustomTextDataset(tokenizer, file_paths, length_of_chunk=1000)\n","                data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n","            except Exception as e:\n","                print(f\"An error occurred while getting the module structure: {e}\")\n","\n","\n","        else:\n","            dataset = load_dataset(args.dataset_name, split=\"train\")\n","    except Exception as e:\n","        logger.error(f\"Error loading dataset: {e}\")\n","        return\n","\n","\n","\n","\n","    try:\n","        # Set compute_dtype based on the argument\n","        compute_dtype = getattr(torch, args.bnb_4bit_compute_dtype)\n","\n","        # Check GPU compatibility with bfloat16\n","        if compute_dtype == torch.float16 and args.use_4bit:\n","            major, _ = torch.cuda.get_device_capability()\n","            if major >= 8:\n","                logger.info(\"=\" * 80)\n","                logger.info(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","                logger.info(\"=\" * 80)\n","\n","        # Load tokenizer and model with QLoRA configuration\n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=args.use_4bit,\n","            bnb_4bit_quant_type=args.bnb_4bit_quant_type,\n","            bnb_4bit_compute_dtype=compute_dtype,\n","            bnb_4bit_use_double_quant=args.use_nested_quant,\n","        )\n","\n","        model = AutoModelForCausalLM.from_pretrained(\n","            args.model_name,\n","            quantization_config=bnb_config,\n","            low_cpu_mem_usage=True\n","        )\n","        model.config.use_cache = False\n","        model.config.pretraining_tp = 1\n","\n","        # Load LLaMA tokenizer and configure padding\n","        tokenizer = AutoTokenizer.from_pretrained(args.model_name, trust_remote_code=True)\n","        tokenizer.pad_token = tokenizer.eos_token\n","        tokenizer.padding_side = \"right\"\n","\n","        # Load LoRA configuration\n","        peft_config = LoraConfig(\n","            lora_alpha=args.lora_alpha,\n","            lora_dropout=args.lora_dropout,\n","            r=args.lora_r,\n","            bias=\"none\",\n","            task_type=\"CAUSAL_LM\",\n","        )\n","\n","        # Set training parameters\n","        training_arguments = TrainingArguments(\n","            output_dir=args.output_dir,\n","            num_train_epochs=args.num_train_epochs,\n","            per_device_train_batch_size=args.per_device_train_batch_size,\n","            gradient_accumulation_steps=args.gradient_accumulation_steps,\n","            optim=args.optim,\n","            save_steps=args.save_steps,\n","            logging_steps=args.logging_steps,\n","            learning_rate=args.learning_rate,\n","            weight_decay=args.weight_decay,\n","            fp16=args.fp16,\n","            bf16=args.bf16,\n","            max_grad_norm=args.max_grad_norm,\n","            max_steps=args.max_steps if args.max_steps >= 0 else None,\n","            warmup_ratio=args.warmup_ratio,\n","            group_by_length=args.group_by_length,\n","            lr_scheduler_type=args.lr_scheduler_type,\n","            report_to=\"tensorboard\"\n","        )\n","\n","    except Exception as e:\n","        logger.error(f\"Error setting up model and tokenizer: {e}\")\n","        return\n","\n","    try:\n","        # Set supervised fine-tuning parameters\n","        trainer = SFTTrainer(\n","            model=model,\n","            train_dataset=dataset,\n","            peft_config=peft_config,\n","            dataset_text_field=\"text\",\n","            max_seq_length=args.max_seq_length,\n","            tokenizer=tokenizer,\n","            args=training_arguments,\n","            packing=args.packing,\n","            data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'][0] for f in data]),\n","                                'attention_mask': torch.stack([f['attention_mask'][0] for f in data]),\n","                                'labels': torch.stack([f['input_ids'][0] for f in data])},\n","        )\n","\n","        # Train model\n","        trainer.train()\n","\n","        # Save trained model\n","        # Save trained model\n","        trainer.save_model(args.output_dir)\n","\n","        # Optionally, evaluate the model after training\n","        if args.do_eval:\n","            # Load evaluation dataset\n","            eval_dataset = load_dataset(args.dataset_name, split=\"validation\")\n","            # Evaluate model\n","            results = trainer.evaluate(eval_dataset)\n","            logger.info(f\"Evaluation results: {results}\")\n","\n","    except Exception as e:\n","        logger.error(f\"Error during training or evaluation: {e}\")\n","        return\n","\n","    logger.info(\"Training and evaluation completed successfully.\")\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"8anZRbqQoVxV"},"execution_count":null,"outputs":[]}]}
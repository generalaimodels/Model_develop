{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNsFqvVkV1irPAdjaVS1XGm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2vbhew2DFDbd"},"outputs":[],"source":["!pip install -q -U deepspeed-mii"]},{"cell_type":"code","source":["import time\n","import mii\n","\n","# Initialize the pipeline\n","pipe = mii.pipeline(\"microsoft/phi-2\")\n","\n","# Define input prompts\n","prompts = [\"DeepSpeed is\", \"Seattle is\"]\n","\n","# Start timing\n","start_time = time.time()\n","\n","# Generate the response using the model\n","response = pipepe(prompts, max_new_tokens=128)\n","\n","# End timing\n","end_time = time.time()\n","\n","# Calculate inference time\n","inference_time = end_time - start_time\n","\n","# Print the response\n","print(response)\n","\n","# Print the inference time\n","print(f\"Inference time: {inference_time} seconds\")"],"metadata":{"id":"BSugXQfCFb3r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import mii\n","client = mii.serve(\"mistralai/Mistral-7B-v0.1\")\n","response = client.generate([\"Deepspeed is\", \"Seattle is\"], max_new_tokens=128)\n","print(response)"],"metadata":{"id":"3SYoP5ryL9om"},"execution_count":null,"outputs":[]}]}
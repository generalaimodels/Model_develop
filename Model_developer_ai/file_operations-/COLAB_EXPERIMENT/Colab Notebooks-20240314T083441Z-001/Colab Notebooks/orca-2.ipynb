{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMjakPFHtb1fribeIxnDu++"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["\n","\n","- Author: Hemanth\n","- Date : 22-11-2023\n","\n","\n","\n","\n","\n"],"metadata":{"id":"XNt36Rc9o8j6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PCWRbLThm3T1"},"outputs":[],"source":["! pip install transformers\n","!pip install accelerate"]},{"cell_type":"markdown","source":["# Orca-2: Teaching Small Language Models How to Reason\n","\n","\n","\n","The progress from Orca 1 to Orca 2 marks a significant leap in empowering smaller Language Models (LMs) with enhanced reasoning abilities. Orca 1 demonstrated remarkable performance by learning from rich signals, surpassing traditional instruction-tuned models in benchmarks like BigBench Hard and AGIEval.\n","\n","In our pursuit with Orca 2, we've delved deeper into refining training signals to augment the reasoning capabilities of smaller LMs. The common approach in training smaller LMs often centered on imitation learning, replicating outputs of more powerful models. However, we argue that over-reliance on imitation could potentially constrain the potential of smaller models.\n","\n","Our focus in Orca 2 has shifted towards teaching these smaller LMs to employ diverse solution strategies tailored to different tasks, distinct from those used by larger models. For instance, while larger models might directly answer complex tasks, smaller models, constrained in capacity, might benefit from alternative approaches.\n","\n","The innovation lies not merely in imparting various reasoning techniques such as step-by-step analysis, recall-based generation, or direct answering to these smaller models, but in enabling them to discern the most effective strategy for each task independently.\n","\n","Evaluating Orca 2 involved a meticulous examination across 15 diverse benchmarks, encompassing roughly 100 tasks and over 36,000 distinct prompts. The remarkable outcome was Orca 2's substantial outperformance of models of similar sizes, even achieving comparable or superior performance levels to models 5-10 times larger. Particularly notable was its prowess in complex tasks that tested advanced reasoning abilities in zero-shot settings.\n","\n","To foster further advancements and encourage collaborative research, we've taken the step to open-source Orca 2. This initiative aims to stimulate ongoing exploration, evaluation, and alignment of smaller Language Models, ushering in a new era of scientific innovation and development in the field."],"metadata":{"id":"IkNuWdwCm4Fn"}},{"cell_type":"code","source":["import torch\n","import transformers\n","\n","if torch.cuda.is_available():\n","    torch.set_default_device(\"cuda\")\n","else:\n","    torch.set_default_device(\"cpu\")\n","\n","model = transformers.AutoModelForCausalLM.from_pretrained(\"microsoft/Orca-2-13b\" )\n","\n","# https://github.com/huggingface/transformers/issues/27132\n","# please use the slow tokenizer since fast and slow tokenizer produces different tokens\n","tokenizer = transformers.AutoTokenizer.from_pretrained(\n","        \"microsoft/Orca-2-13b\",\n","        use_fast=False,\n","    )\n","\n","system_message = \"You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.\"\n","user_message = \"How can you determine if a restaurant is popular among locals or mainly attracts tourists, and why might this information be useful?\"\n","\n","prompt = f\"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\"\n","\n","inputs = tokenizer(prompt, return_tensors='pt')\n","output_ids = model.generate(inputs[\"input_ids\"],)\n","answer = tokenizer.batch_decode(output_ids)[0]\n","\n","print(answer)\n","\n","# This example continues showing how to add a second turn message by the user to the conversation\n","second_turn_user_message = \"Give me a list of the key points of your first answer.\"\n","\n","# we set add_special_tokens=False because we dont want to automatically add a bos_token between messages\n","second_turn_message_in_markup = f\"\\n<|im_start|>user\\n{second_turn_user_message}<|im_end|>\\n<|im_start|>assistant\"\n","second_turn_tokens = tokenizer(second_turn_message_in_markup, return_tensors='pt', add_special_tokens=False)\n","second_turn_input = torch.cat([output_ids, second_turn_tokens['input_ids']], dim=1)\n","\n","output_ids_2 = model.generate(second_turn_input,)\n","second_turn_answer = tokenizer.batch_decode(output_ids_2)[0]\n","\n","print(second_turn_answer)\n"],"metadata":{"id":"6fQx80IIm5XQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import math\n","import transformers\n","import torch\n","\n","from azure.ai.contentsafety import ContentSafetyClient\n","from azure.core.credentials import AzureKeyCredential\n","from azure.core.exceptions import HttpResponseError\n","from azure.ai.contentsafety.models import AnalyzeTextOptions\n","\n","CONTENT_SAFETY_KEY = os.environ[\"CONTENT_SAFETY_KEY\"]\n","CONTENT_SAFETY_ENDPOINT = os.environ[\"CONTENT_SAFETY_ENDPOINT\"]\n","\n","# We use Azure AI Content Safety to filter out any content that reaches \"Medium\" threshold\n","# For more information: https://learn.microsoft.com/en-us/azure/ai-services/content-safety/\n","def should_filter_out(input_text, threshold=4):\n","    # Create an Content Safety client\n","    client = ContentSafetyClient(CONTENT_SAFETY_ENDPOINT, AzureKeyCredential(CONTENT_SAFETY_KEY))\n","\n","    # Construct a request\n","    request = AnalyzeTextOptions(text=input_text)\n","\n","    # Analyze text\n","    try:\n","        response = client.analyze_text(request)\n","    except HttpResponseError as e:\n","        print(\"Analyze text failed.\")\n","        if e.error:\n","            print(f\"Error code: {e.error.code}\")\n","            print(f\"Error message: {e.error.message}\")\n","            raise\n","        print(e)\n","        raise\n","\n","    categories = [\"hate_result\", \"self_harm_result\", \"sexual_result\", \"violence_result\"]\n","    max_score = -math.inf\n","    for category in categories:\n","        max_score = max(max_score, getattr(response, category).severity)\n","\n","    return max_score >= threshold\n","\n","model_path = 'microsoft/Orca-2-13b'\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = transformers.AutoModelForCausalLM.from_pretrained(model_path)\n","model.to(device)\n","\n","tokenizer = transformers.AutoTokenizer.from_pretrained(\n","    model_path,\n","    model_max_length=4096,\n","    padding_side=\"right\",\n","    use_fast=False,\n","    add_special_tokens=False,\n",")\n","\n","system_message = \"You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.\"\n","user_message = \"\\\" \\n :You can't just say, \\\"\\\"that's crap\\\"\\\" and remove it without gaining a consensus. You already know this, based on your block history. â€”/ \\\" \\nIs the comment obscene? \\nOptions : Yes, No.\"\n","\n","prompt =  f\"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\"\n","\n","inputs = tokenizer(prompt, return_tensors='pt')\n","inputs = inputs.to(device)\n","\n","output_ids = model.generate(inputs[\"input_ids\"], max_length=4096, do_sample=False, temperature=0.0, use_cache=True)\n","sequence_length = inputs[\"input_ids\"].shape[1]\n","new_output_ids = output_ids[:, sequence_length:]\n","answers = tokenizer.batch_decode(new_output_ids, skip_special_tokens=True)\n","final_output = answers[0] if not should_filter_out(answers[0]) else \"[Content Filtered]\"\n","\n","print(final_output)\n"],"metadata":{"id":"asfZuGx7n8eW"},"execution_count":null,"outputs":[]}]}
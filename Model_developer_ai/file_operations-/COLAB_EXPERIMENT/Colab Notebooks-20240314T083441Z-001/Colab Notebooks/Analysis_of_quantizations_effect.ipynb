{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNn7SO9jtf73eEeaCfIMGNx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["- **Author:** **Kandimalla Hemanth**\n","- **Date:** **08-01-2024**\n","- **E-mail:** **speechcodehemanth2@gmail.com**\n","- **What this Google Colab is about:** **Performances of LLMs on different types quantization**"],"metadata":{"id":"DTAeHFbvHm6Y"}},{"cell_type":"code","source":["!pip install -q  -U bitsandbytes>=0.39.0\n","!pip install -q  -U git+https://github.com/huggingface/accelerate.git\n","!pip install -q  -U git+https://github.com/huggingface/transformers.git"],"metadata":{"id":"GrDhDbzXBxpU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xdsgc_L0Bcfy"},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from copy import deepcopy\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","import numpy as np\n","\n","def uniform_quantize(X, bits):\n","    qmin = -2**(bits - 1)\n","    qmax = 2**(bits - 1) - 1\n","    scale = (torch.max(X) - torch.min(X)) / (qmax - qmin)\n","    X_quant = torch.round(X / scale) * scale\n","    return X_quant\n","\n","def non_uniform_quantize(X, bits):\n","    # Mu-law non-uniform quantization\n","    mu = 255.0  # Convert mu to a float to avoid integer division\n","    mu_tensor = torch.tensor(mu, device=X.device, dtype=X.dtype)  # Convert mu to a tensor\n","    X_mu = torch.sign(X) * torch.log1p(mu_tensor * torch.abs(X)) / torch.log1p(mu_tensor)\n","\n","    # Scale X_mu to the range [-1, 1] for uniform quantization\n","    X_mu = X_mu / torch.max(torch.abs(X_mu))\n","\n","    # Apply uniform quantization\n","    X_quant = uniform_quantize(X_mu, bits)\n","\n","    # Inverse mu-law transformation\n","    X_dequant = torch.sign(X_quant) * (torch.exp(torch.abs(X_quant) * torch.log1p(mu_tensor)) - 1) / mu_tensor\n","    return X_dequant\n","\n","torch.manual_seed(0)\n","device = 'cpu'\n","model_id = 'gpt2'\n","model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","weights = model.transformer.h[0].attn.c_attn.weight.data\n","\n","# Quantize and dequantize the weights with uniform and non-uniform quantization\n","bits_list = [4, 8, 16]\n","\n","uniform_quant_weights = {}\n","non_uniform_quant_weights = {}\n","\n","for bits in bits_list:\n","    uniform_quant = uniform_quantize(weights, bits=bits).to(device)\n","    non_uniform_quant = non_uniform_quantize(weights, bits=bits).to(device)\n","\n","    uniform_quant_weights[bits] = uniform_quant\n","    non_uniform_quant_weights[bits] = non_uniform_quant\n","\n","# Plotting\n","plt.style.use('ggplot')\n","fig, axs = plt.subplots(len(bits_list), 2, figsize=(12, 6*len(bits_list)), dpi=100, sharex=True)\n","\n","for i, bits in enumerate(bits_list):\n","    axs[i, 0].hist(weights.cpu().numpy().flatten(), bins=150, alpha=0.5, label='Original weights', color='blue', range=(-1, 1))\n","    axs[i, 0].hist(uniform_quant_weights[bits].cpu().numpy().flatten(), bins=150, alpha=0.5, label=f'{bits}-bit Uniform weights', color='red', range=(-1, 1))\n","    axs[i, 0].set_title(f'Original vs {bits}-bit Uniform Quantized Weights', fontsize=16)\n","\n","    axs[i, 1].hist(weights.cpu().numpy().flatten(), bins=150, alpha=0.5, label='Original weights', color='blue', range=(-1, 1))\n","    axs[i, 1].hist(non_uniform_quant_weights[bits].cpu().numpy().flatten(), bins=150, alpha=0.5, label=f'{bits}-bit Non-Uniform weights', color='green', range=(-1, 1))\n","    axs[i, 1].set_title(f'Original vs {bits}-bit Non-Uniform Quantized Weights', fontsize=16)\n","\n","    axs[i, 0].legend()\n","    axs[i, 1].legend()\n","    axs[i, 0].yaxis.set_major_formatter(ticker.EngFormatter())\n","    axs[i, 1].yaxis.set_major_formatter(ticker.EngFormatter())\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","source":["def generate_text(model, input_text, max_length=50):\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n","    output = model.generate(\n","        input_ids=input_ids,\n","        max_length=max_length,\n","        do_sample=True,\n","        top_k=30,\n","        pad_token_id=tokenizer.eos_token_id,\n","        attention_mask=input_ids.new_ones(input_ids.shape)\n","    )\n","    return tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","\n","\n","# Generate text with original weights\n","original_text = generate_text(model, \"I have a dream\")\n","print(f\"Original model:\\n{original_text}\\n\" + \"-\" * 50)\n","\n","# Function to update model weights\n","def update_model_weights(model, new_weights):\n","    own_state = model.state_dict()\n","    for name, param in new_weights.items():\n","        if name in own_state:\n","            own_state[name].copy_(param)\n","\n","# Generate and compare text using quantized weights\n","for bits in bits_list:\n","    # Uniform quantization\n","    model_uniform_quant = deepcopy(model)\n","    uniform_weights = {name: uniform_quantize(param.data, bits).to(device) for name, param in model_uniform_quant.named_parameters()}\n","    update_model_weights(model_uniform_quant, uniform_weights)\n","    uniform_text = generate_text(model_uniform_quant, \"I have a dream\")\n","    print(f\"{bits}-bit Uniform Quantized model:\\n{uniform_text}\\n\" + \"-\" * 50)\n","\n","    # Non-uniform quantization\n","    model_non_uniform_quant = deepcopy(model)\n","    non_uniform_weights = {name: non_uniform_quantize(param.data, bits).to(device) for name, param in model_non_uniform_quant.named_parameters()}\n","    update_model_weights(model_non_uniform_quant, non_uniform_weights)\n","    non_uniform_text = generate_text(model_non_uniform_quant, \"I have a dream\")\n","    print(f\"{bits}-bit Non-Uniform Quantized model:\\n{non_uniform_text}\\n\" + \"-\" * 50)"],"metadata":{"id":"JsNHkK0oBrmX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q -U seaborn"],"metadata":{"id":"sB08YoIAFX7j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from copy import deepcopy\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","import numpy as np\n","\n","def uniform_quantize(X, bits):\n","    qmin = -2**(bits - 1)\n","    qmax = 2**(bits - 1) - 1\n","    scale = (torch.max(X) - torch.min(X)) / (qmax - qmin)\n","    X_quant = torch.round(X / scale) * scale\n","    return X_quant\n","\n","def non_uniform_quantize(X, bits):\n","    # Mu-law non-uniform quantization\n","    mu = 255.0  # Convert mu to a float to avoid integer division\n","    mu_tensor = torch.tensor(mu, device=X.device, dtype=X.dtype)  # Convert mu to a tensor\n","    X_mu = torch.sign(X) * torch.log1p(mu_tensor * torch.abs(X)) / torch.log1p(mu_tensor)\n","\n","    # Scale X_mu to the range [-1, 1] for uniform quantization\n","    X_mu = X_mu / torch.max(torch.abs(X_mu))\n","\n","    # Apply uniform quantization\n","    X_quant = uniform_quantize(X_mu, bits)\n","\n","    # Inverse mu-law transformation\n","    X_dequant = torch.sign(X_quant) * (torch.exp(torch.abs(X_quant) * torch.log1p(mu_tensor)) - 1) / mu_tensor\n","    return X_dequant\n","\n","torch.manual_seed(0)\n","device = 'cpu'\n","model_id = 'gpt2'\n","model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","weights = model.transformer.h[0].attn.c_attn.weight.data\n","\n","# Set the style of the seaborn plot\n","sns.set(style=\"whitegrid\")\n","\n","# Create a figure and axes with subplots\n","fig, axs = plt.subplots(3, 2, figsize=(14, 18))\n","\n","# Define bit levels and corresponding colors for the plot\n","bits_levels = [4, 8, 16]\n","colors = ['red', 'green', 'blue']\n","\n","# Flatten the original weights for plotting\n","original_weights_flat = weights.cpu().numpy().flatten()\n","\n","# Plot density plots for quantized weights\n","for i, bits in enumerate(bits_levels):\n","    sns.histplot(original_weights_flat, bins=150, kde=True, color=\"black\", ax=axs[i, 0], label=\"Original weights\")\n","    sns.histplot(uniform_quant_weights[bits].cpu().numpy().flatten(), bins=150, kde=True, color=colors[i], ax=axs[i, 0], label=f\"{bits}-bit Uniform weights\")\n","    axs[i, 0].set_title(f\"Original vs {bits}-bit Uniform Quantized Weights\")\n","    axs[i, 0].legend()\n","\n","    sns.histplot(original_weights_flat, bins=150, kde=True, color=\"black\", ax=axs[i, 1], label=\"Original weights\")\n","    sns.histplot(non_uniform_quant_weights[bits].cpu().numpy().flatten(), bins=150, kde=True, color=colors[i], ax=axs[i, 1], label=f\"{bits}-bit Non-Uniform weights\")\n","    axs[i, 1].set_title(f\"Original vs {bits}-bit Non-Uniform Quantized Weights\")\n","    axs[i, 1].legend()\n","\n","# Adjust the layout\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"z20F92I3FHYB"},"execution_count":null,"outputs":[]}]}
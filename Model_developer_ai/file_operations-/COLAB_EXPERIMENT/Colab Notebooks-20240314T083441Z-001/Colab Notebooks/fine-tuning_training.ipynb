{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOM13pqhT0uC1Nl34OxYZKU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install  transformers"],"metadata":{"id":"lFqFYAhRCkiv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/google/sentencepiece.git\n","!cd sentencepiece\n","!mkdir build\n","!cd build\n","!cmake ..\n","!make -j $(nproc)\n","!sudo make install\n","!sudo ldconfig -v"],"metadata":{"id":"fbTQX4ZrESWw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install sentencepiece"],"metadata":{"id":"yTyx27_XEiZv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install accelerate"],"metadata":{"id":"TP3x0PHaFFtK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yvAiGpSVCiLN"},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer # ðŸ¤— Transformers, or\n","# from modelscope import AutoModelForCausalLM, AutoTokenizer # ðŸ¤– ModelScope\n","\n","def chat_template(messages):\n","    history = \"\"\n","    for message in messages:\n","        match message:\n","            case {\"role\": \"user\", \"content\": message}:\n","                history += f\"### Human: {message}\\n\\n### Assistant: \"\n","            case {\"role\": \"assistant\", \"content\": message}:\n","                history += message\n","    return history\n","\n","\n","model_path = \"SUSTech/SUS-Chat-34B\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_path\n",").eval()\n","\n","messages = [{\"role\": \"user\", \"content\": \"hi\"}]\n","\n","input_ids = tokenizer.encode(\n","    chat_template(messages), return_tensors=\"pt\", add_special_tokens=False\n",").to(\"cuda\")\n","output_ids = model.generate(input_ids.to(\"cuda\"), max_length=256)\n","response = tokenizer.decode(\n","    output_ids[0][input_ids.shape[1] :], skip_special_tokens=False\n",")\n","\n","messages.append({\"role\": \"assistant\", \"content\": response})\n","\n","# Second round\n","\n","messages.append({\"role\": \"user\", \"content\": \"What is the capital of China?\"})\n","\n","input_ids = tokenizer.encode(\n","    chat_template(messages), return_tensors=\"pt\", add_special_tokens=False\n",").to(\"cuda\")\n","output_ids = model.generate(input_ids.to(\"cuda\"), max_length=256)\n","response = tokenizer.decode(\n","    output_ids[0][input_ids.shape[1] :], skip_special_tokens=False\n",")\n","\n","messages.append({\"role\": \"assistant\", \"content\": response})\n"]}]}
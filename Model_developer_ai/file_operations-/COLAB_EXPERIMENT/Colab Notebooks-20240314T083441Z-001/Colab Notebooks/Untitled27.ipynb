{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPY+7OTlNA4s5hHgsvywljX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -q -U transformers datasets"],"metadata":{"id":"GRwDiJ2u1Fzy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1dth4Kix1jjD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ie4GAH_I77Yr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the dataset\n","dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")\n","\n","# Extract the string \"train\" from the output and print\n","output_str = str(list(dataset.keys())[0]) # train\n","# train\n","k=dataset[output_str].column_names  # LIST of columns\n","\n","inputs=[f'{dataset[output_str][k[0]]}: {X} Label :'for X in dataset[output_str][k[0]] ]\n","\n","# TASK  all list columns  make procesor pre-processing"],"metadata":{"id":"_-IIKlHT5yz1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs[0]"],"metadata":{"id":"dbJecShA7efK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Load the dataset\n","dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")\n","\n","# Extract the string \"train\" from the output and print\n","output_str = str(list(dataset.keys())[0])\n","# train\n","k=dataset[output_str].column_names\n","\n","\n","\n","\n","\n","def preprocess_function(examples):\n","    batch_size = len(examples[text_column])\n","    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n","    targets = [str(x) for x in examples[label_column]]\n","    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding='max_length')\n","    labels = tokenizer(targets, max_length=max_length, truncation=True, padding='max_length', add_special_tokens=False)\n","\n","    model_inputs[\"labels\"] = [\n","        [-100 if input_id == tokenizer.pad_token_id else label_id for input_id, label_id in zip(input_seq, label_seq)]\n","        for input_seq, label_seq in zip(model_inputs[\"input_ids\"], labels[\"input_ids\"])\n","    ]\n","\n","    return model_inputs\n","\n","#list[column, column]"],"metadata":{"id":"Oki3-UTs15qK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CEt-qeT-4Q4q"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v-SRUPo80czG"},"outputs":[],"source":["# data preprocessing\n","from transformers import AutoModelForCausalLM\n","from transformers import AutoTokenizer\n","from datasets import load_dataset\n","\n","\n","tokenizer = AutoTokenizer.from_pretrained('gpt2')\n","if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","def detect_text_and_label_columns(dataset,split:str):\n","    \"\"\"\n","    This function attempts to automatically detect the text and label columns.\n","    The heuristic used here is to find the two longest string columns.\n","    \"\"\"\n","    # Get the list of string columns in the dataset\n","    string_columns = [k for k, v in dataset['train'].features.items() if isinstance(v, datasets.Value) and v.dtype == 'string']\n","\n","# Check if each column exists in the dataset before sorting\n","    sorted_columns = sorted([col for col in string_columns if col in dataset['train'].column_names],\n","                        key=lambda col: max(len(x) for x in dataset['train'][col]),\n","                        reverse=True)\n","\n","    # if len(sorted_columns) < 2:\n","    #     raise ValueError(\"Unable to automatically detect text and label columns. Please specify them manually.\")\n","    return  sorted_columns\n","\n","\n","def preprocess_dataset(dataset_name):\n","    # Load a dataset\n","    dataset = load_dataset(dataset_name)\n","\n","    # Function to preprocess an example based on detected feature types\n","    def preprocess_function(example):\n","        new_example = {}\n","        # Loop over all features in the example\n","        for feature_name, feature_value in example.items():\n","            if isinstance(feature_value, str):  # For text features\n","                # Apply text preprocessing like tokenization, lowercasing, etc.\n","                new_example[feature_name] = feature_value.lower()  # Example operation\n","            elif isinstance(feature_value, list):  # For categorical features\n","                if isinstance(feature_value[0], int):  # Label encoded list\n","                    # Convert numeric labels to strings if names are provided in the feature\n","                    label_feature = dataset[\"train\"].features[feature_name]\n","                    if hasattr(label_feature, 'names'):\n","                        new_example[feature_name] = [label_feature.names[label] for label in feature_value]\n","                    else:\n","                        new_example[feature_name] = feature_value  # Unchanged\n","                else:\n","                    # Handle other list data types if necessary\n","                    pass\n","            else:  # For numerical or other types of features\n","                # Apply numerical preprocessing like normalization, scaling, etc.\n","                new_example[feature_name] = feature_value  # Example operation\n","\n","        return new_example\n","\n","    # Apply the preprocessing function to all splits in the dataset\n","    processed_dataset = dataset.map(preprocess_function, batched=True, num_proc=1)\n","\n","    return processed_dataset\n","\n","\n","\n","def preprocess_function(examples):\n","    batch_size = len(examples[text_column])\n","    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n","    targets = [str(x) for x in examples[label_column]]\n","    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding='max_length')\n","    labels = tokenizer(targets, max_length=max_length, truncation=True, padding='max_length', add_special_tokens=False)\n","\n","    model_inputs[\"labels\"] = [\n","        [-100 if input_id == tokenizer.pad_token_id else label_id for input_id, label_id in zip(input_seq, label_seq)]\n","        for input_seq, label_seq in zip(model_inputs[\"input_ids\"], labels[\"input_ids\"])\n","    ]\n","\n","    return model_inputs\n","\n","# Load and preprocess the dataset\n","dataset_name = \"LDJnr/Capybara\"\n","processed_dataset = preprocess_dataset(dataset_name)\n","\n","dataset=load_dataset(\"LDJnr/Capybara\")\n","sorted_columns=detect_text_and_label_columns(dataset,'train')\n","len(sorted_columns)\n","if len(sorted_columns)==1:\n","    text_column =sorted_columns[0]\n","else:\n","# Define your columns for text and labels based on your dataset\n","  text_column =sorted_columns[0] # Replace with actual text column name\n","  label_column = sorted_columns[1]  # Replace with actual label column name\n","\n","print(text_column )\n","\n","\n","# Additional required parameters\n","max_length = 512  # Example max length, adjust as needed\n","\n","# Now apply your tokenizer processing function to the processed_dataset\n","processed_datasets = processed_dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    num_proc=1,\n","    remove_columns=processed_dataset[\"train\"].column_names,\n","    load_from_cache_file=False,\n","    desc=\"Running tokenizer on dataset\",\n",")\n","\n","# Use the processed_datasets for training\n","train_dataset = processed_datasets[\"train\"][0]\n","print(train_dataset['input_ids'])"]}]}
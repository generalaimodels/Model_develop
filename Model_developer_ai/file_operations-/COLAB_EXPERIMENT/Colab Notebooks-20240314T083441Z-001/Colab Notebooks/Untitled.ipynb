{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1OceOInr6nUpWj2oawX9voY6BPcGmO4Qi","authorship_tag":"ABX9TyOeF8ASSN/NBSGAUYYnmhl4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_6o3piU1aYa5"},"outputs":[],"source":["import locale\n","def getpreferredencoding(do_setlocale=True):\n","  return \"UTF-8\"\n","locale.getpreferredencoding = getpreferredencoding"]},{"cell_type":"code","source":["!pip install -q -U torch transformers transformers accelerate bitsandbytes langchain sentence-transformers faiss-gpu openpyxl  datasets pacmap  unstructured unstructured[pdf]\n","!%reload_ext dotenv\n","!%dotenv"],"metadata":{"id":"YiMGW4-oa_RY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm.notebook import tqdm\n","import pandas as pd\n","from typing import Optional, List, Tuple\n","from datasets import Dataset\n","import matplotlib.pyplot as plt\n","from langchain_community.document_loaders import DirectoryLoader\n","from langchain.text_splitter import CharacterTextSplitter\n","pd.set_option(\n","    \"display.max_colwidth\", None\n",")\n","\n","folderpath='/content/drive/MyDrive/Applied-Deep-Learning'\n","# Loading all or specific extension like .pdf .py .csv .json .txt .md\n","print(\"============================* markdown files *==============================\")\n","ALL_markdown = DirectoryLoader(folderpath, glob= \"**/*.md\",show_progress=True, use_multithreading=True,silent_errors=True) #,txt ,.py, .csv ,.pdf,.md,.csv,.json \"**/[!.]*\"\n","docs_mds = ALL_markdown.load()\n","\n","print(\"============================* pdf files *===========================\")\n","ALL_PDF = DirectoryLoader(folderpath, glob= \"**/*.pdf\",show_progress=True, use_multithreading=True,silent_errors=True) #,txt ,.py, .csv ,.pdf,.md,.csv,.json \"**/[!.]*\"\n","docs_pds = ALL_PDF.load()\n","\n","print(\"============================* py files *===========================\")\n","ALL_Pythonfiles = DirectoryLoader(folderpath, glob= \"**/*.py\",show_progress=True, use_multithreading=True,silent_errors=True) #,txt ,.py, .csv ,.pdf,.md,.csv,.json \"**/[!.]*\"\n","docs_pys = ALL_Pythonfiles.load()\n","\n","print(\"============================* csv files *===========================\")\n","ALL_csvfiles = DirectoryLoader(folderpath, glob= \"**/*.csv\",show_progress=True, use_multithreading=True,silent_errors=True) #,txt ,.py, .csv ,.pdf,.md,.csv,.json \"**/[!.]*\"\n","docs_csvs = ALL_csvfiles.load()\n","\n","print(\"============================* ipynb files *===========================\")\n","ALL_csvfiles = DirectoryLoader(folderpath, glob= \"**/*.ipynb\",show_progress=True, use_multithreading=True,silent_errors=True) #,txt ,.py, .csv ,.pdf,.md,.csv,.json \"**/[!.]*\"\n","docs_ipynbs = ALL_csvfiles.load()\n"],"metadata":{"id":"6gzfGe0GbDYv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"lkP4kBzpaatE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","MARKDOWN_SEPARATORS = [\n","    \"\\n#{1,6} \",\n","    \"```\\n\",\n","    \"\\n\\\\*\\\\*\\\\*+\\n\",\n","    \"\\n---+\\n\",\n","    \"\\n___+\\n\",\n","    \"\\n\\n\",\n","    \"\\n\",\n","    \" \",\n","    \"\",\n","]\n","\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000,  # the maximum number of characters in a chunk: we selected this value arbitrarily\n","    chunk_overlap=100,  # the number of characters to overlap between chunks\n","    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n","    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n","    separators=MARKDOWN_SEPARATORS,\n",")\n","\n","docs_processed = []\n","for doc in docs_mds:\n","    docs_processed += text_splitter.split_documents([doc])\n","print(\"****completed*** md files\")\n","\n","for doc in docs_pds:\n","  docs_processed += text_splitter.split_documents([doc])\n","print(\"****completed*** pdfs files\")\n","\n","for doc in docs_pys:\n","  docs_processed += text_splitter.split_documents([doc])\n","print(\"****completed*** python files\")\n","\n","for doc in docs_csvs:\n","  docs_processed += text_splitter.split_documents([doc])\n","print(\"****completed*** csv files\")\n","\n","print(\"length :\",len(docs_processed))\n"],"metadata":{"id":"IXdk7Iy0cXc9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer\n","\n","# To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object used in the RecursiveCharacterTextSplitter.\n","print(\n","    f\"Model's maximum sequence length: {SentenceTransformer('thenlper/gte-small').max_seq_length}\"\n",")\n","\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\n","lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n","print(\"lengths of data:\",len(lengths))\n"],"metadata":{"id":"fFe8gFvycjDh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","def visualize_distribution(lengths, plot_type='hist'):\n","    \"\"\"\n","    Generate distribution plot for list of document lengths\n","\n","    Args:\n","        lengths (list): List of document lengths\n","        plot_type (str): Type of plot to generate. Options:\n","            - 'hist': Histogram\n","            - 'kde': Kernel Density Estimate\n","            - 'box': Boxplot\n","    \"\"\"\n","\n","    fig, ax = plt.subplots()\n","\n","    if plot_type == 'hist':\n","        ax.hist(lengths)\n","    # elif plot_type == 'kde':\n","    #     ax.sns.kdeplot(lengths)\n","    elif plot_type == 'box':\n","        ax.boxplot(lengths)\n","    else:\n","        raise ValueError(f\"Invalid plot_type: {plot_type}\")\n","\n","    ax.set_title(f\"{plot_type.title()} of Document Lengths\")\n","    ax.set_xlabel(\"Document Length\")\n","    ax.set_ylabel(\"Frequency\")\n","\n","    fig.tight_layout()\n","\n","    return fig\n","\n","\n","\n","fig1 = visualize_distribution(lengths, plot_type='hist')\n","# fig2 = visualize_distribution(lengths, plot_type='kde')\n","plt.show()\n","\n","\n","sns.kdeplot(lengths)\n","plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n","plt.show()\n","# Plot the distrubution of document lengths, counted as the number of tokens\n","fig = pd.Series(lengths).hist()\n","plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n","plt.show()\n","\n"],"metadata":{"id":"_0FMJ9KQcnqE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from transformers import AutoTokenizer\n","\n","EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n","\n","\n","def split_documents(\n","    chunk_size: int,\n","    knowledge_base,\n","    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",") :\n","    \"\"\"\n","    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n","    \"\"\"\n","    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n","        AutoTokenizer.from_pretrained(tokenizer_name),\n","        chunk_size=chunk_size,\n","        chunk_overlap=int(chunk_size / 10),\n","        add_start_index=True,\n","        strip_whitespace=True,\n","        separators=MARKDOWN_SEPARATORS,\n","    )\n","\n","    docs_processed = []\n","    for doc in docs_mds:\n","        docs_processed += text_splitter.split_documents([doc])\n","    print(\"****completed*** md files\")\n","\n","    for doc in docs_pds:\n","        docs_processed += text_splitter.split_documents([doc])\n","    print(\"****completed*** pdfs files\")\n","\n","    for doc in docs_pys:\n","        docs_processed += text_splitter.split_documents([doc])\n","    print(\"****completed*** python files\")\n","\n","    for doc in docs_csvs:\n","        docs_processed += text_splitter.split_documents([doc])\n","    print(\"****completed*** csv files\")\n","       for doc in docs_ipynbs:\n","        docs_processed += text_splitter.split_documents([doc])\n","    print(\"****completed*** ipynbs files\")\n","\n","    # Remove duplicates\n","    unique_texts = {}\n","    docs_processed_unique = []\n","    for doc in docs_processed:\n","        if doc.page_content not in unique_texts:\n","            unique_texts[doc.page_content] = True\n","            docs_processed_unique.append(doc)\n","\n","    return docs_processed_unique\n","\n","\n","docs_processed = split_documents(\n","    512,  # We choose a chunk size adapted to our model\n","    docs_processed,\n","    tokenizer_name=EMBEDDING_MODEL_NAME,\n",")\n","\n","# Let's visualize the chunk sizes we would have in tokens from a common model\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n","lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n","fig = pd.Series(lengths).hist()\n","plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n","plt.show()\n","\n","# Import seaborn\n","import seaborn as sns\n","# Plot the KDE of the document lengths\n","sns.kdeplot(lengths)\n","# Add a title to the plot\n","plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n","# Show the plot\n","plt.show()"],"metadata":{"id":"uutazOrqdXVB"},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMwG1Omod5BwCReS9/XwaPr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ej4E_cF2d7LB"},"outputs":[],"source":["!pip install -q -U transformers datasets"]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","from typing import Any, List, Tuple, Optional\n","from datasets import load_dataset, Dataset\n","\n","# Load the tokenizer for the specific model you want to use\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Replace \"bert-base-uncased\" with the model name you want\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","def transformers_tokenizer_function(examples: Any, tokenizer: tokenizer, columns_to_tokenize: List[str]) -> Any:\n","    # Tokenizes all columns specified in columns_to_tokenize and returns the processed output\n","    tokenized_output = {}\n","\n","    for col in columns_to_tokenize:\n","        # Make sure to pass the text data from each column to the tokenizer\n","        if col in examples:\n","            tokenized_batch = tokenizer(examples[col], padding='max_length', truncation=True, return_tensors='pt')\n","            for key, value in tokenized_batch.items():\n","                if key not in tokenized_output:\n","                    tokenized_output[key] = []\n","                tokenized_output[key].extend(value.numpy().tolist())  # Convert tensor to list and add to outputs\n","        else:\n","            raise ValueError(f\"The specified column '{col}' is not in the dataset.\")\n","\n","    return tokenized_output\n","# Now we'll integrate the tokenization function into the preprocess_dataset function\n","def preprocess_dataset(\n","    dataset_name: str,\n","    tokenizer: AutoTokenizer,  # Pass the initialized tokenizer object from transformers library\n","    columns_to_tokenize: List[str],\n","    random_seed: Optional[int] = None,\n","    split_ratio: float = 0.8,\n","    limit: Optional[int] = None\n",") -> Tuple[Dataset, Dataset]:\n","    # Same preprocess function as described before, but passing the tokenizer object and using to initialize the tokenize_function\n","    dataset = load_dataset(dataset_name)\n","    func_to_apply = lambda examples: transformers_tokenizer_function(examples, tokenizer, columns_to_tokenize)\n","    dataset = dataset.map(func_to_apply, batched=True)\n","\n","    if limit is not None:\n","        for split in dataset.keys():\n","            dataset[split] = dataset[split].select(range(min(limit, len(dataset[split]))))\n","\n","    if random_seed is not None:\n","        dataset = dataset.shuffle(seed=random_seed)\n","\n","    train_test_split = dataset['train'].train_test_split(train_size=split_ratio, seed=random_seed)\n","    return train_test_split['train'], train_test_split['test']\n","\n","# Example usage of the preprocess_dataset function:\n","column_names = ['task']  # Replace with the actual column names from your dataset\n","\n","# Replace 'your_dataset_name' with the actual name of your dataset and 'bert-base-uncased' with your desired model name\n","train_ds, test_ds = preprocess_dataset(\n","    dataset_name=\"TuringsSolutions/NYTWritingStyleGuide\",\n","    tokenizer=tokenizer,\n","    columns_to_tokenize=column_names,\n","    random_seed=42,\n","    split_ratio=0.8\n",")\n","\n","print(f\"Training Dataset Size: {len(train_ds)}\")\n","print(f\"Testing Dataset Size: {len(test_ds)}\")"],"metadata":{"id":"7beLbOOTeX4N"},"execution_count":null,"outputs":[]}]}
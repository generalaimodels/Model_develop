{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNyKBbmWfRDoiX43aUo8qW6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","## Tokenization in Transformers\n","\n","**Name:** Hemanth Kandimalla  \n","**Date:** 3-1-2024\n","\n","---\n","\n","\n"],"metadata":{"id":"bXYRaMaX_ovt"}},{"cell_type":"markdown","source":["The Hugging Face library offers a wide range of transformer models for natural language processing tasks. If you have mastered the basics and now want to dive into more advanced topics, here are some suggestions:\n","\n","- Fine-Tuning Models: Learn how to fine-tune pre-trained models on your specific task. This will allow you to leverage the power of transformers for your specific needs.\n","\n","- Multilingual Models: Hugging Face offers several multilingual models. You can learn how to train a single model on multiple languages.\n","\n","- Custom Models: You can learn how to create and train your own transformer models from scratch.\n","\n","- Pipeline Creation: Hugging Face provides a pipeline feature to easily process data and make predictions. You can learn how to create custom pipelines.\n","\n","- Training on Large Datasets: Handling and training models on large datasets is a common challenge in NLP. You can learn different strategies to handle this, such as gradient accumulation.\n","\n","- Distributed Training: You can learn how to use multiple GPUs to train your models faster.\n","\n","- Optimization Techniques: You can dive into various optimization techniques, like learning rate scheduling, weight decay, etc.\n","\n","- Advanced Tokenization Techniques: There are different tokenization techniques available, such as Byte Pair Encoding (BPE), SentencePiece, etc. Each has its own advantages and use-cases.\n","\n","- Interpretability of Transformer Models: Understanding why a model made a particular prediction is an active field of research. You can dive into this topic to understand your models better.\n","\n","- Use of Callbacks: Callbacks can be used to customize the training process, like saving the model at different stages, changing the learning rate, etc.\n","\n","You can find many tutorials and resources on these topics in the Hugging Face model hub and their documentation. Additionally, there are many courses and books available online that delve into these topics."],"metadata":{"id":"iHui8J3N1Kkq"}},{"cell_type":"markdown","source":["# Tokenization"],"metadata":{"id":"CHb-tf7g_XIs"}},{"cell_type":"markdown","source":["\n","\n","- Text preprocessing - Common preprocessing steps like lowercase, accent removal, punctuation handling that affect tokenization.\n","\n","- Tokenization algorithms - Learn popular algorithms like wordpiece, byte-pair encoding (BPE) used by models. Their pros and cons.\n","\n","- Subword tokenization - How models tokenize text to subwords for large vocabularies. Concepts like unified/non-unified tokenization.\n","\n","- Language-specific tokenizers - Differences in tokenizing patterns across languages like character-based for Chinese/Japanese.\n","\n","- Custom tokenizers - Building custom tokenizers for domain-specific texts using regex, lists etc.\n","\n","- Added token types - Special tokens used by models like CLS, SEP, PAD and their roles.\n","\n","- Contextual tokenizers - How contextual word embeddings capture subword/character level variations.\n","\n","- Benefits of subword tokens - How they handle out-of-vocabulary words, compound words effectively.\n","\n","- Embeddings for tokens - How token embeddings are learned during pretraining and their semantic relationships.\n","\n","- Tokenization for different tasks - Differences in tokenization for tasks like NER, QA, Summarization etc. based on needs.\n","\n","- Tokenization in pipelines - Integrating tokenization as first step in NLP pipelines for tasks.\n","\n","- Tokenization hyperparameters - Tuning parameters like token_min_freq, token_max_len for specific problems.\n","\n","- Analyzing tokenized texts - Visualization, analysis of effect of tokenization parameters qualitatively and quantitatively.\n","\n"],"metadata":{"id":"cHr-d-uV16Lp"}},{"cell_type":"code","source":["!pip install -q -U transformers"],"metadata":{"id":"kgBZSJSd20cD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizerFast\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","class TextDataset(Dataset):\n","    def __init__(self, texts, tokenizer, max_len):\n","        self.texts = texts\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, index):\n","        text = self.texts[index]\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","        return {\n","            'text': text,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten()\n","        }\n","\n","# Initialize the tokenizer\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","\n","# Assuming texts is your large dataset\n","texts = [\"Here's a sample text\"] * 180000 # replace with your own dataset\n","\n","# Create dataset and dataloader\n","dataset = TextDataset(texts, tokenizer, max_len=512)\n","dataloader = DataLoader(dataset, batch_size=16)\n","\n","# Then you can use this dataloader in your training loop with a model."],"metadata":{"id":"sJo5VE7a2KZz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load Tokenizer"],"metadata":{"id":"D0MxbMVB6FXE"}},{"cell_type":"code","source":["from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"metadata":{"id":"z5nj4fXX3O2R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokenize a Single Sentence"],"metadata":{"id":"hN4PomX_6Sna"}},{"cell_type":"code","source":["input_ids = tokenizer.encode(\"Hello, I am a single sentence!\", add_special_tokens=True)\n","print(input_ids)"],"metadata":{"id":"cfGAbk3g6Ujx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokenize a Batch of Sentences"],"metadata":{"id":"y6IYyl4W6hVr"}},{"cell_type":"code","source":["batch_sentences = [\"Hello I am a batch sentence!\", \"This is another one.\"]\n","input_ids = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n","print(input_ids)"],"metadata":{"id":"MWZq0EBr6dxf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokenize Large Text into Chunks"],"metadata":{"id":"du4CU4AH7DPx"}},{"cell_type":"code","source":["def chunk_text(text, chunk_size):\n","    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n","\n","text = \"your large text here...\"*10000\n","chunks = chunk_text(text, 1800)\n","encoded_chunks = [tokenizer.encode(chunk, add_special_tokens=True) for chunk in chunks]\n","print(encoded_chunks )"],"metadata":{"id":"FYlFwILQ7B_M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokenize Large Dataset"],"metadata":{"id":"IQ9lKb-i7kEf"}},{"cell_type":"code","source":["from transformers import BertTokenizer\n","\n","# Initialize tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Function to chunk text\n","def chunk_text(text, chunk_size):\n","    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n","\n","# Function to tokenize large dataset\n","def tokenize_large_dataset(dataset, tokenizer, chunk_size=1800):\n","    tokenized_dataset = []\n","    for text in dataset:\n","        chunks = chunk_text(text, chunk_size)\n","        tokenized_chunks = [tokenizer.encode(chunk, add_special_tokens=True) for chunk in chunks]\n","        tokenized_dataset.append(tokenized_chunks)\n","    return tokenized_dataset\n","\n","# Create a dataset (a list of texts)\n","dataset = [\"your large text here...\"*10000]\n","\n","# Tokenize the dataset\n","tokenized_dataset = tokenize_large_dataset(dataset, tokenizer)"],"metadata":{"id":"rjv8QnEM7oMg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenized_dataset)"],"metadata":{"id":"0sgIIDvz_AOx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokenize with Attention Masks"],"metadata":{"id":"AFQTff3V8Mr0"}},{"cell_type":"code","source":["encoding = tokenizer(\"Hello, I am a single sentence!\", add_special_tokens=True, truncation=True, max_length=50, padding='max_length', return_tensors=\"pt\")\n","input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n","print(input_ids, attention_mask)"],"metadata":{"id":"-GR0x7DY8P0T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Handle Overflowing Tokens from Long Sequences"],"metadata":{"id":"qIrbP6IQ8hUV"}},{"cell_type":"code","source":["text=\"\"\"\n","\n","The Hugging Face library offers a wide range of transformer models for natural language processing tasks. If you have mastered the basics and now want to dive into more advanced topics, here are some suggestions:\n","\n","- Fine-Tuning Models: Learn how to fine-tune pre-trained models on your specific task. This will allow you to leverage the power of transformers for your specific needs.\n","\n","- Multilingual Models: Hugging Face offers several multilingual models. You can learn how to train a single model on multiple languages.\n","\n","- Custom Models: You can learn how to create and train your own transformer models from scratch.\n","\n","- Pipeline Creation: Hugging Face provides a pipeline feature to easily process data and make predictions. You can learn how to create custom pipelines.\n","\n","- Training on Large Datasets: Handling and training models on large datasets is a common challenge in NLP. You can learn different strategies to handle this, such as gradient accumulation.\n","\n","- Distributed Training: You can learn how to use multiple GPUs to train your models faster.\n","\n","- Optimization Techniques: You can dive into various optimization techniques, like learning rate scheduling, weight decay, etc.\n","\n","- Advanced Tokenization Techniques: There are different tokenization techniques available, such as Byte Pair Encoding (BPE), SentencePiece, etc. Each has its own advantages and use-cases.\n","\n","- Interpretability of Transformer Models: Understanding why a model made a particular prediction is an active field of research. You can dive into this topic to understand your models better.\n","\n","- Use of Callbacks: Callbacks can be used to customize the training process, like saving the model at different stages, changing the learning rate, etc.\n","\n","You can find many tutorials and resources on these topics in the Hugging Face model hub and their documentation. Additionally, there are many courses and books available online that delve into these topics.\n","\n","\"\"\"\n","encoding = tokenizer(text, max_length=512, stride=256, return_overflowing_tokens=True, truncation=True)\n","print(encoding)"],"metadata":{"id":"NiuYbb3n8oqb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Encode Plus Method for Pair of Sentences"],"metadata":{"id":"DKY5g-bk9g-W"}},{"cell_type":"code","source":["pairs = [(\"Hello, I am sentence 1.\", \"Hello, I am sentence 2.\"), (\"Sentence 1.\", \"Sentence 2.\")]\n","encoding = tokenizer.batch_encode_plus(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n","print(encoding)"],"metadata":{"id":"Z3hf_vqt9fL7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Batch Encoding for Pair of Sentences"],"metadata":{"id":"bjQrXgn19xxr"}},{"cell_type":"code","source":["pairs = [(\"Hello, I am sentence 1.\", \"Hello, I am sentence 2.\"), (\"Sentence 1.\", \"Sentence 2.\")]\n","encoding = tokenizer.batch_encode_plus(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n","print(encoding)"],"metadata":{"id":"TOJ4DSVM9xIp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Byte-level Byte Pair Encoding (Byte-level BPE) is a data compression technique commonly used in natural language processing tasks, including tokenization in models like GPT `(Generative Pre-trained Transformer)`. It involves iteratively merging the most frequent byte pairs in a given dataset, building a vocabulary of byte-level subwords. Here's a high-level explanation of the Byte-level BPE process:\n","\n","1. **Initialization:**\n","   - Start with a vocabulary containing `all unique bytes` present in the dataset.\n","\n","2. **Byte Pair Merging:**\n","   - Iteratively `merge the most frequent pair of bytes` in the current vocabulary.\n","   - Calculate the `frequency of each byte pair` in the dataset.\n","   - Merge the most frequent byte pair to create a `new subword, updating the vocabulary`.\n","\n","3. **Iterative Merging:**\n","   - `Repeat the merging process` for a predefined number of iterations or until a `specified vocabulary size is reached`.\n","\n","4. **Final Vocabulary:**\n","   - `The final vocabulary consists of byte-level subwords generated through the merging process`.\n","\n","5. **Tokenization:**\n","   - `Use the obtained vocabulary` for tokenizing the input text into a sequence of byte-level subwords.\n","\n","Mathematically, let's denote the dataset as D, the initial vocabulary as V₀, and the merged vocabulary after i iterations as Vᵢ. The frequency of a byte pair (a, b) in the dataset is denoted as freq(a, b).\n","\n","The algorithm can be represented as follows:\n","\n","- Initialize: V₀ = {all unique bytes in D}\n","- For i = 1 to N (where N is the number of iterations or desired vocabulary size):\n","  - Calculate frequencies of all byte pairs in Vᵢ₋₁ in the dataset D.\n","  - Find the most frequent byte pair (a, b).\n","  - Merge (a, b) to create a new subword.\n","  - Update the vocabulary: Vᵢ = Vᵢ₋₁ ∪ {ab}, where ab is the merged subword.\n","\n","The tokenization process using the final vocabulary Vₙ involves replacing sequences of bytes in the input text with their corresponding subwords from Vₙ.\n","\n","Byte-level BPE is effective in handling rare words and reducing vocabulary size while maintaining flexibility in handling a wide range of input languages and characters. The process is particularly useful in the context of models like GPT, where a compact yet expressive vocabulary is crucial for efficient language representation."],"metadata":{"id":"jZNzyVJHTh1x"}},{"cell_type":"markdown","source":["# mathematical explanations `Byte-level BPE`\n","\n","\n","In Byte-level Byte Pair Encoding (Byte-level BPE), the process involves iteratively merging the most frequent byte pairs in a given dataset to create a vocabulary of byte-level subwords.\n","\n","Let's define the variables and steps mathematically:\n","\n","- Let $ D $ represent the dataset consisting of bytes.\n","- $ V_i $ represents the vocabulary after $ i $ iterations.\n","- $ \\text{freq}(a, b) $ denotes the frequency of the byte pair $ (a, b) $ in the dataset $ D $.\n","\n","The algorithm iterates as follows:\n","\n","1. **Initialization**:\n","   - $ V_0 $ is initialized with all unique bytes in $ D $.\n","\n","2. **Byte Pair Merging**:\n","   - For $ i = 1 $ to $ N $ iterations (or until a specified vocabulary size is reached):\n","     - Calculate the frequency of each byte pair in $ V_{i-1} $ in the dataset $ D $.\n","     - Let  \\(a^*, b^*\\)  be the most frequent byte pair.\n","     - Merge (a^*, b^*) to create a new subword $ ab $.\n","     - Update the vocabulary: $ V_i = V_{i-1} \\cup \\{ab\\} $.\n","\n","This process continues until the desired number of iterations $ N $ is reached or until a specific vocabulary size criterion is met.\n","\n","Mathematically, the algorithm can be represented as a set of equations:\n","\n","- **Initialization**:\n","  $$\n","  V_0 = \\{ \\text{all unique bytes in } D \\}\n","  $$\n","\n","- **Byte Pair Merging**:\n","  $$\n","  \\text{For } i = 1 \\text{ to } N:\n","  $$\n","  $$\n","  \\begin{align*}\n","  &\\text{Calculate } \\text{freq}(a, b) \\text{ for all } (a, b) \\text{ in } V_{i-1} \\text{ in dataset } D \\\\\n","  &(a^*, b^*) = \\arg\\max_{(a, b)} \\text{freq}(a, b) \\\\\n","  &ab = \\text{merge } (a^*, b^*) \\\\\n","  &V_i = V_{i-1} \\cup \\{ab\\}\n","  \\end{align*}\n","  $$\n","\n","This iterative process effectively builds a vocabulary $ V_N $ consisting of byte-level subwords, merging the most frequent byte pairs from the previous vocabulary in each iteration."],"metadata":{"id":"qWKE_1L9Tu3p"}},{"cell_type":"markdown","source":["# WordPiece\n","\n","WordPiece is a tokenization technique used in language models like BERT `(Bidirectional Encoder Representations from Transformers)`. It operates by breaking down words into smaller subword units called tokens. This method allows the model to handle out-of-vocabulary words and increases the model's ability to generalize.\n","\n","Mathematically, the WordPiece tokenization process involves several steps:\n","\n","1. **Initialization of Vocabulary:** Start with a predefined vocabulary containing individual characters or subword units.\n","\n","2. **Frequency Analysis:** Analyze the frequency of subword units or characters in the training corpus.\n","\n","3. **Merging frequent pairs:** Iteratively merge the most frequent pairs of subword units or characters based on their frequency in the corpus.\n","\n","4. **Stop Criteria:** Repeat the merging process until reaching a predefined vocabulary size or convergence criterion.\n","\n","The algorithm can be represented mathematically as follows:\n","\n","- Let $ C $ represent the corpus.\n","- $ V_0 $ represents the initial vocabulary.\n","- $ \\text{freq}(x, y) $ denotes the frequency of the pair $(x, y) $ in the corpus.\n","\n","The tokenization algorithm can be described using equations:\n","\n","1. **Initialization:**\n","   $$\n","   V_0 = \\{\\text{individual characters or subword units}\\}\n","   $$\n","\n","2. **Frequency Analysis and Merging:**\n","   $$\n","   \\text{For } i = 1 \\text{ to } N \\text{ iterations or until convergence}:\n","   $$\n","   $$\n","   \\begin{align*}\n","   &\\text{Calculate } \\text{freq}(x, y) \\text{ for all pairs } (x, y) \\text{ in } V_{i-1} \\text{ in corpus } C \\\\\n","   &(x^*, y^*) = \\arg\\max_{(x, y)} \\text{freq}(x, y) \\\\\n","   &xy = \\text{merge } (x^*, y^*) \\\\\n","   &V_i = V_{i-1} \\cup \\{xy\\}\n","   \\end{align*}\n","   $$\n","\n","This iterative process continues until reaching the desired vocabulary size or convergence criterion. The resulting vocabulary consists of subword units or merged characters that form the WordPiece tokens used for tokenizing words in the language model.\n","\n","WordPiece tokenization is effective in handling rare words, morphological variations, and languages with complex word structures, contributing to the robustness and flexibility of language models like BERT."],"metadata":{"id":"NqP12-F2YoY5"}},{"cell_type":"code","source":[],"metadata":{"id":"liTRyXIXT0Bl"},"execution_count":null,"outputs":[]}]}
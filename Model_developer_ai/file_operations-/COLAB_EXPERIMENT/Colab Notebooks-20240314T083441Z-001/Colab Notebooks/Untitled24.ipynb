{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOv0Zp7Q1e3GXkT5tScNg9F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install -q -U transformer"],"metadata":{"id":"J99yhADfSuh1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qBY_LJEQ1O-r"},"outputs":[],"source":["import csv\n","from typing import Tuple\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    PreTrainedModel,\n","    PreTrainedTokenizer\n",")\n","\n","def load_model_and_tokenizer(model_name_or_path: str) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n","    try:\n","        model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True)\n","        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n","\n","        if tokenizer.pad_token is None:\n","            tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n","\n","        return model, tokenizer\n","    except Exception as e:\n","        raise RuntimeError(f\"Failed to load model and tokenizer: {e}\")\n","\n","def generate_text(\n","    model: PreTrainedModel,\n","    tokenizer: PreTrainedTokenizer,\n","    prompt: str,\n","    max_length: int = 100,\n","    temperature: float = 0.9\n",") -> str:\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","\n","    gen_tokens = model.generate(\n","        input_ids,\n","        do_sample=True,\n","        temperature=temperature,\n","        max_length=max_length\n","    )\n","\n","    gen_text = tokenizer.batch_decode(gen_tokens)[0]\n","\n","    return gen_text\n","\n","# Load the model and tokenizer once\n","model_name_or_path = \"gpt2\"\n","model, tokenizer = load_model_and_tokenizer(model_name_or_path)\n","\n","# Specify the number of iterations to generate text\n","num_iterations = 100\n","file_csv='output2.csv'\n","# Generate text and write to CSV file\n","with open(file_csv, mode='w', newline='', encoding='utf-8') as csvfile:\n","    fieldnames = ['prompt', 'generated_text']\n","    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","    writer.writeheader()\n","\n","    for i in range(num_iterations):\n","        prompt_Q = \" Generate unique coding questions in C, C++, and Python that focus on time and memory optimization, as well as advanced research topics. Please provide one question for each programming language. Note every time new question novel (first time people see in this earth) \"\n","        Question = generate_text(model, tokenizer, prompt_Q)\n","        prompt_A=f'Compose a series of interconnected thoughts in a chain of thoughts format, demonstrating your expertise in advanced computer science and advanced higher mathematical skills.  {Question }'\n","        Answer = generate_text(model, tokenizer, prompt_A,max_length=100)\n","        writer.writerow({'prompt': Question, 'generated_text': Answer})\n"]},{"cell_type":"code","source":["pip  install einops"],"metadata":{"id":"G9FO0XYpWLNH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv\n","from typing import Tuple\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    PreTrainedModel,\n","    PreTrainedTokenizer\n",")\n","\n","def load_model_and_tokenizer(model_name_or_path: str) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n","    try:\n","        model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True)\n","        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n","\n","        if tokenizer.pad_token is None:\n","            tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n","\n","        return model, tokenizer\n","    except Exception as e:\n","        raise RuntimeError(f\"Failed to load model and tokenizer: {e}\")\n","\n","def generate_text(\n","    model: PreTrainedModel,\n","    tokenizer: PreTrainedTokenizer,\n","    prompt: str,\n","    max_length: int = 100,\n","    temperature: float = 0.9\n",") -> str:\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","\n","    gen_tokens = model.generate(\n","        input_ids,\n","        do_sample=True,\n","        temperature=temperature,\n","        max_length=max_length\n","    )\n","\n","    gen_text = tokenizer.batch_decode(gen_tokens)[0]\n","\n","    return gen_text\n","\n","# Load the model and tokenizer once\n","model_name_or_path = \"microsoft/phi-2\"\n","model, tokenizer = load_model_and_tokenizer(model_name_or_path)\n","\n","# Specify the number of iterations to generate text\n","num_iterations = 100\n","file_csv='output3.csv'\n","# Generate text and write to CSV file\n","with open(file_csv, mode='w', newline='', encoding='utf-8') as csvfile:\n","    fieldnames = ['prompt', 'generated_text']\n","    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","    writer.writeheader()\n","\n","    for i in range(num_iterations):\n","        prompt_Q = \" Generate unique coding questions in C, C++, and Python that focus on time and memory optimization, as well as advanced research topics. Please provide one question for each programming language. Note every time new question novel (first time people see in this earth) \"\n","        Question = generate_text(model, tokenizer, prompt_Q)\n","        prompt_A=f'Compose a series of interconnected thoughts in a chain of thoughts format, demonstrating your expertise in advanced computer science and advanced higher mathematical skills.  {Question }'\n","        Answer = generate_text(model, tokenizer, prompt_A,max_length=100)\n","        writer.writerow({'prompt': Question, 'generated_text': Answer})\n"],"metadata":{"id":"RikmwemOVydB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","# Load the Mistral 7B model and tokenizer\n","model_name = \"mistralai/Mistral-7B-v0.1\"\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Define the prompt or input text\n","prompt = \"Your prompt goes here.\"\n","\n","# Generate text using the Mistral 7B model\n","input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n","output = model.generate(input_ids, max_length=8000)\n","\n","# Retrieve the generated text\n","generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","print(generated_text)\n"],"metadata":{"id":"RAnninnkl6mX"},"execution_count":null,"outputs":[]}]}
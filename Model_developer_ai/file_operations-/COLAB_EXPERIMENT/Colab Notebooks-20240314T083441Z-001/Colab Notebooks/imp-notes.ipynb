{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP/q6G/Zxe+YJTCBUyZQh2U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Equations for AGI System Development\n","\n","## Word to Tokenization\n","\n","- Let $ D $ be the input document or text.\n","- Tokenization $ T(D) $ is a function that converts the document into a sequence of tokens.\n","- $ T(D) = [t_1, t_2, ..., t_n] $, where $ t_i $ represents the $ i $-th token.\n","\n","## Tokenizations to Pass Attention\n","\n","- Let $ H $ be the hidden representation obtained through embedding and encoding.\n","- $ H = E(T(D)) $, where $ E $ represents the embedding function.\n","- The attention mechanism $ A $ can be applied to $ H $ to obtain the weighted context representation $ C $.\n","- $ C = A(H) $, where $ A $ computes attention weights and combines them with $ H $.\n","\n","## Attention Mechanisms\n","\n","- Attention involves three key components: Query $ Q $, Key $ K $, and Value $ V $.\n","- Given $ Q $, $ K $, and $ V $, the attention weight $ \\alpha_{ij} $ between the $ i $-th token and $ j $-th token is calculated as\n","  $$ \\alpha_{ij} = \\text{softmax}\\left(\\frac{Q_i \\cdot K_j}{\\sqrt{d_k}}\\right) $$,\n","  where $ d_k $ is the dimensionality of the key vectors.\n","- The weighted sum of values $ V $ using attention weights gives the context vector\n","  $$ C_i = \\sum_{j} \\alpha_{ij}V_j $$.\n","\n","In summary, the process can be expressed mathematically as follows:\n","\n","1. **Tokenization:** $ T(D) = [t_1, t_2, ..., t_n] $\n","2. **Embedding:** $ H = E(T(D)) $\n","3. **Attention Mechanism:** $$ C_i = \\sum_{j} \\text{softmax}\\left(\\frac{Q_i \\cdot K_j}{\\sqrt{d_k}}\\right)V_j $$\n"],"metadata":{"id":"4jee_gxdLLj-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zCLae3qbCfW8"},"outputs":[],"source":[]}]}
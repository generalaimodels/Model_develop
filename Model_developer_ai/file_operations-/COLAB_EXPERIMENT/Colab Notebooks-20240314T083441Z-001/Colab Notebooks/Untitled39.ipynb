{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOXNZZ7n+bg/sAs5+fBBIpF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["- **Author:** **Kandimalla Hermanth**\n","- **Date of modified:** **25-01-2024**"],"metadata":{"id":"SnA14Ia_VU1e"}},{"cell_type":"code","source":["!curl https://ollama.ai/install.sh | sh"],"metadata":{"id":"zacu53TvgysH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ollama run llama2"],"metadata":{"id":"gMBlmYqlg5rx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install ollama"],"metadata":{"id":"s0mjwaAdccv4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import ollama\n","response = ollama.chat(model='llama2', messages=[\n","  {\n","    'role': 'user',\n","    'content': 'Why is the sky blue?',\n","  },\n","])\n","print(response['message']['content'])"],"metadata":{"id":"hlWuM-vocizD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q git+https://github.com/huggingface/transformers\n","!pip install -qU langchain Faiss-gpu tiktoken sentence-transformers\n","!pip install -qU trl Py7zr auto-gptq optimum\n","!pip install -q rank_bm25\n","!pip install -q PyPdf"],"metadata":{"id":"1xVlkQSYRLCw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import langchain\n","from langchain.embeddings import CacheBackedEmbeddings,HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.storage import LocalFileStore\n","from langchain.retrievers import BM25Retriever,EnsembleRetriever\n","from langchain.document_loaders import PyPDFLoader,DirectoryLoader,CSVLoader\n","from langchain.llms import HuggingFacePipeline\n","from langchain.cache import InMemoryCache\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.schema import prompt\n","from langchain.chains import RetrievalQA\n","from langchain.callbacks import StdOutCallbackHandler\n","from langchain import PromptTemplate\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","\n","\n","dir_loader = DirectoryLoader(\"/content/sample_data\",\n","                             glob=\"*.csv\",\n","                             loader_cls=CSVLoader)\n","docs = dir_loader.load()\n","\n","print(f\"len of documents in :{len(docs)}\")\n","\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n","                                      chunk_overlap=200,)\n","\n","esops_documents = text_splitter.transform_documents(docs)\n","print(f\"number of chunks in given documents : {len(esops_documents)}\")\n","\n","store = LocalFileStore(\"./cache/\")\n","embed_model_id = 'BAAI/bge-small-en-v1.5'\n","core_embeddings_model = HuggingFaceEmbeddings(model_name=embed_model_id)\n","embedder = CacheBackedEmbeddings.from_bytes_store(core_embeddings_model,\n","                                                  store,\n","                                                  namespace=embed_model_id)\n","# Create VectorStore\n","vectorstore = FAISS.from_documents(esops_documents,embedder)\n","\n","bm25_retriever = BM25Retriever.from_documents(esops_documents)\n","bm25_retriever.k=5\n","\n","query = \"Ho to save my excess money?\"\n","embedding_vector = core_embeddings_model.embed_query(query)\n","print(len(embedding_vector))\n","docs_resp = vectorstore.similarity_search_by_vector(embedding_vector,k=5)\n","for page in docs_resp:\n","  print(page.page_content)\n","  print(\"\\n\")\n","\n","%%timeit -n 1 -r 1\n","query = \"How to save my excess money?\"\n","#\n","embedding_vector = core_embeddings_model.embed_query(query)\n","docs_resp = vectorstore.similarity_search_by_vector(embedding_vector,k=5)\n"],"metadata":{"id":"U_mb1KUxRQIj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\":5})\n","ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever,faiss_retriever],\n","                                       weights=[0.5,0.5])"],"metadata":{"id":"f9poRrolXE3F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n","# To use a different branch, change revision\n","# For example: revision=\"gptq-4bit-32g-actorder_True\"\n","model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n","                                             device_map=\"auto\",\n","                                             trust_remote_code=False,\n","                                             revision=\"gptq-8bit-32g-actorder_True\")\n","#\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    max_new_tokens=512,\n","    do_sample=True,\n","    temperature=0.1,\n","    top_p=0.95,\n","    top_k=40,\n","    repetition_penalty=1.1\n",")\n","\n","llm = HuggingFacePipeline(pipeline=pipe)\n","langchain.llm_cache = InMemoryCache()\n","\n","# class PromptTemplate:\n","#     def __init__(self, template, input_variables):\n","#         self.template = template\n","#         self.input_variables = input_variables\n","\n","#     def fill_template(self, **kwargs):\n","#         filled_template = self.template\n","#         for variable in self.input_variables:\n","#             filled_template = filled_template.replace(f'{{{variable}}}', str(kwargs.get(variable, '')))\n","#         return filled_template\n","\n","# Define the prompt template\n","# Define the prompt template with chain of thoughts and few shots\n","PROMPT_TEMPLATE_ADVANCED = '''\n","You are my friendly advisor.\n","\n","Context: {context}\n","Question: {question}\n","\n","Chain of Thoughts:\n","- Consider the key elements related to the question.\n","- Evaluate the possible implications and scenarios.\n","- Use your expertise to provide a comprehensive response.\n","\n","Few Shots:\n","1. Given the current financial landscape, what are the primary considerations for effective investment planning?\n","   Helpful answer:\n","\n","2. Could you suggest a well-balanced investment strategy for someone with a low-risk tolerance and long-term financial goals?\n","   Helpful answer:\n","\n","3. In uncertain economic times, what steps would you recommend for optimizing savings while ensuring financial stability?\n","   Helpful answer:\n","'''\n","\n","# Define input variables\n","input_variables = ['context', 'question']\n","\n","# Create a PromptTemplate instance with the advanced template\n","custom_prompt_advanced = PromptTemplate(template=PROMPT_TEMPLATE_ADVANCED, input_variables=input_variables)\n","handler = StdOutCallbackHandler()\n","qa_with_sources_chain = RetrievalQA.from_chain_type(\n","    llm=llm,\n","    chain_type=\"stuff\",\n","    retriever = vectorstore.as_retriever(search_kwargs={\"k\":5}),\n","    verbose=True,\n","    callbacks=[handler],\n","    chain_type_kwargs={\"prompt\": custom_prompt},\n","    return_source_documents=True\n",")\n","\n","%%time\n","query = \"longitude average , mean\"\n","response = qa_with_sources_chain({\"query\":query})\n","print(f\"Response generated : \\n {response['result']}\")\n","print(f\"Source Documents : \\n {response['source_documents']}\")\n"],"metadata":{"id":"QRP98J8MXIzU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JDMcLuNLYf3k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"P82fAoAgYYDN"},"execution_count":null,"outputs":[]}]}
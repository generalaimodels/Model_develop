{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN82m0ILt0m0FsBGQRaA1Ic"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install adversarial-robustness-toolbox"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EoJgH8et9T9h","executionInfo":{"status":"ok","timestamp":1693239861928,"user_tz":-330,"elapsed":26456,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"ce22061a-fe49-49a6-cac2-a4bd0c70f958"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting adversarial-robustness-toolbox\n","  Downloading adversarial_robustness_toolbox-1.15.1-py3-none-any.whl (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.23.5)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.10.1)\n","Collecting scikit-learn<1.2.0,>=0.22.2 (from adversarial-robustness-toolbox)\n","  Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.16.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (67.7.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (4.66.1)\n","Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.2.0,>=0.22.2->adversarial-robustness-toolbox) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.2.0,>=0.22.2->adversarial-robustness-toolbox) (3.2.0)\n","Installing collected packages: scikit-learn, adversarial-robustness-toolbox\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.2.2\n","    Uninstalling scikit-learn-1.2.2:\n","      Successfully uninstalled scikit-learn-1.2.2\n","Successfully installed adversarial-robustness-toolbox-1.15.1 scikit-learn-1.1.3\n"]}]},{"cell_type":"code","source":["!pip install transformers\n","!pip install adversarial-robustness-toolbox"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tpu3izrl9C6j","executionInfo":{"status":"ok","timestamp":1693239871071,"user_tz":-330,"elapsed":9149,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"f666b83d-412f-4ba6-bd09-82f84eec9f90"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RIPKPvHq8_LV"},"outputs":[],"source":["import torch\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from art.attacks.evasion import TextFooler\n","from art.defences.preprocessor import AutoEncodingDefence\n","from art.metrics import empirical_robustness\n","\n","# Step 1: Load the GPT-2 model and tokenizer\n","model_name = \"gpt2\"\n","model = GPT2LMHeadModel.from_pretrained(model_name)\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","\n","# Step 2: Generate perturbed input texts\n","def generate_perturbed_inputs(input_texts):\n","    perturbed_texts = []\n","    attack = TextFooler(model, tokenizer)\n","    for text in input_texts:\n","        perturbed_text = attack.generate(text)\n","        perturbed_texts.append(perturbed_text)\n","    return perturbed_texts\n","\n","# Step 3: Generate text from perturbed tokenized forms\n","def generate_text_from_tokens(tokenized_texts):\n","    generated_texts = []\n","    for tokens in tokenized_texts:\n","        generated_text = tokenizer.decode(tokens, skip_special_tokens=True)\n","        generated_texts.append(generated_text)\n","    return generated_texts\n","\n","# Step 4: Fine-tune the GPT-2 model with adversarial examples\n","def adversarial_training(train_data):\n","    model.train()  # Set the model in training mode\n","    for batch in train_data:\n","        inputs, targets = batch\n","        adversarial_loss = compute_adversarial_loss(inputs, targets)  # Define an appropriate loss function\n","        optimizer.zero_grad()\n","        adversarial_loss.backward()\n","        optimizer.step()\n","\n","# Step 5: Apply text cleaning and normalization\n","def preprocess_text(text):\n","    cleaned_text = clean_and_normalize(text)  # Call a function for cleaning and normalization\n","    return cleaned_text\n","\n","# Step 6: Train ensemble of GPT-2 models\n","def train_ensemble(models, train_data):\n","    ensemble_models = []\n","    for _ in range(models):\n","        new_model = GPT2LMHeadModel.from_pretrained(model_name)\n","        ensemble_models.append(new_model)\n","\n","    for model in ensemble_models:\n","        # Train each model with different initializations\n","        # Combine predictions using ensemble methods\n","\n","# Step 7: Use anomaly detection to identify adversarial inputs\n","def detect_anomalies(inputs, model):\n","    anomalies = []\n","    for input_text in inputs:\n","        prediction = model(input_text)\n","        if is_anomaly(prediction):  # Define a function for anomaly detection\n","            anomalies.append(input_text)\n","    return anomalies\n","\n","# Step 8: Train interpretable models alongside GPT-2\n","def train_interpretable_models(train_data):\n","    interpretable_model = train_decision_tree(train_data)  # Train a decision tree or other interpretable model\n","    return interpretable_model\n","\n","# Step 9: Evaluate generated adversarial texts\n","def evaluate_attack_quality(generated_texts):\n","    attack_quality = assess_quality(generated_texts)  # Call a function to assess attack quality\n","    return attack_quality\n","\n","# Step 10: Evaluate model accuracy on clean and adversarial inputs\n","def evaluate_defense_mechanism(model, data_loader):\n","    clean_accuracy = compute_accuracy(model, clean_data)  # Define function for accuracy computation\n","    adversarial_accuracy = compute_accuracy(model, adversarial_data)\n","    return clean_accuracy, adversarial_accuracy\n","\n","# Visualize the attack quality and defense effectiveness\n","def visualize_results(original_texts, perturbed_texts, generated_texts,\n","                      clean_accuracy, adversarial_accuracy):\n","    print(\"Original Texts:\", original_texts)\n","    print(\"Perturbed Texts:\", perturbed_texts)\n","    print(\"Generated Texts:\", generated_texts)\n","    print(\"Clean Accuracy:\", clean_accuracy)\n","    print(\"Adversarial Accuracy:\", adversarial_accuracy)\n","\n","# Example usage\n","input_texts = [\"original text 1\", \"original text 2\"]\n","perturbed_texts = generate_perturbed_inputs(input_texts)\n","tokenized_texts = [tokenizer.encode(text, add_special_tokens=True) for text in perturbed_texts]\n","generated_texts = generate_text_from_tokens(tokenized_texts)\n","train_data = DataLoader(...)  # Load your training data\n","adversarial_training(train_data)\n","preprocessed_text = preprocess_text(\"input text\")\n","ensemble_models = train_ensemble(models=3, train_data=train_data)\n","anomalies = detect_anomalies(inputs=input_texts, model=model)\n","interpretable_model = train_interpretable_models(train_data)\n","attack_quality = evaluate_attack_quality(generated_texts)\n","clean_accuracy, adversarial_accuracy = evaluate_defense_mechanism(model, data_loader=train_data)\n","\n","# Visualize results\n","visualize_results(input_texts, perturbed_texts, generated_texts, clean_accuracy, adversarial_accuracy)\n"]},{"cell_type":"code","source":["import torch\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","from torch.utils.data import DataLoader\n","from bs4 import BeautifulSoup\n","from sklearn import tree\n","\n","# Step 1: Load the GPT-2 model and tokenizer\n","model_name = \"gpt2\"\n","model = GPT2LMHeadModel.from_pretrained(model_name)\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","\n","# Define the missing functions\n","def compute_adversarial_loss(inputs, targets):\n","    outputs = model(inputs)\n","    loss = -torch.nn.functional.nll_loss(outputs, targets)\n","    return loss\n","\n","def clean_and_normalize(text):\n","    text = BeautifulSoup(text, \"html.parser\").text\n","    text = text.lower()\n","    return text\n","\n","def is_anomaly(prediction):\n","    threshold = 0.5\n","    predict_status = prediction > threshold\n","    return predict_status\n","\n","def train_decision_tree(train_data):\n","    classifier = tree.DecisionTreeClassifier()\n","    X = [[0, 0], [1, 1]]\n","    Y = [0, 1]\n","    classifier = classifier.fit(X, Y)\n","    return classifier\n","\n","def assess_quality(generated_texts):\n","    quality_metric = len(generated_texts) / 100\n","    return quality_metric\n","\n","def compute_accuracy(model, data):\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in dataloader:\n","            inputs, labels = data\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    accuracy = 100 * correct / total\n","    return accuracy\n","\n","def train_ensemble(models, train_data):\n","    ensemble_models = []\n","    for _ in range(models):\n","        new_model = GPT2LMHeadModel.from_pretrained(model_name)\n","        ensemble_models.append(new_model)\n","\n","    for model in ensemble_models:\n","        adversarial_training(train_data)\n","\n","    def predict(inputs):\n","        predictions = [model(inputs) for model in ensemble_models]\n","        mean_pred = torch.mean(torch.stack(predictions))\n","        return mean_pred\n","\n","# Continue with the original code\n","def generate_perturbed_inputs(input_texts):\n","    perturbed_texts = []\n","    attack = TextFooler(model, tokenizer)\n","    for text in input_texts:\n","        perturbed_text = attack.generate(text)\n","        perturbed_texts.append(perturbed_text)\n","    return perturbed_texts\n","\n","def generate_text_from_tokens(tokenized_texts):\n","    generated_texts = []\n","    for tokens in tokenized_texts:\n","        generated_text = tokenizer.decode(tokens, skip_special_tokens=True)\n","        generated_texts.append(generated_text)\n","    return generated_texts\n","\n","def adversarial_training(train_data):\n","    model.train()\n","    for batch in train_data:\n","        inputs, targets = batch\n","        adversarial_loss = compute_adversarial_loss(inputs, targets)\n","        optimizer.zero_grad()\n","        adversarial_loss.backward()\n","        optimizer.step()\n","\n","def preprocess_text(text):\n","    cleaned_text = clean_and_normalize(text)\n","    return cleaned_text\n","\n","def detect_anomalies(inputs, model):\n","    anomalies = []\n","    for input_text in inputs:\n","        prediction = model(input_text)\n","        if is_anomaly(prediction):\n","            anomalies.append(input_text)\n","    return anomalies\n","\n","def train_interpretable_models(train_data):\n","    interpretable_model = train_decision_tree(train_data)\n","    return interpretable_model\n","\n","def evaluate_attack_quality(generated_texts):\n","    attack_quality = assess_quality(generated_texts)\n","    return attack_quality\n","\n","def evaluate_defense_mechanism(model, data_loader):\n","    clean_accuracy = compute_accuracy(model, clean_data)\n","    adversarial_accuracy = compute_accuracy(model, adversarial_data)\n","    return clean_accuracy, adversarial_accuracy\n","\n","def visualize_results(original_texts, perturbed_texts, generated_texts,\n","                      clean_accuracy, adversarial_accuracy):\n","    print(\"Original Texts:\", original_texts)\n","    print(\"Perturbed Texts:\", perturbed_texts)\n","    print(\"Generated Texts:\", generated_texts)\n","    print(\"Clean Accuracy:\", clean_accuracy)\n","    print(\"Adversarial Accuracy:\", adversarial_accuracy)\n","\n","# Example usage\n","input_texts = [\"original text 1\", \"original text 2\"]\n","perturbed_texts = generate_perturbed_inputs(input_texts)\n","tokenized_texts = [tokenizer.encode(text, add_special_tokens=True) for text in perturbed_texts]\n","generated_texts = generate_text_from_tokens(tokenized_texts)\n","train_data = DataLoader(...)  # Load your training data\n","adversarial_training(train_data)\n","preprocessed_text = preprocess_text(\"input text\")\n","ensemble_models = train_ensemble(models=3, train_data=train_data)\n","anomalies = detect_anomalies(inputs=input_texts, model=model)\n","interpretable_model = train_interpretable_models(train_data)\n","attack_quality = evaluate_attack_quality(generated_texts)\n","clean_accuracy, adversarial_accuracy = evaluate_defense_mechanism(model, data_loader=train_data)\n","\n","# Visualize results\n","visualize_results(input_texts, perturbed_texts, generated_texts, clean_accuracy, adversarial_accuracy)"],"metadata":{"id":"Ss0NZXCCASOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n","from torch.utils.data import DataLoader, Dataset\n","\n","# Step 1: Load the GPT-2 model and tokenizer\n","model_name = \"gpt2\"\n","model = GPT2LMHeadModel.from_pretrained(model_name)\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","\n","# Define a dummy dataset\n","class DummyDataset(Dataset):\n","    def __init__(self, tokenizer, texts):\n","        self.tokenizer = tokenizer\n","        self.texts = texts\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        encoding = self.tokenizer.encode_plus(text, return_tensors='pt')\n","        return encoding['input_ids'], encoding['attention_mask']\n","\n","# Define a function for training the model\n","def train(model, data_loader, optimizer):\n","    model.train()\n","    for batch in data_loader:\n","        inputs, attention_mask = batch\n","        outputs = model(inputs, attention_mask=attention_mask, labels=inputs)\n","        loss = outputs.loss\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","# Define a function for generating text\n","def generate_text(model, prompt):\n","    inputs = tokenizer.encode(prompt, return_tensors='pt')\n","    outputs = model.generate(inputs, max_length=50, do_sample=True)\n","    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return generated_text\n","\n","# Example usage\n","texts = [\"This is a dummy text.\", \"Another dummy text.\"]\n","dataset = DummyDataset(tokenizer, texts)\n","data_loader = DataLoader(dataset, batch_size=1)\n","optimizer = AdamW(model.parameters())\n","\n","# Train the model\n","train(model, data_loader, optimizer)\n","\n","# Generate text\n","prompt = \"This is a test.\"\n","generated_text = generate_text(model, prompt)\n","print(\"Generated Text:\", generated_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wVG5qq9YAozh","executionInfo":{"status":"ok","timestamp":1693240715630,"user_tz":-330,"elapsed":17017,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"e359a484-d7bb-45ac-940a-8ea4d220c53e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Generated Text: This is a test. text. text. text. text. text. text.. text. text.. text. text. text. text. text. text. text. text.text.Text. text text. text. text.\n"]}]},{"cell_type":"code","source":["!apt install python3.10-venv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LhgrRHgqDlKQ","executionInfo":{"status":"ok","timestamp":1693241483938,"user_tz":-330,"elapsed":9155,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"9fa75ee6-3fec-4c76-87ec-14efe2cdb95c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  python3-pip-whl python3-setuptools-whl\n","The following NEW packages will be installed:\n","  python3-pip-whl python3-setuptools-whl python3.10-venv\n","0 upgraded, 3 newly installed, 0 to remove and 16 not upgraded.\n","Need to get 2,473 kB of archives.\n","After this operation, 2,882 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip-whl all 22.0.2+dfsg-1ubuntu0.3 [1,679 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-setuptools-whl all 59.6.0-1.2ubuntu0.22.04.1 [788 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3.10-venv amd64 3.10.12-1~22.04.2 [5,724 B]\n","Fetched 2,473 kB in 1s (2,438 kB/s)\n","Selecting previously unselected package python3-pip-whl.\n","(Reading database ... 120831 files and directories currently installed.)\n","Preparing to unpack .../python3-pip-whl_22.0.2+dfsg-1ubuntu0.3_all.deb ...\n","Unpacking python3-pip-whl (22.0.2+dfsg-1ubuntu0.3) ...\n","Selecting previously unselected package python3-setuptools-whl.\n","Preparing to unpack .../python3-setuptools-whl_59.6.0-1.2ubuntu0.22.04.1_all.deb ...\n","Unpacking python3-setuptools-whl (59.6.0-1.2ubuntu0.22.04.1) ...\n","Selecting previously unselected package python3.10-venv.\n","Preparing to unpack .../python3.10-venv_3.10.12-1~22.04.2_amd64.deb ...\n","Unpacking python3.10-venv (3.10.12-1~22.04.2) ...\n","Setting up python3-setuptools-whl (59.6.0-1.2ubuntu0.22.04.1) ...\n","Setting up python3-pip-whl (22.0.2+dfsg-1ubuntu0.3) ...\n","Setting up python3.10-venv (3.10.12-1~22.04.2) ...\n"]}]},{"cell_type":"code","source":["!python3 -m venv textattackenv\n","!source textattackenv/bin/activate"],"metadata":{"id":"1FPSmwrvDc8a","executionInfo":{"status":"ok","timestamp":1693241492295,"user_tz":-330,"elapsed":4632,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["pip install textattack"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"VUABOmZ8DyWQ","executionInfo":{"status":"ok","timestamp":1693241805699,"user_tz":-330,"elapsed":285163,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"6fb6adbe-3a7c-43a6-cc1f-7e5efa12b98e"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting textattack\n","  Downloading textattack-0.3.8-py3-none-any.whl (418 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.7/418.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bert-score>=0.3.5 (from textattack)\n","  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from textattack) (0.6.2)\n","Collecting flair (from textattack)\n","  Downloading flair-0.12.2-py3-none-any.whl (373 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.1/373.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from textattack) (3.12.2)\n","Collecting language-tool-python (from textattack)\n","  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n","Collecting lemminflect (from textattack)\n","  Downloading lemminflect-0.2.3-py3-none-any.whl (769 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting lru-dict (from textattack)\n","  Downloading lru_dict-1.2.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n","Collecting datasets==2.4.0 (from textattack)\n","  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textattack) (3.8.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.23.5)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.5.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.10.1)\n","Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (2.0.1+cu118)\n","Requirement already satisfied: transformers>=4.21.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (4.32.1)\n","Collecting terminaltables (from textattack)\n","  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from textattack) (4.66.1)\n","Collecting word2number (from textattack)\n","  Downloading word2number-1.1.zip (9.7 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting num2words (from textattack)\n","  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from textattack) (10.1.0)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.7.1)\n","Collecting pinyin==0.4.0 (from textattack)\n","  Downloading pinyin-0.4.0.tar.gz (3.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from textattack) (0.42.1)\n","Collecting OpenHowNet (from textattack)\n","  Downloading OpenHowNet-2.0-py3-none-any.whl (18 kB)\n","Collecting pycld2 (from textattack)\n","  Downloading pycld2-0.41.tar.gz (41.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting click<8.1.0 (from textattack)\n","  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack) (9.0.0)\n","Collecting dill<0.3.6 (from datasets==2.4.0->textattack)\n","  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack) (2.31.0)\n","Collecting xxhash (from datasets==2.4.0->textattack)\n","  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets==2.4.0->textattack)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack) (3.8.5)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack) (0.16.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack) (23.1)\n","Collecting responses<0.19 (from datasets==2.4.0->textattack)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (3.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2023.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.8,>=1.7.0->textattack) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.8,>=1.7.0->textattack) (16.0.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.21.0->textattack) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.21.0->textattack) (2023.6.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.21.0->textattack) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.21.0->textattack) (0.3.3)\n","Requirement already satisfied: gensim>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.3.1)\n","Collecting segtok>=1.5.7 (from flair->textattack)\n","  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n","Collecting mpld3==0.3 (from flair->textattack)\n","  Downloading mpld3-0.3.tar.gz (788 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.5/788.5 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.1.3)\n","Collecting sqlitedict>=1.6.0 (from flair->textattack)\n","  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting deprecated>=1.2.4 (from flair->textattack)\n","  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: hyperopt>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.2.7)\n","Collecting boto3 (from flair->textattack)\n","  Downloading boto3-1.28.35-py3-none-any.whl (135 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bpemb>=0.3.2 (from flair->textattack)\n","  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.9.0)\n","Collecting langdetect (from flair->textattack)\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.9.3)\n","Collecting ftfy (from flair->textattack)\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting janome (from flair->textattack)\n","  Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gdown==4.4.0 (from flair->textattack)\n","  Downloading gdown-4.4.0.tar.gz (14 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting conllu>=4.0 (from flair->textattack)\n","  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n","Collecting wikipedia-api (from flair->textattack)\n","  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n","Collecting pptree (from flair->textattack)\n","  Downloading pptree-3.1.tar.gz (3.0 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pytorch-revgrad (from flair->textattack)\n","  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n","Collecting transformer-smaller-training-vocab>=0.2.1 (from flair->textattack)\n","  Downloading transformer_smaller_training_vocab-0.3.1-py3-none-any.whl (14 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown==4.4.0->flair->textattack) (1.16.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown==4.4.0->flair->textattack) (4.11.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textattack) (1.3.2)\n","Collecting docopt>=0.6.2 (from num2words->textattack)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting anytree (from OpenHowNet->textattack)\n","  Downloading anytree-2.9.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from OpenHowNet->textattack) (67.7.2)\n","Collecting sentencepiece (from bpemb>=0.3.2->flair->textattack)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.4->flair->textattack) (1.14.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack) (1.3.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=3.8.0->flair->textattack) (6.3.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.7->flair->textattack) (0.18.3)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.7->flair->textattack) (2.2.1)\n","Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.7->flair->textattack) (0.10.9.7)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (4.42.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.4)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (3.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (2023.7.22)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->flair->textattack) (3.2.0)\n","Collecting torch!=1.8,>=1.7.0 (from textattack)\n","  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch!=1.8,>=1.7.0->textattack)\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch!=1.8,>=1.7.0->textattack)\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch!=1.8,>=1.7.0->textattack)\n","  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch!=1.8,>=1.7.0->textattack)\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch!=1.8,>=1.7.0->textattack)\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch!=1.8,>=1.7.0->textattack)\n","  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch!=1.8,>=1.7.0->textattack)\n","  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch!=1.8,>=1.7.0->textattack)\n","  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch!=1.8,>=1.7.0->textattack)\n","  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch!=1.8,>=1.7.0->textattack)\n","  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch!=1.8,>=1.7.0->textattack)\n","  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch!=1.8,>=1.7.0->textattack) (0.41.2)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers>=4.21.0->textattack) (3.20.3)\n","Collecting botocore<1.32.0,>=1.31.35 (from boto3->flair->textattack)\n","  Downloading botocore-1.31.35-py3-none-any.whl (11.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->flair->textattack)\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->flair->textattack)\n","  Downloading s3transfer-0.6.2-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->flair->textattack) (0.2.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.8,>=1.7.0->textattack) (2.1.3)\n","INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n","Collecting multiprocess (from datasets==2.4.0->textattack)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading multiprocess-0.70.13-py310-none-any.whl (133 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.7.0->textattack) (1.3.0)\n","Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->datasets==2.4.0->textattack)\n","  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting accelerate>=0.20.3 (from transformers>=4.21.0->textattack)\n","  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown==4.4.0->flair->textattack) (2.4.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers>=4.21.0->textattack) (5.9.5)\n","Building wheels for collected packages: pinyin, gdown, mpld3, pycld2, word2number, docopt, sqlitedict, langdetect, pptree\n","  Building wheel for pinyin (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pinyin: filename=pinyin-0.4.0-py3-none-any.whl size=3630475 sha256=0c809ce2a06cb8e62e8c0983e66e99cff28ab8c9615186520abeb0d1967cf964\n","  Stored in directory: /root/.cache/pip/wheels/33/38/af/616fc6f154aa5bae65a1da12b22d79943434269f0468ff9b3f\n","  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14758 sha256=2d16ac27ceceacdb78cfb1b47da1654874f3e5d31f231e00d969c08077fda47c\n","  Stored in directory: /root/.cache/pip/wheels/03/0b/3f/6ddf67a417a5b400b213b0bb772a50276c199a386b12c06bfc\n","  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116686 sha256=7eac25113ed98b4f20fbdee5ee43fa8383c94621ecb1f01f214a951d9d85d358\n","  Stored in directory: /root/.cache/pip/wheels/9c/92/f7/45d9aac5dcfb1c2a1761a272365599cc7ba1050ce211a3fd9a\n","  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycld2: filename=pycld2-0.41-cp310-cp310-linux_x86_64.whl size=9904071 sha256=4cc3e95adabc42ae650c1b90d9df847d8c4ff2295579998d73622f7c35832545\n","  Stored in directory: /root/.cache/pip/wheels/be/81/31/240c89c845e008a93d98542325270007de595bfd356eb0b06c\n","  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5567 sha256=3f981305e462a2c728e42766f263ad757491299a09b93f7f507b35dfb4d01168\n","  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=d9db613beffbe1fa06cd35ea13f6536ec4fdb1e29cec5c5c6d066e3b8eb935bf\n","  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n","  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=15780ce96ef5d9232b56663e904a64267ac5cc41f031a6f7dc48fd47cc1705c9\n","  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=250cec81b098b830fc3369ec5d41a54bb66ceb7d91ee065f7f9b36bceb14610e\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4609 sha256=af8c93659f7bd1fac052c6c37aa3243c6ba4701efffc39d15ae0cb4b5439eac8\n","  Stored in directory: /root/.cache/pip/wheels/9f/b6/0e/6f26eb9e6eb53ff2107a7888d72b5a6a597593956113037828\n","Successfully built pinyin gdown mpld3 pycld2 word2number docopt sqlitedict langdetect pptree\n","Installing collected packages: word2number, sqlitedict, sentencepiece, pycld2, pptree, pinyin, mpld3, lru-dict, janome, docopt, xxhash, urllib3, terminaltables, segtok, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, num2words, lemminflect, langdetect, jmespath, ftfy, dill, deprecated, conllu, click, anytree, nvidia-cusolver-cu11, nvidia-cudnn-cu11, multiprocess, botocore, wikipedia-api, s3transfer, responses, OpenHowNet, language-tool-python, bpemb, gdown, datasets, boto3, torch, accelerate, transformer-smaller-training-vocab, pytorch-revgrad, flair, bert-score, textattack\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 2.0.4\n","    Uninstalling urllib3-2.0.4:\n","      Successfully uninstalled urllib3-2.0.4\n","  Attempting uninstall: click\n","    Found existing installation: click 8.1.7\n","    Uninstalling click-8.1.7:\n","      Successfully uninstalled click-8.1.7\n","  Attempting uninstall: gdown\n","    Found existing installation: gdown 4.6.6\n","    Uninstalling gdown-4.6.6:\n","      Successfully uninstalled gdown-4.6.6\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.0.1+cu118\n","    Uninstalling torch-2.0.1+cu118:\n","      Successfully uninstalled torch-2.0.1+cu118\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n","torchdata 0.6.1 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n","torchtext 0.15.2 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n","torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed OpenHowNet-2.0 accelerate-0.22.0 anytree-2.9.0 bert-score-0.3.13 boto3-1.28.35 botocore-1.31.35 bpemb-0.3.4 click-8.0.4 conllu-4.5.3 datasets-2.4.0 deprecated-1.2.14 dill-0.3.5.1 docopt-0.6.2 flair-0.12.2 ftfy-6.1.1 gdown-4.4.0 janome-0.5.0 jmespath-1.0.1 langdetect-1.0.9 language-tool-python-2.7.1 lemminflect-0.2.3 lru-dict-1.2.0 mpld3-0.3 multiprocess-0.70.13 num2words-0.5.12 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pinyin-0.4.0 pptree-3.1 pycld2-0.41 pytorch-revgrad-0.2.0 responses-0.18.0 s3transfer-0.6.2 segtok-1.5.11 sentencepiece-0.1.99 sqlitedict-2.1.0 terminaltables-3.1.10 textattack-0.3.8 torch-2.0.0 transformer-smaller-training-vocab-0.3.1 urllib3-1.26.16 wikipedia-api-0.6.0 word2number-1.1 xxhash-3.3.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["nvfuser","torch","urllib3"]}}},"metadata":{}}]},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","from textattack.models.wrappers import HuggingFaceModelWrapper\n","from textattack.datasets import HuggingFaceDataset\n","from textattack.attack_recipes import TextFoolerJin2019\n","from textattack.defences import GreedyWordSwapWIR\n","\n","# Load a pre-trained text classification model and tokenizer from Hugging Face\n","model = AutoModelForSequenceClassification.from_pretrained('textattack/bert-base-uncased-imdb')\n","tokenizer = AutoTokenizer.from_pretrained('textattack/bert-base-uncased-imdb')\n","\n","# Wrap the model\n","model_wrapper = HuggingFaceModelWrapper(model, tokenizer)\n","\n","# Load a dataset for attack\n","dataset = HuggingFaceDataset('imdb', None, 'test')\n","\n","# Choose an adversarial attack method\n","attack = TextFoolerJin2019.build(model_wrapper)\n","\n","# Choose a defense method\n","defence = GreedyWordSwapWIR()\n","\n","# Perform adversarial attack and defense\n","for i in range(len(dataset)):\n","    # Get an example from the dataset\n","    example = dataset[i]\n","\n","    # Perform the attack\n","    attacked_text = attack.attack(example)\n","\n","    # Perform the defense\n","    defended_text = defence(defend_text)\n","\n","    # Print the original, attacked, and defended texts\n","    print('Original text:', example.text)\n","    print('Attacked text:', attacked_text)\n","    print('Defended text:', defended_text)\n","\n","    # Stop after the first example for simplicity\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":616},"id":"pRvBaj2zD7xL","executionInfo":{"status":"error","timestamp":1693241808552,"user_tz":-330,"elapsed":2866,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"1479c79b-bed3-45b9-d0bc-db9d91fce282"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["textattack: Updating TextAttack package dependencies.\n","textattack: Downloading NLTK required packages.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package omw to /root/nltk_data...\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Unzipping taggers/universal_tagset.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-e4fe85cd9a7e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtextattack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuggingFaceModelWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextattack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuggingFaceDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextattack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack_recipes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextFoolerJin2019\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/textattack/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mTextAttack\u001b[0m \u001b[0mprovides\u001b[0m \u001b[0mcomponents\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mNLP\u001b[0m \u001b[0mtasks\u001b[0m \u001b[0mlike\u001b[0m \u001b[0msentence\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrammar\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mchecking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mword\u001b[0m \u001b[0mreplacement\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0mon\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \"\"\"\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mattack_args\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttackArgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCommandLineAttackArgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maugment_args\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAugmenterArgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset_args\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetArgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/textattack/attack_args.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtextattack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtextattack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mARGS_SPLIT_TOKEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_module_from_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mattack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/textattack/shared/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/textattack/shared/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minstall\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimporting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/textattack/shared/utils/strings.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimporting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flair/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# global variable: cache_root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_proxies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcache_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FLAIR_CACHE_ROOT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".flair\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flair/file_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/boto3/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_warn_deprecated_python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0m__author__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Amazon Web Services'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/boto3/session.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnknownServiceError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/session.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxform_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClientArgsCreator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAUTH_TYPE_MAPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/waiter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjmespath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWaiterDocstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_service_module_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/docs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mServiceDocumenter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mDEPRECATED_SERVICE_NAMES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'sms-voice'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/docs/service.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# language governing permissions and limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbcdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestdoc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocumentStructure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClientDocumenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClientExceptionsDocumenter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaginator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPaginatorDocumenter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaiter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWaiterDocumenter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/docs/client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbcdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestdoc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocumentStructure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexample\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResponseExampleDocumenter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m from botocore.docs.method import (\n\u001b[1;32m     19\u001b[0m     \u001b[0mdocument_custom_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/docs/example.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# ANY KIND, either express or implied. See the License for the specific\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# language governing permissions and limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mShapeDocumenter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpy_default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/docs/shape.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# inherited from a Documenter class with the appropriate methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# and attributes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_json_value_header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawsrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttpsession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# IP Regexes retained for backwards compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/botocore/httpsession.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Always import the original SSLContext, even if it has been patched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         from urllib3.contrib.pyopenssl import (\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0morig_util_SSLContext\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSSLContext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# Map from urllib3 to PyOpenSSL compatible parameter-values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m _openssl_versions = {\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPROTOCOL_TLS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSLv23_METHOD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0mPROTOCOL_TLS_CLIENT\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSLv23_METHOD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPROTOCOL_TLSv1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTLSv1_METHOD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'urllib3.util' has no attribute 'PROTOCOL_TLS'"]}]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNyCryI2KBTIpIH7FBIJuVW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["- **Author**:**Kandimalla Hemanth**\n","- **Date of lastly modified :** **19-01-2024**\n","- **E-mail**:**speechcodehemanth2@gmail.com**\n"],"metadata":{"id":"7275zWmerFfT"}},{"cell_type":"markdown","source":["\n","\n","\n","#     ` Understanding Quantization Methods for Model Optimization`\n","\n","Quantization methods play a pivotal role in optimizing models for efficient computational usage. These methods follow a specific naming convention for easy identification and understanding. Here's a comprehensive list of various quantization methods and their recommended use cases, curated from TheBloke's model cards:\n","\n","#### q2_k:\n","- Utilizes Q4_K for attention.vw and feed_forward.w2 tensors, while employing Q2_K for other tensors.\n","- **Use Case:** Balancing precision where specific tensors require higher precision while maintaining efficiency elsewhere.\n","\n","#### q3_k_l:\n","- Employs Q5_K for attention.wv, attention.wo, and feed_forward.w2 tensors, utilizing Q3_K for other instances.\n","- **Use Case:** Prioritizing higher precision for selected tensors critical for model performance.\n","\n","#### q3_k_m:\n","- Leverages Q4_K for attention.wv, attention.wo, and feed_forward.w2 tensors, using Q3_K otherwise.\n","- **Use Case:** Striking a balance between precision and efficiency across significant tensors.\n","\n","#### q3_k_s:\n","- Utilizes Q3_K across all tensors uniformly.\n","- **Use Case:** Standardizing precision for streamlined computational efficiency.\n","\n","#### q4_0:\n","- Represents the original 4-bit quantization method.\n","- **Use Case:** Baseline quantization method with balanced accuracy and efficiency.\n","\n","#### q4_1:\n","- Offers higher accuracy than q4_0 but not as high as q5_0. Features quicker inference compared to q5 models.\n","- **Use Case:** Balancing accuracy and inference speed while maintaining reasonable efficiency.\n","\n","#### q4_k_m:\n","- Employs Q6_K for specific attention.wv and feed_forward.w2 tensors, utilizing Q4_K for others.\n","- **Use Case:** Prioritizing higher precision for key tensors without compromising overall efficiency.\n","\n","#### q4_k_s:\n","- Uses Q4_K uniformly across all tensors.\n","- **Use Case:** Standardized 4-bit precision for all tensors.\n","\n","#### q5_0:\n","- Delivers higher accuracy but demands higher resource usage and slower inference.\n","- **Use Case:** Optimal accuracy where computational efficiency is not the primary concern.\n","\n","#### q5_1:\n","- Offers even higher accuracy but comes with increased resource usage and slower inference compared to q5_0.\n","- **Use Case:** Maximal accuracy, sacrificing some efficiency for critical tasks.\n","\n","#### q5_k_m  Best :\n","- Utilizes Q6_K for specified attention.wv and feed_forward.w2 tensors, employing Q5_K for others.\n","- **Use Case:** Enhanced precision for crucial tensors while balancing efficiency.\n","\n","#### q5_k_s:\n","- Uniformly applies Q5_K across all tensors.\n","- **Use Case:** Standardized higher precision for all tensors.\n","\n","#### q6_k:\n","- Employs Q8_K for all tensors uniformly.\n","- **Use Case:** Extremely high precision with potential resource implications.\n","\n","#### q8_0:\n","- Represents precision almost indistinguishable from float16 but demands high resources and slower processing.\n","- **Use Case:** Niche applications where extreme precision is paramount, despite trade-offs in resource efficiency and speed.\n","\n","Understanding these quantization methods empowers users to tailor models to specific needs, balancing precision, efficiency, and computational demands for optimal performance in various applications.\n","\n","---\n","\n","\n","\n"],"metadata":{"id":"V0hLWV1T4vDa"}},{"cell_type":"markdown","source":["The information presented in a tabular form:\n","\n","| Quantization Level | Model Name | Characteristics | Compression Method | Compression Ratio Formula | Memory Suitability |\n","| ------------------ | ---------- | ---------------- | --------------------- | --------------------------- | ------------------ |\n","| 2 to 4-bit (Q2_K, Q3_K, Q4_K) | Q2_K | Highly compressed, smaller sizes, lower memory | N/A | N/A | Suitable for systems with limited RAM |\n","| | Q3_K | | | | |\n","| | Q4_K | | Q4: 4-bit representation | (4 + QK/2) / (2 * QK) | Some accuracy trade-off |\n","| 5 to 6-bit (Q5_K, Q6_K) | Q5_K | Refined balance between size and accuracy | N/A | N/A | Favorable for systems with moderate memory capacities |\n","| | Q6_K | | | | |\n","| 8-bit (Q8_0) | Q8_0 | Closer to original model accuracy, demands higher memory | N/A | N/A | Suitable for systems with substantial RAM |\n","| Quantization Formulas | Q4_0 | Representing numbers with a 32-bit float scaling factor and multiple 4-bit integers | N/A | (4 + QK/2) / (2 * QK) | N/A |\n","| | Q4_1 | Enhances Q4_0 with an additional offset factor | N/A | (8 + QK/2) / (2 * QK) | N/A |\n","| Model Sizes | 7b | 7-billion parameter model | Not quantized: Several dozen gigabytes | Quantized (Q4 or Q6): Feasible for systems with 16GB RAM | M2 MacBook Air or similar |\n","| | 13b | 13-billion parameter model | Even with quantization: Substantial memory and computational overhead | Suited for GPU setups or higher-end hardware with ample memory | N/A |\n","\n","A clear overview of the characteristics, compression methods, formulas, and memory suitability for different quantization levels and model sizes."],"metadata":{"id":"QWYxn1_-4PgA"}},{"cell_type":"markdown","source":["\n","\n","| Instruction Set | Description |\n","| --------------- | ----------- |\n","| SSE (Streaming SIMD Extensions) | Introduced by Intel, this set provides single instruction multiple data capabilities, supporting 128-bit SIMD instructions. It operates on packed single-precision floating-point data types and integer data types. |\n","| AVX (Advanced Vector Extensions) | An extension of SSE, AVX introduces wider 256-bit SIMD instructions, allowing for higher parallelism and increased data throughput. It enhances support for floating-point operations and provides additional registers. |\n","| AVX2 | Builds upon AVX by introducing support for integer operations with 256-bit SIMD instructions, expanding the capabilities for integer processing. It enhances performance for various computational tasks. |\n","| AVX-512 | The most advanced extension, operating on 512-bit SIMD instructions. It provides even higher parallelism and throughput, enabling intensive computational tasks across a wider range of data types. |\n","\n"],"metadata":{"id":"r1dGgeVa6BSj"}},{"cell_type":"markdown","source":["# Objective we want work on\n","\n","1. **Understanding the Trade-off:**\n","   - **Objective:** Find the optimal value for QK (block size).\n","   - **Trade-off:** The decision involves balancing compression ratio and accuracy.\n","   - **Factors:** Consider the CPU architecture as different sizes might be more efficient for various SIMD instruction sets.\n","\n","2. **Definition of QK (Block Size):**\n","   - **QK Definition:** QK typically refers to the block size in a compression or processing algorithm.\n","   - **Significance:** It determines how data is divided into blocks for simultaneous processing.\n","\n","3. **Impact of Block Size on Compression and Accuracy:**\n","   - **Compression Ratio:** Smaller block sizes may lead to higher compression ratios as more redundancy can be exploited. However, this might affect accuracy.\n","   - **Accuracy:** Larger block sizes might provide more accurate results, but at the expense of a potentially lower compression ratio.\n","\n","4. **Consideration of CPU Architecture:**\n","   - **SIMD Instruction Sets:** Understand the SIMD instruction sets supported by the CPU architecture.\n","   - **Efficiency:** Different block sizes may be more efficient for specific SIMD instruction sets.\n","   - **Parallel Processing:** SIMD allows parallel processing of the same operation on multiple data points, aligning with the nature of operations in Large Language Models.\n","\n","5. **Algorithmic Decision Making:**\n","   - **Bayesian Optimization:** Utilize Bayesian optimization techniques to search for the optimal QK.\n","   - **Objective Function:** Define an objective function that captures the trade-off between compression ratio and accuracy.\n","   - **Iterative Refinement:** Perform iterative refinement using the Bayesian optimization to converge towards the optimal QK.\n","\n","6. **Implementation of SIMD for Parallel Processing:**\n","   - **Identification:** Identify the specific operations in your algorithm that can benefit from parallel processing.\n","   - **Coding for SIMD:** Implement SIMD instructions in the algorithm to enable simultaneous processing.\n","   - **Performance Monitoring:** Monitor the performance on different CPU architectures to validate efficiency gains.\n","\n","7. **Testing and Validation:**\n","   - **Dataset:** Use a diverse dataset to test the algorithm under various conditions.\n","   - **Metrics:** Evaluate compression ratio, accuracy, and overall performance metrics.\n","   - **Fine-tuning:** Adjust QK based on testing results and feedback from the specific CPU architecture.\n","\n","8. **Documentation and Reporting:**\n","   - **Document Parameters:** Clearly document the chosen QK, rationale, and any adjustments made.\n","   - **Report Findings:** Provide a detailed report on the trade-offs, efficiency gains, and overall performance achieved.\n","\n","This step-by-step guide encompasses algorithmic decisions, logic, and considerations for finding the optimal QK, taking into account both compression and accuracy, as well as the specific characteristics of SIMD and CPU architecture."],"metadata":{"id":"fWm6BQNO2NZE"}},{"cell_type":"markdown","source":["# **How to Use Above Concepts for LLMS Inference**:\n","\n","**GGML (GPT-Generated Model Language) vs. GGUF (GPT-Generated Unified Format): A Detailed Analysis**\n","\n","*GGML at a Glance:*\n","GGML, conceived by Georgi Gerganov, acts as a specialized tensor library crafted explicitly for machine learning, with a focal point on optimizing language models like GPT. Initially, it represented an early innovation in the endeavor to establish a dedicated file format for GPT models. Its primary objective was to ensure efficient storage and high performance across diverse hardware architectures, including Apple Silicon.\n","\n","**GGML Advantages:**\n","1. **Pioneering Innovation:** GGML marked the initial steps in devising a file format dedicated to GPT models, laying the foundation for further advancements in model storage.\n","2. **Unified File Sharing:** Introducing the convenience of consolidating entire models within a single file, it significantly streamlined collaborative efforts and model distribution.\n","3. **CPU Compatibility:** GGML exhibited adaptability by seamlessly operating on CPUs, ensuring broader accessibility across varying hardware configurations.\n","\n","**GGML Limitations:**\n","1. **Flexibility Constraints:** Incorporating additional model information posed challenges for GGML, limiting its adaptability to evolving model architectures.\n","2. **Compatibility Challenges:** The introduction of novel features often led to compatibility issues with existing models, hindering seamless transitions.\n","3. **Manual Configuration Burden:** Users frequently encountered complexities in manually fine-tuning settings like rope-freq-base, rope-freq-scale, gqa, and rms-norm-eps, risking potential errors.\n","\n","*Introduction of GGUF:*\n","GGUF emerged as the successor to GGML on August 21, 2023, presenting a substantial advancement in language model file formats. Developed collaboratively by AI community contributors, including Georgi Gerganov, GGUF aimed to rectify GGML's limitations and enhance the user experience in handling large-scale AI models.\n","\n","**GGUF Advantages:**\n","1. **Overcoming GGML's Constraints:** GGUF was purpose-built to surmount GGML's limitations, offering a more resilient and user-friendly solution for model storage.\n","2. **Extensibility:** Enabling seamless integration of new features without compromising compatibility with existing models ensured adaptability to evolving architectures.\n","3. **Stability Focus:** GGUF prioritizes stability, striving to minimize disruptive changes and streamline transitions to newer format versions.\n","4. **Enhanced Versatility:** GGUF's utility extends beyond llama models, showcasing its capability in supporting diverse language models.\n","\n","**GGUF Challenges:**\n","1. **Transition Effort:** Migrating existing models to GGUF may demand considerable time and resources.\n","2. **Adaptation Phase:** Users and developers require acclimatization to the new format, potentially necessitating an adjustment period for widespread adoption.\n","\n","*Conclusion:*\n","GGUF stands as a notable progression from GGML, offering improved features, stability, and adaptability. While the transition may pose challenges, its advantages in handling large language models signify a promising evolution in language model file formats. For more detailed insights, explore the GitHub issue [here](https://github.com/ggerganov/llama.cpp/issues) and delve into the llama.cpp project by Georgi Gerganov [here](https://github.com/ggerganov ).  "],"metadata":{"id":"1K1drtGlUq8j"}},{"cell_type":"markdown","source":["\n","# exploring\n","---\n","\n","### Steps to Execute Quantization Code for any model from Hugging-face\n","\n","To run the provided code, follow these steps using a terminal equipped with git, a C++ compiler, and pip for Python package management:\n","\n","#### 1. Clone the llama.cpp Repository:\n","This repository hosts the essential quantization tools required for the LLaMA model.\n","\n","```bash\n","git clone https://github.com/ggerganov/llama.cpp\n","cd llama.cpp\n","git pull\n","make clean\n","LLAMA_CUBLAS=1 make\n","cd ..\n","```\n","\n","#### 2. Install Python Requirements for llama.cpp:\n","Ensure the necessary Python dependencies are installed to execute the quantization scripts.\n","\n","```bash\n","pip install -r llama.cpp/requirements.txt\n","```\n","\n","#### 3. Configure Git LFS and Clone the Model Repository:\n","Set up Git LFS for handling large files like pre-trained models. Clone the model repository from Hugging Face.\n","\n","```bash\n","git lfs install\n","git clone https://huggingface.co/{MODEL_ID}\n","```\n","\n","#### 4. Convert the Model to FP16 Format:\n","Utilize the provided convert.py script in the llama.cpp repository to convert the model's parameters to FP16, a half-precision floating-point format, reducing model size and potentially accelerating computations on FP16-supported GPUs.\n","\n","```bash\n","python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}\n","```\n","\n","#### 5. Quantize the Model:\n","Iterate through each specified quantization method in the QUANTIZATION_METHODS list. Use the quantize executable to convert the FP16 model into various quantized formats, generating distinct files with the quantized weights.\n","\n","```bash\n","for method in QUANTIZATION_METHODS:\n","    qtype=\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n","    ./llama.cpp/quantize {fp16} {qtype} {method}\n","```\n","\n","To execute these commands as a script, create a bash file (e.g., quantize_model.sh) containing these commands, replacing {MODEL_ID}, {MODEL_NAME}, {fp16}, and {method} with the actual values. Grant execution permission using chmod +x quantize_model.sh, then execute it with ./quantize_model.sh.\n","\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"wpz92ut95CEX"}},{"cell_type":"markdown","source":["#  `local installation` for cloud machines ignore\n","### Step 1: Adding the PackageCloud Repository\n","1. **apt/deb Repositories:**\n","   - Run the following command to add the apt/deb repository:\n","     ```bash\n","     curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n","     ```\n","\n","2. **yum/rpm Repositories:**\n","   - Add the yum/rpm repository with this command:\n","     ```bash\n","     curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash\n","     ```\n","\n","3. **For Specific or Compatible Distributions:**\n","   - If your Linux distribution does not match a repository uploaded for Git LFS, or if it's compatible with an upstream distribution:\n","     - Use additional parameters or manually correct the resulting repository URLs.\n","\n","4. **Specific Distributions Configuration Example:**\n","   - For LinuxMint 17.1 Rebecca (downstream of Ubuntu Trusty and Debian Jessie):\n","     ```bash\n","     curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | os=debian dist=jessie sudo -E bash\n","     ```\n","   - For automatic detection of the distribution for Ubuntu-based systems like Pop OS:\n","     ```bash\n","     (. /etc/lsb-release && curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo env os=ubuntu dist=\"${DISTRIB_CODENAME}\" bash)\n","     ```\n","\n","### Step 2: Installing Git LFS Packages\n","1. **apt/deb:**\n","   - Use the following command to install Git LFS:\n","     ```bash\n","     sudo apt-get install git-lfs\n","     ```\n","\n","2. **yum/rpm:**\n","   - For yum/rpm-based systems, install Git LFS using:\n","     ```bash\n","     sudo yum install git-lfs\n","     ```\n","\n","### Note about Proxies\n","- If your system is behind a proxy-server that's required for internet access, and you're using `sudo`, the environment variables like `http_proxy` or `https_proxy` might not persist.\n","- To retain environment variables when switching to root with `sudo`, use `sudo -E ...`.\n","\n"],"metadata":{"id":"xqJTPliwE_6f"}},{"cell_type":"code","source":["# Variables\n","MODEL_ID = \"mlabonne/EvolCodeLlama-7b\"\n","QUANTIZATION_METHODS = [\"q4_k_m\", \"q5_k_m\"]\n","\n","# Constants\n","MODEL_NAME = MODEL_ID.split('/')[-1]\n","\n","# Install llama.cpp\n","!git clone https://github.com/ggerganov/llama.cpp\n","!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n","!pip install -r llama.cpp/requirements.txt\n","\n","# Download model\n","!git lfs install\n","!git clone https://huggingface.co/{MODEL_ID}\n","\n","# Convert to fp16\n","fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n","!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}\n","\n","# Quantize the model for each method in the QUANTIZATION_METHODS list\n","for method in QUANTIZATION_METHODS:\n","    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n","    !./llama.cpp/quantize {fp16} {qtype} {method}"],"metadata":{"id":"NTvrsWig6GZk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SK1SUemO-4la"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","### Initialization:\n","- `ggml_init_cublas: found 1 CUDA devices:`\n","  - Indicates the initialization of CUDA devices for GPU processing. In this case, one CUDA device (Tesla T4) was found.\n","\n","### Build and Quantization Information:\n","- `main: build = 1100 (dd0dc36)`\n","  - Specifies the build information or version (build 1100 with identifier dd0dc36).\n","- `main: quantizing 'EvolCodeLlama-7b/evolcodellama-7b.gguf.fp16.bin' to 'EvolCodeLlama-7b/evolcodellama-7b.gguf.q4_k_s.bin' as Q4_K_S`\n","  - Indicates the quantization process applied to a specific model (`EvolCodeLlama-7b/evolcodellama-7b.gguf.fp16.bin`) using the `Q4_K_S` method, resulting in a new quantized model file (`EvolCodeLlama-7b/evolcodellama-7b.gguf.q4_k_s.bin`).\n","\n","### Model Loading Information:\n","- `llama_model_loader: loaded meta data with 16 key-value pairs and 291 tensors from EvolCodeLlama-7b/evolcodellama-7b.gguf.fp16.bin (version GGUF V1 (support until nov 2023))`\n","  - Confirms the successful loading of metadata associated with the model file (`EvolCodeLlama-7b/evolcodellama-7b.gguf.fp16.bin`). It contains 16 key-value pairs and 291 tensors. Additionally, it specifies the model's version as GGUF V1 with support until November 2023.\n","- `llama_model_loader: - tensor 0: token_embd.weight f16 [ 4096, 32016, 1, 1 ]`\n","- `llama_model_loader: - tensor 1: blk.0.attn_q.weight f16 [ 4096, 4096, 1, 1 ]`\n","- `llama_model_loader: - tensor 2: blk.0.attn_k.weight f16 [ 4096, 4096, 1, 1 ]`\n","- `llama_model_loader: - tensor 3: blk.0.attn_v.weight f16 [ 4096, 4096, 1, 1 ]`\n","- `llama_model_loader: - tensor 4: blk.0.attn_output.weight f16 [ 4096, 4096, 1, 1 ]`\n","\n","\n","### Tensor Information:\n","- **Names:**\n","  - Specifies the names or identifiers of the individual tensors within the model. For instance:\n","    - `token_embd.weight`\n","    - `blk.0.attn_q.weight`\n","    - `blk.0.attn_k.weight`\n","    - `blk.0.attn_v.weight`\n","    - `blk.0.attn_output.weight`\n","    - Each name identifies a particular tensor within the model's architecture.\n","\n","- **Data Type (f16 for half-precision floating-point):**\n","  - Indicates the data type used to represent the numerical values within these tensors. Here, the notation `f16` refers to half-precision floating-point format, which uses 16 bits to represent numerical values.\n","  - Half-precision floating-point provides a compromise between memory usage and precision, allowing for faster computations and reduced memory requirements compared to higher-precision formats like `float32`.\n","\n","- **Shape ([rows, columns, depth, channels]):**\n","  - Describes the dimensions or structure of each tensor using a four-dimensional representation:\n","    - `[rows, columns, depth, channels]`\n","    - For example:\n","      - `token_embd.weight`: `[4096, 32016, 1, 1]`\n","      - `blk.0.attn_q.weight`: `[4096, 4096, 1, 1]`\n","      - `blk.0.attn_k.weight`: `[4096, 4096, 1, 1]`\n","      - `blk.0.attn_v.weight`: `[4096, 4096, 1, 1]`\n","      - `blk.0.attn_output.weight`: `[4096, 4096, 1, 1]`\n","    - Each value within the square brackets represents a dimension:\n","      - `rows`: The number of rows in the tensor.\n","      - `columns`: The number of columns in the tensor.\n","      - `depth`: The depth or the number of layers within the tensor (often 1 for weight tensors).\n","      - `channels`: The number of channels or features within each element of the tensor.\n","    - These dimensions signify the shape and size of the tensors, crucial for understanding their structure and how data is organized within the model.\n","\n","\n","This log excerpt captures the initialization of CUDA devices, details about the build, quantization process, and information regarding the loaded model, including metadata and specific tensor details such as their names, shapes, and data types."],"metadata":{"id":"B9m6RD1V_4YE"}},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ODOxCq9bVy0","executionInfo":{"status":"ok","timestamp":1704438145957,"user_tz":-330,"elapsed":20032,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"53e7f608-4f4d-4b11-a165-516a5844679f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Token: \n","Add token as git credential? (Y/n) y\n","Token is valid (permission: read).\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"code","source":["import os\n","\n","model_list = [file for file in os.listdir(MODEL_NAME) if \"gguf\" in file]\n","\n","prompt = input(\"Enter your prompt: \")\n","chosen_method = input(\"Name of the model (options: \" + \", \".join(model_list) + \"): \")\n","\n","# Verify the chosen method is in the list\n","if chosen_method not in model_list:\n","    print(\"Invalid name\")\n","else:\n","    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n","    !./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p \"{prompt}\""],"metadata":{"id":"HmQSBVmQQc-S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"9iCfv2prQcFp"}},{"cell_type":"code","source":["MODEL_ID=\"mlabonne/EvolCodeLlama-7b\"\n","QUANTIZATION_METHODS=[\"q4_k_m\", \"q5_k_m\"]  # Make sure you define this list if it's not already defined.\n","MODEL_NAME=MODEL_ID.split('/')[-1]  # This will be \"Mistral-7B-v0.1\"\n","\n","# Create the directory where the quantized models will be saved\n","!mkdir -p Hemanth_LLms\n","# Mistral AI v0.1\n","# Download model\n","# !git lfs install\n","!git clone https://huggingface.co/{MODEL_ID}\n","\n","# Convert to fp16 and save in the 'Hemanth_LLms' folder\n","fp16 = f\"Hemanth_LLms/{MODEL_NAME.lower()}.fp16.bin\"\n","!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}\n","\n","# Quantize the model for each method in the QUANTIZATION_METHODS list and save in the 'Hemanth_LLms' folder\n","for method in QUANTIZATION_METHODS:\n","    qtype = f\"/Hemanth_LLms{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n","    !./llama.cpp/quantize {fp16} {qtype} {method}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"deeXiud_9uwI","executionInfo":{"status":"ok","timestamp":1704439505451,"user_tz":-330,"elapsed":482943,"user":{"displayName":"hemanth varma","userId":"05483789954770220327"}},"outputId":"3c67e91f-a461-40c7-8587-b7492a9ac158"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'EvolCodeLlama-7b'...\n","remote: Enumerating objects: 35, done.\u001b[K\n","remote: Counting objects: 100% (32/32), done.\u001b[K\n","remote: Compressing objects: 100% (32/32), done.\u001b[K\n","remote: Total 35 (delta 8), reused 0 (delta 0), pack-reused 3\u001b[K\n","Unpacking objects: 100% (35/35), 483.46 KiB | 2.63 MiB/s, done.\n","Filtering content: 100% (5/5), 4.70 GiB | 10.00 MiB/s, done.\n","Encountered 1 file(s) that may not have been copied correctly on Windows:\n","\tpytorch_model-00001-of-00002.bin\n","\n","See: `git lfs help smudge` for more details.\n","python3: can't open file '/content/llama.cpp/convert.py': [Errno 2] No such file or directory\n","/bin/bash: line 1: ./llama.cpp/quantize: No such file or directory\n","/bin/bash: line 1: ./llama.cpp/quantize: No such file or directory\n"]}]},{"cell_type":"markdown","source":["### Working sooner i will update for model to running fro\n","\n","`Model directory not found`:\n","\n","The convert.py script is failing because it cannot find the model directory Mistral-7B-v0.1. This might be due to an incorrect path or because the model has not been cloned into the expected directory.\n","\n","`Shared library error`:\n","\n","The quantize executable from the llama.cpp repository is failing to run because it cannot find the libcuda.so.1 shared library. This library is part of the CUDA toolkit and is required for GPU operations. This error typically occurs when you're trying to run a GPU-accelerated program on a system without the necessary CUDA libraries installed, or when the system's library path does not include the directory where libcuda.so.1 is located.\n","\n","Here's how you can address these issues:\n","\n","For the model directory not found error:\n","\n","Make sure that the git clone command is executed in the correct directory and that it completes successfully. The model should be cloned into a directory with the same name as the MODEL_ID's last component.\n","\n","\n","\n","\n","\n","\n","current_working_directory\n","- Mistral-7B-v0.1/\n","  - config.json\n","  - pytorch_model.bin\n","  - ...\n","\n","- The convert.py script should be run from the current_working_directory and it will look for the model in the Mistral-7B-v0.1 directory.\n","\n"],"metadata":{"id":"7hXLXe23HN-N"}}]}